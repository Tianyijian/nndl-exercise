nohup: ignoring input
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From RNN.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From RNN.py:70: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From RNN.py:71: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.
WARNING:tensorflow:From RNN.py:82: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From RNN.py:86: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /users5/yjtian/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From RNN.py:90: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From RNN.py:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From RNN.py:99: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From RNN.py:131: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From RNN.py:132: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From RNN.py:132: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

2019-09-07 10:13:53.296224: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-09-07 10:13:54.033549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:04:00.0
2019-09-07 10:13:54.044669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-09-07 10:13:54.052432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:08:00.0
2019-09-07 10:13:54.056165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:09:00.0
2019-09-07 10:13:54.064432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:84:00.0
2019-09-07 10:13:54.078117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:85:00.0
2019-09-07 10:13:54.088985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:88:00.0
2019-09-07 10:13:54.096692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:89:00.0
2019-09-07 10:13:54.097163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-07 10:13:54.100035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-09-07 10:13:54.102722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-09-07 10:13:54.104003: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-09-07 10:13:54.107505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-09-07 10:13:54.110105: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-09-07 10:13:54.117379: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-07 10:13:54.190826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-09-07 10:13:54.191618: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-07 10:14:00.297009: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e1127fd5f0 executing computations on platform CUDA. Devices:
2019-09-07 10:14:00.297082: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297098: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297109: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297121: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297133: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297144: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297155: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.297166: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-07 10:14:00.306378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400015000 Hz
2019-09-07 10:14:00.312863: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e112f5cd30 executing computations on platform Host. Devices:
2019-09-07 10:14:00.312924: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-07 10:14:00.318338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:04:00.0
2019-09-07 10:14:00.320481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-09-07 10:14:00.322609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:08:00.0
2019-09-07 10:14:00.324704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:09:00.0
2019-09-07 10:14:00.326699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:84:00.0
2019-09-07 10:14:00.328680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:85:00.0
2019-09-07 10:14:00.330666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:88:00.0
2019-09-07 10:14:00.332690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:89:00.0
2019-09-07 10:14:00.332759: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-07 10:14:00.332782: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-09-07 10:14:00.332802: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-09-07 10:14:00.332830: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-09-07 10:14:00.332847: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-09-07 10:14:00.332866: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-09-07 10:14:00.332886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-07 10:14:00.369639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-09-07 10:14:00.369688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-07 10:14:00.396277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-07 10:14:00.396305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-09-07 10:14:00.396320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N N N N N N N 
2019-09-07 10:14:00.396328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N N N N N N N 
2019-09-07 10:14:00.396335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N N N N N N 
2019-09-07 10:14:00.396342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N N N N N N N 
2019-09-07 10:14:00.396348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N N N N 
2019-09-07 10:14:00.396355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N N N N N 
2019-09-07 10:14:00.396362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N N N N N 
2019-09-07 10:14:00.396368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N N N N N 
2019-09-07 10:14:00.419264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10248 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5)
2019-09-07 10:14:00.424596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10248 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)
2019-09-07 10:14:00.427420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10248 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5)
2019-09-07 10:14:00.430010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10248 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)
2019-09-07 10:14:00.432365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10248 MB memory) -> physical GPU (device: 4, name: GeForce RTX 2080 Ti, pci bus id: 0000:84:00.0, compute capability: 7.5)
2019-09-07 10:14:00.434290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10248 MB memory) -> physical GPU (device: 5, name: GeForce RTX 2080 Ti, pci bus id: 0000:85:00.0, compute capability: 7.5)
2019-09-07 10:14:00.437367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10248 MB memory) -> physical GPU (device: 6, name: GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5)
2019-09-07 10:14:00.441571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10248 MB memory) -> physical GPU (device: 7, name: GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5)
2019-09-07 10:14:00.580886: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-09-07 10:14:01.565044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
[INFO] train tang poem...
[INFO] Epoch: 0 , batch: 0 , training loss: 8.817041
[INFO] Epoch: 0 , batch: 1 , training loss: 7.608752
[INFO] Epoch: 0 , batch: 2 , training loss: 9.954180
[INFO] Epoch: 0 , batch: 3 , training loss: 7.271345
[INFO] Epoch: 0 , batch: 4 , training loss: 8.163298
[INFO] Epoch: 0 , batch: 5 , training loss: 6.776561
[INFO] Epoch: 0 , batch: 6 , training loss: 7.943238
[INFO] Epoch: 0 , batch: 7 , training loss: 7.665838
[INFO] Epoch: 0 , batch: 8 , training loss: 7.613932
[INFO] Epoch: 0 , batch: 9 , training loss: 7.398424
[INFO] Epoch: 0 , batch: 10 , training loss: 7.240931
[INFO] Epoch: 0 , batch: 11 , training loss: 7.168221
[INFO] Epoch: 0 , batch: 12 , training loss: 7.048301
[INFO] Epoch: 0 , batch: 13 , training loss: 6.894137
[INFO] Epoch: 0 , batch: 14 , training loss: 6.794208
[INFO] Epoch: 0 , batch: 15 , training loss: 6.811226
[INFO] Epoch: 0 , batch: 16 , training loss: 6.722122
[INFO] Epoch: 0 , batch: 17 , training loss: 6.664182
[INFO] Epoch: 0 , batch: 18 , training loss: 6.504763
[INFO] Epoch: 0 , batch: 19 , training loss: 6.378535
[INFO] Epoch: 0 , batch: 20 , training loss: 6.278050
[INFO] Epoch: 0 , batch: 21 , training loss: 6.201520
[INFO] Epoch: 0 , batch: 22 , training loss: 6.354535
[INFO] Epoch: 0 , batch: 23 , training loss: 6.194324
[INFO] Epoch: 0 , batch: 24 , training loss: 6.078054
[INFO] Epoch: 0 , batch: 25 , training loss: 6.104011
[INFO] Epoch: 0 , batch: 26 , training loss: 5.964721
[INFO] Epoch: 0 , batch: 27 , training loss: 5.922794
[INFO] Epoch: 0 , batch: 28 , training loss: 6.044622
[INFO] Epoch: 0 , batch: 29 , training loss: 5.857537
[INFO] Epoch: 0 , batch: 30 , training loss: 5.812621
[INFO] Epoch: 0 , batch: 31 , training loss: 5.853666
[INFO] Epoch: 0 , batch: 32 , training loss: 5.817266
[INFO] Epoch: 0 , batch: 33 , training loss: 5.698618
[INFO] Epoch: 0 , batch: 34 , training loss: 5.925082
[INFO] Epoch: 0 , batch: 35 , training loss: 5.789010
[INFO] Epoch: 0 , batch: 36 , training loss: 5.723783
[INFO] Epoch: 0 , batch: 37 , training loss: 5.608320
[INFO] Epoch: 0 , batch: 38 , training loss: 5.717794
[INFO] Epoch: 0 , batch: 39 , training loss: 5.640865
[INFO] Epoch: 0 , batch: 40 , training loss: 5.532425
[INFO] Epoch: 0 , batch: 41 , training loss: 5.817948
[INFO] Epoch: 0 , batch: 42 , training loss: 6.123776
[INFO] Epoch: 0 , batch: 43 , training loss: 5.964821
[INFO] Epoch: 0 , batch: 44 , training loss: 5.882478
[INFO] Epoch: 0 , batch: 45 , training loss: 6.173928
[INFO] Epoch: 0 , batch: 46 , training loss: 6.926503
[INFO] Epoch: 0 , batch: 47 , training loss: 6.109035
[INFO] Epoch: 0 , batch: 48 , training loss: 6.235266
[INFO] Epoch: 0 , batch: 49 , training loss: 6.201226
[INFO] Epoch: 0 , batch: 50 , training loss: 6.160917
[INFO] Epoch: 0 , batch: 51 , training loss: 5.972749
[INFO] Epoch: 0 , batch: 52 , training loss: 5.831112
[INFO] Epoch: 0 , batch: 53 , training loss: 5.962374
[INFO] Epoch: 0 , batch: 54 , training loss: 5.909550
[INFO] Epoch: 0 , batch: 55 , training loss: 6.003722
[INFO] Epoch: 0 , batch: 56 , training loss: 5.876447
[INFO] Epoch: 0 , batch: 57 , training loss: 5.793036
[INFO] Epoch: 0 , batch: 58 , training loss: 5.710519
[INFO] Epoch: 0 , batch: 59 , training loss: 5.894025
[INFO] Epoch: 0 , batch: 60 , training loss: 5.724284
[INFO] Epoch: 0 , batch: 61 , training loss: 5.787101
[INFO] Epoch: 0 , batch: 62 , training loss: 5.752159
[INFO] Epoch: 0 , batch: 63 , training loss: 5.817878
[INFO] Epoch: 0 , batch: 64 , training loss: 5.780619
[INFO] Epoch: 0 , batch: 65 , training loss: 5.825002
[INFO] Epoch: 0 , batch: 66 , training loss: 5.766149
[INFO] Epoch: 0 , batch: 67 , training loss: 5.748316
[INFO] Epoch: 0 , batch: 68 , training loss: 5.978549
[INFO] Epoch: 0 , batch: 69 , training loss: 5.798181
[INFO] Epoch: 0 , batch: 70 , training loss: 5.916843
[INFO] Epoch: 0 , batch: 71 , training loss: 5.787494
[INFO] Epoch: 0 , batch: 72 , training loss: 5.904247
[INFO] Epoch: 0 , batch: 73 , training loss: 5.815618
[INFO] Epoch: 0 , batch: 74 , training loss: 5.777777
[INFO] Epoch: 0 , batch: 75 , training loss: 5.572475
[INFO] Epoch: 0 , batch: 76 , training loss: 5.762399
[INFO] Epoch: 0 , batch: 77 , training loss: 5.659396
[INFO] Epoch: 0 , batch: 78 , training loss: 5.642895
[INFO] Epoch: 0 , batch: 79 , training loss: 5.556409
[INFO] Epoch: 0 , batch: 80 , training loss: 5.682191
[INFO] Epoch: 0 , batch: 81 , training loss: 5.749687
[INFO] Epoch: 0 , batch: 82 , training loss: 5.730507
[INFO] Epoch: 0 , batch: 83 , training loss: 5.809666
[INFO] Epoch: 0 , batch: 84 , training loss: 5.694191
[INFO] Epoch: 0 , batch: 85 , training loss: 5.736580
[INFO] Epoch: 0 , batch: 86 , training loss: 5.727459
[INFO] Epoch: 0 , batch: 87 , training loss: 5.741962
[INFO] Epoch: 0 , batch: 88 , training loss: 5.843868
[INFO] Epoch: 0 , batch: 89 , training loss: 5.693628
[INFO] Epoch: 0 , batch: 90 , training loss: 5.647161
[INFO] Epoch: 0 , batch: 91 , training loss: 5.667345
[INFO] Epoch: 0 , batch: 92 , training loss: 5.688618
[INFO] Epoch: 0 , batch: 93 , training loss: 5.708525
[INFO] Epoch: 0 , batch: 94 , training loss: 5.795129
[INFO] Epoch: 0 , batch: 95 , training loss: 5.686599
[INFO] Epoch: 0 , batch: 96 , training loss: 5.623630
[INFO] Epoch: 0 , batch: 97 , training loss: 5.711191
[INFO] Epoch: 0 , batch: 98 , training loss: 5.682429
[INFO] Epoch: 0 , batch: 99 , training loss: 5.661213
[INFO] Epoch: 0 , batch: 100 , training loss: 5.587075
[INFO] Epoch: 0 , batch: 101 , training loss: 5.600105
[INFO] Epoch: 0 , batch: 102 , training loss: 5.763850
[INFO] Epoch: 0 , batch: 103 , training loss: 5.518012
[INFO] Epoch: 0 , batch: 104 , training loss: 5.591632
[INFO] Epoch: 0 , batch: 105 , training loss: 5.777282
[INFO] Epoch: 0 , batch: 106 , training loss: 5.752025
[INFO] Epoch: 0 , batch: 107 , training loss: 5.712311
[INFO] Epoch: 0 , batch: 108 , training loss: 5.544928
[INFO] Epoch: 0 , batch: 109 , training loss: 5.466185
[INFO] Epoch: 0 , batch: 110 , training loss: 5.676334
[INFO] Epoch: 0 , batch: 111 , training loss: 5.643425
[INFO] Epoch: 0 , batch: 112 , training loss: 5.690267
[INFO] Epoch: 0 , batch: 113 , training loss: 5.562878
[INFO] Epoch: 0 , batch: 114 , training loss: 5.735079
[INFO] Epoch: 0 , batch: 115 , training loss: 5.678294
[INFO] Epoch: 0 , batch: 116 , training loss: 5.755747
[INFO] Epoch: 0 , batch: 117 , training loss: 5.842328
[INFO] Epoch: 0 , batch: 118 , training loss: 5.858711
[INFO] Epoch: 0 , batch: 119 , training loss: 5.792933
[INFO] Epoch: 0 , batch: 120 , training loss: 5.820827
[INFO] Epoch: 0 , batch: 121 , training loss: 5.681917
[INFO] Epoch: 0 , batch: 122 , training loss: 5.664217
[INFO] Epoch: 0 , batch: 123 , training loss: 5.851554
[INFO] Epoch: 0 , batch: 124 , training loss: 5.959411
[INFO] Epoch: 0 , batch: 125 , training loss: 5.537567
[INFO] Epoch: 0 , batch: 126 , training loss: 5.608774
[INFO] Epoch: 0 , batch: 127 , training loss: 5.628359
[INFO] Epoch: 0 , batch: 128 , training loss: 5.816638
[INFO] Epoch: 0 , batch: 129 , training loss: 5.684522
[INFO] Epoch: 0 , batch: 130 , training loss: 5.710655
[INFO] Epoch: 0 , batch: 131 , training loss: 5.719357
[INFO] Epoch: 0 , batch: 132 , training loss: 5.712816
[INFO] Epoch: 0 , batch: 133 , training loss: 5.628986
[INFO] Epoch: 0 , batch: 134 , training loss: 5.568188
[INFO] Epoch: 0 , batch: 135 , training loss: 5.570635
[INFO] Epoch: 0 , batch: 136 , training loss: 5.726341
[INFO] Epoch: 0 , batch: 137 , training loss: 5.642195
[INFO] Epoch: 0 , batch: 138 , training loss: 5.792025
[INFO] Epoch: 0 , batch: 139 , training loss: 6.224902
[INFO] Epoch: 0 , batch: 140 , training loss: 6.262486
[INFO] Epoch: 0 , batch: 141 , training loss: 6.045066
[INFO] Epoch: 0 , batch: 142 , training loss: 5.756962
[INFO] Epoch: 0 , batch: 143 , training loss: 5.743036
[INFO] Epoch: 0 , batch: 144 , training loss: 5.756992
[INFO] Epoch: 0 , batch: 145 , training loss: 5.826560
[INFO] Epoch: 0 , batch: 146 , training loss: 5.867161
[INFO] Epoch: 0 , batch: 147 , training loss: 5.588820
[INFO] Epoch: 0 , batch: 148 , training loss: 5.570608
[INFO] Epoch: 0 , batch: 149 , training loss: 5.665697
[INFO] Epoch: 0 , batch: 150 , training loss: 5.905647
[INFO] Epoch: 0 , batch: 151 , training loss: 5.532143
[INFO] Epoch: 0 , batch: 152 , training loss: 5.504405
[INFO] Epoch: 0 , batch: 153 , training loss: 5.711919
[INFO] Epoch: 0 , batch: 154 , training loss: 5.782740
[INFO] Epoch: 0 , batch: 155 , training loss: 5.954138
[INFO] Epoch: 0 , batch: 156 , training loss: 5.668941
[INFO] Epoch: 0 , batch: 157 , training loss: 5.669891
[INFO] Epoch: 0 , batch: 158 , training loss: 6.002852
[INFO] Epoch: 0 , batch: 159 , training loss: 6.090868
[INFO] Epoch: 0 , batch: 160 , training loss: 6.832015
[INFO] Epoch: 0 , batch: 161 , training loss: 6.288630
[INFO] Epoch: 0 , batch: 162 , training loss: 5.836208
[INFO] Epoch: 0 , batch: 163 , training loss: 5.790142
[INFO] Epoch: 0 , batch: 164 , training loss: 5.866613
[INFO] Epoch: 0 , batch: 165 , training loss: 5.915503
[INFO] Epoch: 0 , batch: 166 , training loss: 7.051206
[INFO] Epoch: 0 , batch: 167 , training loss: 6.839970
[INFO] Epoch: 0 , batch: 168 , training loss: 6.911542
[INFO] Epoch: 0 , batch: 169 , training loss: 6.748403
[INFO] Epoch: 0 , batch: 170 , training loss: 6.621547
[INFO] Epoch: 0 , batch: 171 , training loss: 6.461737
[INFO] Epoch: 0 , batch: 172 , training loss: 6.306173
[INFO] Epoch: 0 , batch: 173 , training loss: 6.200445
[INFO] Epoch: 0 , batch: 174 , training loss: 6.231281
[INFO] Epoch: 0 , batch: 175 , training loss: 6.233505
[INFO] Epoch: 0 , batch: 176 , training loss: 6.274458
[INFO] Epoch: 0 , batch: 177 , training loss: 6.151579
[INFO] Epoch: 0 , batch: 178 , training loss: 6.069261
[INFO] Epoch: 0 , batch: 179 , training loss: 6.044774
[INFO] Epoch: 0 , batch: 180 , training loss: 6.062621
[INFO] Epoch: 0 , batch: 181 , training loss: 6.163666
[INFO] Epoch: 0 , batch: 182 , training loss: 6.029475
[INFO] Epoch: 0 , batch: 183 , training loss: 5.959401
[INFO] Epoch: 0 , batch: 184 , training loss: 5.953166
[INFO] Epoch: 0 , batch: 185 , training loss: 5.923390
[INFO] Epoch: 0 , batch: 186 , training loss: 5.950299
[INFO] Epoch: 0 , batch: 187 , training loss: 6.083073
[INFO] Epoch: 0 , batch: 188 , training loss: 6.018842
[INFO] Epoch: 0 , batch: 189 , training loss: 5.944905
[INFO] Epoch: 0 , batch: 190 , training loss: 5.898119
[INFO] Epoch: 0 , batch: 191 , training loss: 5.966918
[INFO] Epoch: 0 , batch: 192 , training loss: 5.807577
[INFO] Epoch: 0 , batch: 193 , training loss: 5.832719
[INFO] Epoch: 0 , batch: 194 , training loss: 5.798656
[INFO] Epoch: 0 , batch: 195 , training loss: 5.903484
[INFO] Epoch: 0 , batch: 196 , training loss: 5.729307
[INFO] Epoch: 0 , batch: 197 , training loss: 5.883505
[INFO] Epoch: 0 , batch: 198 , training loss: 5.709239
[INFO] Epoch: 0 , batch: 199 , training loss: 5.745778
[INFO] Epoch: 0 , batch: 200 , training loss: 5.690923
[INFO] Epoch: 0 , batch: 201 , training loss: 5.703188
[INFO] Epoch: 0 , batch: 202 , training loss: 5.650114
[INFO] Epoch: 0 , batch: 203 , training loss: 5.519844
[INFO] Epoch: 0 , batch: 204 , training loss: 5.615355
[INFO] Epoch: 0 , batch: 205 , training loss: 5.503911
[INFO] Epoch: 0 , batch: 206 , training loss: 5.400073
[INFO] Epoch: 0 , batch: 207 , training loss: 5.438102
[INFO] Epoch: 0 , batch: 208 , training loss: 5.651500
[INFO] Epoch: 0 , batch: 209 , training loss: 5.635217
[INFO] Epoch: 0 , batch: 210 , training loss: 5.686193
[INFO] Epoch: 0 , batch: 211 , training loss: 5.503516
[INFO] Epoch: 0 , batch: 212 , training loss: 5.495450
[INFO] Epoch: 0 , batch: 213 , training loss: 5.620348
[INFO] Epoch: 0 , batch: 214 , training loss: 5.523924
[INFO] Epoch: 0 , batch: 215 , training loss: 5.635168
[INFO] Epoch: 0 , batch: 216 , training loss: 5.634815
[INFO] Epoch: 0 , batch: 217 , training loss: 5.513525
[INFO] Epoch: 0 , batch: 218 , training loss: 5.540842
[INFO] Epoch: 0 , batch: 219 , training loss: 5.544776
[INFO] Epoch: 0 , batch: 220 , training loss: 5.507414
[INFO] Epoch: 0 , batch: 221 , training loss: 5.473208
[INFO] Epoch: 0 , batch: 222 , training loss: 5.574293
[INFO] Epoch: 0 , batch: 223 , training loss: 5.646117
[INFO] Epoch: 0 , batch: 224 , training loss: 5.683506
[INFO] Epoch: 0 , batch: 225 , training loss: 5.533249
[INFO] Epoch: 0 , batch: 226 , training loss: 5.643741
[INFO] Epoch: 0 , batch: 227 , training loss: 5.620039
[INFO] Epoch: 0 , batch: 228 , training loss: 5.698456
[INFO] Epoch: 0 , batch: 229 , training loss: 5.605090
[INFO] Epoch: 0 , batch: 230 , training loss: 5.443719
[INFO] Epoch: 0 , batch: 231 , training loss: 5.353815
[INFO] Epoch: 0 , batch: 232 , training loss: 5.397672
[INFO] Epoch: 0 , batch: 233 , training loss: 5.465342
[INFO] Epoch: 0 , batch: 234 , training loss: 5.258944
[INFO] Epoch: 0 , batch: 235 , training loss: 5.355487
[INFO] Epoch: 0 , batch: 236 , training loss: 5.486673
[INFO] Epoch: 0 , batch: 237 , training loss: 5.594522
[INFO] Epoch: 0 , batch: 238 , training loss: 5.361600
[INFO] Epoch: 0 , batch: 239 , training loss: 5.393274
[INFO] Epoch: 0 , batch: 240 , training loss: 5.507517
[INFO] Epoch: 0 , batch: 241 , training loss: 5.331344
[INFO] Epoch: 0 , batch: 242 , training loss: 5.327990
[INFO] Epoch: 0 , batch: 243 , training loss: 5.557521
[INFO] Epoch: 0 , batch: 244 , training loss: 5.448551
[INFO] Epoch: 0 , batch: 245 , training loss: 5.434125
[INFO] Epoch: 0 , batch: 246 , training loss: 5.216548
[INFO] Epoch: 0 , batch: 247 , training loss: 5.350358
[INFO] Epoch: 0 , batch: 248 , training loss: 5.354057
[INFO] Epoch: 0 , batch: 249 , training loss: 5.306954
[INFO] Epoch: 0 , batch: 250 , training loss: 5.188824
[INFO] Epoch: 0 , batch: 251 , training loss: 5.489696
[INFO] Epoch: 0 , batch: 252 , training loss: 5.299797
[INFO] Epoch: 0 , batch: 253 , training loss: 5.414243
[INFO] Epoch: 0 , batch: 254 , training loss: 5.602997
[INFO] Epoch: 0 , batch: 255 , training loss: 5.622386
[INFO] Epoch: 0 , batch: 256 , training loss: 5.557362
[INFO] Epoch: 0 , batch: 257 , training loss: 5.574023
[INFO] Epoch: 0 , batch: 258 , training loss: 5.616804
[INFO] Epoch: 0 , batch: 259 , training loss: 5.639162
[INFO] Epoch: 0 , batch: 260 , training loss: 5.402398
[INFO] Epoch: 0 , batch: 261 , training loss: 5.547177
[INFO] Epoch: 0 , batch: 262 , training loss: 5.616562
[INFO] Epoch: 0 , batch: 263 , training loss: 5.647840
[INFO] Epoch: 0 , batch: 264 , training loss: 5.197694
[INFO] Epoch: 0 , batch: 265 , training loss: 5.336829
[INFO] Epoch: 0 , batch: 266 , training loss: 5.767996
[INFO] Epoch: 0 , batch: 267 , training loss: 5.554981
[INFO] Epoch: 0 , batch: 268 , training loss: 5.378335
[INFO] Epoch: 0 , batch: 269 , training loss: 5.565205
[INFO] Epoch: 0 , batch: 270 , training loss: 5.494544
[INFO] Epoch: 0 , batch: 271 , training loss: 5.521799
[INFO] Epoch: 0 , batch: 272 , training loss: 5.446493
[INFO] Epoch: 0 , batch: 273 , training loss: 5.471779
[INFO] Epoch: 0 , batch: 274 , training loss: 5.611642
[INFO] Epoch: 0 , batch: 275 , training loss: 5.491785
[INFO] Epoch: 0 , batch: 276 , training loss: 5.600115
[INFO] Epoch: 0 , batch: 277 , training loss: 5.627605
[INFO] Epoch: 0 , batch: 278 , training loss: 5.239694
[INFO] Epoch: 0 , batch: 279 , training loss: 5.258510
[INFO] Epoch: 0 , batch: 280 , training loss: 5.264488
[INFO] Epoch: 0 , batch: 281 , training loss: 5.341591
[INFO] Epoch: 0 , batch: 282 , training loss: 5.226383
[INFO] Epoch: 0 , batch: 283 , training loss: 5.409834
[INFO] Epoch: 0 , batch: 284 , training loss: 5.332705
[INFO] Epoch: 0 , batch: 285 , training loss: 5.401026
[INFO] Epoch: 0 , batch: 286 , training loss: 5.357347
[INFO] Epoch: 0 , batch: 287 , training loss: 5.143457
[INFO] Epoch: 0 , batch: 288 , training loss: 5.319168
[INFO] Epoch: 0 , batch: 289 , training loss: 5.300439
[INFO] Epoch: 0 , batch: 290 , training loss: 5.231748
[INFO] Epoch: 0 , batch: 291 , training loss: 5.181751
[INFO] Epoch: 0 , batch: 292 , training loss: 5.235670
[INFO] Epoch: 0 , batch: 293 , training loss: 5.233130
[INFO] Epoch: 0 , batch: 294 , training loss: 5.734407
[INFO] Epoch: 0 , batch: 295 , training loss: 5.526926
[INFO] Epoch: 0 , batch: 296 , training loss: 5.406111
[INFO] Epoch: 0 , batch: 297 , training loss: 5.341386
[INFO] Epoch: 0 , batch: 298 , training loss: 5.279057
[INFO] Epoch: 0 , batch: 299 , training loss: 5.160730
[INFO] Epoch: 0 , batch: 300 , training loss: 5.180585
[INFO] Epoch: 0 , batch: 301 , training loss: 5.204640
[INFO] Epoch: 0 , batch: 302 , training loss: 5.319018
[INFO] Epoch: 0 , batch: 303 , training loss: 5.332966
[INFO] Epoch: 0 , batch: 304 , training loss: 5.483124
[INFO] Epoch: 0 , batch: 305 , training loss: 5.242388
[INFO] Epoch: 0 , batch: 306 , training loss: 5.306484
[INFO] Epoch: 0 , batch: 307 , training loss: 5.246646
[INFO] Epoch: 0 , batch: 308 , training loss: 5.315695
[INFO] Epoch: 0 , batch: 309 , training loss: 5.256334
[INFO] Epoch: 0 , batch: 310 , training loss: 5.069703
[INFO] Epoch: 0 , batch: 311 , training loss: 5.077768
[INFO] Epoch: 0 , batch: 312 , training loss: 4.965925
[INFO] Epoch: 0 , batch: 313 , training loss: 5.152587
[INFO] Epoch: 0 , batch: 314 , training loss: 5.191203
[INFO] Epoch: 0 , batch: 315 , training loss: 5.173734
[INFO] Epoch: 0 , batch: 316 , training loss: 5.533474
[INFO] Epoch: 0 , batch: 317 , training loss: 5.991052
[INFO] Epoch: 0 , batch: 318 , training loss: 6.045095
[INFO] Epoch: 0 , batch: 319 , training loss: 5.583567
[INFO] Epoch: 0 , batch: 320 , training loss: 5.265058
[INFO] Epoch: 0 , batch: 321 , training loss: 5.075548
[INFO] Epoch: 0 , batch: 322 , training loss: 5.240570
[INFO] Epoch: 0 , batch: 323 , training loss: 5.192725
[INFO] Epoch: 0 , batch: 324 , training loss: 5.186048
[INFO] Epoch: 0 , batch: 325 , training loss: 5.390100
[INFO] Epoch: 0 , batch: 326 , training loss: 5.448246
[INFO] Epoch: 0 , batch: 327 , training loss: 5.257563
[INFO] Epoch: 0 , batch: 328 , training loss: 5.347287
[INFO] Epoch: 0 , batch: 329 , training loss: 5.183258
[INFO] Epoch: 0 , batch: 330 , training loss: 5.183699
[INFO] Epoch: 0 , batch: 331 , training loss: 5.351594
[INFO] Epoch: 0 , batch: 332 , training loss: 5.035262
[INFO] Epoch: 0 , batch: 333 , training loss: 4.983078
[INFO] Epoch: 0 , batch: 334 , training loss: 5.257806
[INFO] Epoch: 0 , batch: 335 , training loss: 5.272439
[INFO] Epoch: 0 , batch: 336 , training loss: 5.149693
[INFO] Epoch: 0 , batch: 337 , training loss: 5.327307
[INFO] Epoch: 0 , batch: 338 , training loss: 5.349565
[INFO] Epoch: 0 , batch: 339 , training loss: 5.262283
[INFO] Epoch: 0 , batch: 340 , training loss: 5.419767
[INFO] Epoch: 0 , batch: 341 , training loss: 5.176296
[INFO] Epoch: 0 , batch: 342 , training loss: 5.135115
[INFO] Epoch: 0 , batch: 343 , training loss: 5.221327
[INFO] Epoch: 0 , batch: 344 , training loss: 5.141368
[INFO] Epoch: 0 , batch: 345 , training loss: 5.249845
[INFO] Epoch: 0 , batch: 346 , training loss: 5.325043
[INFO] Epoch: 0 , batch: 347 , training loss: 5.156756
[INFO] Epoch: 0 , batch: 348 , training loss: 5.383933
[INFO] Epoch: 0 , batch: 349 , training loss: 5.561235
[INFO] Epoch: 0 , batch: 350 , training loss: 5.157085
[INFO] Epoch: 0 , batch: 351 , training loss: 5.132106
[INFO] Epoch: 0 , batch: 352 , training loss: 5.137103
[INFO] Epoch: 0 , batch: 353 , training loss: 5.099429
[INFO] Epoch: 0 , batch: 354 , training loss: 5.177775
[INFO] Epoch: 0 , batch: 355 , training loss: 5.329806
[INFO] Epoch: 0 , batch: 356 , training loss: 5.191516
[INFO] Epoch: 0 , batch: 357 , training loss: 5.189310
[INFO] Epoch: 0 , batch: 358 , training loss: 5.208226
[INFO] Epoch: 0 , batch: 359 , training loss: 5.203312
[INFO] Epoch: 0 , batch: 360 , training loss: 5.150590
[INFO] Epoch: 0 , batch: 361 , training loss: 5.124709
[INFO] Epoch: 0 , batch: 362 , training loss: 5.203265
[INFO] Epoch: 0 , batch: 363 , training loss: 5.110191
[INFO] Epoch: 0 , batch: 364 , training loss: 5.183859
[INFO] Epoch: 0 , batch: 365 , training loss: 5.042259
[INFO] Epoch: 0 , batch: 366 , training loss: 5.219011
[INFO] Epoch: 0 , batch: 367 , training loss: 5.226007
[INFO] Epoch: 0 , batch: 368 , training loss: 5.808042
[INFO] Epoch: 0 , batch: 369 , training loss: 5.523412
[INFO] Epoch: 0 , batch: 370 , training loss: 5.358206
[INFO] Epoch: 0 , batch: 371 , training loss: 5.998388
[INFO] Epoch: 0 , batch: 372 , training loss: 6.417666
[INFO] Epoch: 0 , batch: 373 , training loss: 6.423954
[INFO] Epoch: 0 , batch: 374 , training loss: 6.135111
[INFO] Epoch: 0 , batch: 375 , training loss: 6.257892
[INFO] Epoch: 0 , batch: 376 , training loss: 6.460439
[INFO] Epoch: 0 , batch: 377 , training loss: 6.064258
[INFO] Epoch: 0 , batch: 378 , training loss: 5.882807
[INFO] Epoch: 0 , batch: 379 , training loss: 5.784977
[INFO] Epoch: 0 , batch: 380 , training loss: 5.720566
[INFO] Epoch: 0 , batch: 381 , training loss: 5.665415
[INFO] Epoch: 0 , batch: 382 , training loss: 5.639481
[INFO] Epoch: 0 , batch: 383 , training loss: 6.194069
[INFO] Epoch: 0 , batch: 384 , training loss: 6.848841
[INFO] Epoch: 0 , batch: 385 , training loss: 6.334593
[INFO] Epoch: 0 , batch: 386 , training loss: 6.284783
[INFO] Epoch: 0 , batch: 387 , training loss: 6.212356
[INFO] Epoch: 0 , batch: 388 , training loss: 6.088438
[INFO] Epoch: 0 , batch: 389 , training loss: 5.907138
[INFO] Epoch: 0 , batch: 390 , training loss: 5.917764
[INFO] Epoch: 0 , batch: 391 , training loss: 6.004946
[INFO] Epoch: 0 , batch: 392 , training loss: 6.174075
[INFO] Epoch: 0 , batch: 393 , training loss: 6.115335
[INFO] Epoch: 0 , batch: 394 , training loss: 6.090684
[INFO] Epoch: 0 , batch: 395 , training loss: 5.939106
[INFO] Epoch: 0 , batch: 396 , training loss: 5.773735
[INFO] Epoch: 0 , batch: 397 , training loss: 5.870582
[INFO] Epoch: 0 , batch: 398 , training loss: 5.739552
[INFO] Epoch: 0 , batch: 399 , training loss: 5.779381
[INFO] Epoch: 0 , batch: 400 , training loss: 5.668964
[INFO] Epoch: 0 , batch: 401 , training loss: 5.967800
[INFO] Epoch: 0 , batch: 402 , training loss: 5.787763
[INFO] Epoch: 0 , batch: 403 , training loss: 5.843188
[INFO] Epoch: 0 , batch: 404 , training loss: 5.894732
[INFO] Epoch: 0 , batch: 405 , training loss: 5.896627
[INFO] Epoch: 0 , batch: 406 , training loss: 5.835914
[INFO] Epoch: 0 , batch: 407 , training loss: 5.801272
[INFO] Epoch: 0 , batch: 408 , training loss: 5.807268
[INFO] Epoch: 0 , batch: 409 , training loss: 5.691772
[INFO] Epoch: 0 , batch: 410 , training loss: 5.657784
[INFO] Epoch: 0 , batch: 411 , training loss: 5.882809
[INFO] Epoch: 0 , batch: 412 , training loss: 5.796046
[INFO] Epoch: 0 , batch: 413 , training loss: 5.702127
[INFO] Epoch: 0 , batch: 414 , training loss: 5.711987
[INFO] Epoch: 0 , batch: 415 , training loss: 5.732862
[INFO] Epoch: 0 , batch: 416 , training loss: 5.786694
[INFO] Epoch: 0 , batch: 417 , training loss: 5.718268
[INFO] Epoch: 0 , batch: 418 , training loss: 5.690829
[INFO] Epoch: 0 , batch: 419 , training loss: 5.701079
[INFO] Epoch: 0 , batch: 420 , training loss: 5.610281
[INFO] Epoch: 0 , batch: 421 , training loss: 5.561204
[INFO] Epoch: 0 , batch: 422 , training loss: 5.578101
[INFO] Epoch: 0 , batch: 423 , training loss: 5.806314
[INFO] Epoch: 0 , batch: 424 , training loss: 5.824068
[INFO] Epoch: 0 , batch: 425 , training loss: 5.761134
[INFO] Epoch: 0 , batch: 426 , training loss: 5.445764
[INFO] Epoch: 0 , batch: 427 , training loss: 5.728564
[INFO] Epoch: 0 , batch: 428 , training loss: 5.669047
[INFO] Epoch: 0 , batch: 429 , training loss: 5.447468
[INFO] Epoch: 0 , batch: 430 , training loss: 5.703938
[INFO] Epoch: 0 , batch: 431 , training loss: 5.383668
[INFO] Epoch: 0 , batch: 432 , training loss: 5.478268
[INFO] Epoch: 0 , batch: 433 , training loss: 5.506588
[INFO] Epoch: 0 , batch: 434 , training loss: 5.366277
[INFO] Epoch: 0 , batch: 435 , training loss: 5.629025
[INFO] Epoch: 0 , batch: 436 , training loss: 5.701736
[INFO] Epoch: 0 , batch: 437 , training loss: 5.587925
[INFO] Epoch: 0 , batch: 438 , training loss: 5.264292
[INFO] Epoch: 0 , batch: 439 , training loss: 5.479771
[INFO] Epoch: 0 , batch: 440 , training loss: 5.605157
[INFO] Epoch: 0 , batch: 441 , training loss: 5.590402
[INFO] Epoch: 0 , batch: 442 , training loss: 5.500680
[INFO] Epoch: 0 , batch: 443 , training loss: 5.619150
[INFO] Epoch: 0 , batch: 444 , training loss: 5.424516
[INFO] Epoch: 0 , batch: 445 , training loss: 5.203709
[INFO] Epoch: 0 , batch: 446 , training loss: 5.079012
[INFO] Epoch: 0 , batch: 447 , training loss: 5.365914
[INFO] Epoch: 0 , batch: 448 , training loss: 5.535789
[INFO] Epoch: 0 , batch: 449 , training loss: 5.816212
[INFO] Epoch: 0 , batch: 450 , training loss: 5.806207
[INFO] Epoch: 0 , batch: 451 , training loss: 5.691466
[INFO] Epoch: 0 , batch: 452 , training loss: 5.480996
[INFO] Epoch: 0 , batch: 453 , training loss: 5.357234
[INFO] Epoch: 0 , batch: 454 , training loss: 5.464755
[INFO] Epoch: 0 , batch: 455 , training loss: 5.492640
[INFO] Epoch: 0 , batch: 456 , training loss: 5.498136
[INFO] Epoch: 0 , batch: 457 , training loss: 5.553630
[INFO] Epoch: 0 , batch: 458 , training loss: 5.315577
[INFO] Epoch: 0 , batch: 459 , training loss: 5.316414
[INFO] Epoch: 0 , batch: 460 , training loss: 5.457800
[INFO] Epoch: 0 , batch: 461 , training loss: 5.465959
[INFO] Epoch: 0 , batch: 462 , training loss: 5.514181
[INFO] Epoch: 0 , batch: 463 , training loss: 5.368185
[INFO] Epoch: 0 , batch: 464 , training loss: 5.504933
[INFO] Epoch: 0 , batch: 465 , training loss: 5.418212
[INFO] Epoch: 0 , batch: 466 , training loss: 5.506477
[INFO] Epoch: 0 , batch: 467 , training loss: 5.607973
[INFO] Epoch: 0 , batch: 468 , training loss: 5.535595
[INFO] Epoch: 0 , batch: 469 , training loss: 5.470766
[INFO] Epoch: 0 , batch: 470 , training loss: 5.226244
[INFO] Epoch: 0 , batch: 471 , training loss: 5.330206
[INFO] Epoch: 0 , batch: 472 , training loss: 5.384817
[INFO] Epoch: 0 , batch: 473 , training loss: 5.305837
[INFO] Epoch: 0 , batch: 474 , training loss: 5.256514
[INFO] Epoch: 0 , batch: 475 , training loss: 5.109923
[INFO] Epoch: 0 , batch: 476 , training loss: 5.399067
[INFO] Epoch: 0 , batch: 477 , training loss: 5.436532
[INFO] Epoch: 0 , batch: 478 , training loss: 5.596025
[INFO] Epoch: 0 , batch: 479 , training loss: 5.565635
[INFO] Epoch: 0 , batch: 480 , training loss: 5.683800
[INFO] Epoch: 0 , batch: 481 , training loss: 5.413454
[INFO] Epoch: 0 , batch: 482 , training loss: 5.567141
[INFO] Epoch: 0 , batch: 483 , training loss: 5.412448
[INFO] Epoch: 0 , batch: 484 , training loss: 5.288302
[INFO] Epoch: 0 , batch: 485 , training loss: 5.449399
[INFO] Epoch: 0 , batch: 486 , training loss: 5.284101
[INFO] Epoch: 0 , batch: 487 , training loss: 5.288563
[INFO] Epoch: 0 , batch: 488 , training loss: 5.457567
[INFO] Epoch: 0 , batch: 489 , training loss: 5.317472
[INFO] Epoch: 0 , batch: 490 , training loss: 5.404995
[INFO] Epoch: 0 , batch: 491 , training loss: 5.403199
[INFO] Epoch: 0 , batch: 492 , training loss: 5.178397
[INFO] Epoch: 0 , batch: 493 , training loss: 5.357388
[INFO] Epoch: 0 , batch: 494 , training loss: 5.274165
[INFO] Epoch: 0 , batch: 495 , training loss: 5.408179
[INFO] Epoch: 0 , batch: 496 , training loss: 5.269817
[INFO] Epoch: 0 , batch: 497 , training loss: 5.272228
[INFO] Epoch: 0 , batch: 498 , training loss: 5.357728
[INFO] Epoch: 0 , batch: 499 , training loss: 5.381447
[INFO] Epoch: 0 , batch: 500 , training loss: 5.742315
[INFO] Epoch: 0 , batch: 501 , training loss: 6.261293
[INFO] Epoch: 0 , batch: 502 , training loss: 6.689173
[INFO] Epoch: 0 , batch: 503 , training loss: 6.588930
[INFO] Epoch: 0 , batch: 504 , training loss: 6.378076
[INFO] Epoch: 0 , batch: 505 , training loss: 5.812894
[INFO] Epoch: 0 , batch: 506 , training loss: 5.696619
[INFO] Epoch: 0 , batch: 507 , training loss: 5.820872
[INFO] Epoch: 0 , batch: 508 , training loss: 5.999417
[INFO] Epoch: 0 , batch: 509 , training loss: 5.679763
[INFO] Epoch: 0 , batch: 510 , training loss: 5.604091
[INFO] Epoch: 0 , batch: 511 , training loss: 5.516549
[INFO] Epoch: 0 , batch: 512 , training loss: 5.536557
[INFO] Epoch: 0 , batch: 513 , training loss: 5.709136
[INFO] Epoch: 0 , batch: 514 , training loss: 5.558301
[INFO] Epoch: 0 , batch: 515 , training loss: 5.715576
[INFO] Epoch: 0 , batch: 516 , training loss: 5.523914
[INFO] Epoch: 0 , batch: 517 , training loss: 5.524890
[INFO] Epoch: 0 , batch: 518 , training loss: 5.475342
[INFO] Epoch: 0 , batch: 519 , training loss: 5.399915
[INFO] Epoch: 0 , batch: 520 , training loss: 5.536732
[INFO] Epoch: 0 , batch: 521 , training loss: 5.609414
[INFO] Epoch: 0 , batch: 522 , training loss: 5.701759
[INFO] Epoch: 0 , batch: 523 , training loss: 5.511961
[INFO] Epoch: 0 , batch: 524 , training loss: 5.584877
[INFO] Epoch: 0 , batch: 525 , training loss: 5.627717
[INFO] Epoch: 0 , batch: 526 , training loss: 5.404349
[INFO] Epoch: 0 , batch: 527 , training loss: 5.416409
[INFO] Epoch: 0 , batch: 528 , training loss: 5.568571
[INFO] Epoch: 0 , batch: 529 , training loss: 5.481399
[INFO] Epoch: 0 , batch: 530 , training loss: 5.334092
[INFO] Epoch: 0 , batch: 531 , training loss: 5.489115
[INFO] Epoch: 0 , batch: 532 , training loss: 5.311722
[INFO] Epoch: 0 , batch: 533 , training loss: 5.367943
[INFO] Epoch: 0 , batch: 534 , training loss: 5.423487
[INFO] Epoch: 0 , batch: 535 , training loss: 5.496063
[INFO] Epoch: 0 , batch: 536 , training loss: 5.401555
[INFO] Epoch: 0 , batch: 537 , training loss: 5.383978
[INFO] Epoch: 0 , batch: 538 , training loss: 5.384222
[INFO] Epoch: 0 , batch: 539 , training loss: 5.604036
[INFO] Epoch: 0 , batch: 540 , training loss: 6.562971
[INFO] Epoch: 0 , batch: 541 , training loss: 6.466595
[INFO] Epoch: 0 , batch: 542 , training loss: 5.996038
[INFO] Epoch: 1 , batch: 0 , training loss: 6.140219
[INFO] Epoch: 1 , batch: 1 , training loss: 5.549098
[INFO] Epoch: 1 , batch: 2 , training loss: 5.565838
[INFO] Epoch: 1 , batch: 3 , training loss: 5.088584
[INFO] Epoch: 1 , batch: 4 , training loss: 5.608641
[INFO] Epoch: 1 , batch: 5 , training loss: 5.232637
[INFO] Epoch: 1 , batch: 6 , training loss: 6.428545
[INFO] Epoch: 1 , batch: 7 , training loss: 5.592478
[INFO] Epoch: 1 , batch: 8 , training loss: 5.375309
[INFO] Epoch: 1 , batch: 9 , training loss: 5.279453
[INFO] Epoch: 1 , batch: 10 , training loss: 5.073075
[INFO] Epoch: 1 , batch: 11 , training loss: 4.897583
[INFO] Epoch: 1 , batch: 12 , training loss: 5.083714
[INFO] Epoch: 1 , batch: 13 , training loss: 4.951553
[INFO] Epoch: 1 , batch: 14 , training loss: 4.803937
[INFO] Epoch: 1 , batch: 15 , training loss: 5.067301
[INFO] Epoch: 1 , batch: 16 , training loss: 4.914058
[INFO] Epoch: 1 , batch: 17 , training loss: 5.000134
[INFO] Epoch: 1 , batch: 18 , training loss: 4.873246
[INFO] Epoch: 1 , batch: 19 , training loss: 4.772565
[INFO] Epoch: 1 , batch: 20 , training loss: 4.724334
[INFO] Epoch: 1 , batch: 21 , training loss: 4.793696
[INFO] Epoch: 1 , batch: 22 , training loss: 4.974977
[INFO] Epoch: 1 , batch: 23 , training loss: 5.003282
[INFO] Epoch: 1 , batch: 24 , training loss: 4.800250
[INFO] Epoch: 1 , batch: 25 , training loss: 4.929700
[INFO] Epoch: 1 , batch: 26 , training loss: 4.738859
[INFO] Epoch: 1 , batch: 27 , training loss: 4.774000
[INFO] Epoch: 1 , batch: 28 , training loss: 5.038096
[INFO] Epoch: 1 , batch: 29 , training loss: 4.790136
[INFO] Epoch: 1 , batch: 30 , training loss: 4.757901
[INFO] Epoch: 1 , batch: 31 , training loss: 4.846786
[INFO] Epoch: 1 , batch: 32 , training loss: 4.844983
[INFO] Epoch: 1 , batch: 33 , training loss: 4.880118
[INFO] Epoch: 1 , batch: 34 , training loss: 4.967823
[INFO] Epoch: 1 , batch: 35 , training loss: 4.837360
[INFO] Epoch: 1 , batch: 36 , training loss: 4.791331
[INFO] Epoch: 1 , batch: 37 , training loss: 4.684605
[INFO] Epoch: 1 , batch: 38 , training loss: 4.821549
[INFO] Epoch: 1 , batch: 39 , training loss: 4.686166
[INFO] Epoch: 1 , batch: 40 , training loss: 4.686796
[INFO] Epoch: 1 , batch: 41 , training loss: 4.992185
[INFO] Epoch: 1 , batch: 42 , training loss: 5.574798
[INFO] Epoch: 1 , batch: 43 , training loss: 5.427615
[INFO] Epoch: 1 , batch: 44 , training loss: 5.325221
[INFO] Epoch: 1 , batch: 45 , training loss: 5.861104
[INFO] Epoch: 1 , batch: 46 , training loss: 6.607404
[INFO] Epoch: 1 , batch: 47 , training loss: 5.348333
[INFO] Epoch: 1 , batch: 48 , training loss: 5.506467
[INFO] Epoch: 1 , batch: 49 , training loss: 5.372458
[INFO] Epoch: 1 , batch: 50 , training loss: 5.267549
[INFO] Epoch: 1 , batch: 51 , training loss: 5.102037
[INFO] Epoch: 1 , batch: 52 , training loss: 4.932194
[INFO] Epoch: 1 , batch: 53 , training loss: 5.146112
[INFO] Epoch: 1 , batch: 54 , training loss: 5.085163
[INFO] Epoch: 1 , batch: 55 , training loss: 5.139056
[INFO] Epoch: 1 , batch: 56 , training loss: 4.992092
[INFO] Epoch: 1 , batch: 57 , training loss: 4.829788
[INFO] Epoch: 1 , batch: 58 , training loss: 4.799769
[INFO] Epoch: 1 , batch: 59 , training loss: 5.028218
[INFO] Epoch: 1 , batch: 60 , training loss: 4.786710
[INFO] Epoch: 1 , batch: 61 , training loss: 4.901700
[INFO] Epoch: 1 , batch: 62 , training loss: 4.800903
[INFO] Epoch: 1 , batch: 63 , training loss: 4.951919
[INFO] Epoch: 1 , batch: 64 , training loss: 5.054970
[INFO] Epoch: 1 , batch: 65 , training loss: 4.929013
[INFO] Epoch: 1 , batch: 66 , training loss: 4.767475
[INFO] Epoch: 1 , batch: 67 , training loss: 4.744607
[INFO] Epoch: 1 , batch: 68 , training loss: 5.098186
[INFO] Epoch: 1 , batch: 69 , training loss: 4.835822
[INFO] Epoch: 1 , batch: 70 , training loss: 5.067974
[INFO] Epoch: 1 , batch: 71 , training loss: 4.902268
[INFO] Epoch: 1 , batch: 72 , training loss: 4.955123
[INFO] Epoch: 1 , batch: 73 , training loss: 4.921986
[INFO] Epoch: 1 , batch: 74 , training loss: 4.943567
[INFO] Epoch: 1 , batch: 75 , training loss: 4.641232
[INFO] Epoch: 1 , batch: 76 , training loss: 4.901557
[INFO] Epoch: 1 , batch: 77 , training loss: 4.735587
[INFO] Epoch: 1 , batch: 78 , training loss: 4.796719
[INFO] Epoch: 1 , batch: 79 , training loss: 4.680272
[INFO] Epoch: 1 , batch: 80 , training loss: 4.871279
[INFO] Epoch: 1 , batch: 81 , training loss: 4.891357
[INFO] Epoch: 1 , batch: 82 , training loss: 4.908623
[INFO] Epoch: 1 , batch: 83 , training loss: 5.027179
[INFO] Epoch: 1 , batch: 84 , training loss: 4.873897
[INFO] Epoch: 1 , batch: 85 , training loss: 4.964468
[INFO] Epoch: 1 , batch: 86 , training loss: 4.977171
[INFO] Epoch: 1 , batch: 87 , training loss: 4.947562
[INFO] Epoch: 1 , batch: 88 , training loss: 5.063706
[INFO] Epoch: 1 , batch: 89 , training loss: 4.855928
[INFO] Epoch: 1 , batch: 90 , training loss: 4.811490
[INFO] Epoch: 1 , batch: 91 , training loss: 4.798538
[INFO] Epoch: 1 , batch: 92 , training loss: 4.787899
[INFO] Epoch: 1 , batch: 93 , training loss: 4.840676
[INFO] Epoch: 1 , batch: 94 , training loss: 5.072644
[INFO] Epoch: 1 , batch: 95 , training loss: 4.853034
[INFO] Epoch: 1 , batch: 96 , training loss: 4.802083
[INFO] Epoch: 1 , batch: 97 , training loss: 4.830358
[INFO] Epoch: 1 , batch: 98 , training loss: 4.835376
[INFO] Epoch: 1 , batch: 99 , training loss: 4.799325
[INFO] Epoch: 1 , batch: 100 , training loss: 4.645916
[INFO] Epoch: 1 , batch: 101 , training loss: 4.735709
[INFO] Epoch: 1 , batch: 102 , training loss: 4.887730
[INFO] Epoch: 1 , batch: 103 , training loss: 4.566777
[INFO] Epoch: 1 , batch: 104 , training loss: 4.655032
[INFO] Epoch: 1 , batch: 105 , training loss: 4.912745
[INFO] Epoch: 1 , batch: 106 , training loss: 4.869501
[INFO] Epoch: 1 , batch: 107 , training loss: 4.825695
[INFO] Epoch: 1 , batch: 108 , training loss: 4.644597
[INFO] Epoch: 1 , batch: 109 , training loss: 4.536220
[INFO] Epoch: 1 , batch: 110 , training loss: 4.865050
[INFO] Epoch: 1 , batch: 111 , training loss: 4.821693
[INFO] Epoch: 1 , batch: 112 , training loss: 4.870599
[INFO] Epoch: 1 , batch: 113 , training loss: 4.735372
[INFO] Epoch: 1 , batch: 114 , training loss: 4.949181
[INFO] Epoch: 1 , batch: 115 , training loss: 4.839721
[INFO] Epoch: 1 , batch: 116 , training loss: 4.815777
[INFO] Epoch: 1 , batch: 117 , training loss: 5.050687
[INFO] Epoch: 1 , batch: 118 , training loss: 5.038912
[INFO] Epoch: 1 , batch: 119 , training loss: 5.078060
[INFO] Epoch: 1 , batch: 120 , training loss: 5.030190
[INFO] Epoch: 1 , batch: 121 , training loss: 4.808139
[INFO] Epoch: 1 , batch: 122 , training loss: 4.763829
[INFO] Epoch: 1 , batch: 123 , training loss: 4.957748
[INFO] Epoch: 1 , batch: 124 , training loss: 5.112363
[INFO] Epoch: 1 , batch: 125 , training loss: 4.656719
[INFO] Epoch: 1 , batch: 126 , training loss: 4.702341
[INFO] Epoch: 1 , batch: 127 , training loss: 4.723234
[INFO] Epoch: 1 , batch: 128 , training loss: 4.968015
[INFO] Epoch: 1 , batch: 129 , training loss: 4.814223
[INFO] Epoch: 1 , batch: 130 , training loss: 4.920055
[INFO] Epoch: 1 , batch: 131 , training loss: 4.875292
[INFO] Epoch: 1 , batch: 132 , training loss: 4.901258
[INFO] Epoch: 1 , batch: 133 , training loss: 4.792348
[INFO] Epoch: 1 , batch: 134 , training loss: 4.629078
[INFO] Epoch: 1 , batch: 135 , training loss: 4.586820
[INFO] Epoch: 1 , batch: 136 , training loss: 4.870995
[INFO] Epoch: 1 , batch: 137 , training loss: 4.788592
[INFO] Epoch: 1 , batch: 138 , training loss: 5.014967
[INFO] Epoch: 1 , batch: 139 , training loss: 5.712340
[INFO] Epoch: 1 , batch: 140 , training loss: 5.698626
[INFO] Epoch: 1 , batch: 141 , training loss: 5.369727
[INFO] Epoch: 1 , batch: 142 , training loss: 4.918353
[INFO] Epoch: 1 , batch: 143 , training loss: 4.967730
[INFO] Epoch: 1 , batch: 144 , training loss: 4.799376
[INFO] Epoch: 1 , batch: 145 , training loss: 4.987031
[INFO] Epoch: 1 , batch: 146 , training loss: 5.105228
[INFO] Epoch: 1 , batch: 147 , training loss: 4.671627
[INFO] Epoch: 1 , batch: 148 , training loss: 4.658615
[INFO] Epoch: 1 , batch: 149 , training loss: 4.802213
[INFO] Epoch: 1 , batch: 150 , training loss: 5.097658
[INFO] Epoch: 1 , batch: 151 , training loss: 4.648655
[INFO] Epoch: 1 , batch: 152 , training loss: 4.703622
[INFO] Epoch: 1 , batch: 153 , training loss: 4.933342
[INFO] Epoch: 1 , batch: 154 , training loss: 4.967849
[INFO] Epoch: 1 , batch: 155 , training loss: 5.261768
[INFO] Epoch: 1 , batch: 156 , training loss: 4.822154
[INFO] Epoch: 1 , batch: 157 , training loss: 4.865078
[INFO] Epoch: 1 , batch: 158 , training loss: 5.326004
[INFO] Epoch: 1 , batch: 159 , training loss: 5.321520
[INFO] Epoch: 1 , batch: 160 , training loss: 6.201687
[INFO] Epoch: 1 , batch: 161 , training loss: 5.672332
[INFO] Epoch: 1 , batch: 162 , training loss: 5.359737
[INFO] Epoch: 1 , batch: 163 , training loss: 5.381639
[INFO] Epoch: 1 , batch: 164 , training loss: 5.135270
[INFO] Epoch: 1 , batch: 165 , training loss: 5.055563
[INFO] Epoch: 1 , batch: 166 , training loss: 5.916889
[INFO] Epoch: 1 , batch: 167 , training loss: 6.618530
[INFO] Epoch: 1 , batch: 168 , training loss: 6.291873
[INFO] Epoch: 1 , batch: 169 , training loss: 6.019051
[INFO] Epoch: 1 , batch: 170 , training loss: 5.912135
[INFO] Epoch: 1 , batch: 171 , training loss: 5.798658
[INFO] Epoch: 1 , batch: 172 , training loss: 5.855879
[INFO] Epoch: 1 , batch: 173 , training loss: 5.926259
[INFO] Epoch: 1 , batch: 174 , training loss: 6.006317
[INFO] Epoch: 1 , batch: 175 , training loss: 5.925982
[INFO] Epoch: 1 , batch: 176 , training loss: 5.897740
[INFO] Epoch: 1 , batch: 177 , training loss: 5.667224
[INFO] Epoch: 1 , batch: 178 , training loss: 5.550251
[INFO] Epoch: 1 , batch: 179 , training loss: 5.492371
[INFO] Epoch: 1 , batch: 180 , training loss: 5.451048
[INFO] Epoch: 1 , batch: 181 , training loss: 5.573154
[INFO] Epoch: 1 , batch: 182 , training loss: 5.458908
[INFO] Epoch: 1 , batch: 183 , training loss: 5.381327
[INFO] Epoch: 1 , batch: 184 , training loss: 5.295696
[INFO] Epoch: 1 , batch: 185 , training loss: 5.255778
[INFO] Epoch: 1 , batch: 186 , training loss: 5.333819
[INFO] Epoch: 1 , batch: 187 , training loss: 5.462849
[INFO] Epoch: 1 , batch: 188 , training loss: 5.402342
[INFO] Epoch: 1 , batch: 189 , training loss: 5.317356
[INFO] Epoch: 1 , batch: 190 , training loss: 5.242816
[INFO] Epoch: 1 , batch: 191 , training loss: 5.395903
[INFO] Epoch: 1 , batch: 192 , training loss: 5.139852
[INFO] Epoch: 1 , batch: 193 , training loss: 5.253980
[INFO] Epoch: 1 , batch: 194 , training loss: 5.186683
[INFO] Epoch: 1 , batch: 195 , training loss: 5.351189
[INFO] Epoch: 1 , batch: 196 , training loss: 5.078581
[INFO] Epoch: 1 , batch: 197 , training loss: 5.300527
[INFO] Epoch: 1 , batch: 198 , training loss: 5.046730
[INFO] Epoch: 1 , batch: 199 , training loss: 5.136235
[INFO] Epoch: 1 , batch: 200 , training loss: 5.035063
[INFO] Epoch: 1 , batch: 201 , training loss: 5.022845
[INFO] Epoch: 1 , batch: 202 , training loss: 4.932194
[INFO] Epoch: 1 , batch: 203 , training loss: 4.897582
[INFO] Epoch: 1 , batch: 204 , training loss: 5.060800
[INFO] Epoch: 1 , batch: 205 , training loss: 4.706377
[INFO] Epoch: 1 , batch: 206 , training loss: 4.540010
[INFO] Epoch: 1 , batch: 207 , training loss: 4.612667
[INFO] Epoch: 1 , batch: 208 , training loss: 5.052909
[INFO] Epoch: 1 , batch: 209 , training loss: 4.965366
[INFO] Epoch: 1 , batch: 210 , training loss: 5.009161
[INFO] Epoch: 1 , batch: 211 , training loss: 4.881416
[INFO] Epoch: 1 , batch: 212 , training loss: 4.964636
[INFO] Epoch: 1 , batch: 213 , training loss: 4.991198
[INFO] Epoch: 1 , batch: 214 , training loss: 4.917889
[INFO] Epoch: 1 , batch: 215 , training loss: 5.126253
[INFO] Epoch: 1 , batch: 216 , training loss: 4.985830
[INFO] Epoch: 1 , batch: 217 , training loss: 4.868659
[INFO] Epoch: 1 , batch: 218 , training loss: 4.917318
[INFO] Epoch: 1 , batch: 219 , training loss: 4.934411
[INFO] Epoch: 1 , batch: 220 , training loss: 4.794436
[INFO] Epoch: 1 , batch: 221 , training loss: 4.795012
[INFO] Epoch: 1 , batch: 222 , training loss: 4.944350
[INFO] Epoch: 1 , batch: 223 , training loss: 5.029160
[INFO] Epoch: 1 , batch: 224 , training loss: 5.071547
[INFO] Epoch: 1 , batch: 225 , training loss: 4.931702
[INFO] Epoch: 1 , batch: 226 , training loss: 5.066983
[INFO] Epoch: 1 , batch: 227 , training loss: 5.036107
[INFO] Epoch: 1 , batch: 228 , training loss: 5.089815
[INFO] Epoch: 1 , batch: 229 , training loss: 4.961593
[INFO] Epoch: 1 , batch: 230 , training loss: 4.786567
[INFO] Epoch: 1 , batch: 231 , training loss: 4.601895
[INFO] Epoch: 1 , batch: 232 , training loss: 4.703806
[INFO] Epoch: 1 , batch: 233 , training loss: 4.802075
[INFO] Epoch: 1 , batch: 234 , training loss: 4.500525
[INFO] Epoch: 1 , batch: 235 , training loss: 4.620083
[INFO] Epoch: 1 , batch: 236 , training loss: 4.818384
[INFO] Epoch: 1 , batch: 237 , training loss: 4.921198
[INFO] Epoch: 1 , batch: 238 , training loss: 4.669099
[INFO] Epoch: 1 , batch: 239 , training loss: 4.742837
[INFO] Epoch: 1 , batch: 240 , training loss: 4.808996
[INFO] Epoch: 1 , batch: 241 , training loss: 4.611854
[INFO] Epoch: 1 , batch: 242 , training loss: 4.606580
[INFO] Epoch: 1 , batch: 243 , training loss: 4.972278
[INFO] Epoch: 1 , batch: 244 , training loss: 4.838062
[INFO] Epoch: 1 , batch: 245 , training loss: 4.830146
[INFO] Epoch: 1 , batch: 246 , training loss: 4.469333
[INFO] Epoch: 1 , batch: 247 , training loss: 4.628779
[INFO] Epoch: 1 , batch: 248 , training loss: 4.715654
[INFO] Epoch: 1 , batch: 249 , training loss: 4.665050
[INFO] Epoch: 1 , batch: 250 , training loss: 4.476891
[INFO] Epoch: 1 , batch: 251 , training loss: 4.933085
[INFO] Epoch: 1 , batch: 252 , training loss: 4.657743
[INFO] Epoch: 1 , batch: 253 , training loss: 4.675941
[INFO] Epoch: 1 , batch: 254 , training loss: 5.010607
[INFO] Epoch: 1 , batch: 255 , training loss: 4.996449
[INFO] Epoch: 1 , batch: 256 , training loss: 4.874694
[INFO] Epoch: 1 , batch: 257 , training loss: 5.023599
[INFO] Epoch: 1 , batch: 258 , training loss: 5.117929
[INFO] Epoch: 1 , batch: 259 , training loss: 5.131464
[INFO] Epoch: 1 , batch: 260 , training loss: 4.787333
[INFO] Epoch: 1 , batch: 261 , training loss: 4.992623
[INFO] Epoch: 1 , batch: 262 , training loss: 5.192726
[INFO] Epoch: 1 , batch: 263 , training loss: 5.228162
[INFO] Epoch: 1 , batch: 264 , training loss: 4.598933
[INFO] Epoch: 1 , batch: 265 , training loss: 4.757391
[INFO] Epoch: 1 , batch: 266 , training loss: 5.286033
[INFO] Epoch: 1 , batch: 267 , training loss: 4.971905
[INFO] Epoch: 1 , batch: 268 , training loss: 4.820736
[INFO] Epoch: 1 , batch: 269 , training loss: 4.940472
[INFO] Epoch: 1 , batch: 270 , training loss: 4.904675
[INFO] Epoch: 1 , batch: 271 , training loss: 4.953711
[INFO] Epoch: 1 , batch: 272 , training loss: 4.895967
[INFO] Epoch: 1 , batch: 273 , training loss: 4.912267
[INFO] Epoch: 1 , batch: 274 , training loss: 5.016339
[INFO] Epoch: 1 , batch: 275 , training loss: 4.921558
[INFO] Epoch: 1 , batch: 276 , training loss: 4.987816
[INFO] Epoch: 1 , batch: 277 , training loss: 5.074536
[INFO] Epoch: 1 , batch: 278 , training loss: 4.667503
[INFO] Epoch: 1 , batch: 279 , training loss: 4.702113
[INFO] Epoch: 1 , batch: 280 , training loss: 4.655053
[INFO] Epoch: 1 , batch: 281 , training loss: 4.780535
[INFO] Epoch: 1 , batch: 282 , training loss: 4.641874
[INFO] Epoch: 1 , batch: 283 , training loss: 4.800431
[INFO] Epoch: 1 , batch: 284 , training loss: 4.771352
[INFO] Epoch: 1 , batch: 285 , training loss: 4.805253
[INFO] Epoch: 1 , batch: 286 , training loss: 4.760082
[INFO] Epoch: 1 , batch: 287 , training loss: 4.582411
[INFO] Epoch: 1 , batch: 288 , training loss: 4.690833
[INFO] Epoch: 1 , batch: 289 , training loss: 4.675859
[INFO] Epoch: 1 , batch: 290 , training loss: 4.574337
[INFO] Epoch: 1 , batch: 291 , training loss: 4.483766
[INFO] Epoch: 1 , batch: 292 , training loss: 4.587451
[INFO] Epoch: 1 , batch: 293 , training loss: 4.564679
[INFO] Epoch: 1 , batch: 294 , training loss: 5.198013
[INFO] Epoch: 1 , batch: 295 , training loss: 4.967923
[INFO] Epoch: 1 , batch: 296 , training loss: 4.879725
[INFO] Epoch: 1 , batch: 297 , training loss: 4.791070
[INFO] Epoch: 1 , batch: 298 , training loss: 4.676196
[INFO] Epoch: 1 , batch: 299 , training loss: 4.626353
[INFO] Epoch: 1 , batch: 300 , training loss: 4.619877
[INFO] Epoch: 1 , batch: 301 , training loss: 4.608031
[INFO] Epoch: 1 , batch: 302 , training loss: 4.752327
[INFO] Epoch: 1 , batch: 303 , training loss: 4.743975
[INFO] Epoch: 1 , batch: 304 , training loss: 4.975636
[INFO] Epoch: 1 , batch: 305 , training loss: 4.671870
[INFO] Epoch: 1 , batch: 306 , training loss: 4.770205
[INFO] Epoch: 1 , batch: 307 , training loss: 4.740668
[INFO] Epoch: 1 , batch: 308 , training loss: 4.704912
[INFO] Epoch: 1 , batch: 309 , training loss: 4.694298
[INFO] Epoch: 1 , batch: 310 , training loss: 4.476509
[INFO] Epoch: 1 , batch: 311 , training loss: 4.516889
[INFO] Epoch: 1 , batch: 312 , training loss: 4.390441
[INFO] Epoch: 1 , batch: 313 , training loss: 4.601255
[INFO] Epoch: 1 , batch: 314 , training loss: 4.606815
[INFO] Epoch: 1 , batch: 315 , training loss: 4.669675
[INFO] Epoch: 1 , batch: 316 , training loss: 5.055204
[INFO] Epoch: 1 , batch: 317 , training loss: 5.696587
[INFO] Epoch: 1 , batch: 318 , training loss: 5.805468
[INFO] Epoch: 1 , batch: 319 , training loss: 5.211456
[INFO] Epoch: 1 , batch: 320 , training loss: 4.699141
[INFO] Epoch: 1 , batch: 321 , training loss: 4.480628
[INFO] Epoch: 1 , batch: 322 , training loss: 4.634450
[INFO] Epoch: 1 , batch: 323 , training loss: 4.628727
[INFO] Epoch: 1 , batch: 324 , training loss: 4.638735
[INFO] Epoch: 1 , batch: 325 , training loss: 4.839240
[INFO] Epoch: 1 , batch: 326 , training loss: 4.871715
[INFO] Epoch: 1 , batch: 327 , training loss: 4.735499
[INFO] Epoch: 1 , batch: 328 , training loss: 4.803287
[INFO] Epoch: 1 , batch: 329 , training loss: 4.623317
[INFO] Epoch: 1 , batch: 330 , training loss: 4.586882
[INFO] Epoch: 1 , batch: 331 , training loss: 4.790586
[INFO] Epoch: 1 , batch: 332 , training loss: 4.513706
[INFO] Epoch: 1 , batch: 333 , training loss: 4.476532
[INFO] Epoch: 1 , batch: 334 , training loss: 4.659163
[INFO] Epoch: 1 , batch: 335 , training loss: 4.757342
[INFO] Epoch: 1 , batch: 336 , training loss: 4.668821
[INFO] Epoch: 1 , batch: 337 , training loss: 4.812777
[INFO] Epoch: 1 , batch: 338 , training loss: 4.912034
[INFO] Epoch: 1 , batch: 339 , training loss: 4.763796
[INFO] Epoch: 1 , batch: 340 , training loss: 4.967743
[INFO] Epoch: 1 , batch: 341 , training loss: 4.665401
[INFO] Epoch: 1 , batch: 342 , training loss: 4.561824
[INFO] Epoch: 1 , batch: 343 , training loss: 4.657152
[INFO] Epoch: 1 , batch: 344 , training loss: 4.525759
[INFO] Epoch: 1 , batch: 345 , training loss: 4.660194
[INFO] Epoch: 1 , batch: 346 , training loss: 4.758745
[INFO] Epoch: 1 , batch: 347 , training loss: 4.604905
[INFO] Epoch: 1 , batch: 348 , training loss: 4.855811
[INFO] Epoch: 1 , batch: 349 , training loss: 4.928117
[INFO] Epoch: 1 , batch: 350 , training loss: 4.596920
[INFO] Epoch: 1 , batch: 351 , training loss: 4.608256
[INFO] Epoch: 1 , batch: 352 , training loss: 4.628837
[INFO] Epoch: 1 , batch: 353 , training loss: 4.614091
[INFO] Epoch: 1 , batch: 354 , training loss: 4.724732
[INFO] Epoch: 1 , batch: 355 , training loss: 4.776872
[INFO] Epoch: 1 , batch: 356 , training loss: 4.658006
[INFO] Epoch: 1 , batch: 357 , training loss: 4.697891
[INFO] Epoch: 1 , batch: 358 , training loss: 4.694776
[INFO] Epoch: 1 , batch: 359 , training loss: 4.676989
[INFO] Epoch: 1 , batch: 360 , training loss: 4.658042
[INFO] Epoch: 1 , batch: 361 , training loss: 4.633063
[INFO] Epoch: 1 , batch: 362 , training loss: 4.717100
[INFO] Epoch: 1 , batch: 363 , training loss: 4.588188
[INFO] Epoch: 1 , batch: 364 , training loss: 4.641339
[INFO] Epoch: 1 , batch: 365 , training loss: 4.544147
[INFO] Epoch: 1 , batch: 366 , training loss: 4.705655
[INFO] Epoch: 1 , batch: 367 , training loss: 4.816602
[INFO] Epoch: 1 , batch: 368 , training loss: 5.482699
[INFO] Epoch: 1 , batch: 369 , training loss: 4.986214
[INFO] Epoch: 1 , batch: 370 , training loss: 4.747562
[INFO] Epoch: 1 , batch: 371 , training loss: 5.435736
[INFO] Epoch: 1 , batch: 372 , training loss: 5.749058
[INFO] Epoch: 1 , batch: 373 , training loss: 5.759704
[INFO] Epoch: 1 , batch: 374 , training loss: 5.663927
[INFO] Epoch: 1 , batch: 375 , training loss: 5.665878
[INFO] Epoch: 1 , batch: 376 , training loss: 5.615041
[INFO] Epoch: 1 , batch: 377 , training loss: 5.250223
[INFO] Epoch: 1 , batch: 378 , training loss: 5.327982
[INFO] Epoch: 1 , batch: 379 , training loss: 5.337905
[INFO] Epoch: 1 , batch: 380 , training loss: 5.367236
[INFO] Epoch: 1 , batch: 381 , training loss: 5.148426
[INFO] Epoch: 1 , batch: 382 , training loss: 5.293303
[INFO] Epoch: 1 , batch: 383 , training loss: 5.509075
[INFO] Epoch: 1 , batch: 384 , training loss: 5.825593
[INFO] Epoch: 1 , batch: 385 , training loss: 5.623570
[INFO] Epoch: 1 , batch: 386 , training loss: 5.703918
[INFO] Epoch: 1 , batch: 387 , training loss: 5.644475
[INFO] Epoch: 1 , batch: 388 , training loss: 5.378065
[INFO] Epoch: 1 , batch: 389 , training loss: 5.124701
[INFO] Epoch: 1 , batch: 390 , training loss: 5.075156
[INFO] Epoch: 1 , batch: 391 , training loss: 5.114562
[INFO] Epoch: 1 , batch: 392 , training loss: 5.440461
[INFO] Epoch: 1 , batch: 393 , training loss: 5.346855
[INFO] Epoch: 1 , batch: 394 , training loss: 5.392987
[INFO] Epoch: 1 , batch: 395 , training loss: 5.189012
[INFO] Epoch: 1 , batch: 396 , training loss: 5.016764
[INFO] Epoch: 1 , batch: 397 , training loss: 5.173579
[INFO] Epoch: 1 , batch: 398 , training loss: 5.001807
[INFO] Epoch: 1 , batch: 399 , training loss: 5.083880
[INFO] Epoch: 1 , batch: 400 , training loss: 5.012473
[INFO] Epoch: 1 , batch: 401 , training loss: 5.372727
[INFO] Epoch: 1 , batch: 402 , training loss: 5.153218
[INFO] Epoch: 1 , batch: 403 , training loss: 5.095680
[INFO] Epoch: 1 , batch: 404 , training loss: 5.187031
[INFO] Epoch: 1 , batch: 405 , training loss: 5.250233
[INFO] Epoch: 1 , batch: 406 , training loss: 5.141106
[INFO] Epoch: 1 , batch: 407 , training loss: 5.194204
[INFO] Epoch: 1 , batch: 408 , training loss: 5.161957
[INFO] Epoch: 1 , batch: 409 , training loss: 5.093730
[INFO] Epoch: 1 , batch: 410 , training loss: 5.083225
[INFO] Epoch: 1 , batch: 411 , training loss: 5.310704
[INFO] Epoch: 1 , batch: 412 , training loss: 5.189017
[INFO] Epoch: 1 , batch: 413 , training loss: 5.069627
[INFO] Epoch: 1 , batch: 414 , training loss: 5.102568
[INFO] Epoch: 1 , batch: 415 , training loss: 5.109010
[INFO] Epoch: 1 , batch: 416 , training loss: 5.200395
[INFO] Epoch: 1 , batch: 417 , training loss: 5.108115
[INFO] Epoch: 1 , batch: 418 , training loss: 5.109641
[INFO] Epoch: 1 , batch: 419 , training loss: 5.080131
[INFO] Epoch: 1 , batch: 420 , training loss: 5.023194
[INFO] Epoch: 1 , batch: 421 , training loss: 4.989126
[INFO] Epoch: 1 , batch: 422 , training loss: 4.922203
[INFO] Epoch: 1 , batch: 423 , training loss: 5.185660
[INFO] Epoch: 1 , batch: 424 , training loss: 5.278047
[INFO] Epoch: 1 , batch: 425 , training loss: 5.185746
[INFO] Epoch: 1 , batch: 426 , training loss: 4.853468
[INFO] Epoch: 1 , batch: 427 , training loss: 5.127913
[INFO] Epoch: 1 , batch: 428 , training loss: 5.069618
[INFO] Epoch: 1 , batch: 429 , training loss: 4.815475
[INFO] Epoch: 1 , batch: 430 , training loss: 5.117700
[INFO] Epoch: 1 , batch: 431 , training loss: 4.710131
[INFO] Epoch: 1 , batch: 432 , training loss: 4.810643
[INFO] Epoch: 1 , batch: 433 , training loss: 4.820626
[INFO] Epoch: 1 , batch: 434 , training loss: 4.677868
[INFO] Epoch: 1 , batch: 435 , training loss: 4.992436
[INFO] Epoch: 1 , batch: 436 , training loss: 5.134386
[INFO] Epoch: 1 , batch: 437 , training loss: 4.959109
[INFO] Epoch: 1 , batch: 438 , training loss: 4.654702
[INFO] Epoch: 1 , batch: 439 , training loss: 4.926067
[INFO] Epoch: 1 , batch: 440 , training loss: 5.046819
[INFO] Epoch: 1 , batch: 441 , training loss: 5.054323
[INFO] Epoch: 1 , batch: 442 , training loss: 4.912209
[INFO] Epoch: 1 , batch: 443 , training loss: 5.100024
[INFO] Epoch: 1 , batch: 444 , training loss: 4.769640
[INFO] Epoch: 1 , batch: 445 , training loss: 4.579852
[INFO] Epoch: 1 , batch: 446 , training loss: 4.435411
[INFO] Epoch: 1 , batch: 447 , training loss: 4.749441
[INFO] Epoch: 1 , batch: 448 , training loss: 4.926764
[INFO] Epoch: 1 , batch: 449 , training loss: 5.308895
[INFO] Epoch: 1 , batch: 450 , training loss: 5.341585
[INFO] Epoch: 1 , batch: 451 , training loss: 5.221922
[INFO] Epoch: 1 , batch: 452 , training loss: 4.940357
[INFO] Epoch: 1 , batch: 453 , training loss: 4.749117
[INFO] Epoch: 1 , batch: 454 , training loss: 4.894612
[INFO] Epoch: 1 , batch: 455 , training loss: 4.931087
[INFO] Epoch: 1 , batch: 456 , training loss: 4.919318
[INFO] Epoch: 1 , batch: 457 , training loss: 5.022099
[INFO] Epoch: 1 , batch: 458 , training loss: 4.736780
[INFO] Epoch: 1 , batch: 459 , training loss: 4.724398
[INFO] Epoch: 1 , batch: 460 , training loss: 4.877255
[INFO] Epoch: 1 , batch: 461 , training loss: 4.904955
[INFO] Epoch: 1 , batch: 462 , training loss: 4.938637
[INFO] Epoch: 1 , batch: 463 , training loss: 4.769516
[INFO] Epoch: 1 , batch: 464 , training loss: 4.961260
[INFO] Epoch: 1 , batch: 465 , training loss: 4.899195
[INFO] Epoch: 1 , batch: 466 , training loss: 4.964610
[INFO] Epoch: 1 , batch: 467 , training loss: 5.090708
[INFO] Epoch: 1 , batch: 468 , training loss: 4.977778
[INFO] Epoch: 1 , batch: 469 , training loss: 4.937344
[INFO] Epoch: 1 , batch: 470 , training loss: 4.697977
[INFO] Epoch: 1 , batch: 471 , training loss: 4.840382
[INFO] Epoch: 1 , batch: 472 , training loss: 4.890885
[INFO] Epoch: 1 , batch: 473 , training loss: 4.811856
[INFO] Epoch: 1 , batch: 474 , training loss: 4.688067
[INFO] Epoch: 1 , batch: 475 , training loss: 4.522696
[INFO] Epoch: 1 , batch: 476 , training loss: 4.861631
[INFO] Epoch: 1 , batch: 477 , training loss: 4.960217
[INFO] Epoch: 1 , batch: 478 , training loss: 5.105993
[INFO] Epoch: 1 , batch: 479 , training loss: 5.045957
[INFO] Epoch: 1 , batch: 480 , training loss: 5.189726
[INFO] Epoch: 1 , batch: 481 , training loss: 4.952067
[INFO] Epoch: 1 , batch: 482 , training loss: 5.102255
[INFO] Epoch: 1 , batch: 483 , training loss: 4.935234
[INFO] Epoch: 1 , batch: 484 , training loss: 4.761853
[INFO] Epoch: 1 , batch: 485 , training loss: 4.935115
[INFO] Epoch: 1 , batch: 486 , training loss: 4.756255
[INFO] Epoch: 1 , batch: 487 , training loss: 4.756558
[INFO] Epoch: 1 , batch: 488 , training loss: 4.965960
[INFO] Epoch: 1 , batch: 489 , training loss: 4.791898
[INFO] Epoch: 1 , batch: 490 , training loss: 4.925961
[INFO] Epoch: 1 , batch: 491 , training loss: 4.906248
[INFO] Epoch: 1 , batch: 492 , training loss: 4.709435
[INFO] Epoch: 1 , batch: 493 , training loss: 4.913402
[INFO] Epoch: 1 , batch: 494 , training loss: 4.833534
[INFO] Epoch: 1 , batch: 495 , training loss: 4.960544
[INFO] Epoch: 1 , batch: 496 , training loss: 4.810629
[INFO] Epoch: 1 , batch: 497 , training loss: 4.834539
[INFO] Epoch: 1 , batch: 498 , training loss: 4.902506
[INFO] Epoch: 1 , batch: 499 , training loss: 4.941814
[INFO] Epoch: 1 , batch: 500 , training loss: 5.305852
[INFO] Epoch: 1 , batch: 501 , training loss: 5.768440
[INFO] Epoch: 1 , batch: 502 , training loss: 5.910386
[INFO] Epoch: 1 , batch: 503 , training loss: 5.629588
[INFO] Epoch: 1 , batch: 504 , training loss: 5.640764
[INFO] Epoch: 1 , batch: 505 , training loss: 5.444631
[INFO] Epoch: 1 , batch: 506 , training loss: 5.384540
[INFO] Epoch: 1 , batch: 507 , training loss: 5.384531
[INFO] Epoch: 1 , batch: 508 , training loss: 5.409054
[INFO] Epoch: 1 , batch: 509 , training loss: 5.104188
[INFO] Epoch: 1 , batch: 510 , training loss: 5.164996
[INFO] Epoch: 1 , batch: 511 , training loss: 5.104573
[INFO] Epoch: 1 , batch: 512 , training loss: 5.150901
[INFO] Epoch: 1 , batch: 513 , training loss: 5.366002
[INFO] Epoch: 1 , batch: 514 , training loss: 5.075185
[INFO] Epoch: 1 , batch: 515 , training loss: 5.325877
[INFO] Epoch: 1 , batch: 516 , training loss: 5.087013
[INFO] Epoch: 1 , batch: 517 , training loss: 5.096253
[INFO] Epoch: 1 , batch: 518 , training loss: 5.052754
[INFO] Epoch: 1 , batch: 519 , training loss: 4.968295
[INFO] Epoch: 1 , batch: 520 , training loss: 5.139162
[INFO] Epoch: 1 , batch: 521 , training loss: 5.210573
[INFO] Epoch: 1 , batch: 522 , training loss: 5.253143
[INFO] Epoch: 1 , batch: 523 , training loss: 5.117594
[INFO] Epoch: 1 , batch: 524 , training loss: 5.257809
[INFO] Epoch: 1 , batch: 525 , training loss: 5.259136
[INFO] Epoch: 1 , batch: 526 , training loss: 5.039589
[INFO] Epoch: 1 , batch: 527 , training loss: 5.046174
[INFO] Epoch: 1 , batch: 528 , training loss: 5.182291
[INFO] Epoch: 1 , batch: 529 , training loss: 5.104917
[INFO] Epoch: 1 , batch: 530 , training loss: 4.946239
[INFO] Epoch: 1 , batch: 531 , training loss: 5.108496
[INFO] Epoch: 1 , batch: 532 , training loss: 4.913845
[INFO] Epoch: 1 , batch: 533 , training loss: 5.037371
[INFO] Epoch: 1 , batch: 534 , training loss: 5.064583
[INFO] Epoch: 1 , batch: 535 , training loss: 5.111322
[INFO] Epoch: 1 , batch: 536 , training loss: 5.013674
[INFO] Epoch: 1 , batch: 537 , training loss: 4.974050
[INFO] Epoch: 1 , batch: 538 , training loss: 5.023566
[INFO] Epoch: 1 , batch: 539 , training loss: 5.199707
[INFO] Epoch: 1 , batch: 540 , training loss: 5.876510
[INFO] Epoch: 1 , batch: 541 , training loss: 5.841218
[INFO] Epoch: 1 , batch: 542 , training loss: 5.476551
[INFO] Epoch: 2 , batch: 0 , training loss: 5.622816
[INFO] Epoch: 2 , batch: 1 , training loss: 5.144134
[INFO] Epoch: 2 , batch: 2 , training loss: 4.969163
[INFO] Epoch: 2 , batch: 3 , training loss: 4.798207
[INFO] Epoch: 2 , batch: 4 , training loss: 5.274718
[INFO] Epoch: 2 , batch: 5 , training loss: 4.866977
[INFO] Epoch: 2 , batch: 6 , training loss: 5.554931
[INFO] Epoch: 2 , batch: 7 , training loss: 4.914225
[INFO] Epoch: 2 , batch: 8 , training loss: 4.678845
[INFO] Epoch: 2 , batch: 9 , training loss: 4.731449
[INFO] Epoch: 2 , batch: 10 , training loss: 4.634030
[INFO] Epoch: 2 , batch: 11 , training loss: 4.466808
[INFO] Epoch: 2 , batch: 12 , training loss: 4.581612
[INFO] Epoch: 2 , batch: 13 , training loss: 4.461450
[INFO] Epoch: 2 , batch: 14 , training loss: 4.229496
[INFO] Epoch: 2 , batch: 15 , training loss: 4.534585
[INFO] Epoch: 2 , batch: 16 , training loss: 4.404361
[INFO] Epoch: 2 , batch: 17 , training loss: 4.589670
[INFO] Epoch: 2 , batch: 18 , training loss: 4.461669
[INFO] Epoch: 2 , batch: 19 , training loss: 4.325854
[INFO] Epoch: 2 , batch: 20 , training loss: 4.271908
[INFO] Epoch: 2 , batch: 21 , training loss: 4.339345
[INFO] Epoch: 2 , batch: 22 , training loss: 4.476436
[INFO] Epoch: 2 , batch: 23 , training loss: 4.606694
[INFO] Epoch: 2 , batch: 24 , training loss: 4.395391
[INFO] Epoch: 2 , batch: 25 , training loss: 4.554085
[INFO] Epoch: 2 , batch: 26 , training loss: 4.360878
[INFO] Epoch: 2 , batch: 27 , training loss: 4.366757
[INFO] Epoch: 2 , batch: 28 , training loss: 4.649666
[INFO] Epoch: 2 , batch: 29 , training loss: 4.386739
[INFO] Epoch: 2 , batch: 30 , training loss: 4.350545
[INFO] Epoch: 2 , batch: 31 , training loss: 4.452255
[INFO] Epoch: 2 , batch: 32 , training loss: 4.464667
[INFO] Epoch: 2 , batch: 33 , training loss: 4.559699
[INFO] Epoch: 2 , batch: 34 , training loss: 4.596798
[INFO] Epoch: 2 , batch: 35 , training loss: 4.501454
[INFO] Epoch: 2 , batch: 36 , training loss: 4.417562
[INFO] Epoch: 2 , batch: 37 , training loss: 4.346344
[INFO] Epoch: 2 , batch: 38 , training loss: 4.526977
[INFO] Epoch: 2 , batch: 39 , training loss: 4.319950
[INFO] Epoch: 2 , batch: 40 , training loss: 4.346394
[INFO] Epoch: 2 , batch: 41 , training loss: 4.652834
[INFO] Epoch: 2 , batch: 42 , training loss: 5.361883
[INFO] Epoch: 2 , batch: 43 , training loss: 5.114202
[INFO] Epoch: 2 , batch: 44 , training loss: 5.103430
[INFO] Epoch: 2 , batch: 45 , training loss: 5.427282
[INFO] Epoch: 2 , batch: 46 , training loss: 6.008996
[INFO] Epoch: 2 , batch: 47 , training loss: 4.901306
[INFO] Epoch: 2 , batch: 48 , training loss: 5.115887
[INFO] Epoch: 2 , batch: 49 , training loss: 4.963093
[INFO] Epoch: 2 , batch: 50 , training loss: 4.832597
[INFO] Epoch: 2 , batch: 51 , training loss: 4.721381
[INFO] Epoch: 2 , batch: 52 , training loss: 4.531025
[INFO] Epoch: 2 , batch: 53 , training loss: 4.725544
[INFO] Epoch: 2 , batch: 54 , training loss: 4.699660
[INFO] Epoch: 2 , batch: 55 , training loss: 4.752906
[INFO] Epoch: 2 , batch: 56 , training loss: 4.567104
[INFO] Epoch: 2 , batch: 57 , training loss: 4.424043
[INFO] Epoch: 2 , batch: 58 , training loss: 4.431358
[INFO] Epoch: 2 , batch: 59 , training loss: 4.611512
[INFO] Epoch: 2 , batch: 60 , training loss: 4.410476
[INFO] Epoch: 2 , batch: 61 , training loss: 4.546101
[INFO] Epoch: 2 , batch: 62 , training loss: 4.449448
[INFO] Epoch: 2 , batch: 63 , training loss: 4.633637
[INFO] Epoch: 2 , batch: 64 , training loss: 4.717245
[INFO] Epoch: 2 , batch: 65 , training loss: 4.510296
[INFO] Epoch: 2 , batch: 66 , training loss: 4.335924
[INFO] Epoch: 2 , batch: 67 , training loss: 4.327439
[INFO] Epoch: 2 , batch: 68 , training loss: 4.711555
[INFO] Epoch: 2 , batch: 69 , training loss: 4.429111
[INFO] Epoch: 2 , batch: 70 , training loss: 4.727516
[INFO] Epoch: 2 , batch: 71 , training loss: 4.524821
[INFO] Epoch: 2 , batch: 72 , training loss: 4.606335
[INFO] Epoch: 2 , batch: 73 , training loss: 4.517094
[INFO] Epoch: 2 , batch: 74 , training loss: 4.640748
[INFO] Epoch: 2 , batch: 75 , training loss: 4.339873
[INFO] Epoch: 2 , batch: 76 , training loss: 4.566441
[INFO] Epoch: 2 , batch: 77 , training loss: 4.413149
[INFO] Epoch: 2 , batch: 78 , training loss: 4.492246
[INFO] Epoch: 2 , batch: 79 , training loss: 4.324883
[INFO] Epoch: 2 , batch: 80 , training loss: 4.530291
[INFO] Epoch: 2 , batch: 81 , training loss: 4.551479
[INFO] Epoch: 2 , batch: 82 , training loss: 4.538517
[INFO] Epoch: 2 , batch: 83 , training loss: 4.649951
[INFO] Epoch: 2 , batch: 84 , training loss: 4.533419
[INFO] Epoch: 2 , batch: 85 , training loss: 4.611467
[INFO] Epoch: 2 , batch: 86 , training loss: 4.612658
[INFO] Epoch: 2 , batch: 87 , training loss: 4.605488
[INFO] Epoch: 2 , batch: 88 , training loss: 4.716812
[INFO] Epoch: 2 , batch: 89 , training loss: 4.494649
[INFO] Epoch: 2 , batch: 90 , training loss: 4.516618
[INFO] Epoch: 2 , batch: 91 , training loss: 4.461396
[INFO] Epoch: 2 , batch: 92 , training loss: 4.434083
[INFO] Epoch: 2 , batch: 93 , training loss: 4.512814
[INFO] Epoch: 2 , batch: 94 , training loss: 4.727866
[INFO] Epoch: 2 , batch: 95 , training loss: 4.517382
[INFO] Epoch: 2 , batch: 96 , training loss: 4.462281
[INFO] Epoch: 2 , batch: 97 , training loss: 4.468629
[INFO] Epoch: 2 , batch: 98 , training loss: 4.458258
[INFO] Epoch: 2 , batch: 99 , training loss: 4.452343
[INFO] Epoch: 2 , batch: 100 , training loss: 4.298806
[INFO] Epoch: 2 , batch: 101 , training loss: 4.395003
[INFO] Epoch: 2 , batch: 102 , training loss: 4.543271
[INFO] Epoch: 2 , batch: 103 , training loss: 4.241095
[INFO] Epoch: 2 , batch: 104 , training loss: 4.316947
[INFO] Epoch: 2 , batch: 105 , training loss: 4.572972
[INFO] Epoch: 2 , batch: 106 , training loss: 4.517450
[INFO] Epoch: 2 , batch: 107 , training loss: 4.445481
[INFO] Epoch: 2 , batch: 108 , training loss: 4.305662
[INFO] Epoch: 2 , batch: 109 , training loss: 4.204236
[INFO] Epoch: 2 , batch: 110 , training loss: 4.530169
[INFO] Epoch: 2 , batch: 111 , training loss: 4.518163
[INFO] Epoch: 2 , batch: 112 , training loss: 4.534446
[INFO] Epoch: 2 , batch: 113 , training loss: 4.429457
[INFO] Epoch: 2 , batch: 114 , training loss: 4.603783
[INFO] Epoch: 2 , batch: 115 , training loss: 4.506289
[INFO] Epoch: 2 , batch: 116 , training loss: 4.461863
[INFO] Epoch: 2 , batch: 117 , training loss: 4.738033
[INFO] Epoch: 2 , batch: 118 , training loss: 4.671109
[INFO] Epoch: 2 , batch: 119 , training loss: 4.757586
[INFO] Epoch: 2 , batch: 120 , training loss: 4.700517
[INFO] Epoch: 2 , batch: 121 , training loss: 4.485249
[INFO] Epoch: 2 , batch: 122 , training loss: 4.454798
[INFO] Epoch: 2 , batch: 123 , training loss: 4.618603
[INFO] Epoch: 2 , batch: 124 , training loss: 4.767062
[INFO] Epoch: 2 , batch: 125 , training loss: 4.332355
[INFO] Epoch: 2 , batch: 126 , training loss: 4.369366
[INFO] Epoch: 2 , batch: 127 , training loss: 4.397282
[INFO] Epoch: 2 , batch: 128 , training loss: 4.629600
[INFO] Epoch: 2 , batch: 129 , training loss: 4.494057
[INFO] Epoch: 2 , batch: 130 , training loss: 4.576507
[INFO] Epoch: 2 , batch: 131 , training loss: 4.537716
[INFO] Epoch: 2 , batch: 132 , training loss: 4.558261
[INFO] Epoch: 2 , batch: 133 , training loss: 4.473316
[INFO] Epoch: 2 , batch: 134 , training loss: 4.292401
[INFO] Epoch: 2 , batch: 135 , training loss: 4.262485
[INFO] Epoch: 2 , batch: 136 , training loss: 4.576381
[INFO] Epoch: 2 , batch: 137 , training loss: 4.485066
[INFO] Epoch: 2 , batch: 138 , training loss: 4.676967
[INFO] Epoch: 2 , batch: 139 , training loss: 5.414825
[INFO] Epoch: 2 , batch: 140 , training loss: 5.359241
[INFO] Epoch: 2 , batch: 141 , training loss: 5.034066
[INFO] Epoch: 2 , batch: 142 , training loss: 4.572402
[INFO] Epoch: 2 , batch: 143 , training loss: 4.646529
[INFO] Epoch: 2 , batch: 144 , training loss: 4.427552
[INFO] Epoch: 2 , batch: 145 , training loss: 4.651331
[INFO] Epoch: 2 , batch: 146 , training loss: 4.802945
[INFO] Epoch: 2 , batch: 147 , training loss: 4.368369
[INFO] Epoch: 2 , batch: 148 , training loss: 4.354779
[INFO] Epoch: 2 , batch: 149 , training loss: 4.494516
[INFO] Epoch: 2 , batch: 150 , training loss: 4.746821
[INFO] Epoch: 2 , batch: 151 , training loss: 4.364798
[INFO] Epoch: 2 , batch: 152 , training loss: 4.435939
[INFO] Epoch: 2 , batch: 153 , training loss: 4.623766
[INFO] Epoch: 2 , batch: 154 , training loss: 4.669677
[INFO] Epoch: 2 , batch: 155 , training loss: 4.969408
[INFO] Epoch: 2 , batch: 156 , training loss: 4.512570
[INFO] Epoch: 2 , batch: 157 , training loss: 4.559461
[INFO] Epoch: 2 , batch: 158 , training loss: 5.038145
[INFO] Epoch: 2 , batch: 159 , training loss: 4.962625
[INFO] Epoch: 2 , batch: 160 , training loss: 5.598399
[INFO] Epoch: 2 , batch: 161 , training loss: 5.162143
[INFO] Epoch: 2 , batch: 162 , training loss: 4.950296
[INFO] Epoch: 2 , batch: 163 , training loss: 5.049685
[INFO] Epoch: 2 , batch: 164 , training loss: 4.902230
[INFO] Epoch: 2 , batch: 165 , training loss: 4.825005
[INFO] Epoch: 2 , batch: 166 , training loss: 5.423745
[INFO] Epoch: 2 , batch: 167 , training loss: 5.999302
[INFO] Epoch: 2 , batch: 168 , training loss: 5.737961
[INFO] Epoch: 2 , batch: 169 , training loss: 5.489681
[INFO] Epoch: 2 , batch: 170 , training loss: 5.440451
[INFO] Epoch: 2 , batch: 171 , training loss: 5.237130
[INFO] Epoch: 2 , batch: 172 , training loss: 5.425391
[INFO] Epoch: 2 , batch: 173 , training loss: 5.628685
[INFO] Epoch: 2 , batch: 174 , training loss: 5.745767
[INFO] Epoch: 2 , batch: 175 , training loss: 5.669060
[INFO] Epoch: 2 , batch: 176 , training loss: 5.498766
[INFO] Epoch: 2 , batch: 177 , training loss: 5.190361
[INFO] Epoch: 2 , batch: 178 , training loss: 5.108689
[INFO] Epoch: 2 , batch: 179 , training loss: 5.063063
[INFO] Epoch: 2 , batch: 180 , training loss: 5.066984
[INFO] Epoch: 2 , batch: 181 , training loss: 5.266511
[INFO] Epoch: 2 , batch: 182 , training loss: 5.173254
[INFO] Epoch: 2 , batch: 183 , training loss: 5.072238
[INFO] Epoch: 2 , batch: 184 , training loss: 4.931437
[INFO] Epoch: 2 , batch: 185 , training loss: 4.903219
[INFO] Epoch: 2 , batch: 186 , training loss: 5.035881
[INFO] Epoch: 2 , batch: 187 , training loss: 5.145753
[INFO] Epoch: 2 , batch: 188 , training loss: 5.103112
[INFO] Epoch: 2 , batch: 189 , training loss: 4.998374
[INFO] Epoch: 2 , batch: 190 , training loss: 4.948519
[INFO] Epoch: 2 , batch: 191 , training loss: 5.102328
[INFO] Epoch: 2 , batch: 192 , training loss: 4.824834
[INFO] Epoch: 2 , batch: 193 , training loss: 4.980399
[INFO] Epoch: 2 , batch: 194 , training loss: 4.901054
[INFO] Epoch: 2 , batch: 195 , training loss: 5.032100
[INFO] Epoch: 2 , batch: 196 , training loss: 4.750868
[INFO] Epoch: 2 , batch: 197 , training loss: 4.953474
[INFO] Epoch: 2 , batch: 198 , training loss: 4.734774
[INFO] Epoch: 2 , batch: 199 , training loss: 4.818823
[INFO] Epoch: 2 , batch: 200 , training loss: 4.734501
[INFO] Epoch: 2 , batch: 201 , training loss: 4.686941
[INFO] Epoch: 2 , batch: 202 , training loss: 4.604830
[INFO] Epoch: 2 , batch: 203 , training loss: 4.602385
[INFO] Epoch: 2 , batch: 204 , training loss: 4.788918
[INFO] Epoch: 2 , batch: 205 , training loss: 4.359765
[INFO] Epoch: 2 , batch: 206 , training loss: 4.219511
[INFO] Epoch: 2 , batch: 207 , training loss: 4.273813
[INFO] Epoch: 2 , batch: 208 , training loss: 4.737907
[INFO] Epoch: 2 , batch: 209 , training loss: 4.613496
[INFO] Epoch: 2 , batch: 210 , training loss: 4.665303
[INFO] Epoch: 2 , batch: 211 , training loss: 4.589996
[INFO] Epoch: 2 , batch: 212 , training loss: 4.681738
[INFO] Epoch: 2 , batch: 213 , training loss: 4.698039
[INFO] Epoch: 2 , batch: 214 , training loss: 4.692416
[INFO] Epoch: 2 , batch: 215 , training loss: 4.923491
[INFO] Epoch: 2 , batch: 216 , training loss: 4.693370
[INFO] Epoch: 2 , batch: 217 , training loss: 4.579027
[INFO] Epoch: 2 , batch: 218 , training loss: 4.600850
[INFO] Epoch: 2 , batch: 219 , training loss: 4.674759
[INFO] Epoch: 2 , batch: 220 , training loss: 4.467808
[INFO] Epoch: 2 , batch: 221 , training loss: 4.510345
[INFO] Epoch: 2 , batch: 222 , training loss: 4.663900
[INFO] Epoch: 2 , batch: 223 , training loss: 4.733878
[INFO] Epoch: 2 , batch: 224 , training loss: 4.783453
[INFO] Epoch: 2 , batch: 225 , training loss: 4.621823
[INFO] Epoch: 2 , batch: 226 , training loss: 4.770821
[INFO] Epoch: 2 , batch: 227 , training loss: 4.731443
[INFO] Epoch: 2 , batch: 228 , training loss: 4.798105
[INFO] Epoch: 2 , batch: 229 , training loss: 4.670614
[INFO] Epoch: 2 , batch: 230 , training loss: 4.487649
[INFO] Epoch: 2 , batch: 231 , training loss: 4.303191
[INFO] Epoch: 2 , batch: 232 , training loss: 4.436334
[INFO] Epoch: 2 , batch: 233 , training loss: 4.516985
[INFO] Epoch: 2 , batch: 234 , training loss: 4.186499
[INFO] Epoch: 2 , batch: 235 , training loss: 4.312171
[INFO] Epoch: 2 , batch: 236 , training loss: 4.539364
[INFO] Epoch: 2 , batch: 237 , training loss: 4.627110
[INFO] Epoch: 2 , batch: 238 , training loss: 4.367322
[INFO] Epoch: 2 , batch: 239 , training loss: 4.452841
[INFO] Epoch: 2 , batch: 240 , training loss: 4.516441
[INFO] Epoch: 2 , batch: 241 , training loss: 4.303292
[INFO] Epoch: 2 , batch: 242 , training loss: 4.296553
[INFO] Epoch: 2 , batch: 243 , training loss: 4.655009
[INFO] Epoch: 2 , batch: 244 , training loss: 4.545981
[INFO] Epoch: 2 , batch: 245 , training loss: 4.561027
[INFO] Epoch: 2 , batch: 246 , training loss: 4.188695
[INFO] Epoch: 2 , batch: 247 , training loss: 4.341080
[INFO] Epoch: 2 , batch: 248 , training loss: 4.472730
[INFO] Epoch: 2 , batch: 249 , training loss: 4.393246
[INFO] Epoch: 2 , batch: 250 , training loss: 4.193959
[INFO] Epoch: 2 , batch: 251 , training loss: 4.694670
[INFO] Epoch: 2 , batch: 252 , training loss: 4.380925
[INFO] Epoch: 2 , batch: 253 , training loss: 4.373410
[INFO] Epoch: 2 , batch: 254 , training loss: 4.701933
[INFO] Epoch: 2 , batch: 255 , training loss: 4.700950
[INFO] Epoch: 2 , batch: 256 , training loss: 4.585503
[INFO] Epoch: 2 , batch: 257 , training loss: 4.772567
[INFO] Epoch: 2 , batch: 258 , training loss: 4.859403
[INFO] Epoch: 2 , batch: 259 , training loss: 4.877052
[INFO] Epoch: 2 , batch: 260 , training loss: 4.510683
[INFO] Epoch: 2 , batch: 261 , training loss: 4.724342
[INFO] Epoch: 2 , batch: 262 , training loss: 4.970023
[INFO] Epoch: 2 , batch: 263 , training loss: 5.058536
[INFO] Epoch: 2 , batch: 264 , training loss: 4.346869
[INFO] Epoch: 2 , batch: 265 , training loss: 4.520597
[INFO] Epoch: 2 , batch: 266 , training loss: 5.061383
[INFO] Epoch: 2 , batch: 267 , training loss: 4.703711
[INFO] Epoch: 2 , batch: 268 , training loss: 4.570363
[INFO] Epoch: 2 , batch: 269 , training loss: 4.639967
[INFO] Epoch: 2 , batch: 270 , training loss: 4.622667
[INFO] Epoch: 2 , batch: 271 , training loss: 4.666731
[INFO] Epoch: 2 , batch: 272 , training loss: 4.630731
[INFO] Epoch: 2 , batch: 273 , training loss: 4.620075
[INFO] Epoch: 2 , batch: 274 , training loss: 4.748408
[INFO] Epoch: 2 , batch: 275 , training loss: 4.666584
[INFO] Epoch: 2 , batch: 276 , training loss: 4.708076
[INFO] Epoch: 2 , batch: 277 , training loss: 4.834046
[INFO] Epoch: 2 , batch: 278 , training loss: 4.392083
[INFO] Epoch: 2 , batch: 279 , training loss: 4.451595
[INFO] Epoch: 2 , batch: 280 , training loss: 4.368622
[INFO] Epoch: 2 , batch: 281 , training loss: 4.504704
[INFO] Epoch: 2 , batch: 282 , training loss: 4.390054
[INFO] Epoch: 2 , batch: 283 , training loss: 4.497898
[INFO] Epoch: 2 , batch: 284 , training loss: 4.508849
[INFO] Epoch: 2 , batch: 285 , training loss: 4.548064
[INFO] Epoch: 2 , batch: 286 , training loss: 4.496214
[INFO] Epoch: 2 , batch: 287 , training loss: 4.348885
[INFO] Epoch: 2 , batch: 288 , training loss: 4.432038
[INFO] Epoch: 2 , batch: 289 , training loss: 4.427429
[INFO] Epoch: 2 , batch: 290 , training loss: 4.252638
[INFO] Epoch: 2 , batch: 291 , training loss: 4.197154
[INFO] Epoch: 2 , batch: 292 , training loss: 4.302288
[INFO] Epoch: 2 , batch: 293 , training loss: 4.280301
[INFO] Epoch: 2 , batch: 294 , training loss: 4.960388
[INFO] Epoch: 2 , batch: 295 , training loss: 4.722631
[INFO] Epoch: 2 , batch: 296 , training loss: 4.624663
[INFO] Epoch: 2 , batch: 297 , training loss: 4.562545
[INFO] Epoch: 2 , batch: 298 , training loss: 4.428978
[INFO] Epoch: 2 , batch: 299 , training loss: 4.392623
[INFO] Epoch: 2 , batch: 300 , training loss: 4.386354
[INFO] Epoch: 2 , batch: 301 , training loss: 4.339245
[INFO] Epoch: 2 , batch: 302 , training loss: 4.515924
[INFO] Epoch: 2 , batch: 303 , training loss: 4.516422
[INFO] Epoch: 2 , batch: 304 , training loss: 4.743563
[INFO] Epoch: 2 , batch: 305 , training loss: 4.442145
[INFO] Epoch: 2 , batch: 306 , training loss: 4.555026
[INFO] Epoch: 2 , batch: 307 , training loss: 4.534841
[INFO] Epoch: 2 , batch: 308 , training loss: 4.455164
[INFO] Epoch: 2 , batch: 309 , training loss: 4.469963
[INFO] Epoch: 2 , batch: 310 , training loss: 4.253382
[INFO] Epoch: 2 , batch: 311 , training loss: 4.291072
[INFO] Epoch: 2 , batch: 312 , training loss: 4.156247
[INFO] Epoch: 2 , batch: 313 , training loss: 4.361328
[INFO] Epoch: 2 , batch: 314 , training loss: 4.399057
[INFO] Epoch: 2 , batch: 315 , training loss: 4.448978
[INFO] Epoch: 2 , batch: 316 , training loss: 4.837132
[INFO] Epoch: 2 , batch: 317 , training loss: 5.476345
[INFO] Epoch: 2 , batch: 318 , training loss: 5.596251
[INFO] Epoch: 2 , batch: 319 , training loss: 5.004231
[INFO] Epoch: 2 , batch: 320 , training loss: 4.492445
[INFO] Epoch: 2 , batch: 321 , training loss: 4.252354
[INFO] Epoch: 2 , batch: 322 , training loss: 4.395373
[INFO] Epoch: 2 , batch: 323 , training loss: 4.389497
[INFO] Epoch: 2 , batch: 324 , training loss: 4.401555
[INFO] Epoch: 2 , batch: 325 , training loss: 4.600148
[INFO] Epoch: 2 , batch: 326 , training loss: 4.650825
[INFO] Epoch: 2 , batch: 327 , training loss: 4.526748
[INFO] Epoch: 2 , batch: 328 , training loss: 4.559391
[INFO] Epoch: 2 , batch: 329 , training loss: 4.404376
[INFO] Epoch: 2 , batch: 330 , training loss: 4.366409
[INFO] Epoch: 2 , batch: 331 , training loss: 4.561702
[INFO] Epoch: 2 , batch: 332 , training loss: 4.313891
[INFO] Epoch: 2 , batch: 333 , training loss: 4.281641
[INFO] Epoch: 2 , batch: 334 , training loss: 4.424785
[INFO] Epoch: 2 , batch: 335 , training loss: 4.543162
[INFO] Epoch: 2 , batch: 336 , training loss: 4.480429
[INFO] Epoch: 2 , batch: 337 , training loss: 4.597315
[INFO] Epoch: 2 , batch: 338 , training loss: 4.750431
[INFO] Epoch: 2 , batch: 339 , training loss: 4.582921
[INFO] Epoch: 2 , batch: 340 , training loss: 4.776122
[INFO] Epoch: 2 , batch: 341 , training loss: 4.483843
[INFO] Epoch: 2 , batch: 342 , training loss: 4.354993
[INFO] Epoch: 2 , batch: 343 , training loss: 4.452823
[INFO] Epoch: 2 , batch: 344 , training loss: 4.281694
[INFO] Epoch: 2 , batch: 345 , training loss: 4.417041
[INFO] Epoch: 2 , batch: 346 , training loss: 4.519162
[INFO] Epoch: 2 , batch: 347 , training loss: 4.365594
[INFO] Epoch: 2 , batch: 348 , training loss: 4.622567
[INFO] Epoch: 2 , batch: 349 , training loss: 4.673326
[INFO] Epoch: 2 , batch: 350 , training loss: 4.382135
[INFO] Epoch: 2 , batch: 351 , training loss: 4.429910
[INFO] Epoch: 2 , batch: 352 , training loss: 4.445076
[INFO] Epoch: 2 , batch: 353 , training loss: 4.425700
[INFO] Epoch: 2 , batch: 354 , training loss: 4.520071
[INFO] Epoch: 2 , batch: 355 , training loss: 4.570485
[INFO] Epoch: 2 , batch: 356 , training loss: 4.442246
[INFO] Epoch: 2 , batch: 357 , training loss: 4.515206
[INFO] Epoch: 2 , batch: 358 , training loss: 4.497048
[INFO] Epoch: 2 , batch: 359 , training loss: 4.456029
[INFO] Epoch: 2 , batch: 360 , training loss: 4.469143
[INFO] Epoch: 2 , batch: 361 , training loss: 4.448403
[INFO] Epoch: 2 , batch: 362 , training loss: 4.549160
[INFO] Epoch: 2 , batch: 363 , training loss: 4.416759
[INFO] Epoch: 2 , batch: 364 , training loss: 4.456659
[INFO] Epoch: 2 , batch: 365 , training loss: 4.358411
[INFO] Epoch: 2 , batch: 366 , training loss: 4.522168
[INFO] Epoch: 2 , batch: 367 , training loss: 4.619728
[INFO] Epoch: 2 , batch: 368 , training loss: 5.276772
[INFO] Epoch: 2 , batch: 369 , training loss: 4.769782
[INFO] Epoch: 2 , batch: 370 , training loss: 4.528384
[INFO] Epoch: 2 , batch: 371 , training loss: 5.189610
[INFO] Epoch: 2 , batch: 372 , training loss: 5.453111
[INFO] Epoch: 2 , batch: 373 , training loss: 5.429154
[INFO] Epoch: 2 , batch: 374 , training loss: 5.425972
[INFO] Epoch: 2 , batch: 375 , training loss: 5.429069
[INFO] Epoch: 2 , batch: 376 , training loss: 5.316462
[INFO] Epoch: 2 , batch: 377 , training loss: 4.945425
[INFO] Epoch: 2 , batch: 378 , training loss: 5.049741
[INFO] Epoch: 2 , batch: 379 , training loss: 5.108812
[INFO] Epoch: 2 , batch: 380 , training loss: 5.195458
[INFO] Epoch: 2 , batch: 381 , training loss: 4.995700
[INFO] Epoch: 2 , batch: 382 , training loss: 5.121123
[INFO] Epoch: 2 , batch: 383 , training loss: 5.294207
[INFO] Epoch: 2 , batch: 384 , training loss: 5.479812
[INFO] Epoch: 2 , batch: 385 , training loss: 5.259521
[INFO] Epoch: 2 , batch: 386 , training loss: 5.381579
[INFO] Epoch: 2 , batch: 387 , training loss: 5.319150
[INFO] Epoch: 2 , batch: 388 , training loss: 5.056329
[INFO] Epoch: 2 , batch: 389 , training loss: 4.838999
[INFO] Epoch: 2 , batch: 390 , training loss: 4.787035
[INFO] Epoch: 2 , batch: 391 , training loss: 4.828082
[INFO] Epoch: 2 , batch: 392 , training loss: 5.161311
[INFO] Epoch: 2 , batch: 393 , training loss: 5.077018
[INFO] Epoch: 2 , batch: 394 , training loss: 5.111868
[INFO] Epoch: 2 , batch: 395 , training loss: 4.918335
[INFO] Epoch: 2 , batch: 396 , training loss: 4.728831
[INFO] Epoch: 2 , batch: 397 , training loss: 4.902161
[INFO] Epoch: 2 , batch: 398 , training loss: 4.721225
[INFO] Epoch: 2 , batch: 399 , training loss: 4.784523
[INFO] Epoch: 2 , batch: 400 , training loss: 4.724933
[INFO] Epoch: 2 , batch: 401 , training loss: 5.129582
[INFO] Epoch: 2 , batch: 402 , training loss: 4.914231
[INFO] Epoch: 2 , batch: 403 , training loss: 4.784830
[INFO] Epoch: 2 , batch: 404 , training loss: 4.899568
[INFO] Epoch: 2 , batch: 405 , training loss: 4.994840
[INFO] Epoch: 2 , batch: 406 , training loss: 4.866978
[INFO] Epoch: 2 , batch: 407 , training loss: 4.913043
[INFO] Epoch: 2 , batch: 408 , training loss: 4.853906
[INFO] Epoch: 2 , batch: 409 , training loss: 4.856462
[INFO] Epoch: 2 , batch: 410 , training loss: 4.867687
[INFO] Epoch: 2 , batch: 411 , training loss: 5.112621
[INFO] Epoch: 2 , batch: 412 , training loss: 4.935081
[INFO] Epoch: 2 , batch: 413 , training loss: 4.787400
[INFO] Epoch: 2 , batch: 414 , training loss: 4.823371
[INFO] Epoch: 2 , batch: 415 , training loss: 4.841288
[INFO] Epoch: 2 , batch: 416 , training loss: 4.926638
[INFO] Epoch: 2 , batch: 417 , training loss: 4.833028
[INFO] Epoch: 2 , batch: 418 , training loss: 4.852522
[INFO] Epoch: 2 , batch: 419 , training loss: 4.802154
[INFO] Epoch: 2 , batch: 420 , training loss: 4.769059
[INFO] Epoch: 2 , batch: 421 , training loss: 4.752384
[INFO] Epoch: 2 , batch: 422 , training loss: 4.663534
[INFO] Epoch: 2 , batch: 423 , training loss: 4.912366
[INFO] Epoch: 2 , batch: 424 , training loss: 5.047773
[INFO] Epoch: 2 , batch: 425 , training loss: 4.942193
[INFO] Epoch: 2 , batch: 426 , training loss: 4.609057
[INFO] Epoch: 2 , batch: 427 , training loss: 4.877385
[INFO] Epoch: 2 , batch: 428 , training loss: 4.822894
[INFO] Epoch: 2 , batch: 429 , training loss: 4.565835
[INFO] Epoch: 2 , batch: 430 , training loss: 4.880221
[INFO] Epoch: 2 , batch: 431 , training loss: 4.436793
[INFO] Epoch: 2 , batch: 432 , training loss: 4.536558
[INFO] Epoch: 2 , batch: 433 , training loss: 4.547626
[INFO] Epoch: 2 , batch: 434 , training loss: 4.422905
[INFO] Epoch: 2 , batch: 435 , training loss: 4.766799
[INFO] Epoch: 2 , batch: 436 , training loss: 4.900301
[INFO] Epoch: 2 , batch: 437 , training loss: 4.697224
[INFO] Epoch: 2 , batch: 438 , training loss: 4.420622
[INFO] Epoch: 2 , batch: 439 , training loss: 4.681109
[INFO] Epoch: 2 , batch: 440 , training loss: 4.822455
[INFO] Epoch: 2 , batch: 441 , training loss: 4.846315
[INFO] Epoch: 2 , batch: 442 , training loss: 4.689813
[INFO] Epoch: 2 , batch: 443 , training loss: 4.899952
[INFO] Epoch: 2 , batch: 444 , training loss: 4.527800
[INFO] Epoch: 2 , batch: 445 , training loss: 4.351150
[INFO] Epoch: 2 , batch: 446 , training loss: 4.220780
[INFO] Epoch: 2 , batch: 447 , training loss: 4.507082
[INFO] Epoch: 2 , batch: 448 , training loss: 4.695108
[INFO] Epoch: 2 , batch: 449 , training loss: 5.100156
[INFO] Epoch: 2 , batch: 450 , training loss: 5.151790
[INFO] Epoch: 2 , batch: 451 , training loss: 5.011599
[INFO] Epoch: 2 , batch: 452 , training loss: 4.748647
[INFO] Epoch: 2 , batch: 453 , training loss: 4.535565
[INFO] Epoch: 2 , batch: 454 , training loss: 4.693211
[INFO] Epoch: 2 , batch: 455 , training loss: 4.733406
[INFO] Epoch: 2 , batch: 456 , training loss: 4.700624
[INFO] Epoch: 2 , batch: 457 , training loss: 4.820567
[INFO] Epoch: 2 , batch: 458 , training loss: 4.520479
[INFO] Epoch: 2 , batch: 459 , training loss: 4.510820
[INFO] Epoch: 2 , batch: 460 , training loss: 4.647234
[INFO] Epoch: 2 , batch: 461 , training loss: 4.698222
[INFO] Epoch: 2 , batch: 462 , training loss: 4.729693
[INFO] Epoch: 2 , batch: 463 , training loss: 4.555922
[INFO] Epoch: 2 , batch: 464 , training loss: 4.769526
[INFO] Epoch: 2 , batch: 465 , training loss: 4.701026
[INFO] Epoch: 2 , batch: 466 , training loss: 4.773221
[INFO] Epoch: 2 , batch: 467 , training loss: 4.855498
[INFO] Epoch: 2 , batch: 468 , training loss: 4.773376
[INFO] Epoch: 2 , batch: 469 , training loss: 4.755851
[INFO] Epoch: 2 , batch: 470 , training loss: 4.519830
[INFO] Epoch: 2 , batch: 471 , training loss: 4.652501
[INFO] Epoch: 2 , batch: 472 , training loss: 4.703272
[INFO] Epoch: 2 , batch: 473 , training loss: 4.628053
[INFO] Epoch: 2 , batch: 474 , training loss: 4.467584
[INFO] Epoch: 2 , batch: 475 , training loss: 4.302698
[INFO] Epoch: 2 , batch: 476 , training loss: 4.672230
[INFO] Epoch: 2 , batch: 477 , training loss: 4.777277
[INFO] Epoch: 2 , batch: 478 , training loss: 4.912763
[INFO] Epoch: 2 , batch: 479 , training loss: 4.850010
[INFO] Epoch: 2 , batch: 480 , training loss: 4.987673
[INFO] Epoch: 2 , batch: 481 , training loss: 4.766021
[INFO] Epoch: 2 , batch: 482 , training loss: 4.919300
[INFO] Epoch: 2 , batch: 483 , training loss: 4.746264
[INFO] Epoch: 2 , batch: 484 , training loss: 4.570483
[INFO] Epoch: 2 , batch: 485 , training loss: 4.709200
[INFO] Epoch: 2 , batch: 486 , training loss: 4.565051
[INFO] Epoch: 2 , batch: 487 , training loss: 4.546152
[INFO] Epoch: 2 , batch: 488 , training loss: 4.776340
[INFO] Epoch: 2 , batch: 489 , training loss: 4.605732
[INFO] Epoch: 2 , batch: 490 , training loss: 4.729823
[INFO] Epoch: 2 , batch: 491 , training loss: 4.706639
[INFO] Epoch: 2 , batch: 492 , training loss: 4.540097
[INFO] Epoch: 2 , batch: 493 , training loss: 4.742971
[INFO] Epoch: 2 , batch: 494 , training loss: 4.662119
[INFO] Epoch: 2 , batch: 495 , training loss: 4.781403
[INFO] Epoch: 2 , batch: 496 , training loss: 4.655958
[INFO] Epoch: 2 , batch: 497 , training loss: 4.658981
[INFO] Epoch: 2 , batch: 498 , training loss: 4.735317
[INFO] Epoch: 2 , batch: 499 , training loss: 4.779114
[INFO] Epoch: 2 , batch: 500 , training loss: 5.099754
[INFO] Epoch: 2 , batch: 501 , training loss: 5.561804
[INFO] Epoch: 2 , batch: 502 , training loss: 5.681600
[INFO] Epoch: 2 , batch: 503 , training loss: 5.345571
[INFO] Epoch: 2 , batch: 504 , training loss: 5.369803
[INFO] Epoch: 2 , batch: 505 , training loss: 5.251884
[INFO] Epoch: 2 , batch: 506 , training loss: 5.209167
[INFO] Epoch: 2 , batch: 507 , training loss: 5.218857
[INFO] Epoch: 2 , batch: 508 , training loss: 5.236219
[INFO] Epoch: 2 , batch: 509 , training loss: 4.907890
[INFO] Epoch: 2 , batch: 510 , training loss: 4.955752
[INFO] Epoch: 2 , batch: 511 , training loss: 4.873888
[INFO] Epoch: 2 , batch: 512 , training loss: 4.930870
[INFO] Epoch: 2 , batch: 513 , training loss: 5.195460
[INFO] Epoch: 2 , batch: 514 , training loss: 4.874137
[INFO] Epoch: 2 , batch: 515 , training loss: 5.141619
[INFO] Epoch: 2 , batch: 516 , training loss: 4.917630
[INFO] Epoch: 2 , batch: 517 , training loss: 4.902287
[INFO] Epoch: 2 , batch: 518 , training loss: 4.853087
[INFO] Epoch: 2 , batch: 519 , training loss: 4.747284
[INFO] Epoch: 2 , batch: 520 , training loss: 4.936546
[INFO] Epoch: 2 , batch: 521 , training loss: 4.972440
[INFO] Epoch: 2 , batch: 522 , training loss: 5.032731
[INFO] Epoch: 2 , batch: 523 , training loss: 4.922651
[INFO] Epoch: 2 , batch: 524 , training loss: 5.134592
[INFO] Epoch: 2 , batch: 525 , training loss: 5.100153
[INFO] Epoch: 2 , batch: 526 , training loss: 4.861260
[INFO] Epoch: 2 , batch: 527 , training loss: 4.863006
[INFO] Epoch: 2 , batch: 528 , training loss: 4.982480
[INFO] Epoch: 2 , batch: 529 , training loss: 4.900129
[INFO] Epoch: 2 , batch: 530 , training loss: 4.735171
[INFO] Epoch: 2 , batch: 531 , training loss: 4.912147
[INFO] Epoch: 2 , batch: 532 , training loss: 4.725501
[INFO] Epoch: 2 , batch: 533 , training loss: 4.894461
[INFO] Epoch: 2 , batch: 534 , training loss: 4.920754
[INFO] Epoch: 2 , batch: 535 , training loss: 4.923462
[INFO] Epoch: 2 , batch: 536 , training loss: 4.810335
[INFO] Epoch: 2 , batch: 537 , training loss: 4.753453
[INFO] Epoch: 2 , batch: 538 , training loss: 4.831935
[INFO] Epoch: 2 , batch: 539 , training loss: 5.011601
[INFO] Epoch: 2 , batch: 540 , training loss: 5.691737
[INFO] Epoch: 2 , batch: 541 , training loss: 5.624672
[INFO] Epoch: 2 , batch: 542 , training loss: 5.320509
[INFO] Epoch: 3 , batch: 0 , training loss: 5.246949
[INFO] Epoch: 3 , batch: 1 , training loss: 4.900979
[INFO] Epoch: 3 , batch: 2 , training loss: 4.681461
[INFO] Epoch: 3 , batch: 3 , training loss: 4.508466
[INFO] Epoch: 3 , batch: 4 , training loss: 4.962070
[INFO] Epoch: 3 , batch: 5 , training loss: 4.553818
[INFO] Epoch: 3 , batch: 6 , training loss: 5.219966
[INFO] Epoch: 3 , batch: 7 , training loss: 4.758005
[INFO] Epoch: 3 , batch: 8 , training loss: 4.495306
[INFO] Epoch: 3 , batch: 9 , training loss: 4.558426
[INFO] Epoch: 3 , batch: 10 , training loss: 4.445944
[INFO] Epoch: 3 , batch: 11 , training loss: 4.296001
[INFO] Epoch: 3 , batch: 12 , training loss: 4.378373
[INFO] Epoch: 3 , batch: 13 , training loss: 4.272735
[INFO] Epoch: 3 , batch: 14 , training loss: 4.033673
[INFO] Epoch: 3 , batch: 15 , training loss: 4.349154
[INFO] Epoch: 3 , batch: 16 , training loss: 4.235374
[INFO] Epoch: 3 , batch: 17 , training loss: 4.389686
[INFO] Epoch: 3 , batch: 18 , training loss: 4.263519
[INFO] Epoch: 3 , batch: 19 , training loss: 4.100891
[INFO] Epoch: 3 , batch: 20 , training loss: 4.055295
[INFO] Epoch: 3 , batch: 21 , training loss: 4.142573
[INFO] Epoch: 3 , batch: 22 , training loss: 4.245557
[INFO] Epoch: 3 , batch: 23 , training loss: 4.396317
[INFO] Epoch: 3 , batch: 24 , training loss: 4.187323
[INFO] Epoch: 3 , batch: 25 , training loss: 4.374068
[INFO] Epoch: 3 , batch: 26 , training loss: 4.180743
[INFO] Epoch: 3 , batch: 27 , training loss: 4.176212
[INFO] Epoch: 3 , batch: 28 , training loss: 4.423994
[INFO] Epoch: 3 , batch: 29 , training loss: 4.178336
[INFO] Epoch: 3 , batch: 30 , training loss: 4.147208
[INFO] Epoch: 3 , batch: 31 , training loss: 4.234231
[INFO] Epoch: 3 , batch: 32 , training loss: 4.270001
[INFO] Epoch: 3 , batch: 33 , training loss: 4.388221
[INFO] Epoch: 3 , batch: 34 , training loss: 4.402852
[INFO] Epoch: 3 , batch: 35 , training loss: 4.313033
[INFO] Epoch: 3 , batch: 36 , training loss: 4.238179
[INFO] Epoch: 3 , batch: 37 , training loss: 4.167677
[INFO] Epoch: 3 , batch: 38 , training loss: 4.338184
[INFO] Epoch: 3 , batch: 39 , training loss: 4.124753
[INFO] Epoch: 3 , batch: 40 , training loss: 4.187707
[INFO] Epoch: 3 , batch: 41 , training loss: 4.430307
[INFO] Epoch: 3 , batch: 42 , training loss: 5.147176
[INFO] Epoch: 3 , batch: 43 , training loss: 4.829236
[INFO] Epoch: 3 , batch: 44 , training loss: 4.882933
[INFO] Epoch: 3 , batch: 45 , training loss: 5.144024
[INFO] Epoch: 3 , batch: 46 , training loss: 5.589787
[INFO] Epoch: 3 , batch: 47 , training loss: 4.616476
[INFO] Epoch: 3 , batch: 48 , training loss: 4.816254
[INFO] Epoch: 3 , batch: 49 , training loss: 4.724995
[INFO] Epoch: 3 , batch: 50 , training loss: 4.542354
[INFO] Epoch: 3 , batch: 51 , training loss: 4.487507
[INFO] Epoch: 3 , batch: 52 , training loss: 4.289880
[INFO] Epoch: 3 , batch: 53 , training loss: 4.478792
[INFO] Epoch: 3 , batch: 54 , training loss: 4.458627
[INFO] Epoch: 3 , batch: 55 , training loss: 4.522270
[INFO] Epoch: 3 , batch: 56 , training loss: 4.321514
[INFO] Epoch: 3 , batch: 57 , training loss: 4.191121
[INFO] Epoch: 3 , batch: 58 , training loss: 4.230683
[INFO] Epoch: 3 , batch: 59 , training loss: 4.374943
[INFO] Epoch: 3 , batch: 60 , training loss: 4.185056
[INFO] Epoch: 3 , batch: 61 , training loss: 4.303413
[INFO] Epoch: 3 , batch: 62 , training loss: 4.221973
[INFO] Epoch: 3 , batch: 63 , training loss: 4.403630
[INFO] Epoch: 3 , batch: 64 , training loss: 4.533176
[INFO] Epoch: 3 , batch: 65 , training loss: 4.273382
[INFO] Epoch: 3 , batch: 66 , training loss: 4.097842
[INFO] Epoch: 3 , batch: 67 , training loss: 4.119264
[INFO] Epoch: 3 , batch: 68 , training loss: 4.476223
[INFO] Epoch: 3 , batch: 69 , training loss: 4.198677
[INFO] Epoch: 3 , batch: 70 , training loss: 4.490387
[INFO] Epoch: 3 , batch: 71 , training loss: 4.298599
[INFO] Epoch: 3 , batch: 72 , training loss: 4.363144
[INFO] Epoch: 3 , batch: 73 , training loss: 4.304924
[INFO] Epoch: 3 , batch: 74 , training loss: 4.462479
[INFO] Epoch: 3 , batch: 75 , training loss: 4.176988
[INFO] Epoch: 3 , batch: 76 , training loss: 4.361173
[INFO] Epoch: 3 , batch: 77 , training loss: 4.240719
[INFO] Epoch: 3 , batch: 78 , training loss: 4.319636
[INFO] Epoch: 3 , batch: 79 , training loss: 4.150480
[INFO] Epoch: 3 , batch: 80 , training loss: 4.349396
[INFO] Epoch: 3 , batch: 81 , training loss: 4.353961
[INFO] Epoch: 3 , batch: 82 , training loss: 4.354208
[INFO] Epoch: 3 , batch: 83 , training loss: 4.439082
[INFO] Epoch: 3 , batch: 84 , training loss: 4.338422
[INFO] Epoch: 3 , batch: 85 , training loss: 4.426574
[INFO] Epoch: 3 , batch: 86 , training loss: 4.430385
[INFO] Epoch: 3 , batch: 87 , training loss: 4.401856
[INFO] Epoch: 3 , batch: 88 , training loss: 4.540318
[INFO] Epoch: 3 , batch: 89 , training loss: 4.308355
[INFO] Epoch: 3 , batch: 90 , training loss: 4.343751
[INFO] Epoch: 3 , batch: 91 , training loss: 4.285560
[INFO] Epoch: 3 , batch: 92 , training loss: 4.284248
[INFO] Epoch: 3 , batch: 93 , training loss: 4.364713
[INFO] Epoch: 3 , batch: 94 , training loss: 4.580316
[INFO] Epoch: 3 , batch: 95 , training loss: 4.344578
[INFO] Epoch: 3 , batch: 96 , training loss: 4.295700
[INFO] Epoch: 3 , batch: 97 , training loss: 4.282448
[INFO] Epoch: 3 , batch: 98 , training loss: 4.258117
[INFO] Epoch: 3 , batch: 99 , training loss: 4.298343
[INFO] Epoch: 3 , batch: 100 , training loss: 4.142765
[INFO] Epoch: 3 , batch: 101 , training loss: 4.243540
[INFO] Epoch: 3 , batch: 102 , training loss: 4.397032
[INFO] Epoch: 3 , batch: 103 , training loss: 4.106032
[INFO] Epoch: 3 , batch: 104 , training loss: 4.135960
[INFO] Epoch: 3 , batch: 105 , training loss: 4.432423
[INFO] Epoch: 3 , batch: 106 , training loss: 4.363318
[INFO] Epoch: 3 , batch: 107 , training loss: 4.276792
[INFO] Epoch: 3 , batch: 108 , training loss: 4.159744
[INFO] Epoch: 3 , batch: 109 , training loss: 4.029554
[INFO] Epoch: 3 , batch: 110 , training loss: 4.368048
[INFO] Epoch: 3 , batch: 111 , training loss: 4.358774
[INFO] Epoch: 3 , batch: 112 , training loss: 4.358689
[INFO] Epoch: 3 , batch: 113 , training loss: 4.262134
[INFO] Epoch: 3 , batch: 114 , training loss: 4.417798
[INFO] Epoch: 3 , batch: 115 , training loss: 4.346244
[INFO] Epoch: 3 , batch: 116 , training loss: 4.274836
[INFO] Epoch: 3 , batch: 117 , training loss: 4.560838
[INFO] Epoch: 3 , batch: 118 , training loss: 4.492754
[INFO] Epoch: 3 , batch: 119 , training loss: 4.603129
[INFO] Epoch: 3 , batch: 120 , training loss: 4.541273
[INFO] Epoch: 3 , batch: 121 , training loss: 4.337385
[INFO] Epoch: 3 , batch: 122 , training loss: 4.305687
[INFO] Epoch: 3 , batch: 123 , training loss: 4.435259
[INFO] Epoch: 3 , batch: 124 , training loss: 4.584020
[INFO] Epoch: 3 , batch: 125 , training loss: 4.199204
[INFO] Epoch: 3 , batch: 126 , training loss: 4.204437
[INFO] Epoch: 3 , batch: 127 , training loss: 4.245438
[INFO] Epoch: 3 , batch: 128 , training loss: 4.469564
[INFO] Epoch: 3 , batch: 129 , training loss: 4.348751
[INFO] Epoch: 3 , batch: 130 , training loss: 4.392593
[INFO] Epoch: 3 , batch: 131 , training loss: 4.379605
[INFO] Epoch: 3 , batch: 132 , training loss: 4.375889
[INFO] Epoch: 3 , batch: 133 , training loss: 4.306262
[INFO] Epoch: 3 , batch: 134 , training loss: 4.130571
[INFO] Epoch: 3 , batch: 135 , training loss: 4.100757
[INFO] Epoch: 3 , batch: 136 , training loss: 4.441017
[INFO] Epoch: 3 , batch: 137 , training loss: 4.344458
[INFO] Epoch: 3 , batch: 138 , training loss: 4.528450
[INFO] Epoch: 3 , batch: 139 , training loss: 5.263918
[INFO] Epoch: 3 , batch: 140 , training loss: 5.149052
[INFO] Epoch: 3 , batch: 141 , training loss: 4.834560
[INFO] Epoch: 3 , batch: 142 , training loss: 4.384704
[INFO] Epoch: 3 , batch: 143 , training loss: 4.475905
[INFO] Epoch: 3 , batch: 144 , training loss: 4.265446
[INFO] Epoch: 3 , batch: 145 , training loss: 4.456025
[INFO] Epoch: 3 , batch: 146 , training loss: 4.634947
[INFO] Epoch: 3 , batch: 147 , training loss: 4.216259
[INFO] Epoch: 3 , batch: 148 , training loss: 4.182628
[INFO] Epoch: 3 , batch: 149 , training loss: 4.321479
[INFO] Epoch: 3 , batch: 150 , training loss: 4.567719
[INFO] Epoch: 3 , batch: 151 , training loss: 4.248176
[INFO] Epoch: 3 , batch: 152 , training loss: 4.303706
[INFO] Epoch: 3 , batch: 153 , training loss: 4.429155
[INFO] Epoch: 3 , batch: 154 , training loss: 4.504712
[INFO] Epoch: 3 , batch: 155 , training loss: 4.803924
[INFO] Epoch: 3 , batch: 156 , training loss: 4.361841
[INFO] Epoch: 3 , batch: 157 , training loss: 4.387360
[INFO] Epoch: 3 , batch: 158 , training loss: 4.862304
[INFO] Epoch: 3 , batch: 159 , training loss: 4.755924
[INFO] Epoch: 3 , batch: 160 , training loss: 5.253180
[INFO] Epoch: 3 , batch: 161 , training loss: 4.922299
[INFO] Epoch: 3 , batch: 162 , training loss: 4.771206
[INFO] Epoch: 3 , batch: 163 , training loss: 4.883766
[INFO] Epoch: 3 , batch: 164 , training loss: 4.768235
[INFO] Epoch: 3 , batch: 165 , training loss: 4.693295
[INFO] Epoch: 3 , batch: 166 , training loss: 5.153903
[INFO] Epoch: 3 , batch: 167 , training loss: 5.682712
[INFO] Epoch: 3 , batch: 168 , training loss: 5.421078
[INFO] Epoch: 3 , batch: 169 , training loss: 5.200802
[INFO] Epoch: 3 , batch: 170 , training loss: 5.132563
[INFO] Epoch: 3 , batch: 171 , training loss: 4.876761
[INFO] Epoch: 3 , batch: 172 , training loss: 5.071015
[INFO] Epoch: 3 , batch: 173 , training loss: 5.330691
[INFO] Epoch: 3 , batch: 174 , training loss: 5.544342
[INFO] Epoch: 3 , batch: 175 , training loss: 5.553396
[INFO] Epoch: 3 , batch: 176 , training loss: 5.304326
[INFO] Epoch: 3 , batch: 177 , training loss: 4.915875
[INFO] Epoch: 3 , batch: 178 , training loss: 4.855012
[INFO] Epoch: 3 , batch: 179 , training loss: 4.853704
[INFO] Epoch: 3 , batch: 180 , training loss: 4.832641
[INFO] Epoch: 3 , batch: 181 , training loss: 5.025823
[INFO] Epoch: 3 , batch: 182 , training loss: 4.935120
[INFO] Epoch: 3 , batch: 183 , training loss: 4.860725
[INFO] Epoch: 3 , batch: 184 , training loss: 4.726063
[INFO] Epoch: 3 , batch: 185 , training loss: 4.682447
[INFO] Epoch: 3 , batch: 186 , training loss: 4.813408
[INFO] Epoch: 3 , batch: 187 , training loss: 4.941283
[INFO] Epoch: 3 , batch: 188 , training loss: 4.916040
[INFO] Epoch: 3 , batch: 189 , training loss: 4.796109
[INFO] Epoch: 3 , batch: 190 , training loss: 4.782214
[INFO] Epoch: 3 , batch: 191 , training loss: 4.947484
[INFO] Epoch: 3 , batch: 192 , training loss: 4.671129
[INFO] Epoch: 3 , batch: 193 , training loss: 4.799662
[INFO] Epoch: 3 , batch: 194 , training loss: 4.711519
[INFO] Epoch: 3 , batch: 195 , training loss: 4.812511
[INFO] Epoch: 3 , batch: 196 , training loss: 4.559814
[INFO] Epoch: 3 , batch: 197 , training loss: 4.777962
[INFO] Epoch: 3 , batch: 198 , training loss: 4.566339
[INFO] Epoch: 3 , batch: 199 , training loss: 4.655620
[INFO] Epoch: 3 , batch: 200 , training loss: 4.571321
[INFO] Epoch: 3 , batch: 201 , training loss: 4.504591
[INFO] Epoch: 3 , batch: 202 , training loss: 4.442364
[INFO] Epoch: 3 , batch: 203 , training loss: 4.453311
[INFO] Epoch: 3 , batch: 204 , training loss: 4.633579
[INFO] Epoch: 3 , batch: 205 , training loss: 4.202580
[INFO] Epoch: 3 , batch: 206 , training loss: 4.085576
[INFO] Epoch: 3 , batch: 207 , training loss: 4.128458
[INFO] Epoch: 3 , batch: 208 , training loss: 4.547905
[INFO] Epoch: 3 , batch: 209 , training loss: 4.458183
[INFO] Epoch: 3 , batch: 210 , training loss: 4.504522
[INFO] Epoch: 3 , batch: 211 , training loss: 4.452140
[INFO] Epoch: 3 , batch: 212 , training loss: 4.543466
[INFO] Epoch: 3 , batch: 213 , training loss: 4.539234
[INFO] Epoch: 3 , batch: 214 , training loss: 4.575373
[INFO] Epoch: 3 , batch: 215 , training loss: 4.809932
[INFO] Epoch: 3 , batch: 216 , training loss: 4.547130
[INFO] Epoch: 3 , batch: 217 , training loss: 4.428504
[INFO] Epoch: 3 , batch: 218 , training loss: 4.441227
[INFO] Epoch: 3 , batch: 219 , training loss: 4.546107
[INFO] Epoch: 3 , batch: 220 , training loss: 4.323694
[INFO] Epoch: 3 , batch: 221 , training loss: 4.372868
[INFO] Epoch: 3 , batch: 222 , training loss: 4.524137
[INFO] Epoch: 3 , batch: 223 , training loss: 4.602003
[INFO] Epoch: 3 , batch: 224 , training loss: 4.662780
[INFO] Epoch: 3 , batch: 225 , training loss: 4.497021
[INFO] Epoch: 3 , batch: 226 , training loss: 4.646957
[INFO] Epoch: 3 , batch: 227 , training loss: 4.604837
[INFO] Epoch: 3 , batch: 228 , training loss: 4.672625
[INFO] Epoch: 3 , batch: 229 , training loss: 4.551544
[INFO] Epoch: 3 , batch: 230 , training loss: 4.371511
[INFO] Epoch: 3 , batch: 231 , training loss: 4.192732
[INFO] Epoch: 3 , batch: 232 , training loss: 4.336755
[INFO] Epoch: 3 , batch: 233 , training loss: 4.380079
[INFO] Epoch: 3 , batch: 234 , training loss: 4.064676
[INFO] Epoch: 3 , batch: 235 , training loss: 4.165865
[INFO] Epoch: 3 , batch: 236 , training loss: 4.389622
[INFO] Epoch: 3 , batch: 237 , training loss: 4.513671
[INFO] Epoch: 3 , batch: 238 , training loss: 4.238716
[INFO] Epoch: 3 , batch: 239 , training loss: 4.337956
[INFO] Epoch: 3 , batch: 240 , training loss: 4.381404
[INFO] Epoch: 3 , batch: 241 , training loss: 4.183136
[INFO] Epoch: 3 , batch: 242 , training loss: 4.173730
[INFO] Epoch: 3 , batch: 243 , training loss: 4.534150
[INFO] Epoch: 3 , batch: 244 , training loss: 4.416721
[INFO] Epoch: 3 , batch: 245 , training loss: 4.433721
[INFO] Epoch: 3 , batch: 246 , training loss: 4.080634
[INFO] Epoch: 3 , batch: 247 , training loss: 4.237547
[INFO] Epoch: 3 , batch: 248 , training loss: 4.350844
[INFO] Epoch: 3 , batch: 249 , training loss: 4.290423
[INFO] Epoch: 3 , batch: 250 , training loss: 4.066884
[INFO] Epoch: 3 , batch: 251 , training loss: 4.592545
[INFO] Epoch: 3 , batch: 252 , training loss: 4.261469
[INFO] Epoch: 3 , batch: 253 , training loss: 4.242758
[INFO] Epoch: 3 , batch: 254 , training loss: 4.557971
[INFO] Epoch: 3 , batch: 255 , training loss: 4.551616
[INFO] Epoch: 3 , batch: 256 , training loss: 4.468981
[INFO] Epoch: 3 , batch: 257 , training loss: 4.663525
[INFO] Epoch: 3 , batch: 258 , training loss: 4.723753
[INFO] Epoch: 3 , batch: 259 , training loss: 4.754831
[INFO] Epoch: 3 , batch: 260 , training loss: 4.400467
[INFO] Epoch: 3 , batch: 261 , training loss: 4.584287
[INFO] Epoch: 3 , batch: 262 , training loss: 4.839884
[INFO] Epoch: 3 , batch: 263 , training loss: 4.963070
[INFO] Epoch: 3 , batch: 264 , training loss: 4.249570
[INFO] Epoch: 3 , batch: 265 , training loss: 4.412677
[INFO] Epoch: 3 , batch: 266 , training loss: 4.930197
[INFO] Epoch: 3 , batch: 267 , training loss: 4.589173
[INFO] Epoch: 3 , batch: 268 , training loss: 4.454087
[INFO] Epoch: 3 , batch: 269 , training loss: 4.512919
[INFO] Epoch: 3 , batch: 270 , training loss: 4.495133
[INFO] Epoch: 3 , batch: 271 , training loss: 4.537027
[INFO] Epoch: 3 , batch: 272 , training loss: 4.504231
[INFO] Epoch: 3 , batch: 273 , training loss: 4.487865
[INFO] Epoch: 3 , batch: 274 , training loss: 4.620760
[INFO] Epoch: 3 , batch: 275 , training loss: 4.545050
[INFO] Epoch: 3 , batch: 276 , training loss: 4.587784
[INFO] Epoch: 3 , batch: 277 , training loss: 4.724283
[INFO] Epoch: 3 , batch: 278 , training loss: 4.296925
[INFO] Epoch: 3 , batch: 279 , training loss: 4.341737
[INFO] Epoch: 3 , batch: 280 , training loss: 4.273343
[INFO] Epoch: 3 , batch: 281 , training loss: 4.420029
[INFO] Epoch: 3 , batch: 282 , training loss: 4.308913
[INFO] Epoch: 3 , batch: 283 , training loss: 4.383064
[INFO] Epoch: 3 , batch: 284 , training loss: 4.396503
[INFO] Epoch: 3 , batch: 285 , training loss: 4.419988
[INFO] Epoch: 3 , batch: 286 , training loss: 4.372328
[INFO] Epoch: 3 , batch: 287 , training loss: 4.241714
[INFO] Epoch: 3 , batch: 288 , training loss: 4.296117
[INFO] Epoch: 3 , batch: 289 , training loss: 4.327300
[INFO] Epoch: 3 , batch: 290 , training loss: 4.134417
[INFO] Epoch: 3 , batch: 291 , training loss: 4.085255
[INFO] Epoch: 3 , batch: 292 , training loss: 4.188731
[INFO] Epoch: 3 , batch: 293 , training loss: 4.154012
[INFO] Epoch: 3 , batch: 294 , training loss: 4.857005
[INFO] Epoch: 3 , batch: 295 , training loss: 4.614707
[INFO] Epoch: 3 , batch: 296 , training loss: 4.513542
[INFO] Epoch: 3 , batch: 297 , training loss: 4.463192
[INFO] Epoch: 3 , batch: 298 , training loss: 4.321619
[INFO] Epoch: 3 , batch: 299 , training loss: 4.298761
[INFO] Epoch: 3 , batch: 300 , training loss: 4.288102
[INFO] Epoch: 3 , batch: 301 , training loss: 4.224627
[INFO] Epoch: 3 , batch: 302 , training loss: 4.410478
[INFO] Epoch: 3 , batch: 303 , training loss: 4.405815
[INFO] Epoch: 3 , batch: 304 , training loss: 4.627132
[INFO] Epoch: 3 , batch: 305 , training loss: 4.342583
[INFO] Epoch: 3 , batch: 306 , training loss: 4.457641
[INFO] Epoch: 3 , batch: 307 , training loss: 4.447281
[INFO] Epoch: 3 , batch: 308 , training loss: 4.337549
[INFO] Epoch: 3 , batch: 309 , training loss: 4.345819
[INFO] Epoch: 3 , batch: 310 , training loss: 4.166921
[INFO] Epoch: 3 , batch: 311 , training loss: 4.198389
[INFO] Epoch: 3 , batch: 312 , training loss: 4.061068
[INFO] Epoch: 3 , batch: 313 , training loss: 4.255852
[INFO] Epoch: 3 , batch: 314 , training loss: 4.305137
[INFO] Epoch: 3 , batch: 315 , training loss: 4.357440
[INFO] Epoch: 3 , batch: 316 , training loss: 4.713550
[INFO] Epoch: 3 , batch: 317 , training loss: 5.326770
[INFO] Epoch: 3 , batch: 318 , training loss: 5.459255
[INFO] Epoch: 3 , batch: 319 , training loss: 4.879572
[INFO] Epoch: 3 , batch: 320 , training loss: 4.375888
[INFO] Epoch: 3 , batch: 321 , training loss: 4.150402
[INFO] Epoch: 3 , batch: 322 , training loss: 4.284339
[INFO] Epoch: 3 , batch: 323 , training loss: 4.291911
[INFO] Epoch: 3 , batch: 324 , training loss: 4.286249
[INFO] Epoch: 3 , batch: 325 , training loss: 4.487436
[INFO] Epoch: 3 , batch: 326 , training loss: 4.541688
[INFO] Epoch: 3 , batch: 327 , training loss: 4.419313
[INFO] Epoch: 3 , batch: 328 , training loss: 4.433059
[INFO] Epoch: 3 , batch: 329 , training loss: 4.307946
[INFO] Epoch: 3 , batch: 330 , training loss: 4.266293
[INFO] Epoch: 3 , batch: 331 , training loss: 4.453514
[INFO] Epoch: 3 , batch: 332 , training loss: 4.223731
[INFO] Epoch: 3 , batch: 333 , training loss: 4.192531
[INFO] Epoch: 3 , batch: 334 , training loss: 4.321435
[INFO] Epoch: 3 , batch: 335 , training loss: 4.442395
[INFO] Epoch: 3 , batch: 336 , training loss: 4.384270
[INFO] Epoch: 3 , batch: 337 , training loss: 4.491695
[INFO] Epoch: 3 , batch: 338 , training loss: 4.656648
[INFO] Epoch: 3 , batch: 339 , training loss: 4.492482
[INFO] Epoch: 3 , batch: 340 , training loss: 4.695389
[INFO] Epoch: 3 , batch: 341 , training loss: 4.393475
[INFO] Epoch: 3 , batch: 342 , training loss: 4.240908
[INFO] Epoch: 3 , batch: 343 , training loss: 4.332400
[INFO] Epoch: 3 , batch: 344 , training loss: 4.173721
[INFO] Epoch: 3 , batch: 345 , training loss: 4.299009
[INFO] Epoch: 3 , batch: 346 , training loss: 4.397020
[INFO] Epoch: 3 , batch: 347 , training loss: 4.253562
[INFO] Epoch: 3 , batch: 348 , training loss: 4.500277
[INFO] Epoch: 3 , batch: 349 , training loss: 4.554146
[INFO] Epoch: 3 , batch: 350 , training loss: 4.281409
[INFO] Epoch: 3 , batch: 351 , training loss: 4.333333
[INFO] Epoch: 3 , batch: 352 , training loss: 4.354042
[INFO] Epoch: 3 , batch: 353 , training loss: 4.321501
[INFO] Epoch: 3 , batch: 354 , training loss: 4.419259
[INFO] Epoch: 3 , batch: 355 , training loss: 4.482709
[INFO] Epoch: 3 , batch: 356 , training loss: 4.342944
[INFO] Epoch: 3 , batch: 357 , training loss: 4.417712
[INFO] Epoch: 3 , batch: 358 , training loss: 4.395743
[INFO] Epoch: 3 , batch: 359 , training loss: 4.350400
[INFO] Epoch: 3 , batch: 360 , training loss: 4.385446
[INFO] Epoch: 3 , batch: 361 , training loss: 4.351351
[INFO] Epoch: 3 , batch: 362 , training loss: 4.456266
[INFO] Epoch: 3 , batch: 363 , training loss: 4.327576
[INFO] Epoch: 3 , batch: 364 , training loss: 4.362457
[INFO] Epoch: 3 , batch: 365 , training loss: 4.272018
[INFO] Epoch: 3 , batch: 366 , training loss: 4.433014
[INFO] Epoch: 3 , batch: 367 , training loss: 4.516627
[INFO] Epoch: 3 , batch: 368 , training loss: 5.141998
[INFO] Epoch: 3 , batch: 369 , training loss: 4.660709
[INFO] Epoch: 3 , batch: 370 , training loss: 4.420970
[INFO] Epoch: 3 , batch: 371 , training loss: 5.032437
[INFO] Epoch: 3 , batch: 372 , training loss: 5.311869
[INFO] Epoch: 3 , batch: 373 , training loss: 5.274584
[INFO] Epoch: 3 , batch: 374 , training loss: 5.299919
[INFO] Epoch: 3 , batch: 375 , training loss: 5.310731
[INFO] Epoch: 3 , batch: 376 , training loss: 5.179871
[INFO] Epoch: 3 , batch: 377 , training loss: 4.821434
[INFO] Epoch: 3 , batch: 378 , training loss: 4.946354
[INFO] Epoch: 3 , batch: 379 , training loss: 4.987268
[INFO] Epoch: 3 , batch: 380 , training loss: 5.081074
[INFO] Epoch: 3 , batch: 381 , training loss: 4.880152
[INFO] Epoch: 3 , batch: 382 , training loss: 5.035046
[INFO] Epoch: 3 , batch: 383 , training loss: 5.183733
[INFO] Epoch: 3 , batch: 384 , training loss: 5.322258
[INFO] Epoch: 3 , batch: 385 , training loss: 5.087289
[INFO] Epoch: 3 , batch: 386 , training loss: 5.233943
[INFO] Epoch: 3 , batch: 387 , training loss: 5.160509
[INFO] Epoch: 3 , batch: 388 , training loss: 4.910006
[INFO] Epoch: 3 , batch: 389 , training loss: 4.705686
[INFO] Epoch: 3 , batch: 390 , training loss: 4.658941
[INFO] Epoch: 3 , batch: 391 , training loss: 4.705451
[INFO] Epoch: 3 , batch: 392 , training loss: 5.053035
[INFO] Epoch: 3 , batch: 393 , training loss: 4.954926
[INFO] Epoch: 3 , batch: 394 , training loss: 4.987207
[INFO] Epoch: 3 , batch: 395 , training loss: 4.808741
[INFO] Epoch: 3 , batch: 396 , training loss: 4.597397
[INFO] Epoch: 3 , batch: 397 , training loss: 4.777905
[INFO] Epoch: 3 , batch: 398 , training loss: 4.599185
[INFO] Epoch: 3 , batch: 399 , training loss: 4.649793
[INFO] Epoch: 3 , batch: 400 , training loss: 4.598038
[INFO] Epoch: 3 , batch: 401 , training loss: 5.023572
[INFO] Epoch: 3 , batch: 402 , training loss: 4.791953
[INFO] Epoch: 3 , batch: 403 , training loss: 4.640082
[INFO] Epoch: 3 , batch: 404 , training loss: 4.776248
[INFO] Epoch: 3 , batch: 405 , training loss: 4.871574
[INFO] Epoch: 3 , batch: 406 , training loss: 4.747212
[INFO] Epoch: 3 , batch: 407 , training loss: 4.792237
[INFO] Epoch: 3 , batch: 408 , training loss: 4.731590
[INFO] Epoch: 3 , batch: 409 , training loss: 4.734815
[INFO] Epoch: 3 , batch: 410 , training loss: 4.764722
[INFO] Epoch: 3 , batch: 411 , training loss: 4.993355
[INFO] Epoch: 3 , batch: 412 , training loss: 4.819574
[INFO] Epoch: 3 , batch: 413 , training loss: 4.666678
[INFO] Epoch: 3 , batch: 414 , training loss: 4.697948
[INFO] Epoch: 3 , batch: 415 , training loss: 4.726557
[INFO] Epoch: 3 , batch: 416 , training loss: 4.804336
[INFO] Epoch: 3 , batch: 417 , training loss: 4.709110
[INFO] Epoch: 3 , batch: 418 , training loss: 4.734993
[INFO] Epoch: 3 , batch: 419 , training loss: 4.680514
[INFO] Epoch: 3 , batch: 420 , training loss: 4.659246
[INFO] Epoch: 3 , batch: 421 , training loss: 4.652539
[INFO] Epoch: 3 , batch: 422 , training loss: 4.549021
[INFO] Epoch: 3 , batch: 423 , training loss: 4.793358
[INFO] Epoch: 3 , batch: 424 , training loss: 4.938599
[INFO] Epoch: 3 , batch: 425 , training loss: 4.828270
[INFO] Epoch: 3 , batch: 426 , training loss: 4.499054
[INFO] Epoch: 3 , batch: 427 , training loss: 4.777426
[INFO] Epoch: 3 , batch: 428 , training loss: 4.696202
[INFO] Epoch: 3 , batch: 429 , training loss: 4.482521
[INFO] Epoch: 3 , batch: 430 , training loss: 4.770478
[INFO] Epoch: 3 , batch: 431 , training loss: 4.320930
[INFO] Epoch: 3 , batch: 432 , training loss: 4.424972
[INFO] Epoch: 3 , batch: 433 , training loss: 4.428988
[INFO] Epoch: 3 , batch: 434 , training loss: 4.307600
[INFO] Epoch: 3 , batch: 435 , training loss: 4.661784
[INFO] Epoch: 3 , batch: 436 , training loss: 4.786342
[INFO] Epoch: 3 , batch: 437 , training loss: 4.568577
[INFO] Epoch: 3 , batch: 438 , training loss: 4.316116
[INFO] Epoch: 3 , batch: 439 , training loss: 4.577583
[INFO] Epoch: 3 , batch: 440 , training loss: 4.721128
[INFO] Epoch: 3 , batch: 441 , training loss: 4.756015
[INFO] Epoch: 3 , batch: 442 , training loss: 4.582137
[INFO] Epoch: 3 , batch: 443 , training loss: 4.792469
[INFO] Epoch: 3 , batch: 444 , training loss: 4.404111
[INFO] Epoch: 3 , batch: 445 , training loss: 4.255984
[INFO] Epoch: 3 , batch: 446 , training loss: 4.125049
[INFO] Epoch: 3 , batch: 447 , training loss: 4.399380
[INFO] Epoch: 3 , batch: 448 , training loss: 4.583922
[INFO] Epoch: 3 , batch: 449 , training loss: 4.988489
[INFO] Epoch: 3 , batch: 450 , training loss: 5.051423
[INFO] Epoch: 3 , batch: 451 , training loss: 4.907895
[INFO] Epoch: 3 , batch: 452 , training loss: 4.653461
[INFO] Epoch: 3 , batch: 453 , training loss: 4.441708
[INFO] Epoch: 3 , batch: 454 , training loss: 4.594378
[INFO] Epoch: 3 , batch: 455 , training loss: 4.635587
[INFO] Epoch: 3 , batch: 456 , training loss: 4.600210
[INFO] Epoch: 3 , batch: 457 , training loss: 4.724998
[INFO] Epoch: 3 , batch: 458 , training loss: 4.424721
[INFO] Epoch: 3 , batch: 459 , training loss: 4.409113
[INFO] Epoch: 3 , batch: 460 , training loss: 4.548429
[INFO] Epoch: 3 , batch: 461 , training loss: 4.586007
[INFO] Epoch: 3 , batch: 462 , training loss: 4.618849
[INFO] Epoch: 3 , batch: 463 , training loss: 4.456386
[INFO] Epoch: 3 , batch: 464 , training loss: 4.679856
[INFO] Epoch: 3 , batch: 465 , training loss: 4.603667
[INFO] Epoch: 3 , batch: 466 , training loss: 4.675035
[INFO] Epoch: 3 , batch: 467 , training loss: 4.733196
[INFO] Epoch: 3 , batch: 468 , training loss: 4.664676
[INFO] Epoch: 3 , batch: 469 , training loss: 4.663643
[INFO] Epoch: 3 , batch: 470 , training loss: 4.431866
[INFO] Epoch: 3 , batch: 471 , training loss: 4.564552
[INFO] Epoch: 3 , batch: 472 , training loss: 4.601717
[INFO] Epoch: 3 , batch: 473 , training loss: 4.530571
[INFO] Epoch: 3 , batch: 474 , training loss: 4.363510
[INFO] Epoch: 3 , batch: 475 , training loss: 4.192149
[INFO] Epoch: 3 , batch: 476 , training loss: 4.587693
[INFO] Epoch: 3 , batch: 477 , training loss: 4.691196
[INFO] Epoch: 3 , batch: 478 , training loss: 4.802023
[INFO] Epoch: 3 , batch: 479 , training loss: 4.739064
[INFO] Epoch: 3 , batch: 480 , training loss: 4.882343
[INFO] Epoch: 3 , batch: 481 , training loss: 4.676506
[INFO] Epoch: 3 , batch: 482 , training loss: 4.822855
[INFO] Epoch: 3 , batch: 483 , training loss: 4.649347
[INFO] Epoch: 3 , batch: 484 , training loss: 4.470783
[INFO] Epoch: 3 , batch: 485 , training loss: 4.596532
[INFO] Epoch: 3 , batch: 486 , training loss: 4.461055
[INFO] Epoch: 3 , batch: 487 , training loss: 4.453018
[INFO] Epoch: 3 , batch: 488 , training loss: 4.673393
[INFO] Epoch: 3 , batch: 489 , training loss: 4.507533
[INFO] Epoch: 3 , batch: 490 , training loss: 4.611639
[INFO] Epoch: 3 , batch: 491 , training loss: 4.593980
[INFO] Epoch: 3 , batch: 492 , training loss: 4.449658
[INFO] Epoch: 3 , batch: 493 , training loss: 4.643942
[INFO] Epoch: 3 , batch: 494 , training loss: 4.567222
[INFO] Epoch: 3 , batch: 495 , training loss: 4.681113
[INFO] Epoch: 3 , batch: 496 , training loss: 4.574104
[INFO] Epoch: 3 , batch: 497 , training loss: 4.563189
[INFO] Epoch: 3 , batch: 498 , training loss: 4.636554
[INFO] Epoch: 3 , batch: 499 , training loss: 4.693499
[INFO] Epoch: 3 , batch: 500 , training loss: 4.968217
[INFO] Epoch: 3 , batch: 501 , training loss: 5.416102
[INFO] Epoch: 3 , batch: 502 , training loss: 5.533632
[INFO] Epoch: 3 , batch: 503 , training loss: 5.213530
[INFO] Epoch: 3 , batch: 504 , training loss: 5.267447
[INFO] Epoch: 3 , batch: 505 , training loss: 5.136642
[INFO] Epoch: 3 , batch: 506 , training loss: 5.105645
[INFO] Epoch: 3 , batch: 507 , training loss: 5.135411
[INFO] Epoch: 3 , batch: 508 , training loss: 5.114522
[INFO] Epoch: 3 , batch: 509 , training loss: 4.795145
[INFO] Epoch: 3 , batch: 510 , training loss: 4.867629
[INFO] Epoch: 3 , batch: 511 , training loss: 4.783812
[INFO] Epoch: 3 , batch: 512 , training loss: 4.839318
[INFO] Epoch: 3 , batch: 513 , training loss: 5.089110
[INFO] Epoch: 3 , batch: 514 , training loss: 4.764233
[INFO] Epoch: 3 , batch: 515 , training loss: 5.032716
[INFO] Epoch: 3 , batch: 516 , training loss: 4.806403
[INFO] Epoch: 3 , batch: 517 , training loss: 4.788888
[INFO] Epoch: 3 , batch: 518 , training loss: 4.765999
[INFO] Epoch: 3 , batch: 519 , training loss: 4.650773
[INFO] Epoch: 3 , batch: 520 , training loss: 4.845278
[INFO] Epoch: 3 , batch: 521 , training loss: 4.869613
[INFO] Epoch: 3 , batch: 522 , training loss: 4.923667
[INFO] Epoch: 3 , batch: 523 , training loss: 4.813673
[INFO] Epoch: 3 , batch: 524 , training loss: 5.032855
[INFO] Epoch: 3 , batch: 525 , training loss: 4.987226
[INFO] Epoch: 3 , batch: 526 , training loss: 4.746986
[INFO] Epoch: 3 , batch: 527 , training loss: 4.760429
[INFO] Epoch: 3 , batch: 528 , training loss: 4.865685
[INFO] Epoch: 3 , batch: 529 , training loss: 4.792915
[INFO] Epoch: 3 , batch: 530 , training loss: 4.623390
[INFO] Epoch: 3 , batch: 531 , training loss: 4.812758
[INFO] Epoch: 3 , batch: 532 , training loss: 4.631410
[INFO] Epoch: 3 , batch: 533 , training loss: 4.808797
[INFO] Epoch: 3 , batch: 534 , training loss: 4.812882
[INFO] Epoch: 3 , batch: 535 , training loss: 4.825835
[INFO] Epoch: 3 , batch: 536 , training loss: 4.690787
[INFO] Epoch: 3 , batch: 537 , training loss: 4.641716
[INFO] Epoch: 3 , batch: 538 , training loss: 4.725945
[INFO] Epoch: 3 , batch: 539 , training loss: 4.903416
[INFO] Epoch: 3 , batch: 540 , training loss: 5.554399
[INFO] Epoch: 3 , batch: 541 , training loss: 5.471925
[INFO] Epoch: 3 , batch: 542 , training loss: 5.216249
[INFO] Epoch: 4 , batch: 0 , training loss: 5.016688
[INFO] Epoch: 4 , batch: 1 , training loss: 4.705666
[INFO] Epoch: 4 , batch: 2 , training loss: 4.529611
[INFO] Epoch: 4 , batch: 3 , training loss: 4.396791
[INFO] Epoch: 4 , batch: 4 , training loss: 4.810591
[INFO] Epoch: 4 , batch: 5 , training loss: 4.403395
[INFO] Epoch: 4 , batch: 6 , training loss: 4.989599
[INFO] Epoch: 4 , batch: 7 , training loss: 4.529648
[INFO] Epoch: 4 , batch: 8 , training loss: 4.248181
[INFO] Epoch: 4 , batch: 9 , training loss: 4.375765
[INFO] Epoch: 4 , batch: 10 , training loss: 4.287931
[INFO] Epoch: 4 , batch: 11 , training loss: 4.191612
[INFO] Epoch: 4 , batch: 12 , training loss: 4.237513
[INFO] Epoch: 4 , batch: 13 , training loss: 4.147872
[INFO] Epoch: 4 , batch: 14 , training loss: 3.938376
[INFO] Epoch: 4 , batch: 15 , training loss: 4.232170
[INFO] Epoch: 4 , batch: 16 , training loss: 4.089904
[INFO] Epoch: 4 , batch: 17 , training loss: 4.262335
[INFO] Epoch: 4 , batch: 18 , training loss: 4.162564
[INFO] Epoch: 4 , batch: 19 , training loss: 3.953346
[INFO] Epoch: 4 , batch: 20 , training loss: 3.928382
[INFO] Epoch: 4 , batch: 21 , training loss: 4.016391
[INFO] Epoch: 4 , batch: 22 , training loss: 4.115625
[INFO] Epoch: 4 , batch: 23 , training loss: 4.253759
[INFO] Epoch: 4 , batch: 24 , training loss: 4.042391
[INFO] Epoch: 4 , batch: 25 , training loss: 4.239206
[INFO] Epoch: 4 , batch: 26 , training loss: 4.032565
[INFO] Epoch: 4 , batch: 27 , training loss: 4.027210
[INFO] Epoch: 4 , batch: 28 , training loss: 4.274925
[INFO] Epoch: 4 , batch: 29 , training loss: 4.020973
[INFO] Epoch: 4 , batch: 30 , training loss: 4.031476
[INFO] Epoch: 4 , batch: 31 , training loss: 4.126449
[INFO] Epoch: 4 , batch: 32 , training loss: 4.152149
[INFO] Epoch: 4 , batch: 33 , training loss: 4.252036
[INFO] Epoch: 4 , batch: 34 , training loss: 4.269888
[INFO] Epoch: 4 , batch: 35 , training loss: 4.184548
[INFO] Epoch: 4 , batch: 36 , training loss: 4.104508
[INFO] Epoch: 4 , batch: 37 , training loss: 4.026020
[INFO] Epoch: 4 , batch: 38 , training loss: 4.206621
[INFO] Epoch: 4 , batch: 39 , training loss: 3.976012
[INFO] Epoch: 4 , batch: 40 , training loss: 4.074190
[INFO] Epoch: 4 , batch: 41 , training loss: 4.276258
[INFO] Epoch: 4 , batch: 42 , training loss: 4.952144
[INFO] Epoch: 4 , batch: 43 , training loss: 4.576622
[INFO] Epoch: 4 , batch: 44 , training loss: 4.653694
[INFO] Epoch: 4 , batch: 45 , training loss: 4.924647
[INFO] Epoch: 4 , batch: 46 , training loss: 5.259624
[INFO] Epoch: 4 , batch: 47 , training loss: 4.424443
[INFO] Epoch: 4 , batch: 48 , training loss: 4.570295
[INFO] Epoch: 4 , batch: 49 , training loss: 4.527093
[INFO] Epoch: 4 , batch: 50 , training loss: 4.353298
[INFO] Epoch: 4 , batch: 51 , training loss: 4.342256
[INFO] Epoch: 4 , batch: 52 , training loss: 4.128746
[INFO] Epoch: 4 , batch: 53 , training loss: 4.322119
[INFO] Epoch: 4 , batch: 54 , training loss: 4.315163
[INFO] Epoch: 4 , batch: 55 , training loss: 4.379414
[INFO] Epoch: 4 , batch: 56 , training loss: 4.197804
[INFO] Epoch: 4 , batch: 57 , training loss: 4.069555
[INFO] Epoch: 4 , batch: 58 , training loss: 4.128968
[INFO] Epoch: 4 , batch: 59 , training loss: 4.257472
[INFO] Epoch: 4 , batch: 60 , training loss: 4.069913
[INFO] Epoch: 4 , batch: 61 , training loss: 4.168034
[INFO] Epoch: 4 , batch: 62 , training loss: 4.093823
[INFO] Epoch: 4 , batch: 63 , training loss: 4.292761
[INFO] Epoch: 4 , batch: 64 , training loss: 4.430336
[INFO] Epoch: 4 , batch: 65 , training loss: 4.154961
[INFO] Epoch: 4 , batch: 66 , training loss: 3.984759
[INFO] Epoch: 4 , batch: 67 , training loss: 4.030314
[INFO] Epoch: 4 , batch: 68 , training loss: 4.347034
[INFO] Epoch: 4 , batch: 69 , training loss: 4.095585
[INFO] Epoch: 4 , batch: 70 , training loss: 4.378519
[INFO] Epoch: 4 , batch: 71 , training loss: 4.186635
[INFO] Epoch: 4 , batch: 72 , training loss: 4.274968
[INFO] Epoch: 4 , batch: 73 , training loss: 4.174659
[INFO] Epoch: 4 , batch: 74 , training loss: 4.361213
[INFO] Epoch: 4 , batch: 75 , training loss: 4.087258
[INFO] Epoch: 4 , batch: 76 , training loss: 4.257757
[INFO] Epoch: 4 , batch: 77 , training loss: 4.152577
[INFO] Epoch: 4 , batch: 78 , training loss: 4.235883
[INFO] Epoch: 4 , batch: 79 , training loss: 4.056402
[INFO] Epoch: 4 , batch: 80 , training loss: 4.282570
[INFO] Epoch: 4 , batch: 81 , training loss: 4.263294
[INFO] Epoch: 4 , batch: 82 , training loss: 4.248429
[INFO] Epoch: 4 , batch: 83 , training loss: 4.329365
[INFO] Epoch: 4 , batch: 84 , training loss: 4.246786
[INFO] Epoch: 4 , batch: 85 , training loss: 4.318720
[INFO] Epoch: 4 , batch: 86 , training loss: 4.321348
[INFO] Epoch: 4 , batch: 87 , training loss: 4.292816
[INFO] Epoch: 4 , batch: 88 , training loss: 4.415768
[INFO] Epoch: 4 , batch: 89 , training loss: 4.207519
[INFO] Epoch: 4 , batch: 90 , training loss: 4.236844
[INFO] Epoch: 4 , batch: 91 , training loss: 4.192767
[INFO] Epoch: 4 , batch: 92 , training loss: 4.188846
[INFO] Epoch: 4 , batch: 93 , training loss: 4.283909
[INFO] Epoch: 4 , batch: 94 , training loss: 4.494075
[INFO] Epoch: 4 , batch: 95 , training loss: 4.234486
[INFO] Epoch: 4 , batch: 96 , training loss: 4.198978
[INFO] Epoch: 4 , batch: 97 , training loss: 4.190182
[INFO] Epoch: 4 , batch: 98 , training loss: 4.153446
[INFO] Epoch: 4 , batch: 99 , training loss: 4.216464
[INFO] Epoch: 4 , batch: 100 , training loss: 4.042484
[INFO] Epoch: 4 , batch: 101 , training loss: 4.153711
[INFO] Epoch: 4 , batch: 102 , training loss: 4.300986
[INFO] Epoch: 4 , batch: 103 , training loss: 4.015279
[INFO] Epoch: 4 , batch: 104 , training loss: 4.049612
[INFO] Epoch: 4 , batch: 105 , training loss: 4.326564
[INFO] Epoch: 4 , batch: 106 , training loss: 4.263277
[INFO] Epoch: 4 , batch: 107 , training loss: 4.182557
[INFO] Epoch: 4 , batch: 108 , training loss: 4.063816
[INFO] Epoch: 4 , batch: 109 , training loss: 3.937322
[INFO] Epoch: 4 , batch: 110 , training loss: 4.245382
[INFO] Epoch: 4 , batch: 111 , training loss: 4.263340
[INFO] Epoch: 4 , batch: 112 , training loss: 4.232159
[INFO] Epoch: 4 , batch: 113 , training loss: 4.151830
[INFO] Epoch: 4 , batch: 114 , training loss: 4.289872
[INFO] Epoch: 4 , batch: 115 , training loss: 4.241045
[INFO] Epoch: 4 , batch: 116 , training loss: 4.159428
[INFO] Epoch: 4 , batch: 117 , training loss: 4.435886
[INFO] Epoch: 4 , batch: 118 , training loss: 4.374233
[INFO] Epoch: 4 , batch: 119 , training loss: 4.499022
[INFO] Epoch: 4 , batch: 120 , training loss: 4.438084
[INFO] Epoch: 4 , batch: 121 , training loss: 4.261075
[INFO] Epoch: 4 , batch: 122 , training loss: 4.204836
[INFO] Epoch: 4 , batch: 123 , training loss: 4.307291
[INFO] Epoch: 4 , batch: 124 , training loss: 4.452715
[INFO] Epoch: 4 , batch: 125 , training loss: 4.095989
[INFO] Epoch: 4 , batch: 126 , training loss: 4.105059
[INFO] Epoch: 4 , batch: 127 , training loss: 4.133104
[INFO] Epoch: 4 , batch: 128 , training loss: 4.349880
[INFO] Epoch: 4 , batch: 129 , training loss: 4.259405
[INFO] Epoch: 4 , batch: 130 , training loss: 4.263052
[INFO] Epoch: 4 , batch: 131 , training loss: 4.279838
[INFO] Epoch: 4 , batch: 132 , training loss: 4.277805
[INFO] Epoch: 4 , batch: 133 , training loss: 4.206905
[INFO] Epoch: 4 , batch: 134 , training loss: 4.024348
[INFO] Epoch: 4 , batch: 135 , training loss: 4.020978
[INFO] Epoch: 4 , batch: 136 , training loss: 4.344349
[INFO] Epoch: 4 , batch: 137 , training loss: 4.252674
[INFO] Epoch: 4 , batch: 138 , training loss: 4.422263
[INFO] Epoch: 4 , batch: 139 , training loss: 5.139288
[INFO] Epoch: 4 , batch: 140 , training loss: 4.989204
[INFO] Epoch: 4 , batch: 141 , training loss: 4.690962
[INFO] Epoch: 4 , batch: 142 , training loss: 4.276540
[INFO] Epoch: 4 , batch: 143 , training loss: 4.361212
[INFO] Epoch: 4 , batch: 144 , training loss: 4.155018
[INFO] Epoch: 4 , batch: 145 , training loss: 4.330508
[INFO] Epoch: 4 , batch: 146 , training loss: 4.519630
[INFO] Epoch: 4 , batch: 147 , training loss: 4.123873
[INFO] Epoch: 4 , batch: 148 , training loss: 4.102303
[INFO] Epoch: 4 , batch: 149 , training loss: 4.199114
[INFO] Epoch: 4 , batch: 150 , training loss: 4.456457
[INFO] Epoch: 4 , batch: 151 , training loss: 4.172912
[INFO] Epoch: 4 , batch: 152 , training loss: 4.230697
[INFO] Epoch: 4 , batch: 153 , training loss: 4.330625
[INFO] Epoch: 4 , batch: 154 , training loss: 4.403425
[INFO] Epoch: 4 , batch: 155 , training loss: 4.699111
[INFO] Epoch: 4 , batch: 156 , training loss: 4.284335
[INFO] Epoch: 4 , batch: 157 , training loss: 4.268900
[INFO] Epoch: 4 , batch: 158 , training loss: 4.725365
[INFO] Epoch: 4 , batch: 159 , training loss: 4.602437
[INFO] Epoch: 4 , batch: 160 , training loss: 5.042410
[INFO] Epoch: 4 , batch: 161 , training loss: 4.773916
[INFO] Epoch: 4 , batch: 162 , training loss: 4.673634
[INFO] Epoch: 4 , batch: 163 , training loss: 4.800259
[INFO] Epoch: 4 , batch: 164 , training loss: 4.716154
[INFO] Epoch: 4 , batch: 165 , training loss: 4.631288
[INFO] Epoch: 4 , batch: 166 , training loss: 4.937018
[INFO] Epoch: 4 , batch: 167 , training loss: 5.407763
[INFO] Epoch: 4 , batch: 168 , training loss: 5.178470
[INFO] Epoch: 4 , batch: 169 , training loss: 4.992182
[INFO] Epoch: 4 , batch: 170 , training loss: 4.982655
[INFO] Epoch: 4 , batch: 171 , training loss: 4.649885
[INFO] Epoch: 4 , batch: 172 , training loss: 4.806343
[INFO] Epoch: 4 , batch: 173 , training loss: 5.062474
[INFO] Epoch: 4 , batch: 174 , training loss: 5.272598
[INFO] Epoch: 4 , batch: 175 , training loss: 5.399182
[INFO] Epoch: 4 , batch: 176 , training loss: 5.172485
[INFO] Epoch: 4 , batch: 177 , training loss: 4.771610
[INFO] Epoch: 4 , batch: 178 , training loss: 4.708676
[INFO] Epoch: 4 , batch: 179 , training loss: 4.722932
[INFO] Epoch: 4 , batch: 180 , training loss: 4.682419
[INFO] Epoch: 4 , batch: 181 , training loss: 4.877115
[INFO] Epoch: 4 , batch: 182 , training loss: 4.799495
[INFO] Epoch: 4 , batch: 183 , training loss: 4.757236
[INFO] Epoch: 4 , batch: 184 , training loss: 4.623539
[INFO] Epoch: 4 , batch: 185 , training loss: 4.570884
[INFO] Epoch: 4 , batch: 186 , training loss: 4.707456
[INFO] Epoch: 4 , batch: 187 , training loss: 4.829318
[INFO] Epoch: 4 , batch: 188 , training loss: 4.806810
[INFO] Epoch: 4 , batch: 189 , training loss: 4.684369
[INFO] Epoch: 4 , batch: 190 , training loss: 4.661146
[INFO] Epoch: 4 , batch: 191 , training loss: 4.835522
[INFO] Epoch: 4 , batch: 192 , training loss: 4.560580
[INFO] Epoch: 4 , batch: 193 , training loss: 4.712502
[INFO] Epoch: 4 , batch: 194 , training loss: 4.609820
[INFO] Epoch: 4 , batch: 195 , training loss: 4.684052
[INFO] Epoch: 4 , batch: 196 , training loss: 4.444457
[INFO] Epoch: 4 , batch: 197 , training loss: 4.638629
[INFO] Epoch: 4 , batch: 198 , training loss: 4.465682
[INFO] Epoch: 4 , batch: 199 , training loss: 4.564418
[INFO] Epoch: 4 , batch: 200 , training loss: 4.485857
[INFO] Epoch: 4 , batch: 201 , training loss: 4.408654
[INFO] Epoch: 4 , batch: 202 , training loss: 4.360254
[INFO] Epoch: 4 , batch: 203 , training loss: 4.367681
[INFO] Epoch: 4 , batch: 204 , training loss: 4.543445
[INFO] Epoch: 4 , batch: 205 , training loss: 4.114053
[INFO] Epoch: 4 , batch: 206 , training loss: 4.004365
[INFO] Epoch: 4 , batch: 207 , training loss: 4.042704
[INFO] Epoch: 4 , batch: 208 , training loss: 4.455131
[INFO] Epoch: 4 , batch: 209 , training loss: 4.345104
[INFO] Epoch: 4 , batch: 210 , training loss: 4.397592
[INFO] Epoch: 4 , batch: 211 , training loss: 4.362154
[INFO] Epoch: 4 , batch: 212 , training loss: 4.462465
[INFO] Epoch: 4 , batch: 213 , training loss: 4.438067
[INFO] Epoch: 4 , batch: 214 , training loss: 4.484457
[INFO] Epoch: 4 , batch: 215 , training loss: 4.716519
[INFO] Epoch: 4 , batch: 216 , training loss: 4.440288
[INFO] Epoch: 4 , batch: 217 , training loss: 4.339917
[INFO] Epoch: 4 , batch: 218 , training loss: 4.352545
[INFO] Epoch: 4 , batch: 219 , training loss: 4.467513
[INFO] Epoch: 4 , batch: 220 , training loss: 4.259281
[INFO] Epoch: 4 , batch: 221 , training loss: 4.291958
[INFO] Epoch: 4 , batch: 222 , training loss: 4.447363
[INFO] Epoch: 4 , batch: 223 , training loss: 4.512881
[INFO] Epoch: 4 , batch: 224 , training loss: 4.589264
[INFO] Epoch: 4 , batch: 225 , training loss: 4.434910
[INFO] Epoch: 4 , batch: 226 , training loss: 4.563915
[INFO] Epoch: 4 , batch: 227 , training loss: 4.526932
[INFO] Epoch: 4 , batch: 228 , training loss: 4.591864
[INFO] Epoch: 4 , batch: 229 , training loss: 4.470978
[INFO] Epoch: 4 , batch: 230 , training loss: 4.284029
[INFO] Epoch: 4 , batch: 231 , training loss: 4.119200
[INFO] Epoch: 4 , batch: 232 , training loss: 4.275996
[INFO] Epoch: 4 , batch: 233 , training loss: 4.304465
[INFO] Epoch: 4 , batch: 234 , training loss: 3.991926
[INFO] Epoch: 4 , batch: 235 , training loss: 4.087675
[INFO] Epoch: 4 , batch: 236 , training loss: 4.290868
[INFO] Epoch: 4 , batch: 237 , training loss: 4.423868
[INFO] Epoch: 4 , batch: 238 , training loss: 4.158249
[INFO] Epoch: 4 , batch: 239 , training loss: 4.256281
[INFO] Epoch: 4 , batch: 240 , training loss: 4.290115
[INFO] Epoch: 4 , batch: 241 , training loss: 4.103971
[INFO] Epoch: 4 , batch: 242 , training loss: 4.101017
[INFO] Epoch: 4 , batch: 243 , training loss: 4.451845
[INFO] Epoch: 4 , batch: 244 , training loss: 4.328485
[INFO] Epoch: 4 , batch: 245 , training loss: 4.352637
[INFO] Epoch: 4 , batch: 246 , training loss: 4.006238
[INFO] Epoch: 4 , batch: 247 , training loss: 4.166342
[INFO] Epoch: 4 , batch: 248 , training loss: 4.267953
[INFO] Epoch: 4 , batch: 249 , training loss: 4.231843
[INFO] Epoch: 4 , batch: 250 , training loss: 3.994485
[INFO] Epoch: 4 , batch: 251 , training loss: 4.519989
[INFO] Epoch: 4 , batch: 252 , training loss: 4.191820
[INFO] Epoch: 4 , batch: 253 , training loss: 4.161036
[INFO] Epoch: 4 , batch: 254 , training loss: 4.464785
[INFO] Epoch: 4 , batch: 255 , training loss: 4.452409
[INFO] Epoch: 4 , batch: 256 , training loss: 4.388666
[INFO] Epoch: 4 , batch: 257 , training loss: 4.597521
[INFO] Epoch: 4 , batch: 258 , training loss: 4.638647
[INFO] Epoch: 4 , batch: 259 , training loss: 4.668248
[INFO] Epoch: 4 , batch: 260 , training loss: 4.331226
[INFO] Epoch: 4 , batch: 261 , training loss: 4.515486
[INFO] Epoch: 4 , batch: 262 , training loss: 4.758340
[INFO] Epoch: 4 , batch: 263 , training loss: 4.892219
[INFO] Epoch: 4 , batch: 264 , training loss: 4.186523
[INFO] Epoch: 4 , batch: 265 , training loss: 4.341681
[INFO] Epoch: 4 , batch: 266 , training loss: 4.832890
[INFO] Epoch: 4 , batch: 267 , training loss: 4.519756
[INFO] Epoch: 4 , batch: 268 , training loss: 4.376014
[INFO] Epoch: 4 , batch: 269 , training loss: 4.429447
[INFO] Epoch: 4 , batch: 270 , training loss: 4.427928
[INFO] Epoch: 4 , batch: 271 , training loss: 4.454243
[INFO] Epoch: 4 , batch: 272 , training loss: 4.430164
[INFO] Epoch: 4 , batch: 273 , training loss: 4.425035
[INFO] Epoch: 4 , batch: 274 , training loss: 4.542356
[INFO] Epoch: 4 , batch: 275 , training loss: 4.447828
[INFO] Epoch: 4 , batch: 276 , training loss: 4.491087
[INFO] Epoch: 4 , batch: 277 , training loss: 4.637274
[INFO] Epoch: 4 , batch: 278 , training loss: 4.236358
[INFO] Epoch: 4 , batch: 279 , training loss: 4.276967
[INFO] Epoch: 4 , batch: 280 , training loss: 4.217053
[INFO] Epoch: 4 , batch: 281 , training loss: 4.355058
[INFO] Epoch: 4 , batch: 282 , training loss: 4.254167
[INFO] Epoch: 4 , batch: 283 , training loss: 4.307792
[INFO] Epoch: 4 , batch: 284 , training loss: 4.327878
[INFO] Epoch: 4 , batch: 285 , training loss: 4.323273
[INFO] Epoch: 4 , batch: 286 , training loss: 4.293108
[INFO] Epoch: 4 , batch: 287 , training loss: 4.176471
[INFO] Epoch: 4 , batch: 288 , training loss: 4.223788
[INFO] Epoch: 4 , batch: 289 , training loss: 4.261023
[INFO] Epoch: 4 , batch: 290 , training loss: 4.052171
[INFO] Epoch: 4 , batch: 291 , training loss: 4.007977
[INFO] Epoch: 4 , batch: 292 , training loss: 4.110221
[INFO] Epoch: 4 , batch: 293 , training loss: 4.063283
[INFO] Epoch: 4 , batch: 294 , training loss: 4.777097
[INFO] Epoch: 4 , batch: 295 , training loss: 4.545835
[INFO] Epoch: 4 , batch: 296 , training loss: 4.447083
[INFO] Epoch: 4 , batch: 297 , training loss: 4.407860
[INFO] Epoch: 4 , batch: 298 , training loss: 4.242422
[INFO] Epoch: 4 , batch: 299 , training loss: 4.243067
[INFO] Epoch: 4 , batch: 300 , training loss: 4.224919
[INFO] Epoch: 4 , batch: 301 , training loss: 4.158861
[INFO] Epoch: 4 , batch: 302 , training loss: 4.345285
[INFO] Epoch: 4 , batch: 303 , training loss: 4.342330
[INFO] Epoch: 4 , batch: 304 , training loss: 4.556716
[INFO] Epoch: 4 , batch: 305 , training loss: 4.264299
[INFO] Epoch: 4 , batch: 306 , training loss: 4.388966
[INFO] Epoch: 4 , batch: 307 , training loss: 4.390007
[INFO] Epoch: 4 , batch: 308 , training loss: 4.269288
[INFO] Epoch: 4 , batch: 309 , training loss: 4.275425
[INFO] Epoch: 4 , batch: 310 , training loss: 4.104148
[INFO] Epoch: 4 , batch: 311 , training loss: 4.142434
[INFO] Epoch: 4 , batch: 312 , training loss: 4.008662
[INFO] Epoch: 4 , batch: 313 , training loss: 4.197338
[INFO] Epoch: 4 , batch: 314 , training loss: 4.239892
[INFO] Epoch: 4 , batch: 315 , training loss: 4.294978
[INFO] Epoch: 4 , batch: 316 , training loss: 4.631794
[INFO] Epoch: 4 , batch: 317 , training loss: 5.228080
[INFO] Epoch: 4 , batch: 318 , training loss: 5.349808
[INFO] Epoch: 4 , batch: 319 , training loss: 4.792184
[INFO] Epoch: 4 , batch: 320 , training loss: 4.302521
[INFO] Epoch: 4 , batch: 321 , training loss: 4.080911
[INFO] Epoch: 4 , batch: 322 , training loss: 4.210937
[INFO] Epoch: 4 , batch: 323 , training loss: 4.243400
[INFO] Epoch: 4 , batch: 324 , training loss: 4.222126
[INFO] Epoch: 4 , batch: 325 , training loss: 4.413169
[INFO] Epoch: 4 , batch: 326 , training loss: 4.478994
[INFO] Epoch: 4 , batch: 327 , training loss: 4.341114
[INFO] Epoch: 4 , batch: 328 , training loss: 4.351897
[INFO] Epoch: 4 , batch: 329 , training loss: 4.249704
[INFO] Epoch: 4 , batch: 330 , training loss: 4.202847
[INFO] Epoch: 4 , batch: 331 , training loss: 4.385720
[INFO] Epoch: 4 , batch: 332 , training loss: 4.170045
[INFO] Epoch: 4 , batch: 333 , training loss: 4.141251
[INFO] Epoch: 4 , batch: 334 , training loss: 4.250182
[INFO] Epoch: 4 , batch: 335 , training loss: 4.370049
[INFO] Epoch: 4 , batch: 336 , training loss: 4.319534
[INFO] Epoch: 4 , batch: 337 , training loss: 4.421795
[INFO] Epoch: 4 , batch: 338 , training loss: 4.600663
[INFO] Epoch: 4 , batch: 339 , training loss: 4.432748
[INFO] Epoch: 4 , batch: 340 , training loss: 4.622546
[INFO] Epoch: 4 , batch: 341 , training loss: 4.339309
[INFO] Epoch: 4 , batch: 342 , training loss: 4.165062
[INFO] Epoch: 4 , batch: 343 , training loss: 4.256534
[INFO] Epoch: 4 , batch: 344 , training loss: 4.101526
[INFO] Epoch: 4 , batch: 345 , training loss: 4.227086
[INFO] Epoch: 4 , batch: 346 , training loss: 4.322118
[INFO] Epoch: 4 , batch: 347 , training loss: 4.180406
[INFO] Epoch: 4 , batch: 348 , training loss: 4.411687
[INFO] Epoch: 4 , batch: 349 , training loss: 4.474268
[INFO] Epoch: 4 , batch: 350 , training loss: 4.200674
[INFO] Epoch: 4 , batch: 351 , training loss: 4.272417
[INFO] Epoch: 4 , batch: 352 , training loss: 4.292702
[INFO] Epoch: 4 , batch: 353 , training loss: 4.263124
[INFO] Epoch: 4 , batch: 354 , training loss: 4.361830
[INFO] Epoch: 4 , batch: 355 , training loss: 4.424476
[INFO] Epoch: 4 , batch: 356 , training loss: 4.272845
[INFO] Epoch: 4 , batch: 357 , training loss: 4.350236
[INFO] Epoch: 4 , batch: 358 , training loss: 4.312460
[INFO] Epoch: 4 , batch: 359 , training loss: 4.270704
[INFO] Epoch: 4 , batch: 360 , training loss: 4.319978
[INFO] Epoch: 4 , batch: 361 , training loss: 4.284725
[INFO] Epoch: 4 , batch: 362 , training loss: 4.388164
[INFO] Epoch: 4 , batch: 363 , training loss: 4.276043
[INFO] Epoch: 4 , batch: 364 , training loss: 4.301600
[INFO] Epoch: 4 , batch: 365 , training loss: 4.211447
[INFO] Epoch: 4 , batch: 366 , training loss: 4.381591
[INFO] Epoch: 4 , batch: 367 , training loss: 4.446774
[INFO] Epoch: 4 , batch: 368 , training loss: 5.042429
[INFO] Epoch: 4 , batch: 369 , training loss: 4.583156
[INFO] Epoch: 4 , batch: 370 , training loss: 4.349313
[INFO] Epoch: 4 , batch: 371 , training loss: 4.937749
[INFO] Epoch: 4 , batch: 372 , training loss: 5.186448
[INFO] Epoch: 4 , batch: 373 , training loss: 5.178958
[INFO] Epoch: 4 , batch: 374 , training loss: 5.222836
[INFO] Epoch: 4 , batch: 375 , training loss: 5.221821
[INFO] Epoch: 4 , batch: 376 , training loss: 5.087106
[INFO] Epoch: 4 , batch: 377 , training loss: 4.743862
[INFO] Epoch: 4 , batch: 378 , training loss: 4.925692
[INFO] Epoch: 4 , batch: 379 , training loss: 4.914966
[INFO] Epoch: 4 , batch: 380 , training loss: 5.012832
[INFO] Epoch: 4 , batch: 381 , training loss: 4.789851
[INFO] Epoch: 4 , batch: 382 , training loss: 4.999228
[INFO] Epoch: 4 , batch: 383 , training loss: 5.107613
[INFO] Epoch: 4 , batch: 384 , training loss: 5.211179
[INFO] Epoch: 4 , batch: 385 , training loss: 4.969733
[INFO] Epoch: 4 , batch: 386 , training loss: 5.121821
[INFO] Epoch: 4 , batch: 387 , training loss: 5.046744
[INFO] Epoch: 4 , batch: 388 , training loss: 4.819803
[INFO] Epoch: 4 , batch: 389 , training loss: 4.628133
[INFO] Epoch: 4 , batch: 390 , training loss: 4.577183
[INFO] Epoch: 4 , batch: 391 , training loss: 4.636744
[INFO] Epoch: 4 , batch: 392 , training loss: 4.985426
[INFO] Epoch: 4 , batch: 393 , training loss: 4.874351
[INFO] Epoch: 4 , batch: 394 , training loss: 4.912071
[INFO] Epoch: 4 , batch: 395 , training loss: 4.734164
[INFO] Epoch: 4 , batch: 396 , training loss: 4.523240
[INFO] Epoch: 4 , batch: 397 , training loss: 4.700006
[INFO] Epoch: 4 , batch: 398 , training loss: 4.528145
[INFO] Epoch: 4 , batch: 399 , training loss: 4.559964
[INFO] Epoch: 4 , batch: 400 , training loss: 4.513264
[INFO] Epoch: 4 , batch: 401 , training loss: 4.949363
[INFO] Epoch: 4 , batch: 402 , training loss: 4.702515
[INFO] Epoch: 4 , batch: 403 , training loss: 4.561622
[INFO] Epoch: 4 , batch: 404 , training loss: 4.709414
[INFO] Epoch: 4 , batch: 405 , training loss: 4.792077
[INFO] Epoch: 4 , batch: 406 , training loss: 4.657208
[INFO] Epoch: 4 , batch: 407 , training loss: 4.729371
[INFO] Epoch: 4 , batch: 408 , training loss: 4.661165
[INFO] Epoch: 4 , batch: 409 , training loss: 4.663333
[INFO] Epoch: 4 , batch: 410 , training loss: 4.702654
[INFO] Epoch: 4 , batch: 411 , training loss: 4.929739
[INFO] Epoch: 4 , batch: 412 , training loss: 4.740912
[INFO] Epoch: 4 , batch: 413 , training loss: 4.589185
[INFO] Epoch: 4 , batch: 414 , training loss: 4.626449
[INFO] Epoch: 4 , batch: 415 , training loss: 4.644509
[INFO] Epoch: 4 , batch: 416 , training loss: 4.721727
[INFO] Epoch: 4 , batch: 417 , training loss: 4.645716
[INFO] Epoch: 4 , batch: 418 , training loss: 4.670980
[INFO] Epoch: 4 , batch: 419 , training loss: 4.607363
[INFO] Epoch: 4 , batch: 420 , training loss: 4.589454
[INFO] Epoch: 4 , batch: 421 , training loss: 4.592033
[INFO] Epoch: 4 , batch: 422 , training loss: 4.482319
[INFO] Epoch: 4 , batch: 423 , training loss: 4.727594
[INFO] Epoch: 4 , batch: 424 , training loss: 4.871403
[INFO] Epoch: 4 , batch: 425 , training loss: 4.749676
[INFO] Epoch: 4 , batch: 426 , training loss: 4.436382
[INFO] Epoch: 4 , batch: 427 , training loss: 4.714441
[INFO] Epoch: 4 , batch: 428 , training loss: 4.616120
[INFO] Epoch: 4 , batch: 429 , training loss: 4.416311
[INFO] Epoch: 4 , batch: 430 , training loss: 4.695542
[INFO] Epoch: 4 , batch: 431 , training loss: 4.254290
[INFO] Epoch: 4 , batch: 432 , training loss: 4.365230
[INFO] Epoch: 4 , batch: 433 , training loss: 4.355140
[INFO] Epoch: 4 , batch: 434 , training loss: 4.236855
[INFO] Epoch: 4 , batch: 435 , training loss: 4.604935
[INFO] Epoch: 4 , batch: 436 , training loss: 4.718583
[INFO] Epoch: 4 , batch: 437 , training loss: 4.483333
[INFO] Epoch: 4 , batch: 438 , training loss: 4.262276
[INFO] Epoch: 4 , batch: 439 , training loss: 4.507362
[INFO] Epoch: 4 , batch: 440 , training loss: 4.653721
[INFO] Epoch: 4 , batch: 441 , training loss: 4.703074
[INFO] Epoch: 4 , batch: 442 , training loss: 4.520126
[INFO] Epoch: 4 , batch: 443 , training loss: 4.724132
[INFO] Epoch: 4 , batch: 444 , training loss: 4.333696
[INFO] Epoch: 4 , batch: 445 , training loss: 4.198299
[INFO] Epoch: 4 , batch: 446 , training loss: 4.066902
[INFO] Epoch: 4 , batch: 447 , training loss: 4.330251
[INFO] Epoch: 4 , batch: 448 , training loss: 4.507809
[INFO] Epoch: 4 , batch: 449 , training loss: 4.909284
[INFO] Epoch: 4 , batch: 450 , training loss: 4.974061
[INFO] Epoch: 4 , batch: 451 , training loss: 4.849817
[INFO] Epoch: 4 , batch: 452 , training loss: 4.600261
[INFO] Epoch: 4 , batch: 453 , training loss: 4.383111
[INFO] Epoch: 4 , batch: 454 , training loss: 4.527148
[INFO] Epoch: 4 , batch: 455 , training loss: 4.565852
[INFO] Epoch: 4 , batch: 456 , training loss: 4.542874
[INFO] Epoch: 4 , batch: 457 , training loss: 4.666828
[INFO] Epoch: 4 , batch: 458 , training loss: 4.370022
[INFO] Epoch: 4 , batch: 459 , training loss: 4.350101
[INFO] Epoch: 4 , batch: 460 , training loss: 4.481843
[INFO] Epoch: 4 , batch: 461 , training loss: 4.505543
[INFO] Epoch: 4 , batch: 462 , training loss: 4.546524
[INFO] Epoch: 4 , batch: 463 , training loss: 4.392254
[INFO] Epoch: 4 , batch: 464 , training loss: 4.622713
[INFO] Epoch: 4 , batch: 465 , training loss: 4.546260
[INFO] Epoch: 4 , batch: 466 , training loss: 4.613741
[INFO] Epoch: 4 , batch: 467 , training loss: 4.649837
[INFO] Epoch: 4 , batch: 468 , training loss: 4.596805
[INFO] Epoch: 4 , batch: 469 , training loss: 4.600082
[INFO] Epoch: 4 , batch: 470 , training loss: 4.379000
[INFO] Epoch: 4 , batch: 471 , training loss: 4.502125
[INFO] Epoch: 4 , batch: 472 , training loss: 4.535183
[INFO] Epoch: 4 , batch: 473 , training loss: 4.460258
[INFO] Epoch: 4 , batch: 474 , training loss: 4.299703
[INFO] Epoch: 4 , batch: 475 , training loss: 4.130099
[INFO] Epoch: 4 , batch: 476 , training loss: 4.529742
[INFO] Epoch: 4 , batch: 477 , training loss: 4.644984
[INFO] Epoch: 4 , batch: 478 , training loss: 4.731024
[INFO] Epoch: 4 , batch: 479 , training loss: 4.679891
[INFO] Epoch: 4 , batch: 480 , training loss: 4.808097
[INFO] Epoch: 4 , batch: 481 , training loss: 4.614886
[INFO] Epoch: 4 , batch: 482 , training loss: 4.762127
[INFO] Epoch: 4 , batch: 483 , training loss: 4.591613
[INFO] Epoch: 4 , batch: 484 , training loss: 4.401695
[INFO] Epoch: 4 , batch: 485 , training loss: 4.533530
[INFO] Epoch: 4 , batch: 486 , training loss: 4.397089
[INFO] Epoch: 4 , batch: 487 , training loss: 4.392562
[INFO] Epoch: 4 , batch: 488 , training loss: 4.599900
[INFO] Epoch: 4 , batch: 489 , training loss: 4.450806
[INFO] Epoch: 4 , batch: 490 , training loss: 4.534608
[INFO] Epoch: 4 , batch: 491 , training loss: 4.521569
[INFO] Epoch: 4 , batch: 492 , training loss: 4.392705
[INFO] Epoch: 4 , batch: 493 , training loss: 4.591657
[INFO] Epoch: 4 , batch: 494 , training loss: 4.511755
[INFO] Epoch: 4 , batch: 495 , training loss: 4.623094
[INFO] Epoch: 4 , batch: 496 , training loss: 4.515266
[INFO] Epoch: 4 , batch: 497 , training loss: 4.515871
[INFO] Epoch: 4 , batch: 498 , training loss: 4.565542
[INFO] Epoch: 4 , batch: 499 , training loss: 4.640955
[INFO] Epoch: 4 , batch: 500 , training loss: 4.883193
[INFO] Epoch: 4 , batch: 501 , training loss: 5.309359
[INFO] Epoch: 4 , batch: 502 , training loss: 5.444067
[INFO] Epoch: 4 , batch: 503 , training loss: 5.122831
[INFO] Epoch: 4 , batch: 504 , training loss: 5.187780
[INFO] Epoch: 4 , batch: 505 , training loss: 5.073215
[INFO] Epoch: 4 , batch: 506 , training loss: 5.053832
[INFO] Epoch: 4 , batch: 507 , training loss: 5.095115
[INFO] Epoch: 4 , batch: 508 , training loss: 5.063745
[INFO] Epoch: 4 , batch: 509 , training loss: 4.755034
[INFO] Epoch: 4 , batch: 510 , training loss: 4.813498
[INFO] Epoch: 4 , batch: 511 , training loss: 4.727512
[INFO] Epoch: 4 , batch: 512 , training loss: 4.791971
[INFO] Epoch: 4 , batch: 513 , training loss: 5.042135
[INFO] Epoch: 4 , batch: 514 , training loss: 4.710183
[INFO] Epoch: 4 , batch: 515 , training loss: 5.002353
[INFO] Epoch: 4 , batch: 516 , training loss: 4.762662
[INFO] Epoch: 4 , batch: 517 , training loss: 4.727619
[INFO] Epoch: 4 , batch: 518 , training loss: 4.694479
[INFO] Epoch: 4 , batch: 519 , training loss: 4.576262
[INFO] Epoch: 4 , batch: 520 , training loss: 4.789958
[INFO] Epoch: 4 , batch: 521 , training loss: 4.812334
[INFO] Epoch: 4 , batch: 522 , training loss: 4.872633
[INFO] Epoch: 4 , batch: 523 , training loss: 4.746818
[INFO] Epoch: 4 , batch: 524 , training loss: 4.988243
[INFO] Epoch: 4 , batch: 525 , training loss: 4.932494
[INFO] Epoch: 4 , batch: 526 , training loss: 4.679291
[INFO] Epoch: 4 , batch: 527 , training loss: 4.708697
[INFO] Epoch: 4 , batch: 528 , training loss: 4.807927
[INFO] Epoch: 4 , batch: 529 , training loss: 4.733807
[INFO] Epoch: 4 , batch: 530 , training loss: 4.555950
[INFO] Epoch: 4 , batch: 531 , training loss: 4.748135
[INFO] Epoch: 4 , batch: 532 , training loss: 4.574919
[INFO] Epoch: 4 , batch: 533 , training loss: 4.768142
[INFO] Epoch: 4 , batch: 534 , training loss: 4.756778
[INFO] Epoch: 4 , batch: 535 , training loss: 4.768507
[INFO] Epoch: 4 , batch: 536 , training loss: 4.619022
[INFO] Epoch: 4 , batch: 537 , training loss: 4.577209
[INFO] Epoch: 4 , batch: 538 , training loss: 4.657579
[INFO] Epoch: 4 , batch: 539 , training loss: 4.836234
[INFO] Epoch: 4 , batch: 540 , training loss: 5.469140
[INFO] Epoch: 4 , batch: 541 , training loss: 5.375324
[INFO] Epoch: 4 , batch: 542 , training loss: 5.150977
[INFO] Epoch: 5 , batch: 0 , training loss: 4.785497
[INFO] Epoch: 5 , batch: 1 , training loss: 4.512996
[INFO] Epoch: 5 , batch: 2 , training loss: 4.399117
[INFO] Epoch: 5 , batch: 3 , training loss: 4.257320
[INFO] Epoch: 5 , batch: 4 , training loss: 4.648371
[INFO] Epoch: 5 , batch: 5 , training loss: 4.249228
[INFO] Epoch: 5 , batch: 6 , training loss: 4.812891
[INFO] Epoch: 5 , batch: 7 , training loss: 4.386424
[INFO] Epoch: 5 , batch: 8 , training loss: 4.108850
[INFO] Epoch: 5 , batch: 9 , training loss: 4.246455
[INFO] Epoch: 5 , batch: 10 , training loss: 4.191227
[INFO] Epoch: 5 , batch: 11 , training loss: 4.102552
[INFO] Epoch: 5 , batch: 12 , training loss: 4.122103
[INFO] Epoch: 5 , batch: 13 , training loss: 4.057038
[INFO] Epoch: 5 , batch: 14 , training loss: 3.846488
[INFO] Epoch: 5 , batch: 15 , training loss: 4.147623
[INFO] Epoch: 5 , batch: 16 , training loss: 3.990452
[INFO] Epoch: 5 , batch: 17 , training loss: 4.172080
[INFO] Epoch: 5 , batch: 18 , training loss: 4.074979
[INFO] Epoch: 5 , batch: 19 , training loss: 3.849896
[INFO] Epoch: 5 , batch: 20 , training loss: 3.823674
[INFO] Epoch: 5 , batch: 21 , training loss: 3.939617
[INFO] Epoch: 5 , batch: 22 , training loss: 4.009722
[INFO] Epoch: 5 , batch: 23 , training loss: 4.156137
[INFO] Epoch: 5 , batch: 24 , training loss: 3.917806
[INFO] Epoch: 5 , batch: 25 , training loss: 4.146542
[INFO] Epoch: 5 , batch: 26 , training loss: 3.949751
[INFO] Epoch: 5 , batch: 27 , training loss: 3.910901
[INFO] Epoch: 5 , batch: 28 , training loss: 4.166362
[INFO] Epoch: 5 , batch: 29 , training loss: 3.912558
[INFO] Epoch: 5 , batch: 30 , training loss: 3.927204
[INFO] Epoch: 5 , batch: 31 , training loss: 4.048230
[INFO] Epoch: 5 , batch: 32 , training loss: 4.061591
[INFO] Epoch: 5 , batch: 33 , training loss: 4.133228
[INFO] Epoch: 5 , batch: 34 , training loss: 4.176750
[INFO] Epoch: 5 , batch: 35 , training loss: 4.086479
[INFO] Epoch: 5 , batch: 36 , training loss: 4.020391
[INFO] Epoch: 5 , batch: 37 , training loss: 3.939170
[INFO] Epoch: 5 , batch: 38 , training loss: 4.091096
[INFO] Epoch: 5 , batch: 39 , training loss: 3.897873
[INFO] Epoch: 5 , batch: 40 , training loss: 3.986038
[INFO] Epoch: 5 , batch: 41 , training loss: 4.171787
[INFO] Epoch: 5 , batch: 42 , training loss: 4.783628
[INFO] Epoch: 5 , batch: 43 , training loss: 4.420005
[INFO] Epoch: 5 , batch: 44 , training loss: 4.554827
[INFO] Epoch: 5 , batch: 45 , training loss: 4.727767
[INFO] Epoch: 5 , batch: 46 , training loss: 5.027696
[INFO] Epoch: 5 , batch: 47 , training loss: 4.287322
[INFO] Epoch: 5 , batch: 48 , training loss: 4.396046
[INFO] Epoch: 5 , batch: 49 , training loss: 4.388232
[INFO] Epoch: 5 , batch: 50 , training loss: 4.199918
[INFO] Epoch: 5 , batch: 51 , training loss: 4.251019
[INFO] Epoch: 5 , batch: 52 , training loss: 4.051233
[INFO] Epoch: 5 , batch: 53 , training loss: 4.221966
[INFO] Epoch: 5 , batch: 54 , training loss: 4.253968
[INFO] Epoch: 5 , batch: 55 , training loss: 4.302395
[INFO] Epoch: 5 , batch: 56 , training loss: 4.127378
[INFO] Epoch: 5 , batch: 57 , training loss: 3.987516
[INFO] Epoch: 5 , batch: 58 , training loss: 4.063526
[INFO] Epoch: 5 , batch: 59 , training loss: 4.181066
[INFO] Epoch: 5 , batch: 60 , training loss: 4.008152
[INFO] Epoch: 5 , batch: 61 , training loss: 4.103683
[INFO] Epoch: 5 , batch: 62 , training loss: 4.026891
[INFO] Epoch: 5 , batch: 63 , training loss: 4.219295
[INFO] Epoch: 5 , batch: 64 , training loss: 4.368782
[INFO] Epoch: 5 , batch: 65 , training loss: 4.071601
[INFO] Epoch: 5 , batch: 66 , training loss: 3.931675
[INFO] Epoch: 5 , batch: 67 , training loss: 3.969363
[INFO] Epoch: 5 , batch: 68 , training loss: 4.272431
[INFO] Epoch: 5 , batch: 69 , training loss: 4.026952
[INFO] Epoch: 5 , batch: 70 , training loss: 4.310979
[INFO] Epoch: 5 , batch: 71 , training loss: 4.117365
[INFO] Epoch: 5 , batch: 72 , training loss: 4.209566
[INFO] Epoch: 5 , batch: 73 , training loss: 4.115489
[INFO] Epoch: 5 , batch: 74 , training loss: 4.277500
[INFO] Epoch: 5 , batch: 75 , training loss: 4.028924
[INFO] Epoch: 5 , batch: 76 , training loss: 4.187699
[INFO] Epoch: 5 , batch: 77 , training loss: 4.093426
[INFO] Epoch: 5 , batch: 78 , training loss: 4.181098
[INFO] Epoch: 5 , batch: 79 , training loss: 3.988581
[INFO] Epoch: 5 , batch: 80 , training loss: 4.225636
[INFO] Epoch: 5 , batch: 81 , training loss: 4.189396
[INFO] Epoch: 5 , batch: 82 , training loss: 4.182657
[INFO] Epoch: 5 , batch: 83 , training loss: 4.263830
[INFO] Epoch: 5 , batch: 84 , training loss: 4.191329
[INFO] Epoch: 5 , batch: 85 , training loss: 4.246886
[INFO] Epoch: 5 , batch: 86 , training loss: 4.239481
[INFO] Epoch: 5 , batch: 87 , training loss: 4.216116
[INFO] Epoch: 5 , batch: 88 , training loss: 4.364277
[INFO] Epoch: 5 , batch: 89 , training loss: 4.146436
[INFO] Epoch: 5 , batch: 90 , training loss: 4.177329
[INFO] Epoch: 5 , batch: 91 , training loss: 4.123352
[INFO] Epoch: 5 , batch: 92 , training loss: 4.120289
[INFO] Epoch: 5 , batch: 93 , training loss: 4.227711
[INFO] Epoch: 5 , batch: 94 , training loss: 4.420252
[INFO] Epoch: 5 , batch: 95 , training loss: 4.171975
[INFO] Epoch: 5 , batch: 96 , training loss: 4.120866
[INFO] Epoch: 5 , batch: 97 , training loss: 4.114373
[INFO] Epoch: 5 , batch: 98 , training loss: 4.077629
[INFO] Epoch: 5 , batch: 99 , training loss: 4.160020
[INFO] Epoch: 5 , batch: 100 , training loss: 3.998870
[INFO] Epoch: 5 , batch: 101 , training loss: 4.088634
[INFO] Epoch: 5 , batch: 102 , training loss: 4.236354
[INFO] Epoch: 5 , batch: 103 , training loss: 3.946012
[INFO] Epoch: 5 , batch: 104 , training loss: 3.981543
[INFO] Epoch: 5 , batch: 105 , training loss: 4.272280
[INFO] Epoch: 5 , batch: 106 , training loss: 4.213387
[INFO] Epoch: 5 , batch: 107 , training loss: 4.105580
[INFO] Epoch: 5 , batch: 108 , training loss: 4.008985
[INFO] Epoch: 5 , batch: 109 , training loss: 3.878257
[INFO] Epoch: 5 , batch: 110 , training loss: 4.151801
[INFO] Epoch: 5 , batch: 111 , training loss: 4.206242
[INFO] Epoch: 5 , batch: 112 , training loss: 4.159298
[INFO] Epoch: 5 , batch: 113 , training loss: 4.088601
[INFO] Epoch: 5 , batch: 114 , training loss: 4.218416
[INFO] Epoch: 5 , batch: 115 , training loss: 4.174100
[INFO] Epoch: 5 , batch: 116 , training loss: 4.082120
[INFO] Epoch: 5 , batch: 117 , training loss: 4.362878
[INFO] Epoch: 5 , batch: 118 , training loss: 4.287439
[INFO] Epoch: 5 , batch: 119 , training loss: 4.423862
[INFO] Epoch: 5 , batch: 120 , training loss: 4.383956
[INFO] Epoch: 5 , batch: 121 , training loss: 4.210203
[INFO] Epoch: 5 , batch: 122 , training loss: 4.126059
[INFO] Epoch: 5 , batch: 123 , training loss: 4.226697
[INFO] Epoch: 5 , batch: 124 , training loss: 4.360332
[INFO] Epoch: 5 , batch: 125 , training loss: 4.033222
[INFO] Epoch: 5 , batch: 126 , training loss: 4.031345
[INFO] Epoch: 5 , batch: 127 , training loss: 4.064512
[INFO] Epoch: 5 , batch: 128 , training loss: 4.282816
[INFO] Epoch: 5 , batch: 129 , training loss: 4.188585
[INFO] Epoch: 5 , batch: 130 , training loss: 4.180827
[INFO] Epoch: 5 , batch: 131 , training loss: 4.193987
[INFO] Epoch: 5 , batch: 132 , training loss: 4.206190
[INFO] Epoch: 5 , batch: 133 , training loss: 4.137924
[INFO] Epoch: 5 , batch: 134 , training loss: 3.948632
[INFO] Epoch: 5 , batch: 135 , training loss: 3.947228
[INFO] Epoch: 5 , batch: 136 , training loss: 4.287459
[INFO] Epoch: 5 , batch: 137 , training loss: 4.182888
[INFO] Epoch: 5 , batch: 138 , training loss: 4.353980
[INFO] Epoch: 5 , batch: 139 , training loss: 5.043190
[INFO] Epoch: 5 , batch: 140 , training loss: 4.867951
[INFO] Epoch: 5 , batch: 141 , training loss: 4.581678
[INFO] Epoch: 5 , batch: 142 , training loss: 4.199004
[INFO] Epoch: 5 , batch: 143 , training loss: 4.289530
[INFO] Epoch: 5 , batch: 144 , training loss: 4.069029
[INFO] Epoch: 5 , batch: 145 , training loss: 4.245230
[INFO] Epoch: 5 , batch: 146 , training loss: 4.421944
[INFO] Epoch: 5 , batch: 147 , training loss: 4.048648
[INFO] Epoch: 5 , batch: 148 , training loss: 4.044528
[INFO] Epoch: 5 , batch: 149 , training loss: 4.116337
[INFO] Epoch: 5 , batch: 150 , training loss: 4.381925
[INFO] Epoch: 5 , batch: 151 , training loss: 4.106210
[INFO] Epoch: 5 , batch: 152 , training loss: 4.189542
[INFO] Epoch: 5 , batch: 153 , training loss: 4.267502
[INFO] Epoch: 5 , batch: 154 , training loss: 4.341102
[INFO] Epoch: 5 , batch: 155 , training loss: 4.634131
[INFO] Epoch: 5 , batch: 156 , training loss: 4.221179
[INFO] Epoch: 5 , batch: 157 , training loss: 4.207136
[INFO] Epoch: 5 , batch: 158 , training loss: 4.607121
[INFO] Epoch: 5 , batch: 159 , training loss: 4.472738
[INFO] Epoch: 5 , batch: 160 , training loss: 4.888785
[INFO] Epoch: 5 , batch: 161 , training loss: 4.712587
[INFO] Epoch: 5 , batch: 162 , training loss: 4.603776
[INFO] Epoch: 5 , batch: 163 , training loss: 4.727725
[INFO] Epoch: 5 , batch: 164 , training loss: 4.673074
[INFO] Epoch: 5 , batch: 165 , training loss: 4.588555
[INFO] Epoch: 5 , batch: 166 , training loss: 4.786325
[INFO] Epoch: 5 , batch: 167 , training loss: 5.202796
[INFO] Epoch: 5 , batch: 168 , training loss: 4.968063
[INFO] Epoch: 5 , batch: 169 , training loss: 4.821397
[INFO] Epoch: 5 , batch: 170 , training loss: 4.851945
[INFO] Epoch: 5 , batch: 171 , training loss: 4.424061
[INFO] Epoch: 5 , batch: 172 , training loss: 4.632119
[INFO] Epoch: 5 , batch: 173 , training loss: 4.883674
[INFO] Epoch: 5 , batch: 174 , training loss: 5.067530
[INFO] Epoch: 5 , batch: 175 , training loss: 5.291970
[INFO] Epoch: 5 , batch: 176 , training loss: 5.052675
[INFO] Epoch: 5 , batch: 177 , training loss: 4.656553
[INFO] Epoch: 5 , batch: 178 , training loss: 4.597874
[INFO] Epoch: 5 , batch: 179 , training loss: 4.599598
[INFO] Epoch: 5 , batch: 180 , training loss: 4.571982
[INFO] Epoch: 5 , batch: 181 , training loss: 4.770357
[INFO] Epoch: 5 , batch: 182 , training loss: 4.709176
[INFO] Epoch: 5 , batch: 183 , training loss: 4.677141
[INFO] Epoch: 5 , batch: 184 , training loss: 4.530938
[INFO] Epoch: 5 , batch: 185 , training loss: 4.496044
[INFO] Epoch: 5 , batch: 186 , training loss: 4.639853
[INFO] Epoch: 5 , batch: 187 , training loss: 4.769991
[INFO] Epoch: 5 , batch: 188 , training loss: 4.742876
[INFO] Epoch: 5 , batch: 189 , training loss: 4.618271
[INFO] Epoch: 5 , batch: 190 , training loss: 4.591684
[INFO] Epoch: 5 , batch: 191 , training loss: 4.774057
[INFO] Epoch: 5 , batch: 192 , training loss: 4.488472
[INFO] Epoch: 5 , batch: 193 , training loss: 4.651947
[INFO] Epoch: 5 , batch: 194 , training loss: 4.560670
[INFO] Epoch: 5 , batch: 195 , training loss: 4.585087
[INFO] Epoch: 5 , batch: 196 , training loss: 4.360887
[INFO] Epoch: 5 , batch: 197 , training loss: 4.555004
[INFO] Epoch: 5 , batch: 198 , training loss: 4.393355
[INFO] Epoch: 5 , batch: 199 , training loss: 4.494829
[INFO] Epoch: 5 , batch: 200 , training loss: 4.420120
[INFO] Epoch: 5 , batch: 201 , training loss: 4.327152
[INFO] Epoch: 5 , batch: 202 , training loss: 4.296088
[INFO] Epoch: 5 , batch: 203 , training loss: 4.330709
[INFO] Epoch: 5 , batch: 204 , training loss: 4.481718
[INFO] Epoch: 5 , batch: 205 , training loss: 4.065118
[INFO] Epoch: 5 , batch: 206 , training loss: 3.961814
[INFO] Epoch: 5 , batch: 207 , training loss: 3.988823
[INFO] Epoch: 5 , batch: 208 , training loss: 4.379982
[INFO] Epoch: 5 , batch: 209 , training loss: 4.280879
[INFO] Epoch: 5 , batch: 210 , training loss: 4.341452
[INFO] Epoch: 5 , batch: 211 , training loss: 4.312463
[INFO] Epoch: 5 , batch: 212 , training loss: 4.398673
[INFO] Epoch: 5 , batch: 213 , training loss: 4.377291
[INFO] Epoch: 5 , batch: 214 , training loss: 4.434121
[INFO] Epoch: 5 , batch: 215 , training loss: 4.675569
[INFO] Epoch: 5 , batch: 216 , training loss: 4.389606
[INFO] Epoch: 5 , batch: 217 , training loss: 4.291026
[INFO] Epoch: 5 , batch: 218 , training loss: 4.288826
[INFO] Epoch: 5 , batch: 219 , training loss: 4.409181
[INFO] Epoch: 5 , batch: 220 , training loss: 4.206163
[INFO] Epoch: 5 , batch: 221 , training loss: 4.245356
[INFO] Epoch: 5 , batch: 222 , training loss: 4.389964
[INFO] Epoch: 5 , batch: 223 , training loss: 4.455478
[INFO] Epoch: 5 , batch: 224 , training loss: 4.526242
[INFO] Epoch: 5 , batch: 225 , training loss: 4.393365
[INFO] Epoch: 5 , batch: 226 , training loss: 4.522594
[INFO] Epoch: 5 , batch: 227 , training loss: 4.483596
[INFO] Epoch: 5 , batch: 228 , training loss: 4.537119
[INFO] Epoch: 5 , batch: 229 , training loss: 4.416074
[INFO] Epoch: 5 , batch: 230 , training loss: 4.244622
[INFO] Epoch: 5 , batch: 231 , training loss: 4.066356
[INFO] Epoch: 5 , batch: 232 , training loss: 4.225157
[INFO] Epoch: 5 , batch: 233 , training loss: 4.247120
[INFO] Epoch: 5 , batch: 234 , training loss: 3.947507
[INFO] Epoch: 5 , batch: 235 , training loss: 4.045114
[INFO] Epoch: 5 , batch: 236 , training loss: 4.227522
[INFO] Epoch: 5 , batch: 237 , training loss: 4.370587
[INFO] Epoch: 5 , batch: 238 , training loss: 4.100447
[INFO] Epoch: 5 , batch: 239 , training loss: 4.187068
[INFO] Epoch: 5 , batch: 240 , training loss: 4.251645
[INFO] Epoch: 5 , batch: 241 , training loss: 4.059756
[INFO] Epoch: 5 , batch: 242 , training loss: 4.059870
[INFO] Epoch: 5 , batch: 243 , training loss: 4.393348
[INFO] Epoch: 5 , batch: 244 , training loss: 4.274657
[INFO] Epoch: 5 , batch: 245 , training loss: 4.300309
[INFO] Epoch: 5 , batch: 246 , training loss: 3.961194
[INFO] Epoch: 5 , batch: 247 , training loss: 4.128103
[INFO] Epoch: 5 , batch: 248 , training loss: 4.232534
[INFO] Epoch: 5 , batch: 249 , training loss: 4.200239
[INFO] Epoch: 5 , batch: 250 , training loss: 3.959530
[INFO] Epoch: 5 , batch: 251 , training loss: 4.469424
[INFO] Epoch: 5 , batch: 252 , training loss: 4.141934
[INFO] Epoch: 5 , batch: 253 , training loss: 4.113346
[INFO] Epoch: 5 , batch: 254 , training loss: 4.417236
[INFO] Epoch: 5 , batch: 255 , training loss: 4.390913
[INFO] Epoch: 5 , batch: 256 , training loss: 4.338620
[INFO] Epoch: 5 , batch: 257 , training loss: 4.537877
[INFO] Epoch: 5 , batch: 258 , training loss: 4.574537
[INFO] Epoch: 5 , batch: 259 , training loss: 4.612517
[INFO] Epoch: 5 , batch: 260 , training loss: 4.283931
[INFO] Epoch: 5 , batch: 261 , training loss: 4.460106
[INFO] Epoch: 5 , batch: 262 , training loss: 4.702901
[INFO] Epoch: 5 , batch: 263 , training loss: 4.844612
[INFO] Epoch: 5 , batch: 264 , training loss: 4.131532
[INFO] Epoch: 5 , batch: 265 , training loss: 4.290541
[INFO] Epoch: 5 , batch: 266 , training loss: 4.768806
[INFO] Epoch: 5 , batch: 267 , training loss: 4.449539
[INFO] Epoch: 5 , batch: 268 , training loss: 4.331972
[INFO] Epoch: 5 , batch: 269 , training loss: 4.372368
[INFO] Epoch: 5 , batch: 270 , training loss: 4.375631
[INFO] Epoch: 5 , batch: 271 , training loss: 4.403210
[INFO] Epoch: 5 , batch: 272 , training loss: 4.380863
[INFO] Epoch: 5 , batch: 273 , training loss: 4.370520
[INFO] Epoch: 5 , batch: 274 , training loss: 4.486295
[INFO] Epoch: 5 , batch: 275 , training loss: 4.390425
[INFO] Epoch: 5 , batch: 276 , training loss: 4.437241
[INFO] Epoch: 5 , batch: 277 , training loss: 4.575695
[INFO] Epoch: 5 , batch: 278 , training loss: 4.195065
[INFO] Epoch: 5 , batch: 279 , training loss: 4.229312
[INFO] Epoch: 5 , batch: 280 , training loss: 4.169137
[INFO] Epoch: 5 , batch: 281 , training loss: 4.318349
[INFO] Epoch: 5 , batch: 282 , training loss: 4.207104
[INFO] Epoch: 5 , batch: 283 , training loss: 4.272077
[INFO] Epoch: 5 , batch: 284 , training loss: 4.286314
[INFO] Epoch: 5 , batch: 285 , training loss: 4.264254
[INFO] Epoch: 5 , batch: 286 , training loss: 4.238516
[INFO] Epoch: 5 , batch: 287 , training loss: 4.136008
[INFO] Epoch: 5 , batch: 288 , training loss: 4.166056
[INFO] Epoch: 5 , batch: 289 , training loss: 4.220329
[INFO] Epoch: 5 , batch: 290 , training loss: 3.998703
[INFO] Epoch: 5 , batch: 291 , training loss: 3.962970
[INFO] Epoch: 5 , batch: 292 , training loss: 4.074733
[INFO] Epoch: 5 , batch: 293 , training loss: 4.006151
[INFO] Epoch: 5 , batch: 294 , training loss: 4.729811
[INFO] Epoch: 5 , batch: 295 , training loss: 4.487870
[INFO] Epoch: 5 , batch: 296 , training loss: 4.404730
[INFO] Epoch: 5 , batch: 297 , training loss: 4.352803
[INFO] Epoch: 5 , batch: 298 , training loss: 4.191986
[INFO] Epoch: 5 , batch: 299 , training loss: 4.203198
[INFO] Epoch: 5 , batch: 300 , training loss: 4.182963
[INFO] Epoch: 5 , batch: 301 , training loss: 4.102772
[INFO] Epoch: 5 , batch: 302 , training loss: 4.304081
[INFO] Epoch: 5 , batch: 303 , training loss: 4.285202
[INFO] Epoch: 5 , batch: 304 , training loss: 4.494823
[INFO] Epoch: 5 , batch: 305 , training loss: 4.215278
[INFO] Epoch: 5 , batch: 306 , training loss: 4.340476
[INFO] Epoch: 5 , batch: 307 , training loss: 4.346656
[INFO] Epoch: 5 , batch: 308 , training loss: 4.210194
[INFO] Epoch: 5 , batch: 309 , training loss: 4.220448
[INFO] Epoch: 5 , batch: 310 , training loss: 4.058921
[INFO] Epoch: 5 , batch: 311 , training loss: 4.084520
[INFO] Epoch: 5 , batch: 312 , training loss: 3.966109
[INFO] Epoch: 5 , batch: 313 , training loss: 4.144082
[INFO] Epoch: 5 , batch: 314 , training loss: 4.190496
[INFO] Epoch: 5 , batch: 315 , training loss: 4.249677
[INFO] Epoch: 5 , batch: 316 , training loss: 4.567818
[INFO] Epoch: 5 , batch: 317 , training loss: 5.155142
[INFO] Epoch: 5 , batch: 318 , training loss: 5.259549
[INFO] Epoch: 5 , batch: 319 , training loss: 4.733068
[INFO] Epoch: 5 , batch: 320 , training loss: 4.253197
[INFO] Epoch: 5 , batch: 321 , training loss: 4.025784
[INFO] Epoch: 5 , batch: 322 , training loss: 4.155253
[INFO] Epoch: 5 , batch: 323 , training loss: 4.186475
[INFO] Epoch: 5 , batch: 324 , training loss: 4.177852
[INFO] Epoch: 5 , batch: 325 , training loss: 4.355054
[INFO] Epoch: 5 , batch: 326 , training loss: 4.421813
[INFO] Epoch: 5 , batch: 327 , training loss: 4.288015
[INFO] Epoch: 5 , batch: 328 , training loss: 4.293451
[INFO] Epoch: 5 , batch: 329 , training loss: 4.205515
[INFO] Epoch: 5 , batch: 330 , training loss: 4.169910
[INFO] Epoch: 5 , batch: 331 , training loss: 4.334369
[INFO] Epoch: 5 , batch: 332 , training loss: 4.134001
[INFO] Epoch: 5 , batch: 333 , training loss: 4.104156
[INFO] Epoch: 5 , batch: 334 , training loss: 4.194362
[INFO] Epoch: 5 , batch: 335 , training loss: 4.322153
[INFO] Epoch: 5 , batch: 336 , training loss: 4.277390
[INFO] Epoch: 5 , batch: 337 , training loss: 4.362474
[INFO] Epoch: 5 , batch: 338 , training loss: 4.560816
[INFO] Epoch: 5 , batch: 339 , training loss: 4.403669
[INFO] Epoch: 5 , batch: 340 , training loss: 4.577995
[INFO] Epoch: 5 , batch: 341 , training loss: 4.302873
[INFO] Epoch: 5 , batch: 342 , training loss: 4.108943
[INFO] Epoch: 5 , batch: 343 , training loss: 4.200279
[INFO] Epoch: 5 , batch: 344 , training loss: 4.050369
[INFO] Epoch: 5 , batch: 345 , training loss: 4.172717
[INFO] Epoch: 5 , batch: 346 , training loss: 4.268763
[INFO] Epoch: 5 , batch: 347 , training loss: 4.132024
[INFO] Epoch: 5 , batch: 348 , training loss: 4.347761
[INFO] Epoch: 5 , batch: 349 , training loss: 4.422156
[INFO] Epoch: 5 , batch: 350 , training loss: 4.152714
[INFO] Epoch: 5 , batch: 351 , training loss: 4.228913
[INFO] Epoch: 5 , batch: 352 , training loss: 4.259465
[INFO] Epoch: 5 , batch: 353 , training loss: 4.212376
[INFO] Epoch: 5 , batch: 354 , training loss: 4.322323
[INFO] Epoch: 5 , batch: 355 , training loss: 4.371818
[INFO] Epoch: 5 , batch: 356 , training loss: 4.227589
[INFO] Epoch: 5 , batch: 357 , training loss: 4.300651
[INFO] Epoch: 5 , batch: 358 , training loss: 4.257117
[INFO] Epoch: 5 , batch: 359 , training loss: 4.208625
[INFO] Epoch: 5 , batch: 360 , training loss: 4.267385
[INFO] Epoch: 5 , batch: 361 , training loss: 4.230739
[INFO] Epoch: 5 , batch: 362 , training loss: 4.355317
[INFO] Epoch: 5 , batch: 363 , training loss: 4.238726
[INFO] Epoch: 5 , batch: 364 , training loss: 4.258501
[INFO] Epoch: 5 , batch: 365 , training loss: 4.169627
[INFO] Epoch: 5 , batch: 366 , training loss: 4.346393
[INFO] Epoch: 5 , batch: 367 , training loss: 4.413724
[INFO] Epoch: 5 , batch: 368 , training loss: 4.988938
[INFO] Epoch: 5 , batch: 369 , training loss: 4.526963
[INFO] Epoch: 5 , batch: 370 , training loss: 4.297785
[INFO] Epoch: 5 , batch: 371 , training loss: 4.860752
[INFO] Epoch: 5 , batch: 372 , training loss: 5.118874
[INFO] Epoch: 5 , batch: 373 , training loss: 5.115156
[INFO] Epoch: 5 , batch: 374 , training loss: 5.154018
[INFO] Epoch: 5 , batch: 375 , training loss: 5.152464
[INFO] Epoch: 5 , batch: 376 , training loss: 5.037172
[INFO] Epoch: 5 , batch: 377 , training loss: 4.709183
[INFO] Epoch: 5 , batch: 378 , training loss: 4.830943
[INFO] Epoch: 5 , batch: 379 , training loss: 4.857181
[INFO] Epoch: 5 , batch: 380 , training loss: 5.016377
[INFO] Epoch: 5 , batch: 381 , training loss: 4.767981
[INFO] Epoch: 5 , batch: 382 , training loss: 4.953003
[INFO] Epoch: 5 , batch: 383 , training loss: 5.052001
[INFO] Epoch: 5 , batch: 384 , training loss: 5.106236
[INFO] Epoch: 5 , batch: 385 , training loss: 4.870056
[INFO] Epoch: 5 , batch: 386 , training loss: 5.017333
[INFO] Epoch: 5 , batch: 387 , training loss: 4.951844
[INFO] Epoch: 5 , batch: 388 , training loss: 4.737963
[INFO] Epoch: 5 , batch: 389 , training loss: 4.561455
[INFO] Epoch: 5 , batch: 390 , training loss: 4.512693
[INFO] Epoch: 5 , batch: 391 , training loss: 4.577376
[INFO] Epoch: 5 , batch: 392 , training loss: 4.941600
[INFO] Epoch: 5 , batch: 393 , training loss: 4.819561
[INFO] Epoch: 5 , batch: 394 , training loss: 4.857442
[INFO] Epoch: 5 , batch: 395 , training loss: 4.697715
[INFO] Epoch: 5 , batch: 396 , training loss: 4.462359
[INFO] Epoch: 5 , batch: 397 , training loss: 4.639115
[INFO] Epoch: 5 , batch: 398 , training loss: 4.473305
[INFO] Epoch: 5 , batch: 399 , training loss: 4.510066
[INFO] Epoch: 5 , batch: 400 , training loss: 4.457689
[INFO] Epoch: 5 , batch: 401 , training loss: 4.898100
[INFO] Epoch: 5 , batch: 402 , training loss: 4.662231
[INFO] Epoch: 5 , batch: 403 , training loss: 4.504824
[INFO] Epoch: 5 , batch: 404 , training loss: 4.659516
[INFO] Epoch: 5 , batch: 405 , training loss: 4.725873
[INFO] Epoch: 5 , batch: 406 , training loss: 4.601495
[INFO] Epoch: 5 , batch: 407 , training loss: 4.675303
[INFO] Epoch: 5 , batch: 408 , training loss: 4.603723
[INFO] Epoch: 5 , batch: 409 , training loss: 4.619607
[INFO] Epoch: 5 , batch: 410 , training loss: 4.661463
[INFO] Epoch: 5 , batch: 411 , training loss: 4.878114
[INFO] Epoch: 5 , batch: 412 , training loss: 4.689745
[INFO] Epoch: 5 , batch: 413 , training loss: 4.548006
[INFO] Epoch: 5 , batch: 414 , training loss: 4.576139
[INFO] Epoch: 5 , batch: 415 , training loss: 4.588254
[INFO] Epoch: 5 , batch: 416 , training loss: 4.675093
[INFO] Epoch: 5 , batch: 417 , training loss: 4.602583
[INFO] Epoch: 5 , batch: 418 , training loss: 4.615748
[INFO] Epoch: 5 , batch: 419 , training loss: 4.558575
[INFO] Epoch: 5 , batch: 420 , training loss: 4.569636
[INFO] Epoch: 5 , batch: 421 , training loss: 4.554913
[INFO] Epoch: 5 , batch: 422 , training loss: 4.448819
[INFO] Epoch: 5 , batch: 423 , training loss: 4.684982
[INFO] Epoch: 5 , batch: 424 , training loss: 4.824189
[INFO] Epoch: 5 , batch: 425 , training loss: 4.696957
[INFO] Epoch: 5 , batch: 426 , training loss: 4.393428
[INFO] Epoch: 5 , batch: 427 , training loss: 4.665531
[INFO] Epoch: 5 , batch: 428 , training loss: 4.570668
[INFO] Epoch: 5 , batch: 429 , training loss: 4.372525
[INFO] Epoch: 5 , batch: 430 , training loss: 4.653212
[INFO] Epoch: 5 , batch: 431 , training loss: 4.204564
[INFO] Epoch: 5 , batch: 432 , training loss: 4.313286
[INFO] Epoch: 5 , batch: 433 , training loss: 4.311263
[INFO] Epoch: 5 , batch: 434 , training loss: 4.187809
[INFO] Epoch: 5 , batch: 435 , training loss: 4.555330
[INFO] Epoch: 5 , batch: 436 , training loss: 4.671514
[INFO] Epoch: 5 , batch: 437 , training loss: 4.425218
[INFO] Epoch: 5 , batch: 438 , training loss: 4.215131
[INFO] Epoch: 5 , batch: 439 , training loss: 4.462728
[INFO] Epoch: 5 , batch: 440 , training loss: 4.607182
[INFO] Epoch: 5 , batch: 441 , training loss: 4.677004
[INFO] Epoch: 5 , batch: 442 , training loss: 4.468102
[INFO] Epoch: 5 , batch: 443 , training loss: 4.678322
[INFO] Epoch: 5 , batch: 444 , training loss: 4.278619
[INFO] Epoch: 5 , batch: 445 , training loss: 4.160065
[INFO] Epoch: 5 , batch: 446 , training loss: 4.038311
[INFO] Epoch: 5 , batch: 447 , training loss: 4.291667
[INFO] Epoch: 5 , batch: 448 , training loss: 4.455293
[INFO] Epoch: 5 , batch: 449 , training loss: 4.856957
[INFO] Epoch: 5 , batch: 450 , training loss: 4.924173
[INFO] Epoch: 5 , batch: 451 , training loss: 4.796945
[INFO] Epoch: 5 , batch: 452 , training loss: 4.559233
[INFO] Epoch: 5 , batch: 453 , training loss: 4.347819
[INFO] Epoch: 5 , batch: 454 , training loss: 4.480667
[INFO] Epoch: 5 , batch: 455 , training loss: 4.519320
[INFO] Epoch: 5 , batch: 456 , training loss: 4.496086
[INFO] Epoch: 5 , batch: 457 , training loss: 4.615681
[INFO] Epoch: 5 , batch: 458 , training loss: 4.331135
[INFO] Epoch: 5 , batch: 459 , training loss: 4.311574
[INFO] Epoch: 5 , batch: 460 , training loss: 4.441764
[INFO] Epoch: 5 , batch: 461 , training loss: 4.454132
[INFO] Epoch: 5 , batch: 462 , training loss: 4.501876
[INFO] Epoch: 5 , batch: 463 , training loss: 4.347198
[INFO] Epoch: 5 , batch: 464 , training loss: 4.583610
[INFO] Epoch: 5 , batch: 465 , training loss: 4.506119
[INFO] Epoch: 5 , batch: 466 , training loss: 4.573860
[INFO] Epoch: 5 , batch: 467 , training loss: 4.602091
[INFO] Epoch: 5 , batch: 468 , training loss: 4.548277
[INFO] Epoch: 5 , batch: 469 , training loss: 4.560447
[INFO] Epoch: 5 , batch: 470 , training loss: 4.341552
[INFO] Epoch: 5 , batch: 471 , training loss: 4.463113
[INFO] Epoch: 5 , batch: 472 , training loss: 4.488939
[INFO] Epoch: 5 , batch: 473 , training loss: 4.425490
[INFO] Epoch: 5 , batch: 474 , training loss: 4.260808
[INFO] Epoch: 5 , batch: 475 , training loss: 4.094463
[INFO] Epoch: 5 , batch: 476 , training loss: 4.486547
[INFO] Epoch: 5 , batch: 477 , training loss: 4.598551
[INFO] Epoch: 5 , batch: 478 , training loss: 4.680013
[INFO] Epoch: 5 , batch: 479 , training loss: 4.636590
[INFO] Epoch: 5 , batch: 480 , training loss: 4.767184
[INFO] Epoch: 5 , batch: 481 , training loss: 4.574388
[INFO] Epoch: 5 , batch: 482 , training loss: 4.723235
[INFO] Epoch: 5 , batch: 483 , training loss: 4.549708
[INFO] Epoch: 5 , batch: 484 , training loss: 4.349646
[INFO] Epoch: 5 , batch: 485 , training loss: 4.480799
[INFO] Epoch: 5 , batch: 486 , training loss: 4.354832
[INFO] Epoch: 5 , batch: 487 , training loss: 4.347712
[INFO] Epoch: 5 , batch: 488 , training loss: 4.554085
[INFO] Epoch: 5 , batch: 489 , training loss: 4.408317
[INFO] Epoch: 5 , batch: 490 , training loss: 4.480906
[INFO] Epoch: 5 , batch: 491 , training loss: 4.473458
[INFO] Epoch: 5 , batch: 492 , training loss: 4.354493
[INFO] Epoch: 5 , batch: 493 , training loss: 4.550200
[INFO] Epoch: 5 , batch: 494 , training loss: 4.477602
[INFO] Epoch: 5 , batch: 495 , training loss: 4.592942
[INFO] Epoch: 5 , batch: 496 , training loss: 4.470367
[INFO] Epoch: 5 , batch: 497 , training loss: 4.477553
[INFO] Epoch: 5 , batch: 498 , training loss: 4.521172
[INFO] Epoch: 5 , batch: 499 , training loss: 4.592188
[INFO] Epoch: 5 , batch: 500 , training loss: 4.824712
[INFO] Epoch: 5 , batch: 501 , training loss: 5.225945
[INFO] Epoch: 5 , batch: 502 , training loss: 5.373989
[INFO] Epoch: 5 , batch: 503 , training loss: 5.049173
[INFO] Epoch: 5 , batch: 504 , training loss: 5.116259
[INFO] Epoch: 5 , batch: 505 , training loss: 5.025279
[INFO] Epoch: 5 , batch: 506 , training loss: 5.008650
[INFO] Epoch: 5 , batch: 507 , training loss: 5.057504
[INFO] Epoch: 5 , batch: 508 , training loss: 5.009703
[INFO] Epoch: 5 , batch: 509 , training loss: 4.713686
[INFO] Epoch: 5 , batch: 510 , training loss: 4.774945
[INFO] Epoch: 5 , batch: 511 , training loss: 4.682263
[INFO] Epoch: 5 , batch: 512 , training loss: 4.766084
[INFO] Epoch: 5 , batch: 513 , training loss: 5.007603
[INFO] Epoch: 5 , batch: 514 , training loss: 4.656960
[INFO] Epoch: 5 , batch: 515 , training loss: 4.934915
[INFO] Epoch: 5 , batch: 516 , training loss: 4.704136
[INFO] Epoch: 5 , batch: 517 , training loss: 4.670468
[INFO] Epoch: 5 , batch: 518 , training loss: 4.645915
[INFO] Epoch: 5 , batch: 519 , training loss: 4.517301
[INFO] Epoch: 5 , batch: 520 , training loss: 4.738089
[INFO] Epoch: 5 , batch: 521 , training loss: 4.754040
[INFO] Epoch: 5 , batch: 522 , training loss: 4.818604
[INFO] Epoch: 5 , batch: 523 , training loss: 4.695920
[INFO] Epoch: 5 , batch: 524 , training loss: 4.944341
[INFO] Epoch: 5 , batch: 525 , training loss: 4.883697
[INFO] Epoch: 5 , batch: 526 , training loss: 4.636358
[INFO] Epoch: 5 , batch: 527 , training loss: 4.649842
[INFO] Epoch: 5 , batch: 528 , training loss: 4.746923
[INFO] Epoch: 5 , batch: 529 , training loss: 4.691550
[INFO] Epoch: 5 , batch: 530 , training loss: 4.503828
[INFO] Epoch: 5 , batch: 531 , training loss: 4.692675
[INFO] Epoch: 5 , batch: 532 , training loss: 4.529432
[INFO] Epoch: 5 , batch: 533 , training loss: 4.721092
[INFO] Epoch: 5 , batch: 534 , training loss: 4.696477
[INFO] Epoch: 5 , batch: 535 , training loss: 4.723569
[INFO] Epoch: 5 , batch: 536 , training loss: 4.564223
[INFO] Epoch: 5 , batch: 537 , training loss: 4.533242
[INFO] Epoch: 5 , batch: 538 , training loss: 4.605660
[INFO] Epoch: 5 , batch: 539 , training loss: 4.777777
[INFO] Epoch: 5 , batch: 540 , training loss: 5.412721
[INFO] Epoch: 5 , batch: 541 , training loss: 5.315782
[INFO] Epoch: 5 , batch: 542 , training loss: 5.100829
[INFO] Epoch: 6 , batch: 0 , training loss: 4.634940
[INFO] Epoch: 6 , batch: 1 , training loss: 4.370125
[INFO] Epoch: 6 , batch: 2 , training loss: 4.294271
[INFO] Epoch: 6 , batch: 3 , training loss: 4.144977
[INFO] Epoch: 6 , batch: 4 , training loss: 4.515247
[INFO] Epoch: 6 , batch: 5 , training loss: 4.141632
[INFO] Epoch: 6 , batch: 6 , training loss: 4.667216
[INFO] Epoch: 6 , batch: 7 , training loss: 4.294902
[INFO] Epoch: 6 , batch: 8 , training loss: 4.012176
[INFO] Epoch: 6 , batch: 9 , training loss: 4.155316
[INFO] Epoch: 6 , batch: 10 , training loss: 4.121698
[INFO] Epoch: 6 , batch: 11 , training loss: 4.046287
[INFO] Epoch: 6 , batch: 12 , training loss: 4.037547
[INFO] Epoch: 6 , batch: 13 , training loss: 3.992429
[INFO] Epoch: 6 , batch: 14 , training loss: 3.779810
[INFO] Epoch: 6 , batch: 15 , training loss: 4.061006
[INFO] Epoch: 6 , batch: 16 , training loss: 3.917824
[INFO] Epoch: 6 , batch: 17 , training loss: 4.096292
[INFO] Epoch: 6 , batch: 18 , training loss: 4.005761
[INFO] Epoch: 6 , batch: 19 , training loss: 3.752641
[INFO] Epoch: 6 , batch: 20 , training loss: 3.750012
[INFO] Epoch: 6 , batch: 21 , training loss: 3.855112
[INFO] Epoch: 6 , batch: 22 , training loss: 3.905800
[INFO] Epoch: 6 , batch: 23 , training loss: 4.058321
[INFO] Epoch: 6 , batch: 24 , training loss: 3.846225
[INFO] Epoch: 6 , batch: 25 , training loss: 4.046005
[INFO] Epoch: 6 , batch: 26 , training loss: 3.864779
[INFO] Epoch: 6 , batch: 27 , training loss: 3.828140
[INFO] Epoch: 6 , batch: 28 , training loss: 4.061073
[INFO] Epoch: 6 , batch: 29 , training loss: 3.791733
[INFO] Epoch: 6 , batch: 30 , training loss: 3.832833
[INFO] Epoch: 6 , batch: 31 , training loss: 3.948287
[INFO] Epoch: 6 , batch: 32 , training loss: 3.987257
[INFO] Epoch: 6 , batch: 33 , training loss: 4.004016
[INFO] Epoch: 6 , batch: 34 , training loss: 4.060910
[INFO] Epoch: 6 , batch: 35 , training loss: 3.985641
[INFO] Epoch: 6 , batch: 36 , training loss: 3.921574
[INFO] Epoch: 6 , batch: 37 , training loss: 3.849617
[INFO] Epoch: 6 , batch: 38 , training loss: 3.987896
[INFO] Epoch: 6 , batch: 39 , training loss: 3.795091
[INFO] Epoch: 6 , batch: 40 , training loss: 3.906350
[INFO] Epoch: 6 , batch: 41 , training loss: 4.071678
[INFO] Epoch: 6 , batch: 42 , training loss: 4.637311
[INFO] Epoch: 6 , batch: 43 , training loss: 4.288353
[INFO] Epoch: 6 , batch: 44 , training loss: 4.447818
[INFO] Epoch: 6 , batch: 45 , training loss: 4.575498
[INFO] Epoch: 6 , batch: 46 , training loss: 4.786710
[INFO] Epoch: 6 , batch: 47 , training loss: 4.165020
[INFO] Epoch: 6 , batch: 48 , training loss: 4.255436
[INFO] Epoch: 6 , batch: 49 , training loss: 4.273798
[INFO] Epoch: 6 , batch: 50 , training loss: 4.063567
[INFO] Epoch: 6 , batch: 51 , training loss: 4.167972
[INFO] Epoch: 6 , batch: 52 , training loss: 3.967951
[INFO] Epoch: 6 , batch: 53 , training loss: 4.118158
[INFO] Epoch: 6 , batch: 54 , training loss: 4.165574
[INFO] Epoch: 6 , batch: 55 , training loss: 4.224223
[INFO] Epoch: 6 , batch: 56 , training loss: 4.075656
[INFO] Epoch: 6 , batch: 57 , training loss: 3.930453
[INFO] Epoch: 6 , batch: 58 , training loss: 3.997829
[INFO] Epoch: 6 , batch: 59 , training loss: 4.108187
[INFO] Epoch: 6 , batch: 60 , training loss: 3.952069
[INFO] Epoch: 6 , batch: 61 , training loss: 4.044152
[INFO] Epoch: 6 , batch: 62 , training loss: 3.957803
[INFO] Epoch: 6 , batch: 63 , training loss: 4.163826
[INFO] Epoch: 6 , batch: 64 , training loss: 4.318377
[INFO] Epoch: 6 , batch: 65 , training loss: 4.018408
[INFO] Epoch: 6 , batch: 66 , training loss: 3.849539
[INFO] Epoch: 6 , batch: 67 , training loss: 3.913810
[INFO] Epoch: 6 , batch: 68 , training loss: 4.204056
[INFO] Epoch: 6 , batch: 69 , training loss: 3.961236
[INFO] Epoch: 6 , batch: 70 , training loss: 4.249054
[INFO] Epoch: 6 , batch: 71 , training loss: 4.061262
[INFO] Epoch: 6 , batch: 72 , training loss: 4.152410
[INFO] Epoch: 6 , batch: 73 , training loss: 4.071313
[INFO] Epoch: 6 , batch: 74 , training loss: 4.209952
[INFO] Epoch: 6 , batch: 75 , training loss: 3.977282
[INFO] Epoch: 6 , batch: 76 , training loss: 4.122493
[INFO] Epoch: 6 , batch: 77 , training loss: 4.056655
[INFO] Epoch: 6 , batch: 78 , training loss: 4.138340
[INFO] Epoch: 6 , batch: 79 , training loss: 3.947576
[INFO] Epoch: 6 , batch: 80 , training loss: 4.194222
[INFO] Epoch: 6 , batch: 81 , training loss: 4.139387
[INFO] Epoch: 6 , batch: 82 , training loss: 4.124634
[INFO] Epoch: 6 , batch: 83 , training loss: 4.216231
[INFO] Epoch: 6 , batch: 84 , training loss: 4.140203
[INFO] Epoch: 6 , batch: 85 , training loss: 4.197672
[INFO] Epoch: 6 , batch: 86 , training loss: 4.201025
[INFO] Epoch: 6 , batch: 87 , training loss: 4.175213
[INFO] Epoch: 6 , batch: 88 , training loss: 4.311109
[INFO] Epoch: 6 , batch: 89 , training loss: 4.096117
[INFO] Epoch: 6 , batch: 90 , training loss: 4.152909
[INFO] Epoch: 6 , batch: 91 , training loss: 4.075727
[INFO] Epoch: 6 , batch: 92 , training loss: 4.073063
[INFO] Epoch: 6 , batch: 93 , training loss: 4.172622
[INFO] Epoch: 6 , batch: 94 , training loss: 4.354068
[INFO] Epoch: 6 , batch: 95 , training loss: 4.131578
[INFO] Epoch: 6 , batch: 96 , training loss: 4.085838
[INFO] Epoch: 6 , batch: 97 , training loss: 4.061870
[INFO] Epoch: 6 , batch: 98 , training loss: 4.032962
[INFO] Epoch: 6 , batch: 99 , training loss: 4.111202
[INFO] Epoch: 6 , batch: 100 , training loss: 3.946197
[INFO] Epoch: 6 , batch: 101 , training loss: 4.038047
[INFO] Epoch: 6 , batch: 102 , training loss: 4.171068
[INFO] Epoch: 6 , batch: 103 , training loss: 3.888438
[INFO] Epoch: 6 , batch: 104 , training loss: 3.924930
[INFO] Epoch: 6 , batch: 105 , training loss: 4.216685
[INFO] Epoch: 6 , batch: 106 , training loss: 4.162017
[INFO] Epoch: 6 , batch: 107 , training loss: 4.052282
[INFO] Epoch: 6 , batch: 108 , training loss: 3.943769
[INFO] Epoch: 6 , batch: 109 , training loss: 3.836157
[INFO] Epoch: 6 , batch: 110 , training loss: 4.104550
[INFO] Epoch: 6 , batch: 111 , training loss: 4.166065
[INFO] Epoch: 6 , batch: 112 , training loss: 4.091314
[INFO] Epoch: 6 , batch: 113 , training loss: 4.039246
[INFO] Epoch: 6 , batch: 114 , training loss: 4.162610
[INFO] Epoch: 6 , batch: 115 , training loss: 4.102292
[INFO] Epoch: 6 , batch: 116 , training loss: 4.037944
[INFO] Epoch: 6 , batch: 117 , training loss: 4.297655
[INFO] Epoch: 6 , batch: 118 , training loss: 4.216694
[INFO] Epoch: 6 , batch: 119 , training loss: 4.371629
[INFO] Epoch: 6 , batch: 120 , training loss: 4.356691
[INFO] Epoch: 6 , batch: 121 , training loss: 4.151947
[INFO] Epoch: 6 , batch: 122 , training loss: 4.079461
[INFO] Epoch: 6 , batch: 123 , training loss: 4.154593
[INFO] Epoch: 6 , batch: 124 , training loss: 4.294846
[INFO] Epoch: 6 , batch: 125 , training loss: 4.001621
[INFO] Epoch: 6 , batch: 126 , training loss: 3.988191
[INFO] Epoch: 6 , batch: 127 , training loss: 4.024497
[INFO] Epoch: 6 , batch: 128 , training loss: 4.218962
[INFO] Epoch: 6 , batch: 129 , training loss: 4.128310
[INFO] Epoch: 6 , batch: 130 , training loss: 4.109651
[INFO] Epoch: 6 , batch: 131 , training loss: 4.133667
[INFO] Epoch: 6 , batch: 132 , training loss: 4.155348
[INFO] Epoch: 6 , batch: 133 , training loss: 4.078112
[INFO] Epoch: 6 , batch: 134 , training loss: 3.894299
[INFO] Epoch: 6 , batch: 135 , training loss: 3.904040
[INFO] Epoch: 6 , batch: 136 , training loss: 4.222077
[INFO] Epoch: 6 , batch: 137 , training loss: 4.128751
[INFO] Epoch: 6 , batch: 138 , training loss: 4.270145
[INFO] Epoch: 6 , batch: 139 , training loss: 4.968448
[INFO] Epoch: 6 , batch: 140 , training loss: 4.765893
[INFO] Epoch: 6 , batch: 141 , training loss: 4.482691
[INFO] Epoch: 6 , batch: 142 , training loss: 4.143180
[INFO] Epoch: 6 , batch: 143 , training loss: 4.221514
[INFO] Epoch: 6 , batch: 144 , training loss: 3.985244
[INFO] Epoch: 6 , batch: 145 , training loss: 4.180844
[INFO] Epoch: 6 , batch: 146 , training loss: 4.362329
[INFO] Epoch: 6 , batch: 147 , training loss: 3.981722
[INFO] Epoch: 6 , batch: 148 , training loss: 3.975710
[INFO] Epoch: 6 , batch: 149 , training loss: 4.071492
[INFO] Epoch: 6 , batch: 150 , training loss: 4.307687
[INFO] Epoch: 6 , batch: 151 , training loss: 4.075694
[INFO] Epoch: 6 , batch: 152 , training loss: 4.156120
[INFO] Epoch: 6 , batch: 153 , training loss: 4.198261
[INFO] Epoch: 6 , batch: 154 , training loss: 4.288812
[INFO] Epoch: 6 , batch: 155 , training loss: 4.545774
[INFO] Epoch: 6 , batch: 156 , training loss: 4.170825
[INFO] Epoch: 6 , batch: 157 , training loss: 4.142831
[INFO] Epoch: 6 , batch: 158 , training loss: 4.517383
[INFO] Epoch: 6 , batch: 159 , training loss: 4.365369
[INFO] Epoch: 6 , batch: 160 , training loss: 4.801800
[INFO] Epoch: 6 , batch: 161 , training loss: 4.652893
[INFO] Epoch: 6 , batch: 162 , training loss: 4.557399
[INFO] Epoch: 6 , batch: 163 , training loss: 4.675625
[INFO] Epoch: 6 , batch: 164 , training loss: 4.649610
[INFO] Epoch: 6 , batch: 165 , training loss: 4.573259
[INFO] Epoch: 6 , batch: 166 , training loss: 4.700083
[INFO] Epoch: 6 , batch: 167 , training loss: 5.052371
[INFO] Epoch: 6 , batch: 168 , training loss: 4.785623
[INFO] Epoch: 6 , batch: 169 , training loss: 4.668866
[INFO] Epoch: 6 , batch: 170 , training loss: 4.724479
[INFO] Epoch: 6 , batch: 171 , training loss: 4.267294
[INFO] Epoch: 6 , batch: 172 , training loss: 4.454377
[INFO] Epoch: 6 , batch: 173 , training loss: 4.728958
[INFO] Epoch: 6 , batch: 174 , training loss: 4.947007
[INFO] Epoch: 6 , batch: 175 , training loss: 5.211223
[INFO] Epoch: 6 , batch: 176 , training loss: 4.967566
[INFO] Epoch: 6 , batch: 177 , training loss: 4.568738
[INFO] Epoch: 6 , batch: 178 , training loss: 4.527876
[INFO] Epoch: 6 , batch: 179 , training loss: 4.543133
[INFO] Epoch: 6 , batch: 180 , training loss: 4.509412
[INFO] Epoch: 6 , batch: 181 , training loss: 4.710192
[INFO] Epoch: 6 , batch: 182 , training loss: 4.645680
[INFO] Epoch: 6 , batch: 183 , training loss: 4.618565
[INFO] Epoch: 6 , batch: 184 , training loss: 4.472994
[INFO] Epoch: 6 , batch: 185 , training loss: 4.434658
[INFO] Epoch: 6 , batch: 186 , training loss: 4.587823
[INFO] Epoch: 6 , batch: 187 , training loss: 4.713974
[INFO] Epoch: 6 , batch: 188 , training loss: 4.681063
[INFO] Epoch: 6 , batch: 189 , training loss: 4.578455
[INFO] Epoch: 6 , batch: 190 , training loss: 4.536572
[INFO] Epoch: 6 , batch: 191 , training loss: 4.724989
[INFO] Epoch: 6 , batch: 192 , training loss: 4.447273
[INFO] Epoch: 6 , batch: 193 , training loss: 4.603627
[INFO] Epoch: 6 , batch: 194 , training loss: 4.509057
[INFO] Epoch: 6 , batch: 195 , training loss: 4.521918
[INFO] Epoch: 6 , batch: 196 , training loss: 4.314799
[INFO] Epoch: 6 , batch: 197 , training loss: 4.487025
[INFO] Epoch: 6 , batch: 198 , training loss: 4.338988
[INFO] Epoch: 6 , batch: 199 , training loss: 4.452824
[INFO] Epoch: 6 , batch: 200 , training loss: 4.376123
[INFO] Epoch: 6 , batch: 201 , training loss: 4.286416
[INFO] Epoch: 6 , batch: 202 , training loss: 4.260290
[INFO] Epoch: 6 , batch: 203 , training loss: 4.297888
[INFO] Epoch: 6 , batch: 204 , training loss: 4.452791
[INFO] Epoch: 6 , batch: 205 , training loss: 4.024332
[INFO] Epoch: 6 , batch: 206 , training loss: 3.935467
[INFO] Epoch: 6 , batch: 207 , training loss: 3.942472
[INFO] Epoch: 6 , batch: 208 , training loss: 4.342107
[INFO] Epoch: 6 , batch: 209 , training loss: 4.240483
[INFO] Epoch: 6 , batch: 210 , training loss: 4.291195
[INFO] Epoch: 6 , batch: 211 , training loss: 4.267025
[INFO] Epoch: 6 , batch: 212 , training loss: 4.363088
[INFO] Epoch: 6 , batch: 213 , training loss: 4.334644
[INFO] Epoch: 6 , batch: 214 , training loss: 4.399105
[INFO] Epoch: 6 , batch: 215 , training loss: 4.619059
[INFO] Epoch: 6 , batch: 216 , training loss: 4.337040
[INFO] Epoch: 6 , batch: 217 , training loss: 4.259016
[INFO] Epoch: 6 , batch: 218 , training loss: 4.259216
[INFO] Epoch: 6 , batch: 219 , training loss: 4.378475
[INFO] Epoch: 6 , batch: 220 , training loss: 4.179698
[INFO] Epoch: 6 , batch: 221 , training loss: 4.210473
[INFO] Epoch: 6 , batch: 222 , training loss: 4.341033
[INFO] Epoch: 6 , batch: 223 , training loss: 4.417543
[INFO] Epoch: 6 , batch: 224 , training loss: 4.474800
[INFO] Epoch: 6 , batch: 225 , training loss: 4.360239
[INFO] Epoch: 6 , batch: 226 , training loss: 4.494513
[INFO] Epoch: 6 , batch: 227 , training loss: 4.449885
[INFO] Epoch: 6 , batch: 228 , training loss: 4.511194
[INFO] Epoch: 6 , batch: 229 , training loss: 4.377855
[INFO] Epoch: 6 , batch: 230 , training loss: 4.202398
[INFO] Epoch: 6 , batch: 231 , training loss: 4.037622
[INFO] Epoch: 6 , batch: 232 , training loss: 4.201256
[INFO] Epoch: 6 , batch: 233 , training loss: 4.212202
[INFO] Epoch: 6 , batch: 234 , training loss: 3.908463
[INFO] Epoch: 6 , batch: 235 , training loss: 4.015967
[INFO] Epoch: 6 , batch: 236 , training loss: 4.179253
[INFO] Epoch: 6 , batch: 237 , training loss: 4.329748
[INFO] Epoch: 6 , batch: 238 , training loss: 4.072319
[INFO] Epoch: 6 , batch: 239 , training loss: 4.157145
[INFO] Epoch: 6 , batch: 240 , training loss: 4.231761
[INFO] Epoch: 6 , batch: 241 , training loss: 4.023715
[INFO] Epoch: 6 , batch: 242 , training loss: 4.014376
[INFO] Epoch: 6 , batch: 243 , training loss: 4.364237
[INFO] Epoch: 6 , batch: 244 , training loss: 4.252464
[INFO] Epoch: 6 , batch: 245 , training loss: 4.263180
[INFO] Epoch: 6 , batch: 246 , training loss: 3.923514
[INFO] Epoch: 6 , batch: 247 , training loss: 4.094950
[INFO] Epoch: 6 , batch: 248 , training loss: 4.200084
[INFO] Epoch: 6 , batch: 249 , training loss: 4.173099
[INFO] Epoch: 6 , batch: 250 , training loss: 3.924921
[INFO] Epoch: 6 , batch: 251 , training loss: 4.434771
[INFO] Epoch: 6 , batch: 252 , training loss: 4.093416
[INFO] Epoch: 6 , batch: 253 , training loss: 4.068819
[INFO] Epoch: 6 , batch: 254 , training loss: 4.371813
[INFO] Epoch: 6 , batch: 255 , training loss: 4.335663
[INFO] Epoch: 6 , batch: 256 , training loss: 4.325169
[INFO] Epoch: 6 , batch: 257 , training loss: 4.486908
[INFO] Epoch: 6 , batch: 258 , training loss: 4.538502
[INFO] Epoch: 6 , batch: 259 , training loss: 4.574279
[INFO] Epoch: 6 , batch: 260 , training loss: 4.250657
[INFO] Epoch: 6 , batch: 261 , training loss: 4.438313
[INFO] Epoch: 6 , batch: 262 , training loss: 4.660685
[INFO] Epoch: 6 , batch: 263 , training loss: 4.814409
[INFO] Epoch: 6 , batch: 264 , training loss: 4.090407
[INFO] Epoch: 6 , batch: 265 , training loss: 4.245996
[INFO] Epoch: 6 , batch: 266 , training loss: 4.746335
[INFO] Epoch: 6 , batch: 267 , training loss: 4.423227
[INFO] Epoch: 6 , batch: 268 , training loss: 4.297226
[INFO] Epoch: 6 , batch: 269 , training loss: 4.325400
[INFO] Epoch: 6 , batch: 270 , training loss: 4.338176
[INFO] Epoch: 6 , batch: 271 , training loss: 4.366059
[INFO] Epoch: 6 , batch: 272 , training loss: 4.345531
[INFO] Epoch: 6 , batch: 273 , training loss: 4.338047
[INFO] Epoch: 6 , batch: 274 , training loss: 4.465329
[INFO] Epoch: 6 , batch: 275 , training loss: 4.357388
[INFO] Epoch: 6 , batch: 276 , training loss: 4.403832
[INFO] Epoch: 6 , batch: 277 , training loss: 4.548063
[INFO] Epoch: 6 , batch: 278 , training loss: 4.156201
[INFO] Epoch: 6 , batch: 279 , training loss: 4.191954
[INFO] Epoch: 6 , batch: 280 , training loss: 4.131869
[INFO] Epoch: 6 , batch: 281 , training loss: 4.285713
[INFO] Epoch: 6 , batch: 282 , training loss: 4.173186
[INFO] Epoch: 6 , batch: 283 , training loss: 4.232894
[INFO] Epoch: 6 , batch: 284 , training loss: 4.246601
[INFO] Epoch: 6 , batch: 285 , training loss: 4.208448
[INFO] Epoch: 6 , batch: 286 , training loss: 4.192400
[INFO] Epoch: 6 , batch: 287 , training loss: 4.116308
[INFO] Epoch: 6 , batch: 288 , training loss: 4.133608
[INFO] Epoch: 6 , batch: 289 , training loss: 4.203223
[INFO] Epoch: 6 , batch: 290 , training loss: 3.963295
[INFO] Epoch: 6 , batch: 291 , training loss: 3.929785
[INFO] Epoch: 6 , batch: 292 , training loss: 4.048138
[INFO] Epoch: 6 , batch: 293 , training loss: 3.977061
[INFO] Epoch: 6 , batch: 294 , training loss: 4.681144
[INFO] Epoch: 6 , batch: 295 , training loss: 4.453592
[INFO] Epoch: 6 , batch: 296 , training loss: 4.383162
[INFO] Epoch: 6 , batch: 297 , training loss: 4.314537
[INFO] Epoch: 6 , batch: 298 , training loss: 4.148970
[INFO] Epoch: 6 , batch: 299 , training loss: 4.170998
[INFO] Epoch: 6 , batch: 300 , training loss: 4.141234
[INFO] Epoch: 6 , batch: 301 , training loss: 4.061493
[INFO] Epoch: 6 , batch: 302 , training loss: 4.255452
[INFO] Epoch: 6 , batch: 303 , training loss: 4.251008
[INFO] Epoch: 6 , batch: 304 , training loss: 4.449810
[INFO] Epoch: 6 , batch: 305 , training loss: 4.177898
[INFO] Epoch: 6 , batch: 306 , training loss: 4.319031
[INFO] Epoch: 6 , batch: 307 , training loss: 4.317343
[INFO] Epoch: 6 , batch: 308 , training loss: 4.179692
[INFO] Epoch: 6 , batch: 309 , training loss: 4.187318
[INFO] Epoch: 6 , batch: 310 , training loss: 4.036198
[INFO] Epoch: 6 , batch: 311 , training loss: 4.063392
[INFO] Epoch: 6 , batch: 312 , training loss: 3.939045
[INFO] Epoch: 6 , batch: 313 , training loss: 4.118677
[INFO] Epoch: 6 , batch: 314 , training loss: 4.169461
[INFO] Epoch: 6 , batch: 315 , training loss: 4.210052
[INFO] Epoch: 6 , batch: 316 , training loss: 4.530443
[INFO] Epoch: 6 , batch: 317 , training loss: 5.079733
[INFO] Epoch: 6 , batch: 318 , training loss: 5.199980
[INFO] Epoch: 6 , batch: 319 , training loss: 4.690238
[INFO] Epoch: 6 , batch: 320 , training loss: 4.210014
[INFO] Epoch: 6 , batch: 321 , training loss: 4.001055
[INFO] Epoch: 6 , batch: 322 , training loss: 4.125679
[INFO] Epoch: 6 , batch: 323 , training loss: 4.146441
[INFO] Epoch: 6 , batch: 324 , training loss: 4.136928
[INFO] Epoch: 6 , batch: 325 , training loss: 4.312627
[INFO] Epoch: 6 , batch: 326 , training loss: 4.382235
[INFO] Epoch: 6 , batch: 327 , training loss: 4.262925
[INFO] Epoch: 6 , batch: 328 , training loss: 4.249179
[INFO] Epoch: 6 , batch: 329 , training loss: 4.177718
[INFO] Epoch: 6 , batch: 330 , training loss: 4.134458
[INFO] Epoch: 6 , batch: 331 , training loss: 4.297397
[INFO] Epoch: 6 , batch: 332 , training loss: 4.107410
[INFO] Epoch: 6 , batch: 333 , training loss: 4.084363
[INFO] Epoch: 6 , batch: 334 , training loss: 4.153451
[INFO] Epoch: 6 , batch: 335 , training loss: 4.278926
[INFO] Epoch: 6 , batch: 336 , training loss: 4.250868
[INFO] Epoch: 6 , batch: 337 , training loss: 4.322177
[INFO] Epoch: 6 , batch: 338 , training loss: 4.528574
[INFO] Epoch: 6 , batch: 339 , training loss: 4.367621
[INFO] Epoch: 6 , batch: 340 , training loss: 4.542217
[INFO] Epoch: 6 , batch: 341 , training loss: 4.284129
[INFO] Epoch: 6 , batch: 342 , training loss: 4.072842
[INFO] Epoch: 6 , batch: 343 , training loss: 4.155273
[INFO] Epoch: 6 , batch: 344 , training loss: 4.014108
[INFO] Epoch: 6 , batch: 345 , training loss: 4.140677
[INFO] Epoch: 6 , batch: 346 , training loss: 4.230675
[INFO] Epoch: 6 , batch: 347 , training loss: 4.083803
[INFO] Epoch: 6 , batch: 348 , training loss: 4.304697
[INFO] Epoch: 6 , batch: 349 , training loss: 4.385501
[INFO] Epoch: 6 , batch: 350 , training loss: 4.114629
[INFO] Epoch: 6 , batch: 351 , training loss: 4.190780
[INFO] Epoch: 6 , batch: 352 , training loss: 4.223350
[INFO] Epoch: 6 , batch: 353 , training loss: 4.181818
[INFO] Epoch: 6 , batch: 354 , training loss: 4.289044
[INFO] Epoch: 6 , batch: 355 , training loss: 4.338741
[INFO] Epoch: 6 , batch: 356 , training loss: 4.175824
[INFO] Epoch: 6 , batch: 357 , training loss: 4.262633
[INFO] Epoch: 6 , batch: 358 , training loss: 4.206472
[INFO] Epoch: 6 , batch: 359 , training loss: 4.159729
[INFO] Epoch: 6 , batch: 360 , training loss: 4.236311
[INFO] Epoch: 6 , batch: 361 , training loss: 4.193919
[INFO] Epoch: 6 , batch: 362 , training loss: 4.308790
[INFO] Epoch: 6 , batch: 363 , training loss: 4.202129
[INFO] Epoch: 6 , batch: 364 , training loss: 4.227083
[INFO] Epoch: 6 , batch: 365 , training loss: 4.141334
[INFO] Epoch: 6 , batch: 366 , training loss: 4.301782
[INFO] Epoch: 6 , batch: 367 , training loss: 4.383516
[INFO] Epoch: 6 , batch: 368 , training loss: 4.921396
[INFO] Epoch: 6 , batch: 369 , training loss: 4.490596
[INFO] Epoch: 6 , batch: 370 , training loss: 4.264554
[INFO] Epoch: 6 , batch: 371 , training loss: 4.805663
[INFO] Epoch: 6 , batch: 372 , training loss: 5.073428
[INFO] Epoch: 6 , batch: 373 , training loss: 5.093656
[INFO] Epoch: 6 , batch: 374 , training loss: 5.121996
[INFO] Epoch: 6 , batch: 375 , training loss: 5.100715
[INFO] Epoch: 6 , batch: 376 , training loss: 4.992369
[INFO] Epoch: 6 , batch: 377 , training loss: 4.671541
[INFO] Epoch: 6 , batch: 378 , training loss: 4.787835
[INFO] Epoch: 6 , batch: 379 , training loss: 4.806067
[INFO] Epoch: 6 , batch: 380 , training loss: 4.953082
[INFO] Epoch: 6 , batch: 381 , training loss: 4.721421
[INFO] Epoch: 6 , batch: 382 , training loss: 4.948550
[INFO] Epoch: 6 , batch: 383 , training loss: 5.025135
[INFO] Epoch: 6 , batch: 384 , training loss: 5.051108
[INFO] Epoch: 6 , batch: 385 , training loss: 4.811406
[INFO] Epoch: 6 , batch: 386 , training loss: 4.955384
[INFO] Epoch: 6 , batch: 387 , training loss: 4.881951
[INFO] Epoch: 6 , batch: 388 , training loss: 4.686535
[INFO] Epoch: 6 , batch: 389 , training loss: 4.529062
[INFO] Epoch: 6 , batch: 390 , training loss: 4.480655
[INFO] Epoch: 6 , batch: 391 , training loss: 4.544237
[INFO] Epoch: 6 , batch: 392 , training loss: 4.908422
[INFO] Epoch: 6 , batch: 393 , training loss: 4.790265
[INFO] Epoch: 6 , batch: 394 , training loss: 4.831267
[INFO] Epoch: 6 , batch: 395 , training loss: 4.663985
[INFO] Epoch: 6 , batch: 396 , training loss: 4.427190
[INFO] Epoch: 6 , batch: 397 , training loss: 4.606890
[INFO] Epoch: 6 , batch: 398 , training loss: 4.437376
[INFO] Epoch: 6 , batch: 399 , training loss: 4.481853
[INFO] Epoch: 6 , batch: 400 , training loss: 4.429920
[INFO] Epoch: 6 , batch: 401 , training loss: 4.874893
[INFO] Epoch: 6 , batch: 402 , training loss: 4.628052
[INFO] Epoch: 6 , batch: 403 , training loss: 4.466238
[INFO] Epoch: 6 , batch: 404 , training loss: 4.634333
[INFO] Epoch: 6 , batch: 405 , training loss: 4.676600
[INFO] Epoch: 6 , batch: 406 , training loss: 4.572638
[INFO] Epoch: 6 , batch: 407 , training loss: 4.641958
[INFO] Epoch: 6 , batch: 408 , training loss: 4.570731
[INFO] Epoch: 6 , batch: 409 , training loss: 4.580855
[INFO] Epoch: 6 , batch: 410 , training loss: 4.626381
[INFO] Epoch: 6 , batch: 411 , training loss: 4.852705
[INFO] Epoch: 6 , batch: 412 , training loss: 4.663907
[INFO] Epoch: 6 , batch: 413 , training loss: 4.513532
[INFO] Epoch: 6 , batch: 414 , training loss: 4.542875
[INFO] Epoch: 6 , batch: 415 , training loss: 4.559737
[INFO] Epoch: 6 , batch: 416 , training loss: 4.645675
[INFO] Epoch: 6 , batch: 417 , training loss: 4.573821
[INFO] Epoch: 6 , batch: 418 , training loss: 4.588944
[INFO] Epoch: 6 , batch: 419 , training loss: 4.531203
[INFO] Epoch: 6 , batch: 420 , training loss: 4.535452
[INFO] Epoch: 6 , batch: 421 , training loss: 4.532077
[INFO] Epoch: 6 , batch: 422 , training loss: 4.410480
[INFO] Epoch: 6 , batch: 423 , training loss: 4.645305
[INFO] Epoch: 6 , batch: 424 , training loss: 4.783158
[INFO] Epoch: 6 , batch: 425 , training loss: 4.654972
[INFO] Epoch: 6 , batch: 426 , training loss: 4.373356
[INFO] Epoch: 6 , batch: 427 , training loss: 4.622159
[INFO] Epoch: 6 , batch: 428 , training loss: 4.539265
[INFO] Epoch: 6 , batch: 429 , training loss: 4.336935
[INFO] Epoch: 6 , batch: 430 , training loss: 4.618075
[INFO] Epoch: 6 , batch: 431 , training loss: 4.182111
[INFO] Epoch: 6 , batch: 432 , training loss: 4.289804
[INFO] Epoch: 6 , batch: 433 , training loss: 4.291861
[INFO] Epoch: 6 , batch: 434 , training loss: 4.163064
[INFO] Epoch: 6 , batch: 435 , training loss: 4.526155
[INFO] Epoch: 6 , batch: 436 , training loss: 4.639789
[INFO] Epoch: 6 , batch: 437 , training loss: 4.392527
[INFO] Epoch: 6 , batch: 438 , training loss: 4.187978
[INFO] Epoch: 6 , batch: 439 , training loss: 4.443139
[INFO] Epoch: 6 , batch: 440 , training loss: 4.583084
[INFO] Epoch: 6 , batch: 441 , training loss: 4.639741
[INFO] Epoch: 6 , batch: 442 , training loss: 4.443092
[INFO] Epoch: 6 , batch: 443 , training loss: 4.643385
[INFO] Epoch: 6 , batch: 444 , training loss: 4.250394
[INFO] Epoch: 6 , batch: 445 , training loss: 4.136372
[INFO] Epoch: 6 , batch: 446 , training loss: 4.024038
[INFO] Epoch: 6 , batch: 447 , training loss: 4.261614
[INFO] Epoch: 6 , batch: 448 , training loss: 4.422266
[INFO] Epoch: 6 , batch: 449 , training loss: 4.816501
[INFO] Epoch: 6 , batch: 450 , training loss: 4.877470
[INFO] Epoch: 6 , batch: 451 , training loss: 4.758811
[INFO] Epoch: 6 , batch: 452 , training loss: 4.528913
[INFO] Epoch: 6 , batch: 453 , training loss: 4.316266
[INFO] Epoch: 6 , batch: 454 , training loss: 4.442724
[INFO] Epoch: 6 , batch: 455 , training loss: 4.486640
[INFO] Epoch: 6 , batch: 456 , training loss: 4.469154
[INFO] Epoch: 6 , batch: 457 , training loss: 4.588559
[INFO] Epoch: 6 , batch: 458 , training loss: 4.309978
[INFO] Epoch: 6 , batch: 459 , training loss: 4.279812
[INFO] Epoch: 6 , batch: 460 , training loss: 4.410748
[INFO] Epoch: 6 , batch: 461 , training loss: 4.419792
[INFO] Epoch: 6 , batch: 462 , training loss: 4.472225
[INFO] Epoch: 6 , batch: 463 , training loss: 4.322432
[INFO] Epoch: 6 , batch: 464 , training loss: 4.566248
[INFO] Epoch: 6 , batch: 465 , training loss: 4.480146
[INFO] Epoch: 6 , batch: 466 , training loss: 4.541646
[INFO] Epoch: 6 , batch: 467 , training loss: 4.568072
[INFO] Epoch: 6 , batch: 468 , training loss: 4.517567
[INFO] Epoch: 6 , batch: 469 , training loss: 4.537682
[INFO] Epoch: 6 , batch: 470 , training loss: 4.314856
[INFO] Epoch: 6 , batch: 471 , training loss: 4.433126
[INFO] Epoch: 6 , batch: 472 , training loss: 4.464459
[INFO] Epoch: 6 , batch: 473 , training loss: 4.386840
[INFO] Epoch: 6 , batch: 474 , training loss: 4.225073
[INFO] Epoch: 6 , batch: 475 , training loss: 4.064203
[INFO] Epoch: 6 , batch: 476 , training loss: 4.453179
[INFO] Epoch: 6 , batch: 477 , training loss: 4.570955
[INFO] Epoch: 6 , batch: 478 , training loss: 4.648299
[INFO] Epoch: 6 , batch: 479 , training loss: 4.604919
[INFO] Epoch: 6 , batch: 480 , training loss: 4.730316
[INFO] Epoch: 6 , batch: 481 , training loss: 4.546853
[INFO] Epoch: 6 , batch: 482 , training loss: 4.698744
[INFO] Epoch: 6 , batch: 483 , training loss: 4.520701
[INFO] Epoch: 6 , batch: 484 , training loss: 4.325367
[INFO] Epoch: 6 , batch: 485 , training loss: 4.451765
[INFO] Epoch: 6 , batch: 486 , training loss: 4.321846
[INFO] Epoch: 6 , batch: 487 , training loss: 4.317164
[INFO] Epoch: 6 , batch: 488 , training loss: 4.520447
[INFO] Epoch: 6 , batch: 489 , training loss: 4.388208
[INFO] Epoch: 6 , batch: 490 , training loss: 4.444073
[INFO] Epoch: 6 , batch: 491 , training loss: 4.432800
[INFO] Epoch: 6 , batch: 492 , training loss: 4.334363
[INFO] Epoch: 6 , batch: 493 , training loss: 4.536594
[INFO] Epoch: 6 , batch: 494 , training loss: 4.450815
[INFO] Epoch: 6 , batch: 495 , training loss: 4.565414
[INFO] Epoch: 6 , batch: 496 , training loss: 4.448653
[INFO] Epoch: 6 , batch: 497 , training loss: 4.449264
[INFO] Epoch: 6 , batch: 498 , training loss: 4.490721
[INFO] Epoch: 6 , batch: 499 , training loss: 4.560875
[INFO] Epoch: 6 , batch: 500 , training loss: 4.778960
[INFO] Epoch: 6 , batch: 501 , training loss: 5.158331
[INFO] Epoch: 6 , batch: 502 , training loss: 5.308862
[INFO] Epoch: 6 , batch: 503 , training loss: 4.995964
[INFO] Epoch: 6 , batch: 504 , training loss: 5.079343
[INFO] Epoch: 6 , batch: 505 , training loss: 4.995764
[INFO] Epoch: 6 , batch: 506 , training loss: 4.953238
[INFO] Epoch: 6 , batch: 507 , training loss: 5.012503
[INFO] Epoch: 6 , batch: 508 , training loss: 4.974786
[INFO] Epoch: 6 , batch: 509 , training loss: 4.700437
[INFO] Epoch: 6 , batch: 510 , training loss: 4.758575
[INFO] Epoch: 6 , batch: 511 , training loss: 4.658462
[INFO] Epoch: 6 , batch: 512 , training loss: 4.737316
[INFO] Epoch: 6 , batch: 513 , training loss: 4.970877
[INFO] Epoch: 6 , batch: 514 , training loss: 4.622857
[INFO] Epoch: 6 , batch: 515 , training loss: 4.888197
[INFO] Epoch: 6 , batch: 516 , training loss: 4.667103
[INFO] Epoch: 6 , batch: 517 , training loss: 4.636513
[INFO] Epoch: 6 , batch: 518 , training loss: 4.613340
[INFO] Epoch: 6 , batch: 519 , training loss: 4.473952
[INFO] Epoch: 6 , batch: 520 , training loss: 4.694215
[INFO] Epoch: 6 , batch: 521 , training loss: 4.711068
[INFO] Epoch: 6 , batch: 522 , training loss: 4.771999
[INFO] Epoch: 6 , batch: 523 , training loss: 4.663207
[INFO] Epoch: 6 , batch: 524 , training loss: 4.914011
[INFO] Epoch: 6 , batch: 525 , training loss: 4.850441
[INFO] Epoch: 6 , batch: 526 , training loss: 4.602679
[INFO] Epoch: 6 , batch: 527 , training loss: 4.619365
[INFO] Epoch: 6 , batch: 528 , training loss: 4.699633
[INFO] Epoch: 6 , batch: 529 , training loss: 4.656185
[INFO] Epoch: 6 , batch: 530 , training loss: 4.466998
[INFO] Epoch: 6 , batch: 531 , training loss: 4.650352
[INFO] Epoch: 6 , batch: 532 , training loss: 4.499086
[INFO] Epoch: 6 , batch: 533 , training loss: 4.702291
[INFO] Epoch: 6 , batch: 534 , training loss: 4.661909
[INFO] Epoch: 6 , batch: 535 , training loss: 4.686421
[INFO] Epoch: 6 , batch: 536 , training loss: 4.516783
[INFO] Epoch: 6 , batch: 537 , training loss: 4.499153
[INFO] Epoch: 6 , batch: 538 , training loss: 4.572443
[INFO] Epoch: 6 , batch: 539 , training loss: 4.735062
[INFO] Epoch: 6 , batch: 540 , training loss: 5.355449
[INFO] Epoch: 6 , batch: 541 , training loss: 5.262835
[INFO] Epoch: 6 , batch: 542 , training loss: 5.051107
[INFO] Epoch: 7 , batch: 0 , training loss: 4.482384
[INFO] Epoch: 7 , batch: 1 , training loss: 4.260304
[INFO] Epoch: 7 , batch: 2 , training loss: 4.241755
[INFO] Epoch: 7 , batch: 3 , training loss: 4.048496
[INFO] Epoch: 7 , batch: 4 , training loss: 4.415342
[INFO] Epoch: 7 , batch: 5 , training loss: 4.067423
[INFO] Epoch: 7 , batch: 6 , training loss: 4.572050
[INFO] Epoch: 7 , batch: 7 , training loss: 4.227583
[INFO] Epoch: 7 , batch: 8 , training loss: 3.925954
[INFO] Epoch: 7 , batch: 9 , training loss: 4.074360
[INFO] Epoch: 7 , batch: 10 , training loss: 4.027781
[INFO] Epoch: 7 , batch: 11 , training loss: 3.987790
[INFO] Epoch: 7 , batch: 12 , training loss: 3.964588
[INFO] Epoch: 7 , batch: 13 , training loss: 3.939189
[INFO] Epoch: 7 , batch: 14 , training loss: 3.742461
[INFO] Epoch: 7 , batch: 15 , training loss: 4.011979
[INFO] Epoch: 7 , batch: 16 , training loss: 3.851936
[INFO] Epoch: 7 , batch: 17 , training loss: 4.029817
[INFO] Epoch: 7 , batch: 18 , training loss: 3.958636
[INFO] Epoch: 7 , batch: 19 , training loss: 3.692022
[INFO] Epoch: 7 , batch: 20 , training loss: 3.695499
[INFO] Epoch: 7 , batch: 21 , training loss: 3.814171
[INFO] Epoch: 7 , batch: 22 , training loss: 3.836507
[INFO] Epoch: 7 , batch: 23 , training loss: 3.988251
[INFO] Epoch: 7 , batch: 24 , training loss: 3.777043
[INFO] Epoch: 7 , batch: 25 , training loss: 3.992621
[INFO] Epoch: 7 , batch: 26 , training loss: 3.827044
[INFO] Epoch: 7 , batch: 27 , training loss: 3.769918
[INFO] Epoch: 7 , batch: 28 , training loss: 3.985439
[INFO] Epoch: 7 , batch: 29 , training loss: 3.729374
[INFO] Epoch: 7 , batch: 30 , training loss: 3.773546
[INFO] Epoch: 7 , batch: 31 , training loss: 3.888635
[INFO] Epoch: 7 , batch: 32 , training loss: 3.932485
[INFO] Epoch: 7 , batch: 33 , training loss: 3.932188
[INFO] Epoch: 7 , batch: 34 , training loss: 3.984922
[INFO] Epoch: 7 , batch: 35 , training loss: 3.915495
[INFO] Epoch: 7 , batch: 36 , training loss: 3.893115
[INFO] Epoch: 7 , batch: 37 , training loss: 3.793322
[INFO] Epoch: 7 , batch: 38 , training loss: 3.910377
[INFO] Epoch: 7 , batch: 39 , training loss: 3.742674
[INFO] Epoch: 7 , batch: 40 , training loss: 3.861982
[INFO] Epoch: 7 , batch: 41 , training loss: 3.973323
[INFO] Epoch: 7 , batch: 42 , training loss: 4.539773
[INFO] Epoch: 7 , batch: 43 , training loss: 4.194838
[INFO] Epoch: 7 , batch: 44 , training loss: 4.373793
[INFO] Epoch: 7 , batch: 45 , training loss: 4.517818
[INFO] Epoch: 7 , batch: 46 , training loss: 4.660304
[INFO] Epoch: 7 , batch: 47 , training loss: 4.092517
[INFO] Epoch: 7 , batch: 48 , training loss: 4.158333
[INFO] Epoch: 7 , batch: 49 , training loss: 4.209468
[INFO] Epoch: 7 , batch: 50 , training loss: 4.004425
[INFO] Epoch: 7 , batch: 51 , training loss: 4.130682
[INFO] Epoch: 7 , batch: 52 , training loss: 3.924926
[INFO] Epoch: 7 , batch: 53 , training loss: 4.069170
[INFO] Epoch: 7 , batch: 54 , training loss: 4.140214
[INFO] Epoch: 7 , batch: 55 , training loss: 4.188040
[INFO] Epoch: 7 , batch: 56 , training loss: 4.024867
[INFO] Epoch: 7 , batch: 57 , training loss: 3.900882
[INFO] Epoch: 7 , batch: 58 , training loss: 3.967563
[INFO] Epoch: 7 , batch: 59 , training loss: 4.064075
[INFO] Epoch: 7 , batch: 60 , training loss: 3.943124
[INFO] Epoch: 7 , batch: 61 , training loss: 4.028828
[INFO] Epoch: 7 , batch: 62 , training loss: 3.930160
[INFO] Epoch: 7 , batch: 63 , training loss: 4.102340
[INFO] Epoch: 7 , batch: 64 , training loss: 4.288544
[INFO] Epoch: 7 , batch: 65 , training loss: 3.983679
[INFO] Epoch: 7 , batch: 66 , training loss: 3.826687
[INFO] Epoch: 7 , batch: 67 , training loss: 3.895171
[INFO] Epoch: 7 , batch: 68 , training loss: 4.159813
[INFO] Epoch: 7 , batch: 69 , training loss: 3.918432
[INFO] Epoch: 7 , batch: 70 , training loss: 4.222692
[INFO] Epoch: 7 , batch: 71 , training loss: 4.031083
[INFO] Epoch: 7 , batch: 72 , training loss: 4.111199
[INFO] Epoch: 7 , batch: 73 , training loss: 4.059961
[INFO] Epoch: 7 , batch: 74 , training loss: 4.160519
[INFO] Epoch: 7 , batch: 75 , training loss: 3.948746
[INFO] Epoch: 7 , batch: 76 , training loss: 4.070093
[INFO] Epoch: 7 , batch: 77 , training loss: 4.043595
[INFO] Epoch: 7 , batch: 78 , training loss: 4.117033
[INFO] Epoch: 7 , batch: 79 , training loss: 3.937367
[INFO] Epoch: 7 , batch: 80 , training loss: 4.158981
[INFO] Epoch: 7 , batch: 81 , training loss: 4.108517
[INFO] Epoch: 7 , batch: 82 , training loss: 4.094217
[INFO] Epoch: 7 , batch: 83 , training loss: 4.191906
[INFO] Epoch: 7 , batch: 84 , training loss: 4.109390
[INFO] Epoch: 7 , batch: 85 , training loss: 4.162791
[INFO] Epoch: 7 , batch: 86 , training loss: 4.167902
[INFO] Epoch: 7 , batch: 87 , training loss: 4.146038
[INFO] Epoch: 7 , batch: 88 , training loss: 4.287873
[INFO] Epoch: 7 , batch: 89 , training loss: 4.054871
[INFO] Epoch: 7 , batch: 90 , training loss: 4.129035
[INFO] Epoch: 7 , batch: 91 , training loss: 4.035854
[INFO] Epoch: 7 , batch: 92 , training loss: 4.021252
[INFO] Epoch: 7 , batch: 93 , training loss: 4.146098
[INFO] Epoch: 7 , batch: 94 , training loss: 4.327010
[INFO] Epoch: 7 , batch: 95 , training loss: 4.077390
[INFO] Epoch: 7 , batch: 96 , training loss: 4.061391
[INFO] Epoch: 7 , batch: 97 , training loss: 4.033757
[INFO] Epoch: 7 , batch: 98 , training loss: 3.977938
[INFO] Epoch: 7 , batch: 99 , training loss: 4.063123
[INFO] Epoch: 7 , batch: 100 , training loss: 3.935552
[INFO] Epoch: 7 , batch: 101 , training loss: 4.017539
[INFO] Epoch: 7 , batch: 102 , training loss: 4.120042
[INFO] Epoch: 7 , batch: 103 , training loss: 3.864609
[INFO] Epoch: 7 , batch: 104 , training loss: 3.883768
[INFO] Epoch: 7 , batch: 105 , training loss: 4.180194
[INFO] Epoch: 7 , batch: 106 , training loss: 4.125058
[INFO] Epoch: 7 , batch: 107 , training loss: 4.026053
[INFO] Epoch: 7 , batch: 108 , training loss: 3.926619
[INFO] Epoch: 7 , batch: 109 , training loss: 3.812292
[INFO] Epoch: 7 , batch: 110 , training loss: 4.084156
[INFO] Epoch: 7 , batch: 111 , training loss: 4.126373
[INFO] Epoch: 7 , batch: 112 , training loss: 4.050546
[INFO] Epoch: 7 , batch: 113 , training loss: 4.003530
[INFO] Epoch: 7 , batch: 114 , training loss: 4.108213
[INFO] Epoch: 7 , batch: 115 , training loss: 4.057862
[INFO] Epoch: 7 , batch: 116 , training loss: 4.012661
[INFO] Epoch: 7 , batch: 117 , training loss: 4.252685
[INFO] Epoch: 7 , batch: 118 , training loss: 4.199665
[INFO] Epoch: 7 , batch: 119 , training loss: 4.331144
[INFO] Epoch: 7 , batch: 120 , training loss: 4.314783
[INFO] Epoch: 7 , batch: 121 , training loss: 4.121594
[INFO] Epoch: 7 , batch: 122 , training loss: 4.048272
[INFO] Epoch: 7 , batch: 123 , training loss: 4.114348
[INFO] Epoch: 7 , batch: 124 , training loss: 4.244543
[INFO] Epoch: 7 , batch: 125 , training loss: 3.954556
[INFO] Epoch: 7 , batch: 126 , training loss: 3.951339
[INFO] Epoch: 7 , batch: 127 , training loss: 3.993833
[INFO] Epoch: 7 , batch: 128 , training loss: 4.181179
[INFO] Epoch: 7 , batch: 129 , training loss: 4.094293
[INFO] Epoch: 7 , batch: 130 , training loss: 4.069285
[INFO] Epoch: 7 , batch: 131 , training loss: 4.112780
[INFO] Epoch: 7 , batch: 132 , training loss: 4.114161
[INFO] Epoch: 7 , batch: 133 , training loss: 4.042478
[INFO] Epoch: 7 , batch: 134 , training loss: 3.851913
[INFO] Epoch: 7 , batch: 135 , training loss: 3.874496
[INFO] Epoch: 7 , batch: 136 , training loss: 4.195086
[INFO] Epoch: 7 , batch: 137 , training loss: 4.092558
[INFO] Epoch: 7 , batch: 138 , training loss: 4.220340
[INFO] Epoch: 7 , batch: 139 , training loss: 4.921779
[INFO] Epoch: 7 , batch: 140 , training loss: 4.689069
[INFO] Epoch: 7 , batch: 141 , training loss: 4.424477
[INFO] Epoch: 7 , batch: 142 , training loss: 4.078350
[INFO] Epoch: 7 , batch: 143 , training loss: 4.187596
[INFO] Epoch: 7 , batch: 144 , training loss: 3.941811
[INFO] Epoch: 7 , batch: 145 , training loss: 4.132165
[INFO] Epoch: 7 , batch: 146 , training loss: 4.299063
[INFO] Epoch: 7 , batch: 147 , training loss: 3.963538
[INFO] Epoch: 7 , batch: 148 , training loss: 3.947243
[INFO] Epoch: 7 , batch: 149 , training loss: 4.026936
[INFO] Epoch: 7 , batch: 150 , training loss: 4.244032
[INFO] Epoch: 7 , batch: 151 , training loss: 4.062021
[INFO] Epoch: 7 , batch: 152 , training loss: 4.109313
[INFO] Epoch: 7 , batch: 153 , training loss: 4.146581
[INFO] Epoch: 7 , batch: 154 , training loss: 4.251006
[INFO] Epoch: 7 , batch: 155 , training loss: 4.492646
[INFO] Epoch: 7 , batch: 156 , training loss: 4.134891
[INFO] Epoch: 7 , batch: 157 , training loss: 4.119670
[INFO] Epoch: 7 , batch: 158 , training loss: 4.442955
[INFO] Epoch: 7 , batch: 159 , training loss: 4.304980
[INFO] Epoch: 7 , batch: 160 , training loss: 4.700682
[INFO] Epoch: 7 , batch: 161 , training loss: 4.615924
[INFO] Epoch: 7 , batch: 162 , training loss: 4.559149
[INFO] Epoch: 7 , batch: 163 , training loss: 4.653255
[INFO] Epoch: 7 , batch: 164 , training loss: 4.622733
[INFO] Epoch: 7 , batch: 165 , training loss: 4.543324
[INFO] Epoch: 7 , batch: 166 , training loss: 4.601176
[INFO] Epoch: 7 , batch: 167 , training loss: 4.975521
[INFO] Epoch: 7 , batch: 168 , training loss: 4.689169
[INFO] Epoch: 7 , batch: 169 , training loss: 4.593624
[INFO] Epoch: 7 , batch: 170 , training loss: 4.649064
[INFO] Epoch: 7 , batch: 171 , training loss: 4.167441
[INFO] Epoch: 7 , batch: 172 , training loss: 4.360061
[INFO] Epoch: 7 , batch: 173 , training loss: 4.687066
[INFO] Epoch: 7 , batch: 174 , training loss: 4.854936
[INFO] Epoch: 7 , batch: 175 , training loss: 5.176306
[INFO] Epoch: 7 , batch: 176 , training loss: 4.915917
[INFO] Epoch: 7 , batch: 177 , training loss: 4.529573
[INFO] Epoch: 7 , batch: 178 , training loss: 4.497377
[INFO] Epoch: 7 , batch: 179 , training loss: 4.507414
[INFO] Epoch: 7 , batch: 180 , training loss: 4.454015
[INFO] Epoch: 7 , batch: 181 , training loss: 4.685064
[INFO] Epoch: 7 , batch: 182 , training loss: 4.607745
[INFO] Epoch: 7 , batch: 183 , training loss: 4.570238
[INFO] Epoch: 7 , batch: 184 , training loss: 4.427236
[INFO] Epoch: 7 , batch: 185 , training loss: 4.387926
[INFO] Epoch: 7 , batch: 186 , training loss: 4.552077
[INFO] Epoch: 7 , batch: 187 , training loss: 4.674014
[INFO] Epoch: 7 , batch: 188 , training loss: 4.632589
[INFO] Epoch: 7 , batch: 189 , training loss: 4.513947
[INFO] Epoch: 7 , batch: 190 , training loss: 4.500525
[INFO] Epoch: 7 , batch: 191 , training loss: 4.686913
[INFO] Epoch: 7 , batch: 192 , training loss: 4.409143
[INFO] Epoch: 7 , batch: 193 , training loss: 4.573253
[INFO] Epoch: 7 , batch: 194 , training loss: 4.470408
[INFO] Epoch: 7 , batch: 195 , training loss: 4.469754
[INFO] Epoch: 7 , batch: 196 , training loss: 4.275151
[INFO] Epoch: 7 , batch: 197 , training loss: 4.432487
[INFO] Epoch: 7 , batch: 198 , training loss: 4.312627
[INFO] Epoch: 7 , batch: 199 , training loss: 4.430984
[INFO] Epoch: 7 , batch: 200 , training loss: 4.342519
[INFO] Epoch: 7 , batch: 201 , training loss: 4.252294
[INFO] Epoch: 7 , batch: 202 , training loss: 4.220621
[INFO] Epoch: 7 , batch: 203 , training loss: 4.273833
[INFO] Epoch: 7 , batch: 204 , training loss: 4.423532
[INFO] Epoch: 7 , batch: 205 , training loss: 4.000905
[INFO] Epoch: 7 , batch: 206 , training loss: 3.922485
[INFO] Epoch: 7 , batch: 207 , training loss: 3.916881
[INFO] Epoch: 7 , batch: 208 , training loss: 4.318936
[INFO] Epoch: 7 , batch: 209 , training loss: 4.220582
[INFO] Epoch: 7 , batch: 210 , training loss: 4.261682
[INFO] Epoch: 7 , batch: 211 , training loss: 4.242252
[INFO] Epoch: 7 , batch: 212 , training loss: 4.349505
[INFO] Epoch: 7 , batch: 213 , training loss: 4.303840
[INFO] Epoch: 7 , batch: 214 , training loss: 4.379981
[INFO] Epoch: 7 , batch: 215 , training loss: 4.598852
[INFO] Epoch: 7 , batch: 216 , training loss: 4.310501
[INFO] Epoch: 7 , batch: 217 , training loss: 4.231641
[INFO] Epoch: 7 , batch: 218 , training loss: 4.219969
[INFO] Epoch: 7 , batch: 219 , training loss: 4.353289
[INFO] Epoch: 7 , batch: 220 , training loss: 4.156200
[INFO] Epoch: 7 , batch: 221 , training loss: 4.168483
[INFO] Epoch: 7 , batch: 222 , training loss: 4.305383
[INFO] Epoch: 7 , batch: 223 , training loss: 4.387491
[INFO] Epoch: 7 , batch: 224 , training loss: 4.461434
[INFO] Epoch: 7 , batch: 225 , training loss: 4.343691
[INFO] Epoch: 7 , batch: 226 , training loss: 4.486084
[INFO] Epoch: 7 , batch: 227 , training loss: 4.426664
[INFO] Epoch: 7 , batch: 228 , training loss: 4.487674
[INFO] Epoch: 7 , batch: 229 , training loss: 4.344167
[INFO] Epoch: 7 , batch: 230 , training loss: 4.176833
[INFO] Epoch: 7 , batch: 231 , training loss: 4.018033
[INFO] Epoch: 7 , batch: 232 , training loss: 4.195908
[INFO] Epoch: 7 , batch: 233 , training loss: 4.183827
[INFO] Epoch: 7 , batch: 234 , training loss: 3.883091
[INFO] Epoch: 7 , batch: 235 , training loss: 3.984078
[INFO] Epoch: 7 , batch: 236 , training loss: 4.147869
[INFO] Epoch: 7 , batch: 237 , training loss: 4.305736
[INFO] Epoch: 7 , batch: 238 , training loss: 4.048207
[INFO] Epoch: 7 , batch: 239 , training loss: 4.127803
[INFO] Epoch: 7 , batch: 240 , training loss: 4.200123
[INFO] Epoch: 7 , batch: 241 , training loss: 3.986490
[INFO] Epoch: 7 , batch: 242 , training loss: 3.999863
[INFO] Epoch: 7 , batch: 243 , training loss: 4.340611
[INFO] Epoch: 7 , batch: 244 , training loss: 4.240924
[INFO] Epoch: 7 , batch: 245 , training loss: 4.241940
[INFO] Epoch: 7 , batch: 246 , training loss: 3.907039
[INFO] Epoch: 7 , batch: 247 , training loss: 4.069326
[INFO] Epoch: 7 , batch: 248 , training loss: 4.163289
[INFO] Epoch: 7 , batch: 249 , training loss: 4.150512
[INFO] Epoch: 7 , batch: 250 , training loss: 3.904958
[INFO] Epoch: 7 , batch: 251 , training loss: 4.406606
[INFO] Epoch: 7 , batch: 252 , training loss: 4.075595
[INFO] Epoch: 7 , batch: 253 , training loss: 4.039272
[INFO] Epoch: 7 , batch: 254 , training loss: 4.349397
[INFO] Epoch: 7 , batch: 255 , training loss: 4.324260
[INFO] Epoch: 7 , batch: 256 , training loss: 4.276273
[INFO] Epoch: 7 , batch: 257 , training loss: 4.456061
[INFO] Epoch: 7 , batch: 258 , training loss: 4.495396
[INFO] Epoch: 7 , batch: 259 , training loss: 4.551747
[INFO] Epoch: 7 , batch: 260 , training loss: 4.230634
[INFO] Epoch: 7 , batch: 261 , training loss: 4.412663
[INFO] Epoch: 7 , batch: 262 , training loss: 4.642911
[INFO] Epoch: 7 , batch: 263 , training loss: 4.783689
[INFO] Epoch: 7 , batch: 264 , training loss: 4.069129
[INFO] Epoch: 7 , batch: 265 , training loss: 4.209068
[INFO] Epoch: 7 , batch: 266 , training loss: 4.696901
[INFO] Epoch: 7 , batch: 267 , training loss: 4.385734
[INFO] Epoch: 7 , batch: 268 , training loss: 4.285302
[INFO] Epoch: 7 , batch: 269 , training loss: 4.304407
[INFO] Epoch: 7 , batch: 270 , training loss: 4.305898
[INFO] Epoch: 7 , batch: 271 , training loss: 4.338274
[INFO] Epoch: 7 , batch: 272 , training loss: 4.320522
[INFO] Epoch: 7 , batch: 273 , training loss: 4.310862
[INFO] Epoch: 7 , batch: 274 , training loss: 4.426020
[INFO] Epoch: 7 , batch: 275 , training loss: 4.321294
[INFO] Epoch: 7 , batch: 276 , training loss: 4.371953
[INFO] Epoch: 7 , batch: 277 , training loss: 4.509605
[INFO] Epoch: 7 , batch: 278 , training loss: 4.128772
[INFO] Epoch: 7 , batch: 279 , training loss: 4.148637
[INFO] Epoch: 7 , batch: 280 , training loss: 4.110619
[INFO] Epoch: 7 , batch: 281 , training loss: 4.263870
[INFO] Epoch: 7 , batch: 282 , training loss: 4.138176
[INFO] Epoch: 7 , batch: 283 , training loss: 4.198394
[INFO] Epoch: 7 , batch: 284 , training loss: 4.222887
[INFO] Epoch: 7 , batch: 285 , training loss: 4.180315
[INFO] Epoch: 7 , batch: 286 , training loss: 4.157843
[INFO] Epoch: 7 , batch: 287 , training loss: 4.088123
[INFO] Epoch: 7 , batch: 288 , training loss: 4.109612
[INFO] Epoch: 7 , batch: 289 , training loss: 4.171312
[INFO] Epoch: 7 , batch: 290 , training loss: 3.935019
[INFO] Epoch: 7 , batch: 291 , training loss: 3.902881
[INFO] Epoch: 7 , batch: 292 , training loss: 4.022048
[INFO] Epoch: 7 , batch: 293 , training loss: 3.944287
[INFO] Epoch: 7 , batch: 294 , training loss: 4.655168
[INFO] Epoch: 7 , batch: 295 , training loss: 4.427152
[INFO] Epoch: 7 , batch: 296 , training loss: 4.361752
[INFO] Epoch: 7 , batch: 297 , training loss: 4.293922
[INFO] Epoch: 7 , batch: 298 , training loss: 4.136139
[INFO] Epoch: 7 , batch: 299 , training loss: 4.157718
[INFO] Epoch: 7 , batch: 300 , training loss: 4.117595
[INFO] Epoch: 7 , batch: 301 , training loss: 4.039764
[INFO] Epoch: 7 , batch: 302 , training loss: 4.230026
[INFO] Epoch: 7 , batch: 303 , training loss: 4.228438
[INFO] Epoch: 7 , batch: 304 , training loss: 4.424211
[INFO] Epoch: 7 , batch: 305 , training loss: 4.167511
[INFO] Epoch: 7 , batch: 306 , training loss: 4.302167
[INFO] Epoch: 7 , batch: 307 , training loss: 4.294454
[INFO] Epoch: 7 , batch: 308 , training loss: 4.146708
[INFO] Epoch: 7 , batch: 309 , training loss: 4.183517
[INFO] Epoch: 7 , batch: 310 , training loss: 4.032963
[INFO] Epoch: 7 , batch: 311 , training loss: 4.039996
[INFO] Epoch: 7 , batch: 312 , training loss: 3.929097
[INFO] Epoch: 7 , batch: 313 , training loss: 4.097142
[INFO] Epoch: 7 , batch: 314 , training loss: 4.134112
[INFO] Epoch: 7 , batch: 315 , training loss: 4.180116
[INFO] Epoch: 7 , batch: 316 , training loss: 4.490047
[INFO] Epoch: 7 , batch: 317 , training loss: 5.065165
[INFO] Epoch: 7 , batch: 318 , training loss: 5.158537
[INFO] Epoch: 7 , batch: 319 , training loss: 4.672256
[INFO] Epoch: 7 , batch: 320 , training loss: 4.196433
[INFO] Epoch: 7 , batch: 321 , training loss: 3.977150
[INFO] Epoch: 7 , batch: 322 , training loss: 4.100186
[INFO] Epoch: 7 , batch: 323 , training loss: 4.120478
[INFO] Epoch: 7 , batch: 324 , training loss: 4.120175
[INFO] Epoch: 7 , batch: 325 , training loss: 4.289633
[INFO] Epoch: 7 , batch: 326 , training loss: 4.355290
[INFO] Epoch: 7 , batch: 327 , training loss: 4.225553
[INFO] Epoch: 7 , batch: 328 , training loss: 4.219777
[INFO] Epoch: 7 , batch: 329 , training loss: 4.140594
[INFO] Epoch: 7 , batch: 330 , training loss: 4.120642
[INFO] Epoch: 7 , batch: 331 , training loss: 4.278008
[INFO] Epoch: 7 , batch: 332 , training loss: 4.088797
[INFO] Epoch: 7 , batch: 333 , training loss: 4.059431
[INFO] Epoch: 7 , batch: 334 , training loss: 4.121883
[INFO] Epoch: 7 , batch: 335 , training loss: 4.252226
[INFO] Epoch: 7 , batch: 336 , training loss: 4.229228
[INFO] Epoch: 7 , batch: 337 , training loss: 4.293638
[INFO] Epoch: 7 , batch: 338 , training loss: 4.502710
[INFO] Epoch: 7 , batch: 339 , training loss: 4.333754
[INFO] Epoch: 7 , batch: 340 , training loss: 4.503345
[INFO] Epoch: 7 , batch: 341 , training loss: 4.273546
[INFO] Epoch: 7 , batch: 342 , training loss: 4.035385
[INFO] Epoch: 7 , batch: 343 , training loss: 4.122590
[INFO] Epoch: 7 , batch: 344 , training loss: 3.984504
[INFO] Epoch: 7 , batch: 345 , training loss: 4.110501
[INFO] Epoch: 7 , batch: 346 , training loss: 4.197371
[INFO] Epoch: 7 , batch: 347 , training loss: 4.056363
[INFO] Epoch: 7 , batch: 348 , training loss: 4.275993
[INFO] Epoch: 7 , batch: 349 , training loss: 4.359414
[INFO] Epoch: 7 , batch: 350 , training loss: 4.080895
[INFO] Epoch: 7 , batch: 351 , training loss: 4.175510
[INFO] Epoch: 7 , batch: 352 , training loss: 4.198408
[INFO] Epoch: 7 , batch: 353 , training loss: 4.156830
[INFO] Epoch: 7 , batch: 354 , training loss: 4.260784
[INFO] Epoch: 7 , batch: 355 , training loss: 4.304730
[INFO] Epoch: 7 , batch: 356 , training loss: 4.142180
[INFO] Epoch: 7 , batch: 357 , training loss: 4.244038
[INFO] Epoch: 7 , batch: 358 , training loss: 4.176629
[INFO] Epoch: 7 , batch: 359 , training loss: 4.149466
[INFO] Epoch: 7 , batch: 360 , training loss: 4.197972
[INFO] Epoch: 7 , batch: 361 , training loss: 4.174740
[INFO] Epoch: 7 , batch: 362 , training loss: 4.290683
[INFO] Epoch: 7 , batch: 363 , training loss: 4.179645
[INFO] Epoch: 7 , batch: 364 , training loss: 4.207712
[INFO] Epoch: 7 , batch: 365 , training loss: 4.118997
[INFO] Epoch: 7 , batch: 366 , training loss: 4.285071
[INFO] Epoch: 7 , batch: 367 , training loss: 4.357157
[INFO] Epoch: 7 , batch: 368 , training loss: 4.899161
[INFO] Epoch: 7 , batch: 369 , training loss: 4.480522
[INFO] Epoch: 7 , batch: 370 , training loss: 4.224885
[INFO] Epoch: 7 , batch: 371 , training loss: 4.765751
[INFO] Epoch: 7 , batch: 372 , training loss: 5.025195
[INFO] Epoch: 7 , batch: 373 , training loss: 5.055190
[INFO] Epoch: 7 , batch: 374 , training loss: 5.114770
[INFO] Epoch: 7 , batch: 375 , training loss: 5.077221
[INFO] Epoch: 7 , batch: 376 , training loss: 4.973334
[INFO] Epoch: 7 , batch: 377 , training loss: 4.642653
[INFO] Epoch: 7 , batch: 378 , training loss: 4.768931
[INFO] Epoch: 7 , batch: 379 , training loss: 4.774856
[INFO] Epoch: 7 , batch: 380 , training loss: 4.908779
[INFO] Epoch: 7 , batch: 381 , training loss: 4.694018
[INFO] Epoch: 7 , batch: 382 , training loss: 4.920003
[INFO] Epoch: 7 , batch: 383 , training loss: 4.991126
[INFO] Epoch: 7 , batch: 384 , training loss: 5.028047
[INFO] Epoch: 7 , batch: 385 , training loss: 4.761460
[INFO] Epoch: 7 , batch: 386 , training loss: 4.929731
[INFO] Epoch: 7 , batch: 387 , training loss: 4.854206
[INFO] Epoch: 7 , batch: 388 , training loss: 4.651423
[INFO] Epoch: 7 , batch: 389 , training loss: 4.511490
[INFO] Epoch: 7 , batch: 390 , training loss: 4.455727
[INFO] Epoch: 7 , batch: 391 , training loss: 4.526901
[INFO] Epoch: 7 , batch: 392 , training loss: 4.878761
[INFO] Epoch: 7 , batch: 393 , training loss: 4.759624
[INFO] Epoch: 7 , batch: 394 , training loss: 4.809078
[INFO] Epoch: 7 , batch: 395 , training loss: 4.640987
[INFO] Epoch: 7 , batch: 396 , training loss: 4.398717
[INFO] Epoch: 7 , batch: 397 , training loss: 4.578274
[INFO] Epoch: 7 , batch: 398 , training loss: 4.414221
[INFO] Epoch: 7 , batch: 399 , training loss: 4.452964
[INFO] Epoch: 7 , batch: 400 , training loss: 4.407919
[INFO] Epoch: 7 , batch: 401 , training loss: 4.851659
[INFO] Epoch: 7 , batch: 402 , training loss: 4.606203
[INFO] Epoch: 7 , batch: 403 , training loss: 4.417135
[INFO] Epoch: 7 , batch: 404 , training loss: 4.603235
[INFO] Epoch: 7 , batch: 405 , training loss: 4.653821
[INFO] Epoch: 7 , batch: 406 , training loss: 4.545314
[INFO] Epoch: 7 , batch: 407 , training loss: 4.622179
[INFO] Epoch: 7 , batch: 408 , training loss: 4.552482
[INFO] Epoch: 7 , batch: 409 , training loss: 4.558288
[INFO] Epoch: 7 , batch: 410 , training loss: 4.607316
[INFO] Epoch: 7 , batch: 411 , training loss: 4.825294
[INFO] Epoch: 7 , batch: 412 , training loss: 4.636060
[INFO] Epoch: 7 , batch: 413 , training loss: 4.492694
[INFO] Epoch: 7 , batch: 414 , training loss: 4.515614
[INFO] Epoch: 7 , batch: 415 , training loss: 4.533653
[INFO] Epoch: 7 , batch: 416 , training loss: 4.619825
[INFO] Epoch: 7 , batch: 417 , training loss: 4.548548
[INFO] Epoch: 7 , batch: 418 , training loss: 4.561103
[INFO] Epoch: 7 , batch: 419 , training loss: 4.517083
[INFO] Epoch: 7 , batch: 420 , training loss: 4.513359
[INFO] Epoch: 7 , batch: 421 , training loss: 4.507305
[INFO] Epoch: 7 , batch: 422 , training loss: 4.384949
[INFO] Epoch: 7 , batch: 423 , training loss: 4.614010
[INFO] Epoch: 7 , batch: 424 , training loss: 4.755996
[INFO] Epoch: 7 , batch: 425 , training loss: 4.628241
[INFO] Epoch: 7 , batch: 426 , training loss: 4.355313
[INFO] Epoch: 7 , batch: 427 , training loss: 4.595572
[INFO] Epoch: 7 , batch: 428 , training loss: 4.507416
[INFO] Epoch: 7 , batch: 429 , training loss: 4.316637
[INFO] Epoch: 7 , batch: 430 , training loss: 4.597407
[INFO] Epoch: 7 , batch: 431 , training loss: 4.173374
[INFO] Epoch: 7 , batch: 432 , training loss: 4.264103
[INFO] Epoch: 7 , batch: 433 , training loss: 4.281030
[INFO] Epoch: 7 , batch: 434 , training loss: 4.145425
[INFO] Epoch: 7 , batch: 435 , training loss: 4.500948
[INFO] Epoch: 7 , batch: 436 , training loss: 4.619902
[INFO] Epoch: 7 , batch: 437 , training loss: 4.364002
[INFO] Epoch: 7 , batch: 438 , training loss: 4.168815
[INFO] Epoch: 7 , batch: 439 , training loss: 4.411306
[INFO] Epoch: 7 , batch: 440 , training loss: 4.553421
[INFO] Epoch: 7 , batch: 441 , training loss: 4.627542
[INFO] Epoch: 7 , batch: 442 , training loss: 4.411965
[INFO] Epoch: 7 , batch: 443 , training loss: 4.616924
[INFO] Epoch: 7 , batch: 444 , training loss: 4.226589
[INFO] Epoch: 7 , batch: 445 , training loss: 4.117514
[INFO] Epoch: 7 , batch: 446 , training loss: 4.008734
[INFO] Epoch: 7 , batch: 447 , training loss: 4.243977
[INFO] Epoch: 7 , batch: 448 , training loss: 4.394246
[INFO] Epoch: 7 , batch: 449 , training loss: 4.810425
[INFO] Epoch: 7 , batch: 450 , training loss: 4.842785
[INFO] Epoch: 7 , batch: 451 , training loss: 4.735278
[INFO] Epoch: 7 , batch: 452 , training loss: 4.503688
[INFO] Epoch: 7 , batch: 453 , training loss: 4.285055
[INFO] Epoch: 7 , batch: 454 , training loss: 4.429211
[INFO] Epoch: 7 , batch: 455 , training loss: 4.478362
[INFO] Epoch: 7 , batch: 456 , training loss: 4.456666
[INFO] Epoch: 7 , batch: 457 , training loss: 4.574679
[INFO] Epoch: 7 , batch: 458 , training loss: 4.291568
[INFO] Epoch: 7 , batch: 459 , training loss: 4.270178
[INFO] Epoch: 7 , batch: 460 , training loss: 4.400344
[INFO] Epoch: 7 , batch: 461 , training loss: 4.388012
[INFO] Epoch: 7 , batch: 462 , training loss: 4.444374
[INFO] Epoch: 7 , batch: 463 , training loss: 4.298256
[INFO] Epoch: 7 , batch: 464 , training loss: 4.541472
[INFO] Epoch: 7 , batch: 465 , training loss: 4.457742
[INFO] Epoch: 7 , batch: 466 , training loss: 4.515978
[INFO] Epoch: 7 , batch: 467 , training loss: 4.532086
[INFO] Epoch: 7 , batch: 468 , training loss: 4.489353
[INFO] Epoch: 7 , batch: 469 , training loss: 4.531127
[INFO] Epoch: 7 , batch: 470 , training loss: 4.295283
[INFO] Epoch: 7 , batch: 471 , training loss: 4.408146
[INFO] Epoch: 7 , batch: 472 , training loss: 4.446569
[INFO] Epoch: 7 , batch: 473 , training loss: 4.377207
[INFO] Epoch: 7 , batch: 474 , training loss: 4.204341
[INFO] Epoch: 7 , batch: 475 , training loss: 4.042687
[INFO] Epoch: 7 , batch: 476 , training loss: 4.431644
[INFO] Epoch: 7 , batch: 477 , training loss: 4.549209
[INFO] Epoch: 7 , batch: 478 , training loss: 4.624718
[INFO] Epoch: 7 , batch: 479 , training loss: 4.578653
[INFO] Epoch: 7 , batch: 480 , training loss: 4.713642
[INFO] Epoch: 7 , batch: 481 , training loss: 4.516137
[INFO] Epoch: 7 , batch: 482 , training loss: 4.684522
[INFO] Epoch: 7 , batch: 483 , training loss: 4.509667
[INFO] Epoch: 7 , batch: 484 , training loss: 4.309374
[INFO] Epoch: 7 , batch: 485 , training loss: 4.416950
[INFO] Epoch: 7 , batch: 486 , training loss: 4.299275
[INFO] Epoch: 7 , batch: 487 , training loss: 4.292608
[INFO] Epoch: 7 , batch: 488 , training loss: 4.497487
[INFO] Epoch: 7 , batch: 489 , training loss: 4.353892
[INFO] Epoch: 7 , batch: 490 , training loss: 4.418612
[INFO] Epoch: 7 , batch: 491 , training loss: 4.406219
[INFO] Epoch: 7 , batch: 492 , training loss: 4.310419
[INFO] Epoch: 7 , batch: 493 , training loss: 4.509051
[INFO] Epoch: 7 , batch: 494 , training loss: 4.427657
[INFO] Epoch: 7 , batch: 495 , training loss: 4.547168
[INFO] Epoch: 7 , batch: 496 , training loss: 4.423283
[INFO] Epoch: 7 , batch: 497 , training loss: 4.429941
[INFO] Epoch: 7 , batch: 498 , training loss: 4.476111
[INFO] Epoch: 7 , batch: 499 , training loss: 4.539823
[INFO] Epoch: 7 , batch: 500 , training loss: 4.744678
[INFO] Epoch: 7 , batch: 501 , training loss: 5.123910
[INFO] Epoch: 7 , batch: 502 , training loss: 5.260447
[INFO] Epoch: 7 , batch: 503 , training loss: 4.968015
[INFO] Epoch: 7 , batch: 504 , training loss: 5.048361
[INFO] Epoch: 7 , batch: 505 , training loss: 4.965922
[INFO] Epoch: 7 , batch: 506 , training loss: 4.922105
[INFO] Epoch: 7 , batch: 507 , training loss: 4.988066
[INFO] Epoch: 7 , batch: 508 , training loss: 4.946923
[INFO] Epoch: 7 , batch: 509 , training loss: 4.671038
[INFO] Epoch: 7 , batch: 510 , training loss: 4.732274
[INFO] Epoch: 7 , batch: 511 , training loss: 4.637097
[INFO] Epoch: 7 , batch: 512 , training loss: 4.710830
[INFO] Epoch: 7 , batch: 513 , training loss: 4.951137
[INFO] Epoch: 7 , batch: 514 , training loss: 4.603143
[INFO] Epoch: 7 , batch: 515 , training loss: 4.878388
[INFO] Epoch: 7 , batch: 516 , training loss: 4.651732
[INFO] Epoch: 7 , batch: 517 , training loss: 4.617163
[INFO] Epoch: 7 , batch: 518 , training loss: 4.592388
[INFO] Epoch: 7 , batch: 519 , training loss: 4.443529
[INFO] Epoch: 7 , batch: 520 , training loss: 4.667418
[INFO] Epoch: 7 , batch: 521 , training loss: 4.670073
[INFO] Epoch: 7 , batch: 522 , training loss: 4.738947
[INFO] Epoch: 7 , batch: 523 , training loss: 4.641074
[INFO] Epoch: 7 , batch: 524 , training loss: 4.909865
[INFO] Epoch: 7 , batch: 525 , training loss: 4.822081
[INFO] Epoch: 7 , batch: 526 , training loss: 4.575196
[INFO] Epoch: 7 , batch: 527 , training loss: 4.606896
[INFO] Epoch: 7 , batch: 528 , training loss: 4.679842
[INFO] Epoch: 7 , batch: 529 , training loss: 4.637194
[INFO] Epoch: 7 , batch: 530 , training loss: 4.453793
[INFO] Epoch: 7 , batch: 531 , training loss: 4.617590
[INFO] Epoch: 7 , batch: 532 , training loss: 4.483152
[INFO] Epoch: 7 , batch: 533 , training loss: 4.677801
[INFO] Epoch: 7 , batch: 534 , training loss: 4.643456
[INFO] Epoch: 7 , batch: 535 , training loss: 4.660699
[INFO] Epoch: 7 , batch: 536 , training loss: 4.486793
[INFO] Epoch: 7 , batch: 537 , training loss: 4.481053
[INFO] Epoch: 7 , batch: 538 , training loss: 4.544319
[INFO] Epoch: 7 , batch: 539 , training loss: 4.710902
[INFO] Epoch: 7 , batch: 540 , training loss: 5.312783
[INFO] Epoch: 7 , batch: 541 , training loss: 5.215867
[INFO] Epoch: 7 , batch: 542 , training loss: 5.016628
[INFO] Epoch: 8 , batch: 0 , training loss: 4.351546
[INFO] Epoch: 8 , batch: 1 , training loss: 4.179166
[INFO] Epoch: 8 , batch: 2 , training loss: 4.130691
[INFO] Epoch: 8 , batch: 3 , training loss: 3.999635
[INFO] Epoch: 8 , batch: 4 , training loss: 4.352924
[INFO] Epoch: 8 , batch: 5 , training loss: 4.000208
[INFO] Epoch: 8 , batch: 6 , training loss: 4.488484
[INFO] Epoch: 8 , batch: 7 , training loss: 4.190958
[INFO] Epoch: 8 , batch: 8 , training loss: 3.891541
[INFO] Epoch: 8 , batch: 9 , training loss: 4.034272
[INFO] Epoch: 8 , batch: 10 , training loss: 3.978878
[INFO] Epoch: 8 , batch: 11 , training loss: 3.945225
[INFO] Epoch: 8 , batch: 12 , training loss: 3.902672
[INFO] Epoch: 8 , batch: 13 , training loss: 3.899430
[INFO] Epoch: 8 , batch: 14 , training loss: 3.698182
[INFO] Epoch: 8 , batch: 15 , training loss: 3.961867
[INFO] Epoch: 8 , batch: 16 , training loss: 3.809257
[INFO] Epoch: 8 , batch: 17 , training loss: 3.986265
[INFO] Epoch: 8 , batch: 18 , training loss: 3.934035
[INFO] Epoch: 8 , batch: 19 , training loss: 3.658717
[INFO] Epoch: 8 , batch: 20 , training loss: 3.659547
[INFO] Epoch: 8 , batch: 21 , training loss: 3.770034
[INFO] Epoch: 8 , batch: 22 , training loss: 3.783875
[INFO] Epoch: 8 , batch: 23 , training loss: 3.927394
[INFO] Epoch: 8 , batch: 24 , training loss: 3.751301
[INFO] Epoch: 8 , batch: 25 , training loss: 3.941273
[INFO] Epoch: 8 , batch: 26 , training loss: 3.785007
[INFO] Epoch: 8 , batch: 27 , training loss: 3.734141
[INFO] Epoch: 8 , batch: 28 , training loss: 3.938657
[INFO] Epoch: 8 , batch: 29 , training loss: 3.691190
[INFO] Epoch: 8 , batch: 30 , training loss: 3.751137
[INFO] Epoch: 8 , batch: 31 , training loss: 3.857623
[INFO] Epoch: 8 , batch: 32 , training loss: 3.881002
[INFO] Epoch: 8 , batch: 33 , training loss: 3.892283
[INFO] Epoch: 8 , batch: 34 , training loss: 3.925289
[INFO] Epoch: 8 , batch: 35 , training loss: 3.868235
[INFO] Epoch: 8 , batch: 36 , training loss: 3.837312
[INFO] Epoch: 8 , batch: 37 , training loss: 3.758728
[INFO] Epoch: 8 , batch: 38 , training loss: 3.854948
[INFO] Epoch: 8 , batch: 39 , training loss: 3.711034
[INFO] Epoch: 8 , batch: 40 , training loss: 3.834771
[INFO] Epoch: 8 , batch: 41 , training loss: 3.923202
[INFO] Epoch: 8 , batch: 42 , training loss: 4.443604
[INFO] Epoch: 8 , batch: 43 , training loss: 4.101609
[INFO] Epoch: 8 , batch: 44 , training loss: 4.309522
[INFO] Epoch: 8 , batch: 45 , training loss: 4.427662
[INFO] Epoch: 8 , batch: 46 , training loss: 4.538431
[INFO] Epoch: 8 , batch: 47 , training loss: 4.071179
[INFO] Epoch: 8 , batch: 48 , training loss: 4.082870
[INFO] Epoch: 8 , batch: 49 , training loss: 4.165360
[INFO] Epoch: 8 , batch: 50 , training loss: 3.960356
[INFO] Epoch: 8 , batch: 51 , training loss: 4.079254
[INFO] Epoch: 8 , batch: 52 , training loss: 3.915980
[INFO] Epoch: 8 , batch: 53 , training loss: 4.044668
[INFO] Epoch: 8 , batch: 54 , training loss: 4.106012
[INFO] Epoch: 8 , batch: 55 , training loss: 4.141826
[INFO] Epoch: 8 , batch: 56 , training loss: 3.979816
[INFO] Epoch: 8 , batch: 57 , training loss: 3.880167
[INFO] Epoch: 8 , batch: 58 , training loss: 3.923174
[INFO] Epoch: 8 , batch: 59 , training loss: 4.033473
[INFO] Epoch: 8 , batch: 60 , training loss: 3.913750
[INFO] Epoch: 8 , batch: 61 , training loss: 3.996848
[INFO] Epoch: 8 , batch: 62 , training loss: 3.901168
[INFO] Epoch: 8 , batch: 63 , training loss: 4.096815
[INFO] Epoch: 8 , batch: 64 , training loss: 4.253395
[INFO] Epoch: 8 , batch: 65 , training loss: 3.953507
[INFO] Epoch: 8 , batch: 66 , training loss: 3.780385
[INFO] Epoch: 8 , batch: 67 , training loss: 3.866927
[INFO] Epoch: 8 , batch: 68 , training loss: 4.127937
[INFO] Epoch: 8 , batch: 69 , training loss: 3.906708
[INFO] Epoch: 8 , batch: 70 , training loss: 4.188502
[INFO] Epoch: 8 , batch: 71 , training loss: 4.000196
[INFO] Epoch: 8 , batch: 72 , training loss: 4.081781
[INFO] Epoch: 8 , batch: 73 , training loss: 4.023245
[INFO] Epoch: 8 , batch: 74 , training loss: 4.132715
[INFO] Epoch: 8 , batch: 75 , training loss: 3.921031
[INFO] Epoch: 8 , batch: 76 , training loss: 4.041992
[INFO] Epoch: 8 , batch: 77 , training loss: 4.017281
[INFO] Epoch: 8 , batch: 78 , training loss: 4.087196
[INFO] Epoch: 8 , batch: 79 , training loss: 3.919217
[INFO] Epoch: 8 , batch: 80 , training loss: 4.132068
[INFO] Epoch: 8 , batch: 81 , training loss: 4.081531
[INFO] Epoch: 8 , batch: 82 , training loss: 4.067150
[INFO] Epoch: 8 , batch: 83 , training loss: 4.159051
[INFO] Epoch: 8 , batch: 84 , training loss: 4.078354
[INFO] Epoch: 8 , batch: 85 , training loss: 4.139002
[INFO] Epoch: 8 , batch: 86 , training loss: 4.131599
[INFO] Epoch: 8 , batch: 87 , training loss: 4.120239
[INFO] Epoch: 8 , batch: 88 , training loss: 4.241903
[INFO] Epoch: 8 , batch: 89 , training loss: 4.027870
[INFO] Epoch: 8 , batch: 90 , training loss: 4.099288
[INFO] Epoch: 8 , batch: 91 , training loss: 4.011576
[INFO] Epoch: 8 , batch: 92 , training loss: 4.003241
[INFO] Epoch: 8 , batch: 93 , training loss: 4.128655
[INFO] Epoch: 8 , batch: 94 , training loss: 4.300694
[INFO] Epoch: 8 , batch: 95 , training loss: 4.055945
[INFO] Epoch: 8 , batch: 96 , training loss: 4.033477
[INFO] Epoch: 8 , batch: 97 , training loss: 3.992797
[INFO] Epoch: 8 , batch: 98 , training loss: 3.945442
[INFO] Epoch: 8 , batch: 99 , training loss: 4.024601
[INFO] Epoch: 8 , batch: 100 , training loss: 3.902842
[INFO] Epoch: 8 , batch: 101 , training loss: 3.991594
[INFO] Epoch: 8 , batch: 102 , training loss: 4.095417
[INFO] Epoch: 8 , batch: 103 , training loss: 3.853916
[INFO] Epoch: 8 , batch: 104 , training loss: 3.857279
[INFO] Epoch: 8 , batch: 105 , training loss: 4.148068
[INFO] Epoch: 8 , batch: 106 , training loss: 4.109563
[INFO] Epoch: 8 , batch: 107 , training loss: 3.976225
[INFO] Epoch: 8 , batch: 108 , training loss: 3.918542
[INFO] Epoch: 8 , batch: 109 , training loss: 3.803024
[INFO] Epoch: 8 , batch: 110 , training loss: 4.032495
[INFO] Epoch: 8 , batch: 111 , training loss: 4.087414
[INFO] Epoch: 8 , batch: 112 , training loss: 4.010562
[INFO] Epoch: 8 , batch: 113 , training loss: 3.979554
[INFO] Epoch: 8 , batch: 114 , training loss: 4.074033
[INFO] Epoch: 8 , batch: 115 , training loss: 4.037663
[INFO] Epoch: 8 , batch: 116 , training loss: 3.969572
[INFO] Epoch: 8 , batch: 117 , training loss: 4.226594
[INFO] Epoch: 8 , batch: 118 , training loss: 4.142799
[INFO] Epoch: 8 , batch: 119 , training loss: 4.331396
[INFO] Epoch: 8 , batch: 120 , training loss: 4.276455
[INFO] Epoch: 8 , batch: 121 , training loss: 4.078017
[INFO] Epoch: 8 , batch: 122 , training loss: 4.006390
[INFO] Epoch: 8 , batch: 123 , training loss: 4.089889
[INFO] Epoch: 8 , batch: 124 , training loss: 4.216069
[INFO] Epoch: 8 , batch: 125 , training loss: 3.931795
[INFO] Epoch: 8 , batch: 126 , training loss: 3.927896
[INFO] Epoch: 8 , batch: 127 , training loss: 3.970568
[INFO] Epoch: 8 , batch: 128 , training loss: 4.148172
[INFO] Epoch: 8 , batch: 129 , training loss: 4.058722
[INFO] Epoch: 8 , batch: 130 , training loss: 4.032785
[INFO] Epoch: 8 , batch: 131 , training loss: 4.087275
[INFO] Epoch: 8 , batch: 132 , training loss: 4.078253
[INFO] Epoch: 8 , batch: 133 , training loss: 4.012581
[INFO] Epoch: 8 , batch: 134 , training loss: 3.847111
[INFO] Epoch: 8 , batch: 135 , training loss: 3.855442
[INFO] Epoch: 8 , batch: 136 , training loss: 4.170391
[INFO] Epoch: 8 , batch: 137 , training loss: 4.069951
[INFO] Epoch: 8 , batch: 138 , training loss: 4.169698
[INFO] Epoch: 8 , batch: 139 , training loss: 4.880884
[INFO] Epoch: 8 , batch: 140 , training loss: 4.644325
[INFO] Epoch: 8 , batch: 141 , training loss: 4.363318
[INFO] Epoch: 8 , batch: 142 , training loss: 4.060337
[INFO] Epoch: 8 , batch: 143 , training loss: 4.169302
[INFO] Epoch: 8 , batch: 144 , training loss: 3.891160
[INFO] Epoch: 8 , batch: 145 , training loss: 4.080242
[INFO] Epoch: 8 , batch: 146 , training loss: 4.253026
[INFO] Epoch: 8 , batch: 147 , training loss: 3.929754
[INFO] Epoch: 8 , batch: 148 , training loss: 3.898281
[INFO] Epoch: 8 , batch: 149 , training loss: 3.985461
[INFO] Epoch: 8 , batch: 150 , training loss: 4.216667
[INFO] Epoch: 8 , batch: 151 , training loss: 4.031243
[INFO] Epoch: 8 , batch: 152 , training loss: 4.085905
[INFO] Epoch: 8 , batch: 153 , training loss: 4.102628
[INFO] Epoch: 8 , batch: 154 , training loss: 4.215504
[INFO] Epoch: 8 , batch: 155 , training loss: 4.450191
[INFO] Epoch: 8 , batch: 156 , training loss: 4.101067
[INFO] Epoch: 8 , batch: 157 , training loss: 4.084542
[INFO] Epoch: 8 , batch: 158 , training loss: 4.381447
[INFO] Epoch: 8 , batch: 159 , training loss: 4.241035
[INFO] Epoch: 8 , batch: 160 , training loss: 4.632326
[INFO] Epoch: 8 , batch: 161 , training loss: 4.586828
[INFO] Epoch: 8 , batch: 162 , training loss: 4.540496
[INFO] Epoch: 8 , batch: 163 , training loss: 4.619887
[INFO] Epoch: 8 , batch: 164 , training loss: 4.626886
[INFO] Epoch: 8 , batch: 165 , training loss: 4.525028
[INFO] Epoch: 8 , batch: 166 , training loss: 4.559096
[INFO] Epoch: 8 , batch: 167 , training loss: 4.882598
[INFO] Epoch: 8 , batch: 168 , training loss: 4.569039
[INFO] Epoch: 8 , batch: 169 , training loss: 4.510438
[INFO] Epoch: 8 , batch: 170 , training loss: 4.577126
[INFO] Epoch: 8 , batch: 171 , training loss: 4.092789
[INFO] Epoch: 8 , batch: 172 , training loss: 4.259072
[INFO] Epoch: 8 , batch: 173 , training loss: 4.618385
[INFO] Epoch: 8 , batch: 174 , training loss: 4.794849
[INFO] Epoch: 8 , batch: 175 , training loss: 5.164928
[INFO] Epoch: 8 , batch: 176 , training loss: 4.886635
[INFO] Epoch: 8 , batch: 177 , training loss: 4.476240
[INFO] Epoch: 8 , batch: 178 , training loss: 4.457637
[INFO] Epoch: 8 , batch: 179 , training loss: 4.473328
[INFO] Epoch: 8 , batch: 180 , training loss: 4.394885
[INFO] Epoch: 8 , batch: 181 , training loss: 4.634325
[INFO] Epoch: 8 , batch: 182 , training loss: 4.565194
[INFO] Epoch: 8 , batch: 183 , training loss: 4.534942
[INFO] Epoch: 8 , batch: 184 , training loss: 4.397243
[INFO] Epoch: 8 , batch: 185 , training loss: 4.357329
[INFO] Epoch: 8 , batch: 186 , training loss: 4.528792
[INFO] Epoch: 8 , batch: 187 , training loss: 4.646881
[INFO] Epoch: 8 , batch: 188 , training loss: 4.601223
[INFO] Epoch: 8 , batch: 189 , training loss: 4.486948
[INFO] Epoch: 8 , batch: 190 , training loss: 4.481742
[INFO] Epoch: 8 , batch: 191 , training loss: 4.652704
[INFO] Epoch: 8 , batch: 192 , training loss: 4.407442
[INFO] Epoch: 8 , batch: 193 , training loss: 4.543897
[INFO] Epoch: 8 , batch: 194 , training loss: 4.434658
[INFO] Epoch: 8 , batch: 195 , training loss: 4.416944
[INFO] Epoch: 8 , batch: 196 , training loss: 4.245517
[INFO] Epoch: 8 , batch: 197 , training loss: 4.396847
[INFO] Epoch: 8 , batch: 198 , training loss: 4.271208
[INFO] Epoch: 8 , batch: 199 , training loss: 4.389851
[INFO] Epoch: 8 , batch: 200 , training loss: 4.314337
[INFO] Epoch: 8 , batch: 201 , training loss: 4.237974
[INFO] Epoch: 8 , batch: 202 , training loss: 4.197492
[INFO] Epoch: 8 , batch: 203 , training loss: 4.248388
[INFO] Epoch: 8 , batch: 204 , training loss: 4.407359
[INFO] Epoch: 8 , batch: 205 , training loss: 3.978384
[INFO] Epoch: 8 , batch: 206 , training loss: 3.907050
[INFO] Epoch: 8 , batch: 207 , training loss: 3.917552
[INFO] Epoch: 8 , batch: 208 , training loss: 4.283715
[INFO] Epoch: 8 , batch: 209 , training loss: 4.211615
[INFO] Epoch: 8 , batch: 210 , training loss: 4.239461
[INFO] Epoch: 8 , batch: 211 , training loss: 4.228929
[INFO] Epoch: 8 , batch: 212 , training loss: 4.327799
[INFO] Epoch: 8 , batch: 213 , training loss: 4.297033
[INFO] Epoch: 8 , batch: 214 , training loss: 4.356624
[INFO] Epoch: 8 , batch: 215 , training loss: 4.585260
[INFO] Epoch: 8 , batch: 216 , training loss: 4.280655
[INFO] Epoch: 8 , batch: 217 , training loss: 4.217638
[INFO] Epoch: 8 , batch: 218 , training loss: 4.200057
[INFO] Epoch: 8 , batch: 219 , training loss: 4.335814
[INFO] Epoch: 8 , batch: 220 , training loss: 4.134929
[INFO] Epoch: 8 , batch: 221 , training loss: 4.153566
[INFO] Epoch: 8 , batch: 222 , training loss: 4.279142
[INFO] Epoch: 8 , batch: 223 , training loss: 4.370282
[INFO] Epoch: 8 , batch: 224 , training loss: 4.434028
[INFO] Epoch: 8 , batch: 225 , training loss: 4.307759
[INFO] Epoch: 8 , batch: 226 , training loss: 4.458643
[INFO] Epoch: 8 , batch: 227 , training loss: 4.408210
[INFO] Epoch: 8 , batch: 228 , training loss: 4.462894
[INFO] Epoch: 8 , batch: 229 , training loss: 4.310464
[INFO] Epoch: 8 , batch: 230 , training loss: 4.156409
[INFO] Epoch: 8 , batch: 231 , training loss: 3.999216
[INFO] Epoch: 8 , batch: 232 , training loss: 4.165466
[INFO] Epoch: 8 , batch: 233 , training loss: 4.171824
[INFO] Epoch: 8 , batch: 234 , training loss: 3.864910
[INFO] Epoch: 8 , batch: 235 , training loss: 3.975866
[INFO] Epoch: 8 , batch: 236 , training loss: 4.125854
[INFO] Epoch: 8 , batch: 237 , training loss: 4.280634
[INFO] Epoch: 8 , batch: 238 , training loss: 4.024549
[INFO] Epoch: 8 , batch: 239 , training loss: 4.107052
[INFO] Epoch: 8 , batch: 240 , training loss: 4.180734
[INFO] Epoch: 8 , batch: 241 , training loss: 3.973287
[INFO] Epoch: 8 , batch: 242 , training loss: 3.970312
[INFO] Epoch: 8 , batch: 243 , training loss: 4.315199
[INFO] Epoch: 8 , batch: 244 , training loss: 4.214507
[INFO] Epoch: 8 , batch: 245 , training loss: 4.219923
[INFO] Epoch: 8 , batch: 246 , training loss: 3.878467
[INFO] Epoch: 8 , batch: 247 , training loss: 4.050284
[INFO] Epoch: 8 , batch: 248 , training loss: 4.147329
[INFO] Epoch: 8 , batch: 249 , training loss: 4.129793
[INFO] Epoch: 8 , batch: 250 , training loss: 3.890676
[INFO] Epoch: 8 , batch: 251 , training loss: 4.389960
[INFO] Epoch: 8 , batch: 252 , training loss: 4.067065
[INFO] Epoch: 8 , batch: 253 , training loss: 4.026552
[INFO] Epoch: 8 , batch: 254 , training loss: 4.318079
[INFO] Epoch: 8 , batch: 255 , training loss: 4.307212
[INFO] Epoch: 8 , batch: 256 , training loss: 4.271553
[INFO] Epoch: 8 , batch: 257 , training loss: 4.427648
[INFO] Epoch: 8 , batch: 258 , training loss: 4.480231
[INFO] Epoch: 8 , batch: 259 , training loss: 4.524129
[INFO] Epoch: 8 , batch: 260 , training loss: 4.204133
[INFO] Epoch: 8 , batch: 261 , training loss: 4.394094
[INFO] Epoch: 8 , batch: 262 , training loss: 4.609310
[INFO] Epoch: 8 , batch: 263 , training loss: 4.758475
[INFO] Epoch: 8 , batch: 264 , training loss: 4.054087
[INFO] Epoch: 8 , batch: 265 , training loss: 4.197252
[INFO] Epoch: 8 , batch: 266 , training loss: 4.671776
[INFO] Epoch: 8 , batch: 267 , training loss: 4.378956
[INFO] Epoch: 8 , batch: 268 , training loss: 4.260531
[INFO] Epoch: 8 , batch: 269 , training loss: 4.278256
[INFO] Epoch: 8 , batch: 270 , training loss: 4.284184
[INFO] Epoch: 8 , batch: 271 , training loss: 4.318999
[INFO] Epoch: 8 , batch: 272 , training loss: 4.299874
[INFO] Epoch: 8 , batch: 273 , training loss: 4.301609
[INFO] Epoch: 8 , batch: 274 , training loss: 4.409020
[INFO] Epoch: 8 , batch: 275 , training loss: 4.307287
[INFO] Epoch: 8 , batch: 276 , training loss: 4.346579
[INFO] Epoch: 8 , batch: 277 , training loss: 4.479737
[INFO] Epoch: 8 , batch: 278 , training loss: 4.116505
[INFO] Epoch: 8 , batch: 279 , training loss: 4.138846
[INFO] Epoch: 8 , batch: 280 , training loss: 4.082562
[INFO] Epoch: 8 , batch: 281 , training loss: 4.245245
[INFO] Epoch: 8 , batch: 282 , training loss: 4.137380
[INFO] Epoch: 8 , batch: 283 , training loss: 4.186430
[INFO] Epoch: 8 , batch: 284 , training loss: 4.205986
[INFO] Epoch: 8 , batch: 285 , training loss: 4.155459
[INFO] Epoch: 8 , batch: 286 , training loss: 4.145601
[INFO] Epoch: 8 , batch: 287 , training loss: 4.066517
[INFO] Epoch: 8 , batch: 288 , training loss: 4.095912
[INFO] Epoch: 8 , batch: 289 , training loss: 4.156141
[INFO] Epoch: 8 , batch: 290 , training loss: 3.917669
[INFO] Epoch: 8 , batch: 291 , training loss: 3.891000
[INFO] Epoch: 8 , batch: 292 , training loss: 4.003595
[INFO] Epoch: 8 , batch: 293 , training loss: 3.918500
[INFO] Epoch: 8 , batch: 294 , training loss: 4.625985
[INFO] Epoch: 8 , batch: 295 , training loss: 4.402070
[INFO] Epoch: 8 , batch: 296 , training loss: 4.316376
[INFO] Epoch: 8 , batch: 297 , training loss: 4.275109
[INFO] Epoch: 8 , batch: 298 , training loss: 4.111602
[INFO] Epoch: 8 , batch: 299 , training loss: 4.145218
[INFO] Epoch: 8 , batch: 300 , training loss: 4.105013
[INFO] Epoch: 8 , batch: 301 , training loss: 4.020948
[INFO] Epoch: 8 , batch: 302 , training loss: 4.202047
[INFO] Epoch: 8 , batch: 303 , training loss: 4.202308
[INFO] Epoch: 8 , batch: 304 , training loss: 4.402061
[INFO] Epoch: 8 , batch: 305 , training loss: 4.149393
[INFO] Epoch: 8 , batch: 306 , training loss: 4.287934
[INFO] Epoch: 8 , batch: 307 , training loss: 4.267562
[INFO] Epoch: 8 , batch: 308 , training loss: 4.133471
[INFO] Epoch: 8 , batch: 309 , training loss: 4.173498
[INFO] Epoch: 8 , batch: 310 , training loss: 4.019728
[INFO] Epoch: 8 , batch: 311 , training loss: 4.022972
[INFO] Epoch: 8 , batch: 312 , training loss: 3.913602
[INFO] Epoch: 8 , batch: 313 , training loss: 4.089094
[INFO] Epoch: 8 , batch: 314 , training loss: 4.112198
[INFO] Epoch: 8 , batch: 315 , training loss: 4.160117
[INFO] Epoch: 8 , batch: 316 , training loss: 4.474617
[INFO] Epoch: 8 , batch: 317 , training loss: 5.036541
[INFO] Epoch: 8 , batch: 318 , training loss: 5.122596
[INFO] Epoch: 8 , batch: 319 , training loss: 4.644014
[INFO] Epoch: 8 , batch: 320 , training loss: 4.187871
[INFO] Epoch: 8 , batch: 321 , training loss: 3.960390
[INFO] Epoch: 8 , batch: 322 , training loss: 4.085265
[INFO] Epoch: 8 , batch: 323 , training loss: 4.103782
[INFO] Epoch: 8 , batch: 324 , training loss: 4.090969
[INFO] Epoch: 8 , batch: 325 , training loss: 4.268940
[INFO] Epoch: 8 , batch: 326 , training loss: 4.331767
[INFO] Epoch: 8 , batch: 327 , training loss: 4.208086
[INFO] Epoch: 8 , batch: 328 , training loss: 4.193158
[INFO] Epoch: 8 , batch: 329 , training loss: 4.119808
[INFO] Epoch: 8 , batch: 330 , training loss: 4.095949
[INFO] Epoch: 8 , batch: 331 , training loss: 4.247914
[INFO] Epoch: 8 , batch: 332 , training loss: 4.062999
[INFO] Epoch: 8 , batch: 333 , training loss: 4.056871
[INFO] Epoch: 8 , batch: 334 , training loss: 4.107061
[INFO] Epoch: 8 , batch: 335 , training loss: 4.227035
[INFO] Epoch: 8 , batch: 336 , training loss: 4.220246
[INFO] Epoch: 8 , batch: 337 , training loss: 4.256862
[INFO] Epoch: 8 , batch: 338 , training loss: 4.485399
[INFO] Epoch: 8 , batch: 339 , training loss: 4.326188
[INFO] Epoch: 8 , batch: 340 , training loss: 4.491350
[INFO] Epoch: 8 , batch: 341 , training loss: 4.254744
[INFO] Epoch: 8 , batch: 342 , training loss: 4.025094
[INFO] Epoch: 8 , batch: 343 , training loss: 4.102420
[INFO] Epoch: 8 , batch: 344 , training loss: 3.956065
[INFO] Epoch: 8 , batch: 345 , training loss: 4.098862
[INFO] Epoch: 8 , batch: 346 , training loss: 4.168222
[INFO] Epoch: 8 , batch: 347 , training loss: 4.040913
[INFO] Epoch: 8 , batch: 348 , training loss: 4.245509
[INFO] Epoch: 8 , batch: 349 , training loss: 4.341684
[INFO] Epoch: 8 , batch: 350 , training loss: 4.057366
[INFO] Epoch: 8 , batch: 351 , training loss: 4.142992
[INFO] Epoch: 8 , batch: 352 , training loss: 4.177893
[INFO] Epoch: 8 , batch: 353 , training loss: 4.145465
[INFO] Epoch: 8 , batch: 354 , training loss: 4.246326
[INFO] Epoch: 8 , batch: 355 , training loss: 4.290932
[INFO] Epoch: 8 , batch: 356 , training loss: 4.133516
[INFO] Epoch: 8 , batch: 357 , training loss: 4.231540
[INFO] Epoch: 8 , batch: 358 , training loss: 4.156214
[INFO] Epoch: 8 , batch: 359 , training loss: 4.117558
[INFO] Epoch: 8 , batch: 360 , training loss: 4.195547
[INFO] Epoch: 8 , batch: 361 , training loss: 4.155034
[INFO] Epoch: 8 , batch: 362 , training loss: 4.279249
[INFO] Epoch: 8 , batch: 363 , training loss: 4.160456
[INFO] Epoch: 8 , batch: 364 , training loss: 4.180789
[INFO] Epoch: 8 , batch: 365 , training loss: 4.100758
[INFO] Epoch: 8 , batch: 366 , training loss: 4.276975
[INFO] Epoch: 8 , batch: 367 , training loss: 4.337427
[INFO] Epoch: 8 , batch: 368 , training loss: 4.868361
[INFO] Epoch: 8 , batch: 369 , training loss: 4.457636
[INFO] Epoch: 8 , batch: 370 , training loss: 4.205985
[INFO] Epoch: 8 , batch: 371 , training loss: 4.720691
[INFO] Epoch: 8 , batch: 372 , training loss: 4.998756
[INFO] Epoch: 8 , batch: 373 , training loss: 5.036098
[INFO] Epoch: 8 , batch: 374 , training loss: 5.055015
[INFO] Epoch: 8 , batch: 375 , training loss: 5.049247
[INFO] Epoch: 8 , batch: 376 , training loss: 4.947597
[INFO] Epoch: 8 , batch: 377 , training loss: 4.619636
[INFO] Epoch: 8 , batch: 378 , training loss: 4.762921
[INFO] Epoch: 8 , batch: 379 , training loss: 4.758400
[INFO] Epoch: 8 , batch: 380 , training loss: 4.900666
[INFO] Epoch: 8 , batch: 381 , training loss: 4.656962
[INFO] Epoch: 8 , batch: 382 , training loss: 4.892021
[INFO] Epoch: 8 , batch: 383 , training loss: 4.952712
[INFO] Epoch: 8 , batch: 384 , training loss: 4.976679
[INFO] Epoch: 8 , batch: 385 , training loss: 4.712823
[INFO] Epoch: 8 , batch: 386 , training loss: 4.886246
[INFO] Epoch: 8 , batch: 387 , training loss: 4.827802
[INFO] Epoch: 8 , batch: 388 , training loss: 4.633425
[INFO] Epoch: 8 , batch: 389 , training loss: 4.494462
[INFO] Epoch: 8 , batch: 390 , training loss: 4.441815
[INFO] Epoch: 8 , batch: 391 , training loss: 4.493028
[INFO] Epoch: 8 , batch: 392 , training loss: 4.849944
[INFO] Epoch: 8 , batch: 393 , training loss: 4.726490
[INFO] Epoch: 8 , batch: 394 , training loss: 4.792101
[INFO] Epoch: 8 , batch: 395 , training loss: 4.635445
[INFO] Epoch: 8 , batch: 396 , training loss: 4.399774
[INFO] Epoch: 8 , batch: 397 , training loss: 4.574180
[INFO] Epoch: 8 , batch: 398 , training loss: 4.406750
[INFO] Epoch: 8 , batch: 399 , training loss: 4.449223
[INFO] Epoch: 8 , batch: 400 , training loss: 4.399368
[INFO] Epoch: 8 , batch: 401 , training loss: 4.831831
[INFO] Epoch: 8 , batch: 402 , training loss: 4.578464
[INFO] Epoch: 8 , batch: 403 , training loss: 4.402682
[INFO] Epoch: 8 , batch: 404 , training loss: 4.579657
[INFO] Epoch: 8 , batch: 405 , training loss: 4.625281
[INFO] Epoch: 8 , batch: 406 , training loss: 4.521033
[INFO] Epoch: 8 , batch: 407 , training loss: 4.602238
[INFO] Epoch: 8 , batch: 408 , training loss: 4.518689
[INFO] Epoch: 8 , batch: 409 , training loss: 4.535170
[INFO] Epoch: 8 , batch: 410 , training loss: 4.592541
[INFO] Epoch: 8 , batch: 411 , training loss: 4.803832
[INFO] Epoch: 8 , batch: 412 , training loss: 4.628768
[INFO] Epoch: 8 , batch: 413 , training loss: 4.478198
[INFO] Epoch: 8 , batch: 414 , training loss: 4.509917
[INFO] Epoch: 8 , batch: 415 , training loss: 4.525331
[INFO] Epoch: 8 , batch: 416 , training loss: 4.609944
[INFO] Epoch: 8 , batch: 417 , training loss: 4.537447
[INFO] Epoch: 8 , batch: 418 , training loss: 4.554961
[INFO] Epoch: 8 , batch: 419 , training loss: 4.511666
[INFO] Epoch: 8 , batch: 420 , training loss: 4.485777
[INFO] Epoch: 8 , batch: 421 , training loss: 4.495053
[INFO] Epoch: 8 , batch: 422 , training loss: 4.366524
[INFO] Epoch: 8 , batch: 423 , training loss: 4.588152
[INFO] Epoch: 8 , batch: 424 , training loss: 4.745194
[INFO] Epoch: 8 , batch: 425 , training loss: 4.608678
[INFO] Epoch: 8 , batch: 426 , training loss: 4.337598
[INFO] Epoch: 8 , batch: 427 , training loss: 4.573334
[INFO] Epoch: 8 , batch: 428 , training loss: 4.489591
[INFO] Epoch: 8 , batch: 429 , training loss: 4.310268
[INFO] Epoch: 8 , batch: 430 , training loss: 4.579774
[INFO] Epoch: 8 , batch: 431 , training loss: 4.149523
[INFO] Epoch: 8 , batch: 432 , training loss: 4.254206
[INFO] Epoch: 8 , batch: 433 , training loss: 4.270366
[INFO] Epoch: 8 , batch: 434 , training loss: 4.128163
[INFO] Epoch: 8 , batch: 435 , training loss: 4.502601
[INFO] Epoch: 8 , batch: 436 , training loss: 4.595853
[INFO] Epoch: 8 , batch: 437 , training loss: 4.352959
[INFO] Epoch: 8 , batch: 438 , training loss: 4.167034
[INFO] Epoch: 8 , batch: 439 , training loss: 4.396819
[INFO] Epoch: 8 , batch: 440 , training loss: 4.540376
[INFO] Epoch: 8 , batch: 441 , training loss: 4.615623
[INFO] Epoch: 8 , batch: 442 , training loss: 4.405388
[INFO] Epoch: 8 , batch: 443 , training loss: 4.612409
[INFO] Epoch: 8 , batch: 444 , training loss: 4.211492
[INFO] Epoch: 8 , batch: 445 , training loss: 4.101119
[INFO] Epoch: 8 , batch: 446 , training loss: 3.998659
[INFO] Epoch: 8 , batch: 447 , training loss: 4.224588
[INFO] Epoch: 8 , batch: 448 , training loss: 4.374344
[INFO] Epoch: 8 , batch: 449 , training loss: 4.785869
[INFO] Epoch: 8 , batch: 450 , training loss: 4.820390
[INFO] Epoch: 8 , batch: 451 , training loss: 4.720054
[INFO] Epoch: 8 , batch: 452 , training loss: 4.490218
[INFO] Epoch: 8 , batch: 453 , training loss: 4.279818
[INFO] Epoch: 8 , batch: 454 , training loss: 4.419636
[INFO] Epoch: 8 , batch: 455 , training loss: 4.465497
[INFO] Epoch: 8 , batch: 456 , training loss: 4.436993
[INFO] Epoch: 8 , batch: 457 , training loss: 4.554643
[INFO] Epoch: 8 , batch: 458 , training loss: 4.276471
[INFO] Epoch: 8 , batch: 459 , training loss: 4.254396
[INFO] Epoch: 8 , batch: 460 , training loss: 4.388045
[INFO] Epoch: 8 , batch: 461 , training loss: 4.356425
[INFO] Epoch: 8 , batch: 462 , training loss: 4.421083
[INFO] Epoch: 8 , batch: 463 , training loss: 4.283619
[INFO] Epoch: 8 , batch: 464 , training loss: 4.520952
[INFO] Epoch: 8 , batch: 465 , training loss: 4.430954
[INFO] Epoch: 8 , batch: 466 , training loss: 4.509292
[INFO] Epoch: 8 , batch: 467 , training loss: 4.514603
[INFO] Epoch: 8 , batch: 468 , training loss: 4.471960
[INFO] Epoch: 8 , batch: 469 , training loss: 4.504742
[INFO] Epoch: 8 , batch: 470 , training loss: 4.284775
[INFO] Epoch: 8 , batch: 471 , training loss: 4.381973
[INFO] Epoch: 8 , batch: 472 , training loss: 4.431304
[INFO] Epoch: 8 , batch: 473 , training loss: 4.366354
[INFO] Epoch: 8 , batch: 474 , training loss: 4.185399
[INFO] Epoch: 8 , batch: 475 , training loss: 4.035345
[INFO] Epoch: 8 , batch: 476 , training loss: 4.415398
[INFO] Epoch: 8 , batch: 477 , training loss: 4.536972
[INFO] Epoch: 8 , batch: 478 , training loss: 4.608378
[INFO] Epoch: 8 , batch: 479 , training loss: 4.571946
[INFO] Epoch: 8 , batch: 480 , training loss: 4.697139
[INFO] Epoch: 8 , batch: 481 , training loss: 4.501321
[INFO] Epoch: 8 , batch: 482 , training loss: 4.658079
[INFO] Epoch: 8 , batch: 483 , training loss: 4.494935
[INFO] Epoch: 8 , batch: 484 , training loss: 4.284294
[INFO] Epoch: 8 , batch: 485 , training loss: 4.390155
[INFO] Epoch: 8 , batch: 486 , training loss: 4.281141
[INFO] Epoch: 8 , batch: 487 , training loss: 4.272733
[INFO] Epoch: 8 , batch: 488 , training loss: 4.479770
[INFO] Epoch: 8 , batch: 489 , training loss: 4.340069
[INFO] Epoch: 8 , batch: 490 , training loss: 4.406619
[INFO] Epoch: 8 , batch: 491 , training loss: 4.378440
[INFO] Epoch: 8 , batch: 492 , training loss: 4.293288
[INFO] Epoch: 8 , batch: 493 , training loss: 4.482037
[INFO] Epoch: 8 , batch: 494 , training loss: 4.409158
[INFO] Epoch: 8 , batch: 495 , training loss: 4.531942
[INFO] Epoch: 8 , batch: 496 , training loss: 4.398552
[INFO] Epoch: 8 , batch: 497 , training loss: 4.419306
[INFO] Epoch: 8 , batch: 498 , training loss: 4.456923
[INFO] Epoch: 8 , batch: 499 , training loss: 4.532327
[INFO] Epoch: 8 , batch: 500 , training loss: 4.723679
[INFO] Epoch: 8 , batch: 501 , training loss: 5.070447
[INFO] Epoch: 8 , batch: 502 , training loss: 5.219275
[INFO] Epoch: 8 , batch: 503 , training loss: 4.930977
[INFO] Epoch: 8 , batch: 504 , training loss: 5.012323
[INFO] Epoch: 8 , batch: 505 , training loss: 4.945391
[INFO] Epoch: 8 , batch: 506 , training loss: 4.906719
[INFO] Epoch: 8 , batch: 507 , training loss: 4.967727
[INFO] Epoch: 8 , batch: 508 , training loss: 4.917209
[INFO] Epoch: 8 , batch: 509 , training loss: 4.651376
[INFO] Epoch: 8 , batch: 510 , training loss: 4.707680
[INFO] Epoch: 8 , batch: 511 , training loss: 4.620669
[INFO] Epoch: 8 , batch: 512 , training loss: 4.689956
[INFO] Epoch: 8 , batch: 513 , training loss: 4.939712
[INFO] Epoch: 8 , batch: 514 , training loss: 4.588421
[INFO] Epoch: 8 , batch: 515 , training loss: 4.853179
[INFO] Epoch: 8 , batch: 516 , training loss: 4.641130
[INFO] Epoch: 8 , batch: 517 , training loss: 4.607314
[INFO] Epoch: 8 , batch: 518 , training loss: 4.578018
[INFO] Epoch: 8 , batch: 519 , training loss: 4.432476
[INFO] Epoch: 8 , batch: 520 , training loss: 4.649330
[INFO] Epoch: 8 , batch: 521 , training loss: 4.637383
[INFO] Epoch: 8 , batch: 522 , training loss: 4.710884
[INFO] Epoch: 8 , batch: 523 , training loss: 4.626027
[INFO] Epoch: 8 , batch: 524 , training loss: 4.903900
[INFO] Epoch: 8 , batch: 525 , training loss: 4.809915
[INFO] Epoch: 8 , batch: 526 , training loss: 4.563720
[INFO] Epoch: 8 , batch: 527 , training loss: 4.585025
[INFO] Epoch: 8 , batch: 528 , training loss: 4.669322
[INFO] Epoch: 8 , batch: 529 , training loss: 4.624009
[INFO] Epoch: 8 , batch: 530 , training loss: 4.450552
[INFO] Epoch: 8 , batch: 531 , training loss: 4.599336
[INFO] Epoch: 8 , batch: 532 , training loss: 4.459592
[INFO] Epoch: 8 , batch: 533 , training loss: 4.660226
[INFO] Epoch: 8 , batch: 534 , training loss: 4.623670
[INFO] Epoch: 8 , batch: 535 , training loss: 4.636302
[INFO] Epoch: 8 , batch: 536 , training loss: 4.474643
[INFO] Epoch: 8 , batch: 537 , training loss: 4.467638
[INFO] Epoch: 8 , batch: 538 , training loss: 4.527315
[INFO] Epoch: 8 , batch: 539 , training loss: 4.692120
[INFO] Epoch: 8 , batch: 540 , training loss: 5.274663
[INFO] Epoch: 8 , batch: 541 , training loss: 5.186649
[INFO] Epoch: 8 , batch: 542 , training loss: 4.982640
[INFO] Epoch: 9 , batch: 0 , training loss: 4.233680
[INFO] Epoch: 9 , batch: 1 , training loss: 4.104440
[INFO] Epoch: 9 , batch: 2 , training loss: 4.051523
[INFO] Epoch: 9 , batch: 3 , training loss: 3.906619
[INFO] Epoch: 9 , batch: 4 , training loss: 4.299026
[INFO] Epoch: 9 , batch: 5 , training loss: 3.892251
[INFO] Epoch: 9 , batch: 6 , training loss: 4.398838
[INFO] Epoch: 9 , batch: 7 , training loss: 4.139552
[INFO] Epoch: 9 , batch: 8 , training loss: 3.829740
[INFO] Epoch: 9 , batch: 9 , training loss: 4.002625
[INFO] Epoch: 9 , batch: 10 , training loss: 3.935388
[INFO] Epoch: 9 , batch: 11 , training loss: 3.898872
[INFO] Epoch: 9 , batch: 12 , training loss: 3.862114
[INFO] Epoch: 9 , batch: 13 , training loss: 3.854433
[INFO] Epoch: 9 , batch: 14 , training loss: 3.656004
[INFO] Epoch: 9 , batch: 15 , training loss: 3.911563
[INFO] Epoch: 9 , batch: 16 , training loss: 3.781992
[INFO] Epoch: 9 , batch: 17 , training loss: 3.941446
[INFO] Epoch: 9 , batch: 18 , training loss: 3.894160
[INFO] Epoch: 9 , batch: 19 , training loss: 3.621062
[INFO] Epoch: 9 , batch: 20 , training loss: 3.617063
[INFO] Epoch: 9 , batch: 21 , training loss: 3.716213
[INFO] Epoch: 9 , batch: 22 , training loss: 3.708547
[INFO] Epoch: 9 , batch: 23 , training loss: 3.880523
[INFO] Epoch: 9 , batch: 24 , training loss: 3.701552
[INFO] Epoch: 9 , batch: 25 , training loss: 3.901248
[INFO] Epoch: 9 , batch: 26 , training loss: 3.720612
[INFO] Epoch: 9 , batch: 27 , training loss: 3.679072
[INFO] Epoch: 9 , batch: 28 , training loss: 3.886436
[INFO] Epoch: 9 , batch: 29 , training loss: 3.650894
[INFO] Epoch: 9 , batch: 30 , training loss: 3.723912
[INFO] Epoch: 9 , batch: 31 , training loss: 3.800990
[INFO] Epoch: 9 , batch: 32 , training loss: 3.817336
[INFO] Epoch: 9 , batch: 33 , training loss: 3.839216
[INFO] Epoch: 9 , batch: 34 , training loss: 3.847615
[INFO] Epoch: 9 , batch: 35 , training loss: 3.832676
[INFO] Epoch: 9 , batch: 36 , training loss: 3.820698
[INFO] Epoch: 9 , batch: 37 , training loss: 3.718640
[INFO] Epoch: 9 , batch: 38 , training loss: 3.807957
[INFO] Epoch: 9 , batch: 39 , training loss: 3.666763
[INFO] Epoch: 9 , batch: 40 , training loss: 3.812807
[INFO] Epoch: 9 , batch: 41 , training loss: 3.866479
[INFO] Epoch: 9 , batch: 42 , training loss: 4.364500
[INFO] Epoch: 9 , batch: 43 , training loss: 4.026365
[INFO] Epoch: 9 , batch: 44 , training loss: 4.283188
[INFO] Epoch: 9 , batch: 45 , training loss: 4.345216
[INFO] Epoch: 9 , batch: 46 , training loss: 4.436815
[INFO] Epoch: 9 , batch: 47 , training loss: 3.999336
[INFO] Epoch: 9 , batch: 48 , training loss: 4.005063
[INFO] Epoch: 9 , batch: 49 , training loss: 4.123853
[INFO] Epoch: 9 , batch: 50 , training loss: 3.924625
[INFO] Epoch: 9 , batch: 51 , training loss: 4.035425
[INFO] Epoch: 9 , batch: 52 , training loss: 3.882434
[INFO] Epoch: 9 , batch: 53 , training loss: 4.015058
[INFO] Epoch: 9 , batch: 54 , training loss: 4.061310
[INFO] Epoch: 9 , batch: 55 , training loss: 4.088099
[INFO] Epoch: 9 , batch: 56 , training loss: 3.945508
[INFO] Epoch: 9 , batch: 57 , training loss: 3.841835
[INFO] Epoch: 9 , batch: 58 , training loss: 3.891585
[INFO] Epoch: 9 , batch: 59 , training loss: 3.990523
[INFO] Epoch: 9 , batch: 60 , training loss: 3.900897
[INFO] Epoch: 9 , batch: 61 , training loss: 3.973295
[INFO] Epoch: 9 , batch: 62 , training loss: 3.860395
[INFO] Epoch: 9 , batch: 63 , training loss: 4.049728
[INFO] Epoch: 9 , batch: 64 , training loss: 4.253421
[INFO] Epoch: 9 , batch: 65 , training loss: 3.928130
[INFO] Epoch: 9 , batch: 66 , training loss: 3.770552
[INFO] Epoch: 9 , batch: 67 , training loss: 3.836686
[INFO] Epoch: 9 , batch: 68 , training loss: 4.071743
[INFO] Epoch: 9 , batch: 69 , training loss: 3.879412
[INFO] Epoch: 9 , batch: 70 , training loss: 4.140762
[INFO] Epoch: 9 , batch: 71 , training loss: 3.974868
[INFO] Epoch: 9 , batch: 72 , training loss: 4.044975
[INFO] Epoch: 9 , batch: 73 , training loss: 3.979187
[INFO] Epoch: 9 , batch: 74 , training loss: 4.123187
[INFO] Epoch: 9 , batch: 75 , training loss: 3.890983
[INFO] Epoch: 9 , batch: 76 , training loss: 4.020031
[INFO] Epoch: 9 , batch: 77 , training loss: 4.007676
[INFO] Epoch: 9 , batch: 78 , training loss: 4.054144
[INFO] Epoch: 9 , batch: 79 , training loss: 3.892617
[INFO] Epoch: 9 , batch: 80 , training loss: 4.113753
[INFO] Epoch: 9 , batch: 81 , training loss: 4.042312
[INFO] Epoch: 9 , batch: 82 , training loss: 4.036581
[INFO] Epoch: 9 , batch: 83 , training loss: 4.134660
[INFO] Epoch: 9 , batch: 84 , training loss: 4.061423
[INFO] Epoch: 9 , batch: 85 , training loss: 4.134214
[INFO] Epoch: 9 , batch: 86 , training loss: 4.084561
[INFO] Epoch: 9 , batch: 87 , training loss: 4.082565
[INFO] Epoch: 9 , batch: 88 , training loss: 4.225932
[INFO] Epoch: 9 , batch: 89 , training loss: 3.999701
[INFO] Epoch: 9 , batch: 90 , training loss: 4.057183
[INFO] Epoch: 9 , batch: 91 , training loss: 4.003675
[INFO] Epoch: 9 , batch: 92 , training loss: 3.983819
[INFO] Epoch: 9 , batch: 93 , training loss: 4.121160
[INFO] Epoch: 9 , batch: 94 , training loss: 4.264188
[INFO] Epoch: 9 , batch: 95 , training loss: 4.044027
[INFO] Epoch: 9 , batch: 96 , training loss: 4.026018
[INFO] Epoch: 9 , batch: 97 , training loss: 3.978388
[INFO] Epoch: 9 , batch: 98 , training loss: 3.914650
[INFO] Epoch: 9 , batch: 99 , training loss: 4.007908
[INFO] Epoch: 9 , batch: 100 , training loss: 3.893148
[INFO] Epoch: 9 , batch: 101 , training loss: 3.963188
[INFO] Epoch: 9 , batch: 102 , training loss: 4.069828
[INFO] Epoch: 9 , batch: 103 , training loss: 3.868962
[INFO] Epoch: 9 , batch: 104 , training loss: 3.831583
[INFO] Epoch: 9 , batch: 105 , training loss: 4.130998
[INFO] Epoch: 9 , batch: 106 , training loss: 4.095963
[INFO] Epoch: 9 , batch: 107 , training loss: 3.957887
[INFO] Epoch: 9 , batch: 108 , training loss: 3.892788
[INFO] Epoch: 9 , batch: 109 , training loss: 3.788684
[INFO] Epoch: 9 , batch: 110 , training loss: 4.003733
[INFO] Epoch: 9 , batch: 111 , training loss: 4.065348
[INFO] Epoch: 9 , batch: 112 , training loss: 3.993246
[INFO] Epoch: 9 , batch: 113 , training loss: 3.979729
[INFO] Epoch: 9 , batch: 114 , training loss: 4.041690
[INFO] Epoch: 9 , batch: 115 , training loss: 4.009806
[INFO] Epoch: 9 , batch: 116 , training loss: 3.935573
[INFO] Epoch: 9 , batch: 117 , training loss: 4.193519
[INFO] Epoch: 9 , batch: 118 , training loss: 4.115056
[INFO] Epoch: 9 , batch: 119 , training loss: 4.292956
[INFO] Epoch: 9 , batch: 120 , training loss: 4.258551
[INFO] Epoch: 9 , batch: 121 , training loss: 4.063010
[INFO] Epoch: 9 , batch: 122 , training loss: 3.982610
[INFO] Epoch: 9 , batch: 123 , training loss: 4.029966
[INFO] Epoch: 9 , batch: 124 , training loss: 4.180268
[INFO] Epoch: 9 , batch: 125 , training loss: 3.921804
[INFO] Epoch: 9 , batch: 126 , training loss: 3.910632
[INFO] Epoch: 9 , batch: 127 , training loss: 3.932985
[INFO] Epoch: 9 , batch: 128 , training loss: 4.125395
[INFO] Epoch: 9 , batch: 129 , training loss: 4.045648
[INFO] Epoch: 9 , batch: 130 , training loss: 4.013082
[INFO] Epoch: 9 , batch: 131 , training loss: 4.055863
[INFO] Epoch: 9 , batch: 132 , training loss: 4.053342
[INFO] Epoch: 9 , batch: 133 , training loss: 3.994616
[INFO] Epoch: 9 , batch: 134 , training loss: 3.831920
[INFO] Epoch: 9 , batch: 135 , training loss: 3.829654
[INFO] Epoch: 9 , batch: 136 , training loss: 4.141043
[INFO] Epoch: 9 , batch: 137 , training loss: 4.021091
[INFO] Epoch: 9 , batch: 138 , training loss: 4.147188
[INFO] Epoch: 9 , batch: 139 , training loss: 4.817791
[INFO] Epoch: 9 , batch: 140 , training loss: 4.576649
[INFO] Epoch: 9 , batch: 141 , training loss: 4.305550
[INFO] Epoch: 9 , batch: 142 , training loss: 4.013611
[INFO] Epoch: 9 , batch: 143 , training loss: 4.114341
[INFO] Epoch: 9 , batch: 144 , training loss: 3.879160
[INFO] Epoch: 9 , batch: 145 , training loss: 4.030436
[INFO] Epoch: 9 , batch: 146 , training loss: 4.233326
[INFO] Epoch: 9 , batch: 147 , training loss: 3.904341
[INFO] Epoch: 9 , batch: 148 , training loss: 3.877426
[INFO] Epoch: 9 , batch: 149 , training loss: 3.952056
[INFO] Epoch: 9 , batch: 150 , training loss: 4.201649
[INFO] Epoch: 9 , batch: 151 , training loss: 3.993877
[INFO] Epoch: 9 , batch: 152 , training loss: 4.056663
[INFO] Epoch: 9 , batch: 153 , training loss: 4.112233
[INFO] Epoch: 9 , batch: 154 , training loss: 4.187904
[INFO] Epoch: 9 , batch: 155 , training loss: 4.420569
[INFO] Epoch: 9 , batch: 156 , training loss: 4.075371
[INFO] Epoch: 9 , batch: 157 , training loss: 4.041799
[INFO] Epoch: 9 , batch: 158 , training loss: 4.343835
[INFO] Epoch: 9 , batch: 159 , training loss: 4.182169
[INFO] Epoch: 9 , batch: 160 , training loss: 4.573285
[INFO] Epoch: 9 , batch: 161 , training loss: 4.570801
[INFO] Epoch: 9 , batch: 162 , training loss: 4.505160
[INFO] Epoch: 9 , batch: 163 , training loss: 4.611878
[INFO] Epoch: 9 , batch: 164 , training loss: 4.587531
[INFO] Epoch: 9 , batch: 165 , training loss: 4.517457
[INFO] Epoch: 9 , batch: 166 , training loss: 4.481344
[INFO] Epoch: 9 , batch: 167 , training loss: 4.780310
[INFO] Epoch: 9 , batch: 168 , training loss: 4.498355
[INFO] Epoch: 9 , batch: 169 , training loss: 4.444176
[INFO] Epoch: 9 , batch: 170 , training loss: 4.539911
[INFO] Epoch: 9 , batch: 171 , training loss: 3.990268
[INFO] Epoch: 9 , batch: 172 , training loss: 4.198718
[INFO] Epoch: 9 , batch: 173 , training loss: 4.516642
[INFO] Epoch: 9 , batch: 174 , training loss: 4.777399
[INFO] Epoch: 9 , batch: 175 , training loss: 5.131337
[INFO] Epoch: 9 , batch: 176 , training loss: 4.848395
[INFO] Epoch: 9 , batch: 177 , training loss: 4.444253
[INFO] Epoch: 9 , batch: 178 , training loss: 4.433326
[INFO] Epoch: 9 , batch: 179 , training loss: 4.439721
[INFO] Epoch: 9 , batch: 180 , training loss: 4.385098
[INFO] Epoch: 9 , batch: 181 , training loss: 4.605731
[INFO] Epoch: 9 , batch: 182 , training loss: 4.528034
[INFO] Epoch: 9 , batch: 183 , training loss: 4.500399
[INFO] Epoch: 9 , batch: 184 , training loss: 4.381371
[INFO] Epoch: 9 , batch: 185 , training loss: 4.339916
[INFO] Epoch: 9 , batch: 186 , training loss: 4.506692
[INFO] Epoch: 9 , batch: 187 , training loss: 4.625988
[INFO] Epoch: 9 , batch: 188 , training loss: 4.578125
[INFO] Epoch: 9 , batch: 189 , training loss: 4.467231
[INFO] Epoch: 9 , batch: 190 , training loss: 4.464355
[INFO] Epoch: 9 , batch: 191 , training loss: 4.611213
[INFO] Epoch: 9 , batch: 192 , training loss: 4.379316
[INFO] Epoch: 9 , batch: 193 , training loss: 4.522461
[INFO] Epoch: 9 , batch: 194 , training loss: 4.408661
[INFO] Epoch: 9 , batch: 195 , training loss: 4.413971
[INFO] Epoch: 9 , batch: 196 , training loss: 4.230747
[INFO] Epoch: 9 , batch: 197 , training loss: 4.352473
[INFO] Epoch: 9 , batch: 198 , training loss: 4.261113
[INFO] Epoch: 9 , batch: 199 , training loss: 4.370697
[INFO] Epoch: 9 , batch: 200 , training loss: 4.279762
[INFO] Epoch: 9 , batch: 201 , training loss: 4.202106
[INFO] Epoch: 9 , batch: 202 , training loss: 4.180060
[INFO] Epoch: 9 , batch: 203 , training loss: 4.253743
[INFO] Epoch: 9 , batch: 204 , training loss: 4.396031
[INFO] Epoch: 9 , batch: 205 , training loss: 3.953348
[INFO] Epoch: 9 , batch: 206 , training loss: 3.892847
[INFO] Epoch: 9 , batch: 207 , training loss: 3.894478
[INFO] Epoch: 9 , batch: 208 , training loss: 4.263147
[INFO] Epoch: 9 , batch: 209 , training loss: 4.182601
[INFO] Epoch: 9 , batch: 210 , training loss: 4.216386
[INFO] Epoch: 9 , batch: 211 , training loss: 4.196331
[INFO] Epoch: 9 , batch: 212 , training loss: 4.308675
[INFO] Epoch: 9 , batch: 213 , training loss: 4.278281
[INFO] Epoch: 9 , batch: 214 , training loss: 4.342141
[INFO] Epoch: 9 , batch: 215 , training loss: 4.577101
[INFO] Epoch: 9 , batch: 216 , training loss: 4.290467
[INFO] Epoch: 9 , batch: 217 , training loss: 4.201627
[INFO] Epoch: 9 , batch: 218 , training loss: 4.192979
[INFO] Epoch: 9 , batch: 219 , training loss: 4.307730
[INFO] Epoch: 9 , batch: 220 , training loss: 4.116938
[INFO] Epoch: 9 , batch: 221 , training loss: 4.125029
[INFO] Epoch: 9 , batch: 222 , training loss: 4.272517
[INFO] Epoch: 9 , batch: 223 , training loss: 4.362346
[INFO] Epoch: 9 , batch: 224 , training loss: 4.407449
[INFO] Epoch: 9 , batch: 225 , training loss: 4.302415
[INFO] Epoch: 9 , batch: 226 , training loss: 4.439639
[INFO] Epoch: 9 , batch: 227 , training loss: 4.394806
[INFO] Epoch: 9 , batch: 228 , training loss: 4.449233
[INFO] Epoch: 9 , batch: 229 , training loss: 4.294779
[INFO] Epoch: 9 , batch: 230 , training loss: 4.141416
[INFO] Epoch: 9 , batch: 231 , training loss: 3.995372
[INFO] Epoch: 9 , batch: 232 , training loss: 4.156488
[INFO] Epoch: 9 , batch: 233 , training loss: 4.141033
[INFO] Epoch: 9 , batch: 234 , training loss: 3.839819
[INFO] Epoch: 9 , batch: 235 , training loss: 3.942701
[INFO] Epoch: 9 , batch: 236 , training loss: 4.105100
[INFO] Epoch: 9 , batch: 237 , training loss: 4.274749
[INFO] Epoch: 9 , batch: 238 , training loss: 4.014092
[INFO] Epoch: 9 , batch: 239 , training loss: 4.105131
[INFO] Epoch: 9 , batch: 240 , training loss: 4.165108
[INFO] Epoch: 9 , batch: 241 , training loss: 3.956045
[INFO] Epoch: 9 , batch: 242 , training loss: 3.973809
[INFO] Epoch: 9 , batch: 243 , training loss: 4.287795
[INFO] Epoch: 9 , batch: 244 , training loss: 4.209100
[INFO] Epoch: 9 , batch: 245 , training loss: 4.191160
[INFO] Epoch: 9 , batch: 246 , training loss: 3.860268
[INFO] Epoch: 9 , batch: 247 , training loss: 4.052956
[INFO] Epoch: 9 , batch: 248 , training loss: 4.106497
[INFO] Epoch: 9 , batch: 249 , training loss: 4.120639
[INFO] Epoch: 9 , batch: 250 , training loss: 3.879419
[INFO] Epoch: 9 , batch: 251 , training loss: 4.362952
[INFO] Epoch: 9 , batch: 252 , training loss: 4.060794
[INFO] Epoch: 9 , batch: 253 , training loss: 3.991117
[INFO] Epoch: 9 , batch: 254 , training loss: 4.291730
[INFO] Epoch: 9 , batch: 255 , training loss: 4.271678
[INFO] Epoch: 9 , batch: 256 , training loss: 4.242000
[INFO] Epoch: 9 , batch: 257 , training loss: 4.407481
[INFO] Epoch: 9 , batch: 258 , training loss: 4.460288
[INFO] Epoch: 9 , batch: 259 , training loss: 4.499597
[INFO] Epoch: 9 , batch: 260 , training loss: 4.185408
[INFO] Epoch: 9 , batch: 261 , training loss: 4.378258
[INFO] Epoch: 9 , batch: 262 , training loss: 4.599113
[INFO] Epoch: 9 , batch: 263 , training loss: 4.735773
[INFO] Epoch: 9 , batch: 264 , training loss: 4.037897
[INFO] Epoch: 9 , batch: 265 , training loss: 4.167686
[INFO] Epoch: 9 , batch: 266 , training loss: 4.650148
[INFO] Epoch: 9 , batch: 267 , training loss: 4.360142
[INFO] Epoch: 9 , batch: 268 , training loss: 4.252800
[INFO] Epoch: 9 , batch: 269 , training loss: 4.252203
[INFO] Epoch: 9 , batch: 270 , training loss: 4.260862
[INFO] Epoch: 9 , batch: 271 , training loss: 4.298289
[INFO] Epoch: 9 , batch: 272 , training loss: 4.270663
[INFO] Epoch: 9 , batch: 273 , training loss: 4.276674
[INFO] Epoch: 9 , batch: 274 , training loss: 4.386180
[INFO] Epoch: 9 , batch: 275 , training loss: 4.292026
[INFO] Epoch: 9 , batch: 276 , training loss: 4.334252
[INFO] Epoch: 9 , batch: 277 , training loss: 4.464150
[INFO] Epoch: 9 , batch: 278 , training loss: 4.102220
[INFO] Epoch: 9 , batch: 279 , training loss: 4.130641
[INFO] Epoch: 9 , batch: 280 , training loss: 4.066863
[INFO] Epoch: 9 , batch: 281 , training loss: 4.226478
[INFO] Epoch: 9 , batch: 282 , training loss: 4.134073
[INFO] Epoch: 9 , batch: 283 , training loss: 4.164675
[INFO] Epoch: 9 , batch: 284 , training loss: 4.173315
[INFO] Epoch: 9 , batch: 285 , training loss: 4.129533
[INFO] Epoch: 9 , batch: 286 , training loss: 4.133465
[INFO] Epoch: 9 , batch: 287 , training loss: 4.053524
[INFO] Epoch: 9 , batch: 288 , training loss: 4.081342
[INFO] Epoch: 9 , batch: 289 , training loss: 4.146203
[INFO] Epoch: 9 , batch: 290 , training loss: 3.910848
[INFO] Epoch: 9 , batch: 291 , training loss: 3.869941
[INFO] Epoch: 9 , batch: 292 , training loss: 3.995297
[INFO] Epoch: 9 , batch: 293 , training loss: 3.906185
[INFO] Epoch: 9 , batch: 294 , training loss: 4.611618
[INFO] Epoch: 9 , batch: 295 , training loss: 4.382614
[INFO] Epoch: 9 , batch: 296 , training loss: 4.298780
[INFO] Epoch: 9 , batch: 297 , training loss: 4.252874
[INFO] Epoch: 9 , batch: 298 , training loss: 4.089270
[INFO] Epoch: 9 , batch: 299 , training loss: 4.120984
[INFO] Epoch: 9 , batch: 300 , training loss: 4.089594
[INFO] Epoch: 9 , batch: 301 , training loss: 4.002953
[INFO] Epoch: 9 , batch: 302 , training loss: 4.183262
[INFO] Epoch: 9 , batch: 303 , training loss: 4.195652
[INFO] Epoch: 9 , batch: 304 , training loss: 4.388547
[INFO] Epoch: 9 , batch: 305 , training loss: 4.138461
[INFO] Epoch: 9 , batch: 306 , training loss: 4.274749
[INFO] Epoch: 9 , batch: 307 , training loss: 4.255649
[INFO] Epoch: 9 , batch: 308 , training loss: 4.125298
[INFO] Epoch: 9 , batch: 309 , training loss: 4.159144
[INFO] Epoch: 9 , batch: 310 , training loss: 3.985189
[INFO] Epoch: 9 , batch: 311 , training loss: 4.012457
[INFO] Epoch: 9 , batch: 312 , training loss: 3.904450
[INFO] Epoch: 9 , batch: 313 , training loss: 4.066484
[INFO] Epoch: 9 , batch: 314 , training loss: 4.102780
[INFO] Epoch: 9 , batch: 315 , training loss: 4.148578
[INFO] Epoch: 9 , batch: 316 , training loss: 4.453723
[INFO] Epoch: 9 , batch: 317 , training loss: 4.995033
[INFO] Epoch: 9 , batch: 318 , training loss: 5.108885
[INFO] Epoch: 9 , batch: 319 , training loss: 4.619197
[INFO] Epoch: 9 , batch: 320 , training loss: 4.157755
[INFO] Epoch: 9 , batch: 321 , training loss: 3.951662
[INFO] Epoch: 9 , batch: 322 , training loss: 4.073427
[INFO] Epoch: 9 , batch: 323 , training loss: 4.098173
[INFO] Epoch: 9 , batch: 324 , training loss: 4.076009
[INFO] Epoch: 9 , batch: 325 , training loss: 4.251898
[INFO] Epoch: 9 , batch: 326 , training loss: 4.313285
[INFO] Epoch: 9 , batch: 327 , training loss: 4.190565
[INFO] Epoch: 9 , batch: 328 , training loss: 4.173506
[INFO] Epoch: 9 , batch: 329 , training loss: 4.113491
[INFO] Epoch: 9 , batch: 330 , training loss: 4.091313
[INFO] Epoch: 9 , batch: 331 , training loss: 4.228441
[INFO] Epoch: 9 , batch: 332 , training loss: 4.051043
[INFO] Epoch: 9 , batch: 333 , training loss: 4.043436
[INFO] Epoch: 9 , batch: 334 , training loss: 4.086650
[INFO] Epoch: 9 , batch: 335 , training loss: 4.218707
[INFO] Epoch: 9 , batch: 336 , training loss: 4.209918
[INFO] Epoch: 9 , batch: 337 , training loss: 4.241313
[INFO] Epoch: 9 , batch: 338 , training loss: 4.456834
[INFO] Epoch: 9 , batch: 339 , training loss: 4.308424
[INFO] Epoch: 9 , batch: 340 , training loss: 4.474740
[INFO] Epoch: 9 , batch: 341 , training loss: 4.239023
[INFO] Epoch: 9 , batch: 342 , training loss: 4.009779
[INFO] Epoch: 9 , batch: 343 , training loss: 4.092935
[INFO] Epoch: 9 , batch: 344 , training loss: 3.948393
[INFO] Epoch: 9 , batch: 345 , training loss: 4.081746
[INFO] Epoch: 9 , batch: 346 , training loss: 4.162930
[INFO] Epoch: 9 , batch: 347 , training loss: 4.019248
[INFO] Epoch: 9 , batch: 348 , training loss: 4.223328
[INFO] Epoch: 9 , batch: 349 , training loss: 4.315329
[INFO] Epoch: 9 , batch: 350 , training loss: 4.056137
[INFO] Epoch: 9 , batch: 351 , training loss: 4.132992
[INFO] Epoch: 9 , batch: 352 , training loss: 4.166148
[INFO] Epoch: 9 , batch: 353 , training loss: 4.132424
[INFO] Epoch: 9 , batch: 354 , training loss: 4.227179
[INFO] Epoch: 9 , batch: 355 , training loss: 4.288872
[INFO] Epoch: 9 , batch: 356 , training loss: 4.120176
[INFO] Epoch: 9 , batch: 357 , training loss: 4.217581
[INFO] Epoch: 9 , batch: 358 , training loss: 4.141102
[INFO] Epoch: 9 , batch: 359 , training loss: 4.098225
[INFO] Epoch: 9 , batch: 360 , training loss: 4.166471
[INFO] Epoch: 9 , batch: 361 , training loss: 4.145358
[INFO] Epoch: 9 , batch: 362 , training loss: 4.257737
[INFO] Epoch: 9 , batch: 363 , training loss: 4.132814
[INFO] Epoch: 9 , batch: 364 , training loss: 4.163810
[INFO] Epoch: 9 , batch: 365 , training loss: 4.084197
[INFO] Epoch: 9 , batch: 366 , training loss: 4.252417
[INFO] Epoch: 9 , batch: 367 , training loss: 4.317049
[INFO] Epoch: 9 , batch: 368 , training loss: 4.847730
[INFO] Epoch: 9 , batch: 369 , training loss: 4.423191
[INFO] Epoch: 9 , batch: 370 , training loss: 4.184840
[INFO] Epoch: 9 , batch: 371 , training loss: 4.689765
[INFO] Epoch: 9 , batch: 372 , training loss: 4.955146
[INFO] Epoch: 9 , batch: 373 , training loss: 5.005435
[INFO] Epoch: 9 , batch: 374 , training loss: 5.037049
[INFO] Epoch: 9 , batch: 375 , training loss: 5.034414
[INFO] Epoch: 9 , batch: 376 , training loss: 4.931082
[INFO] Epoch: 9 , batch: 377 , training loss: 4.607741
[INFO] Epoch: 9 , batch: 378 , training loss: 4.718918
[INFO] Epoch: 9 , batch: 379 , training loss: 4.719150
[INFO] Epoch: 9 , batch: 380 , training loss: 4.858808
[INFO] Epoch: 9 , batch: 381 , training loss: 4.635816
[INFO] Epoch: 9 , batch: 382 , training loss: 4.860186
[INFO] Epoch: 9 , batch: 383 , training loss: 4.920737
[INFO] Epoch: 9 , batch: 384 , training loss: 4.946797
[INFO] Epoch: 9 , batch: 385 , training loss: 4.688293
[INFO] Epoch: 9 , batch: 386 , training loss: 4.861998
[INFO] Epoch: 9 , batch: 387 , training loss: 4.792645
[INFO] Epoch: 9 , batch: 388 , training loss: 4.598773
[INFO] Epoch: 9 , batch: 389 , training loss: 4.449259
[INFO] Epoch: 9 , batch: 390 , training loss: 4.403698
[INFO] Epoch: 9 , batch: 391 , training loss: 4.475014
[INFO] Epoch: 9 , batch: 392 , training loss: 4.822896
[INFO] Epoch: 9 , batch: 393 , training loss: 4.720390
[INFO] Epoch: 9 , batch: 394 , training loss: 4.761962
[INFO] Epoch: 9 , batch: 395 , training loss: 4.607302
[INFO] Epoch: 9 , batch: 396 , training loss: 4.364707
[INFO] Epoch: 9 , batch: 397 , training loss: 4.546703
[INFO] Epoch: 9 , batch: 398 , training loss: 4.378376
[INFO] Epoch: 9 , batch: 399 , training loss: 4.425450
[INFO] Epoch: 9 , batch: 400 , training loss: 4.380458
[INFO] Epoch: 9 , batch: 401 , training loss: 4.816518
[INFO] Epoch: 9 , batch: 402 , training loss: 4.568496
[INFO] Epoch: 9 , batch: 403 , training loss: 4.389207
[INFO] Epoch: 9 , batch: 404 , training loss: 4.558328
[INFO] Epoch: 9 , batch: 405 , training loss: 4.599311
[INFO] Epoch: 9 , batch: 406 , training loss: 4.501998
[INFO] Epoch: 9 , batch: 407 , training loss: 4.581686
[INFO] Epoch: 9 , batch: 408 , training loss: 4.515231
[INFO] Epoch: 9 , batch: 409 , training loss: 4.526894
[INFO] Epoch: 9 , batch: 410 , training loss: 4.570107
[INFO] Epoch: 9 , batch: 411 , training loss: 4.787934
[INFO] Epoch: 9 , batch: 412 , training loss: 4.598440
[INFO] Epoch: 9 , batch: 413 , training loss: 4.459991
[INFO] Epoch: 9 , batch: 414 , training loss: 4.497366
[INFO] Epoch: 9 , batch: 415 , training loss: 4.512073
[INFO] Epoch: 9 , batch: 416 , training loss: 4.587142
[INFO] Epoch: 9 , batch: 417 , training loss: 4.514883
[INFO] Epoch: 9 , batch: 418 , training loss: 4.535434
[INFO] Epoch: 9 , batch: 419 , training loss: 4.494545
[INFO] Epoch: 9 , batch: 420 , training loss: 4.479147
[INFO] Epoch: 9 , batch: 421 , training loss: 4.479813
[INFO] Epoch: 9 , batch: 422 , training loss: 4.353970
[INFO] Epoch: 9 , batch: 423 , training loss: 4.575657
[INFO] Epoch: 9 , batch: 424 , training loss: 4.729508
[INFO] Epoch: 9 , batch: 425 , training loss: 4.600581
[INFO] Epoch: 9 , batch: 426 , training loss: 4.322714
[INFO] Epoch: 9 , batch: 427 , training loss: 4.565200
[INFO] Epoch: 9 , batch: 428 , training loss: 4.463997
[INFO] Epoch: 9 , batch: 429 , training loss: 4.283967
[INFO] Epoch: 9 , batch: 430 , training loss: 4.555237
[INFO] Epoch: 9 , batch: 431 , training loss: 4.131220
[INFO] Epoch: 9 , batch: 432 , training loss: 4.234670
[INFO] Epoch: 9 , batch: 433 , training loss: 4.253147
[INFO] Epoch: 9 , batch: 434 , training loss: 4.106621
[INFO] Epoch: 9 , batch: 435 , training loss: 4.494530
[INFO] Epoch: 9 , batch: 436 , training loss: 4.582211
[INFO] Epoch: 9 , batch: 437 , training loss: 4.318432
[INFO] Epoch: 9 , batch: 438 , training loss: 4.152164
[INFO] Epoch: 9 , batch: 439 , training loss: 4.398073
[INFO] Epoch: 9 , batch: 440 , training loss: 4.522457
[INFO] Epoch: 9 , batch: 441 , training loss: 4.609296
[INFO] Epoch: 9 , batch: 442 , training loss: 4.393011
[INFO] Epoch: 9 , batch: 443 , training loss: 4.584473
[INFO] Epoch: 9 , batch: 444 , training loss: 4.194231
[INFO] Epoch: 9 , batch: 445 , training loss: 4.092200
[INFO] Epoch: 9 , batch: 446 , training loss: 3.991103
[INFO] Epoch: 9 , batch: 447 , training loss: 4.210781
[INFO] Epoch: 9 , batch: 448 , training loss: 4.355327
[INFO] Epoch: 9 , batch: 449 , training loss: 4.781038
[INFO] Epoch: 9 , batch: 450 , training loss: 4.808972
[INFO] Epoch: 9 , batch: 451 , training loss: 4.695907
[INFO] Epoch: 9 , batch: 452 , training loss: 4.484211
[INFO] Epoch: 9 , batch: 453 , training loss: 4.262183
[INFO] Epoch: 9 , batch: 454 , training loss: 4.401280
[INFO] Epoch: 9 , batch: 455 , training loss: 4.444328
[INFO] Epoch: 9 , batch: 456 , training loss: 4.419732
[INFO] Epoch: 9 , batch: 457 , training loss: 4.547560
[INFO] Epoch: 9 , batch: 458 , training loss: 4.270969
[INFO] Epoch: 9 , batch: 459 , training loss: 4.235560
[INFO] Epoch: 9 , batch: 460 , training loss: 4.372498
[INFO] Epoch: 9 , batch: 461 , training loss: 4.336980
[INFO] Epoch: 9 , batch: 462 , training loss: 4.405557
[INFO] Epoch: 9 , batch: 463 , training loss: 4.275530
[INFO] Epoch: 9 , batch: 464 , training loss: 4.505402
[INFO] Epoch: 9 , batch: 465 , training loss: 4.415616
[INFO] Epoch: 9 , batch: 466 , training loss: 4.497550
[INFO] Epoch: 9 , batch: 467 , training loss: 4.508365
[INFO] Epoch: 9 , batch: 468 , training loss: 4.451542
[INFO] Epoch: 9 , batch: 469 , training loss: 4.491339
[INFO] Epoch: 9 , batch: 470 , training loss: 4.278690
[INFO] Epoch: 9 , batch: 471 , training loss: 4.368978
[INFO] Epoch: 9 , batch: 472 , training loss: 4.424814
[INFO] Epoch: 9 , batch: 473 , training loss: 4.350410
[INFO] Epoch: 9 , batch: 474 , training loss: 4.163428
[INFO] Epoch: 9 , batch: 475 , training loss: 4.023722
[INFO] Epoch: 9 , batch: 476 , training loss: 4.402244
[INFO] Epoch: 9 , batch: 477 , training loss: 4.524699
[INFO] Epoch: 9 , batch: 478 , training loss: 4.586551
[INFO] Epoch: 9 , batch: 479 , training loss: 4.553333
[INFO] Epoch: 9 , batch: 480 , training loss: 4.675613
[INFO] Epoch: 9 , batch: 481 , training loss: 4.490710
[INFO] Epoch: 9 , batch: 482 , training loss: 4.642162
[INFO] Epoch: 9 , batch: 483 , training loss: 4.482422
[INFO] Epoch: 9 , batch: 484 , training loss: 4.279377
[INFO] Epoch: 9 , batch: 485 , training loss: 4.380088
[INFO] Epoch: 9 , batch: 486 , training loss: 4.257387
[INFO] Epoch: 9 , batch: 487 , training loss: 4.262366
[INFO] Epoch: 9 , batch: 488 , training loss: 4.476529
[INFO] Epoch: 9 , batch: 489 , training loss: 4.313179
[INFO] Epoch: 9 , batch: 490 , training loss: 4.382678
[INFO] Epoch: 9 , batch: 491 , training loss: 4.366229
[INFO] Epoch: 9 , batch: 492 , training loss: 4.276953
[INFO] Epoch: 9 , batch: 493 , training loss: 4.468369
[INFO] Epoch: 9 , batch: 494 , training loss: 4.383763
[INFO] Epoch: 9 , batch: 495 , training loss: 4.517089
[INFO] Epoch: 9 , batch: 496 , training loss: 4.387357
[INFO] Epoch: 9 , batch: 497 , training loss: 4.414265
[INFO] Epoch: 9 , batch: 498 , training loss: 4.447110
[INFO] Epoch: 9 , batch: 499 , training loss: 4.505253
[INFO] Epoch: 9 , batch: 500 , training loss: 4.702033
[INFO] Epoch: 9 , batch: 501 , training loss: 5.049520
[INFO] Epoch: 9 , batch: 502 , training loss: 5.199515
[INFO] Epoch: 9 , batch: 503 , training loss: 4.900819
[INFO] Epoch: 9 , batch: 504 , training loss: 4.981884
[INFO] Epoch: 9 , batch: 505 , training loss: 4.925355
[INFO] Epoch: 9 , batch: 506 , training loss: 4.878413
[INFO] Epoch: 9 , batch: 507 , training loss: 4.937294
[INFO] Epoch: 9 , batch: 508 , training loss: 4.893087
[INFO] Epoch: 9 , batch: 509 , training loss: 4.635254
[INFO] Epoch: 9 , batch: 510 , training loss: 4.690680
[INFO] Epoch: 9 , batch: 511 , training loss: 4.601958
[INFO] Epoch: 9 , batch: 512 , training loss: 4.673967
[INFO] Epoch: 9 , batch: 513 , training loss: 4.932381
[INFO] Epoch: 9 , batch: 514 , training loss: 4.567522
[INFO] Epoch: 9 , batch: 515 , training loss: 4.823122
[INFO] Epoch: 9 , batch: 516 , training loss: 4.617733
[INFO] Epoch: 9 , batch: 517 , training loss: 4.593020
[INFO] Epoch: 9 , batch: 518 , training loss: 4.563156
[INFO] Epoch: 9 , batch: 519 , training loss: 4.416770
[INFO] Epoch: 9 , batch: 520 , training loss: 4.643579
[INFO] Epoch: 9 , batch: 521 , training loss: 4.616416
[INFO] Epoch: 9 , batch: 522 , training loss: 4.694599
[INFO] Epoch: 9 , batch: 523 , training loss: 4.606108
[INFO] Epoch: 9 , batch: 524 , training loss: 4.874728
[INFO] Epoch: 9 , batch: 525 , training loss: 4.801034
[INFO] Epoch: 9 , batch: 526 , training loss: 4.554631
[INFO] Epoch: 9 , batch: 527 , training loss: 4.582892
[INFO] Epoch: 9 , batch: 528 , training loss: 4.658230
[INFO] Epoch: 9 , batch: 529 , training loss: 4.607015
[INFO] Epoch: 9 , batch: 530 , training loss: 4.436684
[INFO] Epoch: 9 , batch: 531 , training loss: 4.583896
[INFO] Epoch: 9 , batch: 532 , training loss: 4.453398
[INFO] Epoch: 9 , batch: 533 , training loss: 4.655569
[INFO] Epoch: 9 , batch: 534 , training loss: 4.603654
[INFO] Epoch: 9 , batch: 535 , training loss: 4.616344
[INFO] Epoch: 9 , batch: 536 , training loss: 4.451229
[INFO] Epoch: 9 , batch: 537 , training loss: 4.454029
[INFO] Epoch: 9 , batch: 538 , training loss: 4.517427
[INFO] Epoch: 9 , batch: 539 , training loss: 4.667728
[INFO] Epoch: 9 , batch: 540 , training loss: 5.241630
[INFO] Epoch: 9 , batch: 541 , training loss: 5.152357
[INFO] Epoch: 9 , batch: 542 , training loss: 4.973704
[INFO] Epoch: 10 , batch: 0 , training loss: 4.182194
[INFO] Epoch: 10 , batch: 1 , training loss: 4.025028
[INFO] Epoch: 10 , batch: 2 , training loss: 3.986687
[INFO] Epoch: 10 , batch: 3 , training loss: 3.888474
[INFO] Epoch: 10 , batch: 4 , training loss: 4.240695
[INFO] Epoch: 10 , batch: 5 , training loss: 3.857806
[INFO] Epoch: 10 , batch: 6 , training loss: 4.341650
[INFO] Epoch: 10 , batch: 7 , training loss: 4.088877
[INFO] Epoch: 10 , batch: 8 , training loss: 3.807528
[INFO] Epoch: 10 , batch: 9 , training loss: 3.975041
[INFO] Epoch: 10 , batch: 10 , training loss: 3.909954
[INFO] Epoch: 10 , batch: 11 , training loss: 3.873811
[INFO] Epoch: 10 , batch: 12 , training loss: 3.827410
[INFO] Epoch: 10 , batch: 13 , training loss: 3.812550
[INFO] Epoch: 10 , batch: 14 , training loss: 3.618286
[INFO] Epoch: 10 , batch: 15 , training loss: 3.903213
[INFO] Epoch: 10 , batch: 16 , training loss: 3.754765
[INFO] Epoch: 10 , batch: 17 , training loss: 3.932457
[INFO] Epoch: 10 , batch: 18 , training loss: 3.879832
[INFO] Epoch: 10 , batch: 19 , training loss: 3.583319
[INFO] Epoch: 10 , batch: 20 , training loss: 3.578121
[INFO] Epoch: 10 , batch: 21 , training loss: 3.692280
[INFO] Epoch: 10 , batch: 22 , training loss: 3.672523
[INFO] Epoch: 10 , batch: 23 , training loss: 3.831566
[INFO] Epoch: 10 , batch: 24 , training loss: 3.679041
[INFO] Epoch: 10 , batch: 25 , training loss: 3.872269
[INFO] Epoch: 10 , batch: 26 , training loss: 3.702126
[INFO] Epoch: 10 , batch: 27 , training loss: 3.647217
[INFO] Epoch: 10 , batch: 28 , training loss: 3.838582
[INFO] Epoch: 10 , batch: 29 , training loss: 3.621057
[INFO] Epoch: 10 , batch: 30 , training loss: 3.708369
[INFO] Epoch: 10 , batch: 31 , training loss: 3.765821
[INFO] Epoch: 10 , batch: 32 , training loss: 3.781350
[INFO] Epoch: 10 , batch: 33 , training loss: 3.802703
[INFO] Epoch: 10 , batch: 34 , training loss: 3.840354
[INFO] Epoch: 10 , batch: 35 , training loss: 3.787856
[INFO] Epoch: 10 , batch: 36 , training loss: 3.807065
[INFO] Epoch: 10 , batch: 37 , training loss: 3.705298
[INFO] Epoch: 10 , batch: 38 , training loss: 3.756570
[INFO] Epoch: 10 , batch: 39 , training loss: 3.634365
[INFO] Epoch: 10 , batch: 40 , training loss: 3.771600
[INFO] Epoch: 10 , batch: 41 , training loss: 3.798282
[INFO] Epoch: 10 , batch: 42 , training loss: 4.281011
[INFO] Epoch: 10 , batch: 43 , training loss: 3.975655
[INFO] Epoch: 10 , batch: 44 , training loss: 4.210784
[INFO] Epoch: 10 , batch: 45 , training loss: 4.289829
[INFO] Epoch: 10 , batch: 46 , training loss: 4.379390
[INFO] Epoch: 10 , batch: 47 , training loss: 3.944110
[INFO] Epoch: 10 , batch: 48 , training loss: 3.956311
[INFO] Epoch: 10 , batch: 49 , training loss: 4.060912
[INFO] Epoch: 10 , batch: 50 , training loss: 3.836922
[INFO] Epoch: 10 , batch: 51 , training loss: 3.997629
[INFO] Epoch: 10 , batch: 52 , training loss: 3.840103
[INFO] Epoch: 10 , batch: 53 , training loss: 3.987810
[INFO] Epoch: 10 , batch: 54 , training loss: 4.018614
[INFO] Epoch: 10 , batch: 55 , training loss: 4.049052
[INFO] Epoch: 10 , batch: 56 , training loss: 3.910454
[INFO] Epoch: 10 , batch: 57 , training loss: 3.816269
[INFO] Epoch: 10 , batch: 58 , training loss: 3.867851
[INFO] Epoch: 10 , batch: 59 , training loss: 3.954309
[INFO] Epoch: 10 , batch: 60 , training loss: 3.893904
[INFO] Epoch: 10 , batch: 61 , training loss: 3.943378
[INFO] Epoch: 10 , batch: 62 , training loss: 3.851753
[INFO] Epoch: 10 , batch: 63 , training loss: 4.025494
[INFO] Epoch: 10 , batch: 64 , training loss: 4.200979
[INFO] Epoch: 10 , batch: 65 , training loss: 3.911556
[INFO] Epoch: 10 , batch: 66 , training loss: 3.750253
[INFO] Epoch: 10 , batch: 67 , training loss: 3.835037
[INFO] Epoch: 10 , batch: 68 , training loss: 4.046737
[INFO] Epoch: 10 , batch: 69 , training loss: 3.879888
[INFO] Epoch: 10 , batch: 70 , training loss: 4.117792
[INFO] Epoch: 10 , batch: 71 , training loss: 3.959342
[INFO] Epoch: 10 , batch: 72 , training loss: 3.994883
[INFO] Epoch: 10 , batch: 73 , training loss: 3.974427
[INFO] Epoch: 10 , batch: 74 , training loss: 4.096060
[INFO] Epoch: 10 , batch: 75 , training loss: 3.863788
[INFO] Epoch: 10 , batch: 76 , training loss: 4.011044
[INFO] Epoch: 10 , batch: 77 , training loss: 3.988404
[INFO] Epoch: 10 , batch: 78 , training loss: 4.052859
[INFO] Epoch: 10 , batch: 79 , training loss: 3.881551
[INFO] Epoch: 10 , batch: 80 , training loss: 4.115817
[INFO] Epoch: 10 , batch: 81 , training loss: 4.040969
[INFO] Epoch: 10 , batch: 82 , training loss: 4.006860
[INFO] Epoch: 10 , batch: 83 , training loss: 4.109792
[INFO] Epoch: 10 , batch: 84 , training loss: 4.049511
[INFO] Epoch: 10 , batch: 85 , training loss: 4.101380
[INFO] Epoch: 10 , batch: 86 , training loss: 4.092031
[INFO] Epoch: 10 , batch: 87 , training loss: 4.068566
[INFO] Epoch: 10 , batch: 88 , training loss: 4.192235
[INFO] Epoch: 10 , batch: 89 , training loss: 3.991213
[INFO] Epoch: 10 , batch: 90 , training loss: 4.056160
[INFO] Epoch: 10 , batch: 91 , training loss: 3.960200
[INFO] Epoch: 10 , batch: 92 , training loss: 3.994878
[INFO] Epoch: 10 , batch: 93 , training loss: 4.089605
[INFO] Epoch: 10 , batch: 94 , training loss: 4.266914
[INFO] Epoch: 10 , batch: 95 , training loss: 4.037952
[INFO] Epoch: 10 , batch: 96 , training loss: 4.000453
[INFO] Epoch: 10 , batch: 97 , training loss: 3.951794
[INFO] Epoch: 10 , batch: 98 , training loss: 3.925316
[INFO] Epoch: 10 , batch: 99 , training loss: 4.004141
[INFO] Epoch: 10 , batch: 100 , training loss: 3.891318
[INFO] Epoch: 10 , batch: 101 , training loss: 3.925106
[INFO] Epoch: 10 , batch: 102 , training loss: 4.074396
[INFO] Epoch: 10 , batch: 103 , training loss: 3.853908
[INFO] Epoch: 10 , batch: 104 , training loss: 3.826225
[INFO] Epoch: 10 , batch: 105 , training loss: 4.096375
[INFO] Epoch: 10 , batch: 106 , training loss: 4.081707
[INFO] Epoch: 10 , batch: 107 , training loss: 3.938030
[INFO] Epoch: 10 , batch: 108 , training loss: 3.869361
[INFO] Epoch: 10 , batch: 109 , training loss: 3.773566
[INFO] Epoch: 10 , batch: 110 , training loss: 3.976220
[INFO] Epoch: 10 , batch: 111 , training loss: 4.039082
[INFO] Epoch: 10 , batch: 112 , training loss: 3.982592
[INFO] Epoch: 10 , batch: 113 , training loss: 3.939745
[INFO] Epoch: 10 , batch: 114 , training loss: 4.031956
[INFO] Epoch: 10 , batch: 115 , training loss: 3.988914
[INFO] Epoch: 10 , batch: 116 , training loss: 3.908858
[INFO] Epoch: 10 , batch: 117 , training loss: 4.169461
[INFO] Epoch: 10 , batch: 118 , training loss: 4.098834
[INFO] Epoch: 10 , batch: 119 , training loss: 4.253040
[INFO] Epoch: 10 , batch: 120 , training loss: 4.214653
[INFO] Epoch: 10 , batch: 121 , training loss: 4.032159
[INFO] Epoch: 10 , batch: 122 , training loss: 3.965504
[INFO] Epoch: 10 , batch: 123 , training loss: 4.017512
[INFO] Epoch: 10 , batch: 124 , training loss: 4.153634
[INFO] Epoch: 10 , batch: 125 , training loss: 3.894995
[INFO] Epoch: 10 , batch: 126 , training loss: 3.879189
[INFO] Epoch: 10 , batch: 127 , training loss: 3.924497
[INFO] Epoch: 10 , batch: 128 , training loss: 4.104663
[INFO] Epoch: 10 , batch: 129 , training loss: 4.024332
[INFO] Epoch: 10 , batch: 130 , training loss: 4.002859
[INFO] Epoch: 10 , batch: 131 , training loss: 4.031486
[INFO] Epoch: 10 , batch: 132 , training loss: 4.040486
[INFO] Epoch: 10 , batch: 133 , training loss: 3.985023
[INFO] Epoch: 10 , batch: 134 , training loss: 3.800597
[INFO] Epoch: 10 , batch: 135 , training loss: 3.816687
[INFO] Epoch: 10 , batch: 136 , training loss: 4.124467
[INFO] Epoch: 10 , batch: 137 , training loss: 4.013378
[INFO] Epoch: 10 , batch: 138 , training loss: 4.125568
[INFO] Epoch: 10 , batch: 139 , training loss: 4.808295
[INFO] Epoch: 10 , batch: 140 , training loss: 4.536134
[INFO] Epoch: 10 , batch: 141 , training loss: 4.286150
[INFO] Epoch: 10 , batch: 142 , training loss: 3.987196
[INFO] Epoch: 10 , batch: 143 , training loss: 4.085924
[INFO] Epoch: 10 , batch: 144 , training loss: 3.876906
[INFO] Epoch: 10 , batch: 145 , training loss: 4.029067
[INFO] Epoch: 10 , batch: 146 , training loss: 4.206532
[INFO] Epoch: 10 , batch: 147 , training loss: 3.880693
[INFO] Epoch: 10 , batch: 148 , training loss: 3.857592
[INFO] Epoch: 10 , batch: 149 , training loss: 3.926097
[INFO] Epoch: 10 , batch: 150 , training loss: 4.167545
[INFO] Epoch: 10 , batch: 151 , training loss: 3.983950
[INFO] Epoch: 10 , batch: 152 , training loss: 4.035593
[INFO] Epoch: 10 , batch: 153 , training loss: 4.080212
[INFO] Epoch: 10 , batch: 154 , training loss: 4.170250
[INFO] Epoch: 10 , batch: 155 , training loss: 4.394305
[INFO] Epoch: 10 , batch: 156 , training loss: 4.071209
[INFO] Epoch: 10 , batch: 157 , training loss: 4.044569
[INFO] Epoch: 10 , batch: 158 , training loss: 4.316775
[INFO] Epoch: 10 , batch: 159 , training loss: 4.166788
[INFO] Epoch: 10 , batch: 160 , training loss: 4.540491
[INFO] Epoch: 10 , batch: 161 , training loss: 4.561110
[INFO] Epoch: 10 , batch: 162 , training loss: 4.498541
[INFO] Epoch: 10 , batch: 163 , training loss: 4.579698
[INFO] Epoch: 10 , batch: 164 , training loss: 4.545966
[INFO] Epoch: 10 , batch: 165 , training loss: 4.477035
[INFO] Epoch: 10 , batch: 166 , training loss: 4.469262
[INFO] Epoch: 10 , batch: 167 , training loss: 4.763450
[INFO] Epoch: 10 , batch: 168 , training loss: 4.462651
[INFO] Epoch: 10 , batch: 169 , training loss: 4.395041
[INFO] Epoch: 10 , batch: 170 , training loss: 4.470478
[INFO] Epoch: 10 , batch: 171 , training loss: 3.945274
[INFO] Epoch: 10 , batch: 172 , training loss: 4.144835
[INFO] Epoch: 10 , batch: 173 , training loss: 4.477774
[INFO] Epoch: 10 , batch: 174 , training loss: 4.739722
[INFO] Epoch: 10 , batch: 175 , training loss: 5.101535
[INFO] Epoch: 10 , batch: 176 , training loss: 4.789871
[INFO] Epoch: 10 , batch: 177 , training loss: 4.378677
[INFO] Epoch: 10 , batch: 178 , training loss: 4.374429
[INFO] Epoch: 10 , batch: 179 , training loss: 4.378241
[INFO] Epoch: 10 , batch: 180 , training loss: 4.317644
[INFO] Epoch: 10 , batch: 181 , training loss: 4.595495
[INFO] Epoch: 10 , batch: 182 , training loss: 4.512672
[INFO] Epoch: 10 , batch: 183 , training loss: 4.487450
[INFO] Epoch: 10 , batch: 184 , training loss: 4.367761
[INFO] Epoch: 10 , batch: 185 , training loss: 4.327241
[INFO] Epoch: 10 , batch: 186 , training loss: 4.486892
[INFO] Epoch: 10 , batch: 187 , training loss: 4.594117
[INFO] Epoch: 10 , batch: 188 , training loss: 4.553645
[INFO] Epoch: 10 , batch: 189 , training loss: 4.453604
[INFO] Epoch: 10 , batch: 190 , training loss: 4.452504
[INFO] Epoch: 10 , batch: 191 , training loss: 4.577263
[INFO] Epoch: 10 , batch: 192 , training loss: 4.376074
[INFO] Epoch: 10 , batch: 193 , training loss: 4.510520
[INFO] Epoch: 10 , batch: 194 , training loss: 4.393390
[INFO] Epoch: 10 , batch: 195 , training loss: 4.367340
[INFO] Epoch: 10 , batch: 196 , training loss: 4.214843
[INFO] Epoch: 10 , batch: 197 , training loss: 4.349888
[INFO] Epoch: 10 , batch: 198 , training loss: 4.250508
[INFO] Epoch: 10 , batch: 199 , training loss: 4.353664
[INFO] Epoch: 10 , batch: 200 , training loss: 4.251161
[INFO] Epoch: 10 , batch: 201 , training loss: 4.179131
[INFO] Epoch: 10 , batch: 202 , training loss: 4.166842
[INFO] Epoch: 10 , batch: 203 , training loss: 4.243456
[INFO] Epoch: 10 , batch: 204 , training loss: 4.378910
[INFO] Epoch: 10 , batch: 205 , training loss: 3.939597
[INFO] Epoch: 10 , batch: 206 , training loss: 3.888985
[INFO] Epoch: 10 , batch: 207 , training loss: 3.884796
[INFO] Epoch: 10 , batch: 208 , training loss: 4.239986
[INFO] Epoch: 10 , batch: 209 , training loss: 4.178201
[INFO] Epoch: 10 , batch: 210 , training loss: 4.212265
[INFO] Epoch: 10 , batch: 211 , training loss: 4.188764
[INFO] Epoch: 10 , batch: 212 , training loss: 4.296532
[INFO] Epoch: 10 , batch: 213 , training loss: 4.266315
[INFO] Epoch: 10 , batch: 214 , training loss: 4.331217
[INFO] Epoch: 10 , batch: 215 , training loss: 4.541786
[INFO] Epoch: 10 , batch: 216 , training loss: 4.261618
[INFO] Epoch: 10 , batch: 217 , training loss: 4.182726
[INFO] Epoch: 10 , batch: 218 , training loss: 4.166102
[INFO] Epoch: 10 , batch: 219 , training loss: 4.295311
[INFO] Epoch: 10 , batch: 220 , training loss: 4.099667
[INFO] Epoch: 10 , batch: 221 , training loss: 4.103121
[INFO] Epoch: 10 , batch: 222 , training loss: 4.256680
[INFO] Epoch: 10 , batch: 223 , training loss: 4.341778
[INFO] Epoch: 10 , batch: 224 , training loss: 4.397529
[INFO] Epoch: 10 , batch: 225 , training loss: 4.291208
[INFO] Epoch: 10 , batch: 226 , training loss: 4.433311
[INFO] Epoch: 10 , batch: 227 , training loss: 4.377364
[INFO] Epoch: 10 , batch: 228 , training loss: 4.436795
[INFO] Epoch: 10 , batch: 229 , training loss: 4.264408
[INFO] Epoch: 10 , batch: 230 , training loss: 4.132597
[INFO] Epoch: 10 , batch: 231 , training loss: 3.987219
[INFO] Epoch: 10 , batch: 232 , training loss: 4.143104
[INFO] Epoch: 10 , batch: 233 , training loss: 4.132840
[INFO] Epoch: 10 , batch: 234 , training loss: 3.828275
[INFO] Epoch: 10 , batch: 235 , training loss: 3.939999
[INFO] Epoch: 10 , batch: 236 , training loss: 4.096694
[INFO] Epoch: 10 , batch: 237 , training loss: 4.261208
[INFO] Epoch: 10 , batch: 238 , training loss: 4.007782
[INFO] Epoch: 10 , batch: 239 , training loss: 4.075606
[INFO] Epoch: 10 , batch: 240 , training loss: 4.148392
[INFO] Epoch: 10 , batch: 241 , training loss: 3.933592
[INFO] Epoch: 10 , batch: 242 , training loss: 3.943131
[INFO] Epoch: 10 , batch: 243 , training loss: 4.281600
[INFO] Epoch: 10 , batch: 244 , training loss: 4.192350
[INFO] Epoch: 10 , batch: 245 , training loss: 4.184111
[INFO] Epoch: 10 , batch: 246 , training loss: 3.854237
[INFO] Epoch: 10 , batch: 247 , training loss: 4.047796
[INFO] Epoch: 10 , batch: 248 , training loss: 4.105645
[INFO] Epoch: 10 , batch: 249 , training loss: 4.108930
[INFO] Epoch: 10 , batch: 250 , training loss: 3.865762
[INFO] Epoch: 10 , batch: 251 , training loss: 4.350471
[INFO] Epoch: 10 , batch: 252 , training loss: 4.031780
[INFO] Epoch: 10 , batch: 253 , training loss: 3.971139
[INFO] Epoch: 10 , batch: 254 , training loss: 4.280696
[INFO] Epoch: 10 , batch: 255 , training loss: 4.243789
[INFO] Epoch: 10 , batch: 256 , training loss: 4.226565
[INFO] Epoch: 10 , batch: 257 , training loss: 4.395906
[INFO] Epoch: 10 , batch: 258 , training loss: 4.441436
[INFO] Epoch: 10 , batch: 259 , training loss: 4.504475
[INFO] Epoch: 10 , batch: 260 , training loss: 4.184030
[INFO] Epoch: 10 , batch: 261 , training loss: 4.367954
[INFO] Epoch: 10 , batch: 262 , training loss: 4.583221
[INFO] Epoch: 10 , batch: 263 , training loss: 4.733661
[INFO] Epoch: 10 , batch: 264 , training loss: 4.023712
[INFO] Epoch: 10 , batch: 265 , training loss: 4.151441
[INFO] Epoch: 10 , batch: 266 , training loss: 4.623157
[INFO] Epoch: 10 , batch: 267 , training loss: 4.334001
[INFO] Epoch: 10 , batch: 268 , training loss: 4.230989
[INFO] Epoch: 10 , batch: 269 , training loss: 4.230388
[INFO] Epoch: 10 , batch: 270 , training loss: 4.238597
[INFO] Epoch: 10 , batch: 271 , training loss: 4.276930
[INFO] Epoch: 10 , batch: 272 , training loss: 4.258007
[INFO] Epoch: 10 , batch: 273 , training loss: 4.257761
[INFO] Epoch: 10 , batch: 274 , training loss: 4.384768
[INFO] Epoch: 10 , batch: 275 , training loss: 4.268618
[INFO] Epoch: 10 , batch: 276 , training loss: 4.315379
[INFO] Epoch: 10 , batch: 277 , training loss: 4.464088
[INFO] Epoch: 10 , batch: 278 , training loss: 4.102617
[INFO] Epoch: 10 , batch: 279 , training loss: 4.104488
[INFO] Epoch: 10 , batch: 280 , training loss: 4.055392
[INFO] Epoch: 10 , batch: 281 , training loss: 4.218128
[INFO] Epoch: 10 , batch: 282 , training loss: 4.115730
[INFO] Epoch: 10 , batch: 283 , training loss: 4.152065
[INFO] Epoch: 10 , batch: 284 , training loss: 4.157449
[INFO] Epoch: 10 , batch: 285 , training loss: 4.125830
[INFO] Epoch: 10 , batch: 286 , training loss: 4.109990
[INFO] Epoch: 10 , batch: 287 , training loss: 4.044706
[INFO] Epoch: 10 , batch: 288 , training loss: 4.067688
[INFO] Epoch: 10 , batch: 289 , training loss: 4.112535
[INFO] Epoch: 10 , batch: 290 , training loss: 3.897175
[INFO] Epoch: 10 , batch: 291 , training loss: 3.862037
[INFO] Epoch: 10 , batch: 292 , training loss: 3.988805
[INFO] Epoch: 10 , batch: 293 , training loss: 3.888955
[INFO] Epoch: 10 , batch: 294 , training loss: 4.601696
[INFO] Epoch: 10 , batch: 295 , training loss: 4.358579
[INFO] Epoch: 10 , batch: 296 , training loss: 4.283219
[INFO] Epoch: 10 , batch: 297 , training loss: 4.232549
[INFO] Epoch: 10 , batch: 298 , training loss: 4.075250
[INFO] Epoch: 10 , batch: 299 , training loss: 4.124276
[INFO] Epoch: 10 , batch: 300 , training loss: 4.068836
[INFO] Epoch: 10 , batch: 301 , training loss: 3.992507
[INFO] Epoch: 10 , batch: 302 , training loss: 4.172714
[INFO] Epoch: 10 , batch: 303 , training loss: 4.187653
[INFO] Epoch: 10 , batch: 304 , training loss: 4.378384
[INFO] Epoch: 10 , batch: 305 , training loss: 4.137142
[INFO] Epoch: 10 , batch: 306 , training loss: 4.257017
[INFO] Epoch: 10 , batch: 307 , training loss: 4.255512
[INFO] Epoch: 10 , batch: 308 , training loss: 4.105455
[INFO] Epoch: 10 , batch: 309 , training loss: 4.143531
[INFO] Epoch: 10 , batch: 310 , training loss: 3.978398
[INFO] Epoch: 10 , batch: 311 , training loss: 3.997079
[INFO] Epoch: 10 , batch: 312 , training loss: 3.894439
[INFO] Epoch: 10 , batch: 313 , training loss: 4.041087
[INFO] Epoch: 10 , batch: 314 , training loss: 4.099560
[INFO] Epoch: 10 , batch: 315 , training loss: 4.143118
[INFO] Epoch: 10 , batch: 316 , training loss: 4.426786
[INFO] Epoch: 10 , batch: 317 , training loss: 4.952056
[INFO] Epoch: 10 , batch: 318 , training loss: 5.065069
[INFO] Epoch: 10 , batch: 319 , training loss: 4.604494
[INFO] Epoch: 10 , batch: 320 , training loss: 4.147038
[INFO] Epoch: 10 , batch: 321 , training loss: 3.945110
[INFO] Epoch: 10 , batch: 322 , training loss: 4.075770
[INFO] Epoch: 10 , batch: 323 , training loss: 4.095638
[INFO] Epoch: 10 , batch: 324 , training loss: 4.055105
[INFO] Epoch: 10 , batch: 325 , training loss: 4.230265
[INFO] Epoch: 10 , batch: 326 , training loss: 4.285932
[INFO] Epoch: 10 , batch: 327 , training loss: 4.168182
[INFO] Epoch: 10 , batch: 328 , training loss: 4.159768
[INFO] Epoch: 10 , batch: 329 , training loss: 4.093627
[INFO] Epoch: 10 , batch: 330 , training loss: 4.076819
[INFO] Epoch: 10 , batch: 331 , training loss: 4.223326
[INFO] Epoch: 10 , batch: 332 , training loss: 4.023305
[INFO] Epoch: 10 , batch: 333 , training loss: 4.034233
[INFO] Epoch: 10 , batch: 334 , training loss: 4.074060
[INFO] Epoch: 10 , batch: 335 , training loss: 4.195005
[INFO] Epoch: 10 , batch: 336 , training loss: 4.206214
[INFO] Epoch: 10 , batch: 337 , training loss: 4.236392
[INFO] Epoch: 10 , batch: 338 , training loss: 4.443958
[INFO] Epoch: 10 , batch: 339 , training loss: 4.286932
[INFO] Epoch: 10 , batch: 340 , training loss: 4.459503
[INFO] Epoch: 10 , batch: 341 , training loss: 4.235972
[INFO] Epoch: 10 , batch: 342 , training loss: 4.004290
[INFO] Epoch: 10 , batch: 343 , training loss: 4.084274
[INFO] Epoch: 10 , batch: 344 , training loss: 3.939173
[INFO] Epoch: 10 , batch: 345 , training loss: 4.053876
[INFO] Epoch: 10 , batch: 346 , training loss: 4.157324
[INFO] Epoch: 10 , batch: 347 , training loss: 4.018288
[INFO] Epoch: 10 , batch: 348 , training loss: 4.200135
[INFO] Epoch: 10 , batch: 349 , training loss: 4.314798
[INFO] Epoch: 10 , batch: 350 , training loss: 4.058689
[INFO] Epoch: 10 , batch: 351 , training loss: 4.121006
[INFO] Epoch: 10 , batch: 352 , training loss: 4.159827
[INFO] Epoch: 10 , batch: 353 , training loss: 4.119199
[INFO] Epoch: 10 , batch: 354 , training loss: 4.227614
[INFO] Epoch: 10 , batch: 355 , training loss: 4.280282
[INFO] Epoch: 10 , batch: 356 , training loss: 4.105389
[INFO] Epoch: 10 , batch: 357 , training loss: 4.197766
[INFO] Epoch: 10 , batch: 358 , training loss: 4.123213
[INFO] Epoch: 10 , batch: 359 , training loss: 4.089952
[INFO] Epoch: 10 , batch: 360 , training loss: 4.139647
[INFO] Epoch: 10 , batch: 361 , training loss: 4.134815
[INFO] Epoch: 10 , batch: 362 , training loss: 4.235902
[INFO] Epoch: 10 , batch: 363 , training loss: 4.132136
[INFO] Epoch: 10 , batch: 364 , training loss: 4.162549
[INFO] Epoch: 10 , batch: 365 , training loss: 4.075237
[INFO] Epoch: 10 , batch: 366 , training loss: 4.226355
[INFO] Epoch: 10 , batch: 367 , training loss: 4.312177
[INFO] Epoch: 10 , batch: 368 , training loss: 4.805494
[INFO] Epoch: 10 , batch: 369 , training loss: 4.409511
[INFO] Epoch: 10 , batch: 370 , training loss: 4.169547
[INFO] Epoch: 10 , batch: 371 , training loss: 4.653424
[INFO] Epoch: 10 , batch: 372 , training loss: 4.944292
[INFO] Epoch: 10 , batch: 373 , training loss: 4.987169
[INFO] Epoch: 10 , batch: 374 , training loss: 5.022067
[INFO] Epoch: 10 , batch: 375 , training loss: 5.018732
[INFO] Epoch: 10 , batch: 376 , training loss: 4.916525
[INFO] Epoch: 10 , batch: 377 , training loss: 4.588849
[INFO] Epoch: 10 , batch: 378 , training loss: 4.719172
[INFO] Epoch: 10 , batch: 379 , training loss: 4.710552
[INFO] Epoch: 10 , batch: 380 , training loss: 4.855535
[INFO] Epoch: 10 , batch: 381 , training loss: 4.616220
[INFO] Epoch: 10 , batch: 382 , training loss: 4.827445
[INFO] Epoch: 10 , batch: 383 , training loss: 4.887090
[INFO] Epoch: 10 , batch: 384 , training loss: 4.911201
[INFO] Epoch: 10 , batch: 385 , training loss: 4.660012
[INFO] Epoch: 10 , batch: 386 , training loss: 4.833959
[INFO] Epoch: 10 , batch: 387 , training loss: 4.775660
[INFO] Epoch: 10 , batch: 388 , training loss: 4.575751
[INFO] Epoch: 10 , batch: 389 , training loss: 4.436063
[INFO] Epoch: 10 , batch: 390 , training loss: 4.398783
[INFO] Epoch: 10 , batch: 391 , training loss: 4.459995
[INFO] Epoch: 10 , batch: 392 , training loss: 4.797878
[INFO] Epoch: 10 , batch: 393 , training loss: 4.686157
[INFO] Epoch: 10 , batch: 394 , training loss: 4.745991
[INFO] Epoch: 10 , batch: 395 , training loss: 4.579922
[INFO] Epoch: 10 , batch: 396 , training loss: 4.358426
[INFO] Epoch: 10 , batch: 397 , training loss: 4.536065
[INFO] Epoch: 10 , batch: 398 , training loss: 4.358656
[INFO] Epoch: 10 , batch: 399 , training loss: 4.425493
[INFO] Epoch: 10 , batch: 400 , training loss: 4.371665
[INFO] Epoch: 10 , batch: 401 , training loss: 4.798923
[INFO] Epoch: 10 , batch: 402 , training loss: 4.552677
[INFO] Epoch: 10 , batch: 403 , training loss: 4.374379
[INFO] Epoch: 10 , batch: 404 , training loss: 4.542562
[INFO] Epoch: 10 , batch: 405 , training loss: 4.585020
[INFO] Epoch: 10 , batch: 406 , training loss: 4.488510
[INFO] Epoch: 10 , batch: 407 , training loss: 4.570673
[INFO] Epoch: 10 , batch: 408 , training loss: 4.494349
[INFO] Epoch: 10 , batch: 409 , training loss: 4.518775
[INFO] Epoch: 10 , batch: 410 , training loss: 4.547041
[INFO] Epoch: 10 , batch: 411 , training loss: 4.766581
[INFO] Epoch: 10 , batch: 412 , training loss: 4.575579
[INFO] Epoch: 10 , batch: 413 , training loss: 4.445093
[INFO] Epoch: 10 , batch: 414 , training loss: 4.479512
[INFO] Epoch: 10 , batch: 415 , training loss: 4.497889
[INFO] Epoch: 10 , batch: 416 , training loss: 4.577226
[INFO] Epoch: 10 , batch: 417 , training loss: 4.504436
[INFO] Epoch: 10 , batch: 418 , training loss: 4.523784
[INFO] Epoch: 10 , batch: 419 , training loss: 4.487248
[INFO] Epoch: 10 , batch: 420 , training loss: 4.459517
[INFO] Epoch: 10 , batch: 421 , training loss: 4.463115
[INFO] Epoch: 10 , batch: 422 , training loss: 4.334922
[INFO] Epoch: 10 , batch: 423 , training loss: 4.557635
[INFO] Epoch: 10 , batch: 424 , training loss: 4.720033
[INFO] Epoch: 10 , batch: 425 , training loss: 4.581598
[INFO] Epoch: 10 , batch: 426 , training loss: 4.306726
[INFO] Epoch: 10 , batch: 427 , training loss: 4.547317
[INFO] Epoch: 10 , batch: 428 , training loss: 4.447184
[INFO] Epoch: 10 , batch: 429 , training loss: 4.277489
[INFO] Epoch: 10 , batch: 430 , training loss: 4.543301
[INFO] Epoch: 10 , batch: 431 , training loss: 4.126320
[INFO] Epoch: 10 , batch: 432 , training loss: 4.222612
[INFO] Epoch: 10 , batch: 433 , training loss: 4.245757
[INFO] Epoch: 10 , batch: 434 , training loss: 4.099607
[INFO] Epoch: 10 , batch: 435 , training loss: 4.469280
[INFO] Epoch: 10 , batch: 436 , training loss: 4.567469
[INFO] Epoch: 10 , batch: 437 , training loss: 4.298960
[INFO] Epoch: 10 , batch: 438 , training loss: 4.138286
[INFO] Epoch: 10 , batch: 439 , training loss: 4.387836
[INFO] Epoch: 10 , batch: 440 , training loss: 4.514010
[INFO] Epoch: 10 , batch: 441 , training loss: 4.602042
[INFO] Epoch: 10 , batch: 442 , training loss: 4.382013
[INFO] Epoch: 10 , batch: 443 , training loss: 4.573471
[INFO] Epoch: 10 , batch: 444 , training loss: 4.169878
[INFO] Epoch: 10 , batch: 445 , training loss: 4.076568
[INFO] Epoch: 10 , batch: 446 , training loss: 3.981653
[INFO] Epoch: 10 , batch: 447 , training loss: 4.196145
[INFO] Epoch: 10 , batch: 448 , training loss: 4.335578
[INFO] Epoch: 10 , batch: 449 , training loss: 4.759957
[INFO] Epoch: 10 , batch: 450 , training loss: 4.794068
[INFO] Epoch: 10 , batch: 451 , training loss: 4.688908
[INFO] Epoch: 10 , batch: 452 , training loss: 4.473790
[INFO] Epoch: 10 , batch: 453 , training loss: 4.249591
[INFO] Epoch: 10 , batch: 454 , training loss: 4.397516
[INFO] Epoch: 10 , batch: 455 , training loss: 4.435133
[INFO] Epoch: 10 , batch: 456 , training loss: 4.418651
[INFO] Epoch: 10 , batch: 457 , training loss: 4.529649
[INFO] Epoch: 10 , batch: 458 , training loss: 4.259762
[INFO] Epoch: 10 , batch: 459 , training loss: 4.227795
[INFO] Epoch: 10 , batch: 460 , training loss: 4.354949
[INFO] Epoch: 10 , batch: 461 , training loss: 4.319949
[INFO] Epoch: 10 , batch: 462 , training loss: 4.389867
[INFO] Epoch: 10 , batch: 463 , training loss: 4.269936
[INFO] Epoch: 10 , batch: 464 , training loss: 4.492224
[INFO] Epoch: 10 , batch: 465 , training loss: 4.422873
[INFO] Epoch: 10 , batch: 466 , training loss: 4.481836
[INFO] Epoch: 10 , batch: 467 , training loss: 4.488715
[INFO] Epoch: 10 , batch: 468 , training loss: 4.437402
[INFO] Epoch: 10 , batch: 469 , training loss: 4.478280
[INFO] Epoch: 10 , batch: 470 , training loss: 4.268499
[INFO] Epoch: 10 , batch: 471 , training loss: 4.356388
[INFO] Epoch: 10 , batch: 472 , training loss: 4.417619
[INFO] Epoch: 10 , batch: 473 , training loss: 4.344107
[INFO] Epoch: 10 , batch: 474 , training loss: 4.149093
[INFO] Epoch: 10 , batch: 475 , training loss: 4.010343
[INFO] Epoch: 10 , batch: 476 , training loss: 4.394217
[INFO] Epoch: 10 , batch: 477 , training loss: 4.508148
[INFO] Epoch: 10 , batch: 478 , training loss: 4.578975
[INFO] Epoch: 10 , batch: 479 , training loss: 4.540984
[INFO] Epoch: 10 , batch: 480 , training loss: 4.642854
[INFO] Epoch: 10 , batch: 481 , training loss: 4.485385
[INFO] Epoch: 10 , batch: 482 , training loss: 4.632293
[INFO] Epoch: 10 , batch: 483 , training loss: 4.470035
[INFO] Epoch: 10 , batch: 484 , training loss: 4.267792
[INFO] Epoch: 10 , batch: 485 , training loss: 4.368334
[INFO] Epoch: 10 , batch: 486 , training loss: 4.255332
[INFO] Epoch: 10 , batch: 487 , training loss: 4.247322
[INFO] Epoch: 10 , batch: 488 , training loss: 4.460854
[INFO] Epoch: 10 , batch: 489 , training loss: 4.296667
[INFO] Epoch: 10 , batch: 490 , training loss: 4.374458
[INFO] Epoch: 10 , batch: 491 , training loss: 4.349238
[INFO] Epoch: 10 , batch: 492 , training loss: 4.263074
[INFO] Epoch: 10 , batch: 493 , training loss: 4.466242
[INFO] Epoch: 10 , batch: 494 , training loss: 4.380874
[INFO] Epoch: 10 , batch: 495 , training loss: 4.505740
[INFO] Epoch: 10 , batch: 496 , training loss: 4.373004
[INFO] Epoch: 10 , batch: 497 , training loss: 4.408308
[INFO] Epoch: 10 , batch: 498 , training loss: 4.423269
[INFO] Epoch: 10 , batch: 499 , training loss: 4.493509
[INFO] Epoch: 10 , batch: 500 , training loss: 4.678991
[INFO] Epoch: 10 , batch: 501 , training loss: 5.015894
[INFO] Epoch: 10 , batch: 502 , training loss: 5.153881
[INFO] Epoch: 10 , batch: 503 , training loss: 4.878573
[INFO] Epoch: 10 , batch: 504 , training loss: 4.972262
[INFO] Epoch: 10 , batch: 505 , training loss: 4.910907
[INFO] Epoch: 10 , batch: 506 , training loss: 4.865336
[INFO] Epoch: 10 , batch: 507 , training loss: 4.920166
[INFO] Epoch: 10 , batch: 508 , training loss: 4.876749
[INFO] Epoch: 10 , batch: 509 , training loss: 4.615811
[INFO] Epoch: 10 , batch: 510 , training loss: 4.667422
[INFO] Epoch: 10 , batch: 511 , training loss: 4.585121
[INFO] Epoch: 10 , batch: 512 , training loss: 4.665267
[INFO] Epoch: 10 , batch: 513 , training loss: 4.915941
[INFO] Epoch: 10 , batch: 514 , training loss: 4.554830
[INFO] Epoch: 10 , batch: 515 , training loss: 4.810578
[INFO] Epoch: 10 , batch: 516 , training loss: 4.612031
[INFO] Epoch: 10 , batch: 517 , training loss: 4.572355
[INFO] Epoch: 10 , batch: 518 , training loss: 4.544570
[INFO] Epoch: 10 , batch: 519 , training loss: 4.403723
[INFO] Epoch: 10 , batch: 520 , training loss: 4.630579
[INFO] Epoch: 10 , batch: 521 , training loss: 4.598531
[INFO] Epoch: 10 , batch: 522 , training loss: 4.665575
[INFO] Epoch: 10 , batch: 523 , training loss: 4.570638
[INFO] Epoch: 10 , batch: 524 , training loss: 4.857678
[INFO] Epoch: 10 , batch: 525 , training loss: 4.789224
[INFO] Epoch: 10 , batch: 526 , training loss: 4.536363
[INFO] Epoch: 10 , batch: 527 , training loss: 4.558623
[INFO] Epoch: 10 , batch: 528 , training loss: 4.627272
[INFO] Epoch: 10 , batch: 529 , training loss: 4.582279
[INFO] Epoch: 10 , batch: 530 , training loss: 4.420104
[INFO] Epoch: 10 , batch: 531 , training loss: 4.566270
[INFO] Epoch: 10 , batch: 532 , training loss: 4.450677
[INFO] Epoch: 10 , batch: 533 , training loss: 4.634095
[INFO] Epoch: 10 , batch: 534 , training loss: 4.601236
[INFO] Epoch: 10 , batch: 535 , training loss: 4.610279
[INFO] Epoch: 10 , batch: 536 , training loss: 4.450398
[INFO] Epoch: 10 , batch: 537 , training loss: 4.439089
[INFO] Epoch: 10 , batch: 538 , training loss: 4.512794
[INFO] Epoch: 10 , batch: 539 , training loss: 4.658075
[INFO] Epoch: 10 , batch: 540 , training loss: 5.231182
[INFO] Epoch: 10 , batch: 541 , training loss: 5.117690
[INFO] Epoch: 10 , batch: 542 , training loss: 4.972746
[INFO] Epoch: 11 , batch: 0 , training loss: 4.107814
[INFO] Epoch: 11 , batch: 1 , training loss: 4.013072
[INFO] Epoch: 11 , batch: 2 , training loss: 3.974149
[INFO] Epoch: 11 , batch: 3 , training loss: 3.855675
[INFO] Epoch: 11 , batch: 4 , training loss: 4.296480
[INFO] Epoch: 11 , batch: 5 , training loss: 3.854286
[INFO] Epoch: 11 , batch: 6 , training loss: 4.294712
[INFO] Epoch: 11 , batch: 7 , training loss: 4.066035
[INFO] Epoch: 11 , batch: 8 , training loss: 3.807600
[INFO] Epoch: 11 , batch: 9 , training loss: 3.951658
[INFO] Epoch: 11 , batch: 10 , training loss: 3.878263
[INFO] Epoch: 11 , batch: 11 , training loss: 3.847647
[INFO] Epoch: 11 , batch: 12 , training loss: 3.800848
[INFO] Epoch: 11 , batch: 13 , training loss: 3.782430
[INFO] Epoch: 11 , batch: 14 , training loss: 3.591085
[INFO] Epoch: 11 , batch: 15 , training loss: 3.862178
[INFO] Epoch: 11 , batch: 16 , training loss: 3.725807
[INFO] Epoch: 11 , batch: 17 , training loss: 3.900656
[INFO] Epoch: 11 , batch: 18 , training loss: 3.900099
[INFO] Epoch: 11 , batch: 19 , training loss: 3.555797
[INFO] Epoch: 11 , batch: 20 , training loss: 3.553453
[INFO] Epoch: 11 , batch: 21 , training loss: 3.683810
[INFO] Epoch: 11 , batch: 22 , training loss: 3.668343
[INFO] Epoch: 11 , batch: 23 , training loss: 3.822692
[INFO] Epoch: 11 , batch: 24 , training loss: 3.658369
[INFO] Epoch: 11 , batch: 25 , training loss: 3.860516
[INFO] Epoch: 11 , batch: 26 , training loss: 3.680551
[INFO] Epoch: 11 , batch: 27 , training loss: 3.647574
[INFO] Epoch: 11 , batch: 28 , training loss: 3.822616
[INFO] Epoch: 11 , batch: 29 , training loss: 3.629629
[INFO] Epoch: 11 , batch: 30 , training loss: 3.683422
[INFO] Epoch: 11 , batch: 31 , training loss: 3.754303
[INFO] Epoch: 11 , batch: 32 , training loss: 3.754294
[INFO] Epoch: 11 , batch: 33 , training loss: 3.774195
[INFO] Epoch: 11 , batch: 34 , training loss: 3.805592
[INFO] Epoch: 11 , batch: 35 , training loss: 3.758447
[INFO] Epoch: 11 , batch: 36 , training loss: 3.759075
[INFO] Epoch: 11 , batch: 37 , training loss: 3.669589
[INFO] Epoch: 11 , batch: 38 , training loss: 3.732955
[INFO] Epoch: 11 , batch: 39 , training loss: 3.596939
[INFO] Epoch: 11 , batch: 40 , training loss: 3.767138
[INFO] Epoch: 11 , batch: 41 , training loss: 3.767502
[INFO] Epoch: 11 , batch: 42 , training loss: 4.288870
[INFO] Epoch: 11 , batch: 43 , training loss: 3.934586
[INFO] Epoch: 11 , batch: 44 , training loss: 4.236038
[INFO] Epoch: 11 , batch: 45 , training loss: 4.259437
[INFO] Epoch: 11 , batch: 46 , training loss: 4.331214
[INFO] Epoch: 11 , batch: 47 , training loss: 3.928808
[INFO] Epoch: 11 , batch: 48 , training loss: 3.923334
[INFO] Epoch: 11 , batch: 49 , training loss: 4.057549
[INFO] Epoch: 11 , batch: 50 , training loss: 3.821879
[INFO] Epoch: 11 , batch: 51 , training loss: 3.968800
[INFO] Epoch: 11 , batch: 52 , training loss: 3.833460
[INFO] Epoch: 11 , batch: 53 , training loss: 3.964771
[INFO] Epoch: 11 , batch: 54 , training loss: 4.005787
[INFO] Epoch: 11 , batch: 55 , training loss: 4.028681
[INFO] Epoch: 11 , batch: 56 , training loss: 3.922438
[INFO] Epoch: 11 , batch: 57 , training loss: 3.804084
[INFO] Epoch: 11 , batch: 58 , training loss: 3.875468
[INFO] Epoch: 11 , batch: 59 , training loss: 3.950414
[INFO] Epoch: 11 , batch: 60 , training loss: 3.870405
[INFO] Epoch: 11 , batch: 61 , training loss: 3.950498
[INFO] Epoch: 11 , batch: 62 , training loss: 3.824327
[INFO] Epoch: 11 , batch: 63 , training loss: 4.003850
[INFO] Epoch: 11 , batch: 64 , training loss: 4.195805
[INFO] Epoch: 11 , batch: 65 , training loss: 3.888226
[INFO] Epoch: 11 , batch: 66 , training loss: 3.747036
[INFO] Epoch: 11 , batch: 67 , training loss: 3.830236
[INFO] Epoch: 11 , batch: 68 , training loss: 4.024755
[INFO] Epoch: 11 , batch: 69 , training loss: 3.874151
[INFO] Epoch: 11 , batch: 70 , training loss: 4.108986
[INFO] Epoch: 11 , batch: 71 , training loss: 3.937606
[INFO] Epoch: 11 , batch: 72 , training loss: 3.981424
[INFO] Epoch: 11 , batch: 73 , training loss: 3.919281
[INFO] Epoch: 11 , batch: 74 , training loss: 4.077358
[INFO] Epoch: 11 , batch: 75 , training loss: 3.885491
[INFO] Epoch: 11 , batch: 76 , training loss: 4.020005
[INFO] Epoch: 11 , batch: 77 , training loss: 3.972961
[INFO] Epoch: 11 , batch: 78 , training loss: 4.054135
[INFO] Epoch: 11 , batch: 79 , training loss: 3.852087
[INFO] Epoch: 11 , batch: 80 , training loss: 4.101391
[INFO] Epoch: 11 , batch: 81 , training loss: 4.043389
[INFO] Epoch: 11 , batch: 82 , training loss: 3.999790
[INFO] Epoch: 11 , batch: 83 , training loss: 4.110550
[INFO] Epoch: 11 , batch: 84 , training loss: 4.019811
[INFO] Epoch: 11 , batch: 85 , training loss: 4.084003
[INFO] Epoch: 11 , batch: 86 , training loss: 4.053781
[INFO] Epoch: 11 , batch: 87 , training loss: 4.067099
[INFO] Epoch: 11 , batch: 88 , training loss: 4.202049
[INFO] Epoch: 11 , batch: 89 , training loss: 3.963400
[INFO] Epoch: 11 , batch: 90 , training loss: 4.045578
[INFO] Epoch: 11 , batch: 91 , training loss: 3.954310
[INFO] Epoch: 11 , batch: 92 , training loss: 3.959580
[INFO] Epoch: 11 , batch: 93 , training loss: 4.081758
[INFO] Epoch: 11 , batch: 94 , training loss: 4.250371
[INFO] Epoch: 11 , batch: 95 , training loss: 4.008665
[INFO] Epoch: 11 , batch: 96 , training loss: 3.999248
[INFO] Epoch: 11 , batch: 97 , training loss: 3.921906
[INFO] Epoch: 11 , batch: 98 , training loss: 3.905788
[INFO] Epoch: 11 , batch: 99 , training loss: 3.961876
[INFO] Epoch: 11 , batch: 100 , training loss: 3.875864
[INFO] Epoch: 11 , batch: 101 , training loss: 3.935960
[INFO] Epoch: 11 , batch: 102 , training loss: 4.055621
[INFO] Epoch: 11 , batch: 103 , training loss: 3.832013
[INFO] Epoch: 11 , batch: 104 , training loss: 3.825024
[INFO] Epoch: 11 , batch: 105 , training loss: 4.119905
[INFO] Epoch: 11 , batch: 106 , training loss: 4.075776
[INFO] Epoch: 11 , batch: 107 , training loss: 3.917870
[INFO] Epoch: 11 , batch: 108 , training loss: 3.841928
[INFO] Epoch: 11 , batch: 109 , training loss: 3.752786
[INFO] Epoch: 11 , batch: 110 , training loss: 3.978492
[INFO] Epoch: 11 , batch: 111 , training loss: 4.039631
[INFO] Epoch: 11 , batch: 112 , training loss: 3.959142
[INFO] Epoch: 11 , batch: 113 , training loss: 3.929125
[INFO] Epoch: 11 , batch: 114 , training loss: 4.014142
[INFO] Epoch: 11 , batch: 115 , training loss: 3.972378
[INFO] Epoch: 11 , batch: 116 , training loss: 3.892880
[INFO] Epoch: 11 , batch: 117 , training loss: 4.122041
[INFO] Epoch: 11 , batch: 118 , training loss: 4.066162
[INFO] Epoch: 11 , batch: 119 , training loss: 4.234176
[INFO] Epoch: 11 , batch: 120 , training loss: 4.232784
[INFO] Epoch: 11 , batch: 121 , training loss: 4.025353
[INFO] Epoch: 11 , batch: 122 , training loss: 3.963648
[INFO] Epoch: 11 , batch: 123 , training loss: 3.996390
[INFO] Epoch: 11 , batch: 124 , training loss: 4.132526
[INFO] Epoch: 11 , batch: 125 , training loss: 3.865804
[INFO] Epoch: 11 , batch: 126 , training loss: 3.867058
[INFO] Epoch: 11 , batch: 127 , training loss: 3.905100
[INFO] Epoch: 11 , batch: 128 , training loss: 4.074510
[INFO] Epoch: 11 , batch: 129 , training loss: 4.013467
[INFO] Epoch: 11 , batch: 130 , training loss: 3.989761
[INFO] Epoch: 11 , batch: 131 , training loss: 4.021338
[INFO] Epoch: 11 , batch: 132 , training loss: 4.018031
[INFO] Epoch: 11 , batch: 133 , training loss: 3.956691
[INFO] Epoch: 11 , batch: 134 , training loss: 3.764914
[INFO] Epoch: 11 , batch: 135 , training loss: 3.797557
[INFO] Epoch: 11 , batch: 136 , training loss: 4.115300
[INFO] Epoch: 11 , batch: 137 , training loss: 4.011827
[INFO] Epoch: 11 , batch: 138 , training loss: 4.122104
[INFO] Epoch: 11 , batch: 139 , training loss: 4.763859
[INFO] Epoch: 11 , batch: 140 , training loss: 4.521766
[INFO] Epoch: 11 , batch: 141 , training loss: 4.241621
[INFO] Epoch: 11 , batch: 142 , training loss: 3.965402
[INFO] Epoch: 11 , batch: 143 , training loss: 4.085203
[INFO] Epoch: 11 , batch: 144 , training loss: 3.873396
[INFO] Epoch: 11 , batch: 145 , training loss: 4.014939
[INFO] Epoch: 11 , batch: 146 , training loss: 4.175013
[INFO] Epoch: 11 , batch: 147 , training loss: 3.885943
[INFO] Epoch: 11 , batch: 148 , training loss: 3.843193
[INFO] Epoch: 11 , batch: 149 , training loss: 3.906102
[INFO] Epoch: 11 , batch: 150 , training loss: 4.154001
[INFO] Epoch: 11 , batch: 151 , training loss: 3.979464
[INFO] Epoch: 11 , batch: 152 , training loss: 4.012428
[INFO] Epoch: 11 , batch: 153 , training loss: 4.048984
[INFO] Epoch: 11 , batch: 154 , training loss: 4.145233
[INFO] Epoch: 11 , batch: 155 , training loss: 4.383211
[INFO] Epoch: 11 , batch: 156 , training loss: 4.058272
[INFO] Epoch: 11 , batch: 157 , training loss: 4.027276
[INFO] Epoch: 11 , batch: 158 , training loss: 4.299201
[INFO] Epoch: 11 , batch: 159 , training loss: 4.139030
[INFO] Epoch: 11 , batch: 160 , training loss: 4.501410
[INFO] Epoch: 11 , batch: 161 , training loss: 4.533073
[INFO] Epoch: 11 , batch: 162 , training loss: 4.479723
[INFO] Epoch: 11 , batch: 163 , training loss: 4.560004
[INFO] Epoch: 11 , batch: 164 , training loss: 4.551617
[INFO] Epoch: 11 , batch: 165 , training loss: 4.466500
[INFO] Epoch: 11 , batch: 166 , training loss: 4.442320
[INFO] Epoch: 11 , batch: 167 , training loss: 4.691844
[INFO] Epoch: 11 , batch: 168 , training loss: 4.419586
[INFO] Epoch: 11 , batch: 169 , training loss: 4.349035
[INFO] Epoch: 11 , batch: 170 , training loss: 4.427956
[INFO] Epoch: 11 , batch: 171 , training loss: 3.896319
[INFO] Epoch: 11 , batch: 172 , training loss: 4.091327
[INFO] Epoch: 11 , batch: 173 , training loss: 4.380774
[INFO] Epoch: 11 , batch: 174 , training loss: 4.693491
[INFO] Epoch: 11 , batch: 175 , training loss: 5.049697
[INFO] Epoch: 11 , batch: 176 , training loss: 4.782440
[INFO] Epoch: 11 , batch: 177 , training loss: 4.349888
[INFO] Epoch: 11 , batch: 178 , training loss: 4.363661
[INFO] Epoch: 11 , batch: 179 , training loss: 4.386501
[INFO] Epoch: 11 , batch: 180 , training loss: 4.302088
[INFO] Epoch: 11 , batch: 181 , training loss: 4.579039
[INFO] Epoch: 11 , batch: 182 , training loss: 4.504838
[INFO] Epoch: 11 , batch: 183 , training loss: 4.461112
[INFO] Epoch: 11 , batch: 184 , training loss: 4.346261
[INFO] Epoch: 11 , batch: 185 , training loss: 4.302002
[INFO] Epoch: 11 , batch: 186 , training loss: 4.469338
[INFO] Epoch: 11 , batch: 187 , training loss: 4.568000
[INFO] Epoch: 11 , batch: 188 , training loss: 4.538280
[INFO] Epoch: 11 , batch: 189 , training loss: 4.419310
[INFO] Epoch: 11 , batch: 190 , training loss: 4.426856
[INFO] Epoch: 11 , batch: 191 , training loss: 4.568697
[INFO] Epoch: 11 , batch: 192 , training loss: 4.376168
[INFO] Epoch: 11 , batch: 193 , training loss: 4.482415
[INFO] Epoch: 11 , batch: 194 , training loss: 4.407604
[INFO] Epoch: 11 , batch: 195 , training loss: 4.353630
[INFO] Epoch: 11 , batch: 196 , training loss: 4.195747
[INFO] Epoch: 11 , batch: 197 , training loss: 4.334861
[INFO] Epoch: 11 , batch: 198 , training loss: 4.244380
[INFO] Epoch: 11 , batch: 199 , training loss: 4.335160
[INFO] Epoch: 11 , batch: 200 , training loss: 4.248569
[INFO] Epoch: 11 , batch: 201 , training loss: 4.143164
[INFO] Epoch: 11 , batch: 202 , training loss: 4.160660
[INFO] Epoch: 11 , batch: 203 , training loss: 4.215883
[INFO] Epoch: 11 , batch: 204 , training loss: 4.356928
[INFO] Epoch: 11 , batch: 205 , training loss: 3.928074
[INFO] Epoch: 11 , batch: 206 , training loss: 3.874074
[INFO] Epoch: 11 , batch: 207 , training loss: 3.864383
[INFO] Epoch: 11 , batch: 208 , training loss: 4.230254
[INFO] Epoch: 11 , batch: 209 , training loss: 4.159829
[INFO] Epoch: 11 , batch: 210 , training loss: 4.196631
[INFO] Epoch: 11 , batch: 211 , training loss: 4.168233
[INFO] Epoch: 11 , batch: 212 , training loss: 4.295384
[INFO] Epoch: 11 , batch: 213 , training loss: 4.233942
[INFO] Epoch: 11 , batch: 214 , training loss: 4.324229
[INFO] Epoch: 11 , batch: 215 , training loss: 4.542074
[INFO] Epoch: 11 , batch: 216 , training loss: 4.241942
[INFO] Epoch: 11 , batch: 217 , training loss: 4.169789
[INFO] Epoch: 11 , batch: 218 , training loss: 4.145590
[INFO] Epoch: 11 , batch: 219 , training loss: 4.278591
[INFO] Epoch: 11 , batch: 220 , training loss: 4.088138
[INFO] Epoch: 11 , batch: 221 , training loss: 4.093585
[INFO] Epoch: 11 , batch: 222 , training loss: 4.245054
[INFO] Epoch: 11 , batch: 223 , training loss: 4.342479
[INFO] Epoch: 11 , batch: 224 , training loss: 4.368033
[INFO] Epoch: 11 , batch: 225 , training loss: 4.287155
[INFO] Epoch: 11 , batch: 226 , training loss: 4.411798
[INFO] Epoch: 11 , batch: 227 , training loss: 4.364920
[INFO] Epoch: 11 , batch: 228 , training loss: 4.409472
[INFO] Epoch: 11 , batch: 229 , training loss: 4.242643
[INFO] Epoch: 11 , batch: 230 , training loss: 4.113448
[INFO] Epoch: 11 , batch: 231 , training loss: 3.982130
[INFO] Epoch: 11 , batch: 232 , training loss: 4.143458
[INFO] Epoch: 11 , batch: 233 , training loss: 4.125853
[INFO] Epoch: 11 , batch: 234 , training loss: 3.824655
[INFO] Epoch: 11 , batch: 235 , training loss: 3.932012
[INFO] Epoch: 11 , batch: 236 , training loss: 4.078788
[INFO] Epoch: 11 , batch: 237 , training loss: 4.255562
[INFO] Epoch: 11 , batch: 238 , training loss: 3.994612
[INFO] Epoch: 11 , batch: 239 , training loss: 4.078896
[INFO] Epoch: 11 , batch: 240 , training loss: 4.132090
[INFO] Epoch: 11 , batch: 241 , training loss: 3.907155
[INFO] Epoch: 11 , batch: 242 , training loss: 3.926510
[INFO] Epoch: 11 , batch: 243 , training loss: 4.272324
[INFO] Epoch: 11 , batch: 244 , training loss: 4.181651
[INFO] Epoch: 11 , batch: 245 , training loss: 4.166963
[INFO] Epoch: 11 , batch: 246 , training loss: 3.843817
[INFO] Epoch: 11 , batch: 247 , training loss: 4.030142
[INFO] Epoch: 11 , batch: 248 , training loss: 4.095511
[INFO] Epoch: 11 , batch: 249 , training loss: 4.086927
[INFO] Epoch: 11 , batch: 250 , training loss: 3.855540
[INFO] Epoch: 11 , batch: 251 , training loss: 4.341083
[INFO] Epoch: 11 , batch: 252 , training loss: 4.031433
[INFO] Epoch: 11 , batch: 253 , training loss: 3.961682
[INFO] Epoch: 11 , batch: 254 , training loss: 4.264752
[INFO] Epoch: 11 , batch: 255 , training loss: 4.226308
[INFO] Epoch: 11 , batch: 256 , training loss: 4.232583
[INFO] Epoch: 11 , batch: 257 , training loss: 4.377897
[INFO] Epoch: 11 , batch: 258 , training loss: 4.429168
[INFO] Epoch: 11 , batch: 259 , training loss: 4.490809
[INFO] Epoch: 11 , batch: 260 , training loss: 4.173120
[INFO] Epoch: 11 , batch: 261 , training loss: 4.332574
[INFO] Epoch: 11 , batch: 262 , training loss: 4.555900
[INFO] Epoch: 11 , batch: 263 , training loss: 4.693149
[INFO] Epoch: 11 , batch: 264 , training loss: 4.008174
[INFO] Epoch: 11 , batch: 265 , training loss: 4.144208
[INFO] Epoch: 11 , batch: 266 , training loss: 4.602582
[INFO] Epoch: 11 , batch: 267 , training loss: 4.325213
[INFO] Epoch: 11 , batch: 268 , training loss: 4.225141
[INFO] Epoch: 11 , batch: 269 , training loss: 4.222182
[INFO] Epoch: 11 , batch: 270 , training loss: 4.231332
[INFO] Epoch: 11 , batch: 271 , training loss: 4.278813
[INFO] Epoch: 11 , batch: 272 , training loss: 4.241646
[INFO] Epoch: 11 , batch: 273 , training loss: 4.263760
[INFO] Epoch: 11 , batch: 274 , training loss: 4.358653
[INFO] Epoch: 11 , batch: 275 , training loss: 4.242470
[INFO] Epoch: 11 , batch: 276 , training loss: 4.304145
[INFO] Epoch: 11 , batch: 277 , training loss: 4.457894
[INFO] Epoch: 11 , batch: 278 , training loss: 4.090814
[INFO] Epoch: 11 , batch: 279 , training loss: 4.092003
[INFO] Epoch: 11 , batch: 280 , training loss: 4.047416
[INFO] Epoch: 11 , batch: 281 , training loss: 4.198072
[INFO] Epoch: 11 , batch: 282 , training loss: 4.108669
[INFO] Epoch: 11 , batch: 283 , training loss: 4.139110
[INFO] Epoch: 11 , batch: 284 , training loss: 4.160485
[INFO] Epoch: 11 , batch: 285 , training loss: 4.106731
[INFO] Epoch: 11 , batch: 286 , training loss: 4.105263
[INFO] Epoch: 11 , batch: 287 , training loss: 4.024549
[INFO] Epoch: 11 , batch: 288 , training loss: 4.063478
[INFO] Epoch: 11 , batch: 289 , training loss: 4.117219
[INFO] Epoch: 11 , batch: 290 , training loss: 3.874369
[INFO] Epoch: 11 , batch: 291 , training loss: 3.853148
[INFO] Epoch: 11 , batch: 292 , training loss: 3.963518
[INFO] Epoch: 11 , batch: 293 , training loss: 3.883554
[INFO] Epoch: 11 , batch: 294 , training loss: 4.576604
[INFO] Epoch: 11 , batch: 295 , training loss: 4.344464
[INFO] Epoch: 11 , batch: 296 , training loss: 4.265754
[INFO] Epoch: 11 , batch: 297 , training loss: 4.233345
[INFO] Epoch: 11 , batch: 298 , training loss: 4.058901
[INFO] Epoch: 11 , batch: 299 , training loss: 4.098809
[INFO] Epoch: 11 , batch: 300 , training loss: 4.052187
[INFO] Epoch: 11 , batch: 301 , training loss: 3.987475
[INFO] Epoch: 11 , batch: 302 , training loss: 4.156054
[INFO] Epoch: 11 , batch: 303 , training loss: 4.162631
[INFO] Epoch: 11 , batch: 304 , training loss: 4.346920
[INFO] Epoch: 11 , batch: 305 , training loss: 4.118375
[INFO] Epoch: 11 , batch: 306 , training loss: 4.252865
[INFO] Epoch: 11 , batch: 307 , training loss: 4.251834
[INFO] Epoch: 11 , batch: 308 , training loss: 4.097357
[INFO] Epoch: 11 , batch: 309 , training loss: 4.124757
[INFO] Epoch: 11 , batch: 310 , training loss: 3.969007
[INFO] Epoch: 11 , batch: 311 , training loss: 3.984323
[INFO] Epoch: 11 , batch: 312 , training loss: 3.886223
[INFO] Epoch: 11 , batch: 313 , training loss: 4.034590
[INFO] Epoch: 11 , batch: 314 , training loss: 4.074241
[INFO] Epoch: 11 , batch: 315 , training loss: 4.127268
[INFO] Epoch: 11 , batch: 316 , training loss: 4.420290
[INFO] Epoch: 11 , batch: 317 , training loss: 4.943607
[INFO] Epoch: 11 , batch: 318 , training loss: 5.052046
[INFO] Epoch: 11 , batch: 319 , training loss: 4.587400
[INFO] Epoch: 11 , batch: 320 , training loss: 4.135261
[INFO] Epoch: 11 , batch: 321 , training loss: 3.944281
[INFO] Epoch: 11 , batch: 322 , training loss: 4.065498
[INFO] Epoch: 11 , batch: 323 , training loss: 4.074225
[INFO] Epoch: 11 , batch: 324 , training loss: 4.040344
[INFO] Epoch: 11 , batch: 325 , training loss: 4.217770
[INFO] Epoch: 11 , batch: 326 , training loss: 4.267290
[INFO] Epoch: 11 , batch: 327 , training loss: 4.163705
[INFO] Epoch: 11 , batch: 328 , training loss: 4.147309
[INFO] Epoch: 11 , batch: 329 , training loss: 4.086499
[INFO] Epoch: 11 , batch: 330 , training loss: 4.059409
[INFO] Epoch: 11 , batch: 331 , training loss: 4.204369
[INFO] Epoch: 11 , batch: 332 , training loss: 4.018585
[INFO] Epoch: 11 , batch: 333 , training loss: 4.011921
[INFO] Epoch: 11 , batch: 334 , training loss: 4.070266
[INFO] Epoch: 11 , batch: 335 , training loss: 4.190322
[INFO] Epoch: 11 , batch: 336 , training loss: 4.198545
[INFO] Epoch: 11 , batch: 337 , training loss: 4.221121
[INFO] Epoch: 11 , batch: 338 , training loss: 4.439053
[INFO] Epoch: 11 , batch: 339 , training loss: 4.277505
[INFO] Epoch: 11 , batch: 340 , training loss: 4.451101
[INFO] Epoch: 11 , batch: 341 , training loss: 4.218429
[INFO] Epoch: 11 , batch: 342 , training loss: 3.993018
[INFO] Epoch: 11 , batch: 343 , training loss: 4.078129
[INFO] Epoch: 11 , batch: 344 , training loss: 3.922765
[INFO] Epoch: 11 , batch: 345 , training loss: 4.050308
[INFO] Epoch: 11 , batch: 346 , training loss: 4.129386
[INFO] Epoch: 11 , batch: 347 , training loss: 4.001640
[INFO] Epoch: 11 , batch: 348 , training loss: 4.183796
[INFO] Epoch: 11 , batch: 349 , training loss: 4.291801
[INFO] Epoch: 11 , batch: 350 , training loss: 4.048562
[INFO] Epoch: 11 , batch: 351 , training loss: 4.111374
[INFO] Epoch: 11 , batch: 352 , training loss: 4.146371
[INFO] Epoch: 11 , batch: 353 , training loss: 4.107882
[INFO] Epoch: 11 , batch: 354 , training loss: 4.222629
[INFO] Epoch: 11 , batch: 355 , training loss: 4.262813
[INFO] Epoch: 11 , batch: 356 , training loss: 4.082473
[INFO] Epoch: 11 , batch: 357 , training loss: 4.188292
[INFO] Epoch: 11 , batch: 358 , training loss: 4.112675
[INFO] Epoch: 11 , batch: 359 , training loss: 4.076059
[INFO] Epoch: 11 , batch: 360 , training loss: 4.128755
[INFO] Epoch: 11 , batch: 361 , training loss: 4.135807
[INFO] Epoch: 11 , batch: 362 , training loss: 4.220606
[INFO] Epoch: 11 , batch: 363 , training loss: 4.115470
[INFO] Epoch: 11 , batch: 364 , training loss: 4.138558
[INFO] Epoch: 11 , batch: 365 , training loss: 4.075725
[INFO] Epoch: 11 , batch: 366 , training loss: 4.208310
[INFO] Epoch: 11 , batch: 367 , training loss: 4.299345
[INFO] Epoch: 11 , batch: 368 , training loss: 4.801350
[INFO] Epoch: 11 , batch: 369 , training loss: 4.390404
[INFO] Epoch: 11 , batch: 370 , training loss: 4.152339
[INFO] Epoch: 11 , batch: 371 , training loss: 4.602781
[INFO] Epoch: 11 , batch: 372 , training loss: 4.924060
[INFO] Epoch: 11 , batch: 373 , training loss: 4.966754
[INFO] Epoch: 11 , batch: 374 , training loss: 5.020334
[INFO] Epoch: 11 , batch: 375 , training loss: 5.023518
[INFO] Epoch: 11 , batch: 376 , training loss: 4.911485
[INFO] Epoch: 11 , batch: 377 , training loss: 4.596345
[INFO] Epoch: 11 , batch: 378 , training loss: 4.703390
[INFO] Epoch: 11 , batch: 379 , training loss: 4.697636
[INFO] Epoch: 11 , batch: 380 , training loss: 4.831273
[INFO] Epoch: 11 , batch: 381 , training loss: 4.594583
[INFO] Epoch: 11 , batch: 382 , training loss: 4.826925
[INFO] Epoch: 11 , batch: 383 , training loss: 4.874024
[INFO] Epoch: 11 , batch: 384 , training loss: 4.915003
[INFO] Epoch: 11 , batch: 385 , training loss: 4.646937
[INFO] Epoch: 11 , batch: 386 , training loss: 4.825895
[INFO] Epoch: 11 , batch: 387 , training loss: 4.771921
[INFO] Epoch: 11 , batch: 388 , training loss: 4.578590
[INFO] Epoch: 11 , batch: 389 , training loss: 4.414738
[INFO] Epoch: 11 , batch: 390 , training loss: 4.402072
[INFO] Epoch: 11 , batch: 391 , training loss: 4.453686
[INFO] Epoch: 11 , batch: 392 , training loss: 4.801369
[INFO] Epoch: 11 , batch: 393 , training loss: 4.686755
[INFO] Epoch: 11 , batch: 394 , training loss: 4.729448
[INFO] Epoch: 11 , batch: 395 , training loss: 4.573935
[INFO] Epoch: 11 , batch: 396 , training loss: 4.359904
[INFO] Epoch: 11 , batch: 397 , training loss: 4.536759
[INFO] Epoch: 11 , batch: 398 , training loss: 4.366496
[INFO] Epoch: 11 , batch: 399 , training loss: 4.420389
[INFO] Epoch: 11 , batch: 400 , training loss: 4.379345
[INFO] Epoch: 11 , batch: 401 , training loss: 4.798675
[INFO] Epoch: 11 , batch: 402 , training loss: 4.545353
[INFO] Epoch: 11 , batch: 403 , training loss: 4.368883
[INFO] Epoch: 11 , batch: 404 , training loss: 4.547500
[INFO] Epoch: 11 , batch: 405 , training loss: 4.572442
[INFO] Epoch: 11 , batch: 406 , training loss: 4.485916
[INFO] Epoch: 11 , batch: 407 , training loss: 4.552989
[INFO] Epoch: 11 , batch: 408 , training loss: 4.488803
[INFO] Epoch: 11 , batch: 409 , training loss: 4.503581
[INFO] Epoch: 11 , batch: 410 , training loss: 4.527790
[INFO] Epoch: 11 , batch: 411 , training loss: 4.746542
[INFO] Epoch: 11 , batch: 412 , training loss: 4.565955
[INFO] Epoch: 11 , batch: 413 , training loss: 4.446141
[INFO] Epoch: 11 , batch: 414 , training loss: 4.471339
[INFO] Epoch: 11 , batch: 415 , training loss: 4.495746
[INFO] Epoch: 11 , batch: 416 , training loss: 4.563975
[INFO] Epoch: 11 , batch: 417 , training loss: 4.492929
[INFO] Epoch: 11 , batch: 418 , training loss: 4.514978
[INFO] Epoch: 11 , batch: 419 , training loss: 4.479224
[INFO] Epoch: 11 , batch: 420 , training loss: 4.458015
[INFO] Epoch: 11 , batch: 421 , training loss: 4.446776
[INFO] Epoch: 11 , batch: 422 , training loss: 4.323838
[INFO] Epoch: 11 , batch: 423 , training loss: 4.550166
[INFO] Epoch: 11 , batch: 424 , training loss: 4.704864
[INFO] Epoch: 11 , batch: 425 , training loss: 4.566780
[INFO] Epoch: 11 , batch: 426 , training loss: 4.295114
[INFO] Epoch: 11 , batch: 427 , training loss: 4.528564
[INFO] Epoch: 11 , batch: 428 , training loss: 4.438745
[INFO] Epoch: 11 , batch: 429 , training loss: 4.269283
[INFO] Epoch: 11 , batch: 430 , training loss: 4.536980
[INFO] Epoch: 11 , batch: 431 , training loss: 4.116608
[INFO] Epoch: 11 , batch: 432 , training loss: 4.198143
[INFO] Epoch: 11 , batch: 433 , training loss: 4.226846
[INFO] Epoch: 11 , batch: 434 , training loss: 4.077550
[INFO] Epoch: 11 , batch: 435 , training loss: 4.458302
[INFO] Epoch: 11 , batch: 436 , training loss: 4.551676
[INFO] Epoch: 11 , batch: 437 , training loss: 4.293383
[INFO] Epoch: 11 , batch: 438 , training loss: 4.124173
[INFO] Epoch: 11 , batch: 439 , training loss: 4.372974
[INFO] Epoch: 11 , batch: 440 , training loss: 4.515204
[INFO] Epoch: 11 , batch: 441 , training loss: 4.595912
[INFO] Epoch: 11 , batch: 442 , training loss: 4.376719
[INFO] Epoch: 11 , batch: 443 , training loss: 4.559855
[INFO] Epoch: 11 , batch: 444 , training loss: 4.159305
[INFO] Epoch: 11 , batch: 445 , training loss: 4.074809
[INFO] Epoch: 11 , batch: 446 , training loss: 3.976902
[INFO] Epoch: 11 , batch: 447 , training loss: 4.189582
[INFO] Epoch: 11 , batch: 448 , training loss: 4.318771
[INFO] Epoch: 11 , batch: 449 , training loss: 4.754182
[INFO] Epoch: 11 , batch: 450 , training loss: 4.771080
[INFO] Epoch: 11 , batch: 451 , training loss: 4.672121
[INFO] Epoch: 11 , batch: 452 , training loss: 4.460356
[INFO] Epoch: 11 , batch: 453 , training loss: 4.238809
[INFO] Epoch: 11 , batch: 454 , training loss: 4.380439
[INFO] Epoch: 11 , batch: 455 , training loss: 4.418253
[INFO] Epoch: 11 , batch: 456 , training loss: 4.402781
[INFO] Epoch: 11 , batch: 457 , training loss: 4.536831
[INFO] Epoch: 11 , batch: 458 , training loss: 4.246539
[INFO] Epoch: 11 , batch: 459 , training loss: 4.221947
[INFO] Epoch: 11 , batch: 460 , training loss: 4.353886
[INFO] Epoch: 11 , batch: 461 , training loss: 4.326465
[INFO] Epoch: 11 , batch: 462 , training loss: 4.381442
[INFO] Epoch: 11 , batch: 463 , training loss: 4.260475
[INFO] Epoch: 11 , batch: 464 , training loss: 4.481468
[INFO] Epoch: 11 , batch: 465 , training loss: 4.404732
[INFO] Epoch: 11 , batch: 466 , training loss: 4.475687
[INFO] Epoch: 11 , batch: 467 , training loss: 4.469633
[INFO] Epoch: 11 , batch: 468 , training loss: 4.431427
[INFO] Epoch: 11 , batch: 469 , training loss: 4.481129
[INFO] Epoch: 11 , batch: 470 , training loss: 4.263923
[INFO] Epoch: 11 , batch: 471 , training loss: 4.346467
[INFO] Epoch: 11 , batch: 472 , training loss: 4.405832
[INFO] Epoch: 11 , batch: 473 , training loss: 4.323507
[INFO] Epoch: 11 , batch: 474 , training loss: 4.132071
[INFO] Epoch: 11 , batch: 475 , training loss: 3.984241
[INFO] Epoch: 11 , batch: 476 , training loss: 4.393105
[INFO] Epoch: 11 , batch: 477 , training loss: 4.492419
[INFO] Epoch: 11 , batch: 478 , training loss: 4.562205
[INFO] Epoch: 11 , batch: 479 , training loss: 4.512980
[INFO] Epoch: 11 , batch: 480 , training loss: 4.640686
[INFO] Epoch: 11 , batch: 481 , training loss: 4.482099
[INFO] Epoch: 11 , batch: 482 , training loss: 4.618807
[INFO] Epoch: 11 , batch: 483 , training loss: 4.460326
[INFO] Epoch: 11 , batch: 484 , training loss: 4.272002
[INFO] Epoch: 11 , batch: 485 , training loss: 4.361854
[INFO] Epoch: 11 , batch: 486 , training loss: 4.231087
[INFO] Epoch: 11 , batch: 487 , training loss: 4.238026
[INFO] Epoch: 11 , batch: 488 , training loss: 4.438624
[INFO] Epoch: 11 , batch: 489 , training loss: 4.287759
[INFO] Epoch: 11 , batch: 490 , training loss: 4.356763
[INFO] Epoch: 11 , batch: 491 , training loss: 4.342371
[INFO] Epoch: 11 , batch: 492 , training loss: 4.254167
[INFO] Epoch: 11 , batch: 493 , training loss: 4.454669
[INFO] Epoch: 11 , batch: 494 , training loss: 4.375644
[INFO] Epoch: 11 , batch: 495 , training loss: 4.493169
[INFO] Epoch: 11 , batch: 496 , training loss: 4.367896
[INFO] Epoch: 11 , batch: 497 , training loss: 4.395994
[INFO] Epoch: 11 , batch: 498 , training loss: 4.418409
[INFO] Epoch: 11 , batch: 499 , training loss: 4.489579
[INFO] Epoch: 11 , batch: 500 , training loss: 4.664002
[INFO] Epoch: 11 , batch: 501 , training loss: 4.997288
[INFO] Epoch: 11 , batch: 502 , training loss: 5.139568
[INFO] Epoch: 11 , batch: 503 , training loss: 4.854287
[INFO] Epoch: 11 , batch: 504 , training loss: 4.958855
[INFO] Epoch: 11 , batch: 505 , training loss: 4.909189
[INFO] Epoch: 11 , batch: 506 , training loss: 4.831136
[INFO] Epoch: 11 , batch: 507 , training loss: 4.902805
[INFO] Epoch: 11 , batch: 508 , training loss: 4.853609
[INFO] Epoch: 11 , batch: 509 , training loss: 4.586472
[INFO] Epoch: 11 , batch: 510 , training loss: 4.643245
[INFO] Epoch: 11 , batch: 511 , training loss: 4.546395
[INFO] Epoch: 11 , batch: 512 , training loss: 4.646033
[INFO] Epoch: 11 , batch: 513 , training loss: 4.900753
[INFO] Epoch: 11 , batch: 514 , training loss: 4.547857
[INFO] Epoch: 11 , batch: 515 , training loss: 4.784286
[INFO] Epoch: 11 , batch: 516 , training loss: 4.590542
[INFO] Epoch: 11 , batch: 517 , training loss: 4.563339
[INFO] Epoch: 11 , batch: 518 , training loss: 4.530839
[INFO] Epoch: 11 , batch: 519 , training loss: 4.388633
[INFO] Epoch: 11 , batch: 520 , training loss: 4.621380
[INFO] Epoch: 11 , batch: 521 , training loss: 4.602554
[INFO] Epoch: 11 , batch: 522 , training loss: 4.655937
[INFO] Epoch: 11 , batch: 523 , training loss: 4.568187
[INFO] Epoch: 11 , batch: 524 , training loss: 4.843456
[INFO] Epoch: 11 , batch: 525 , training loss: 4.762911
[INFO] Epoch: 11 , batch: 526 , training loss: 4.528096
[INFO] Epoch: 11 , batch: 527 , training loss: 4.547719
[INFO] Epoch: 11 , batch: 528 , training loss: 4.616406
[INFO] Epoch: 11 , batch: 529 , training loss: 4.570467
[INFO] Epoch: 11 , batch: 530 , training loss: 4.413167
[INFO] Epoch: 11 , batch: 531 , training loss: 4.559274
[INFO] Epoch: 11 , batch: 532 , training loss: 4.443161
[INFO] Epoch: 11 , batch: 533 , training loss: 4.624550
[INFO] Epoch: 11 , batch: 534 , training loss: 4.589790
[INFO] Epoch: 11 , batch: 535 , training loss: 4.604260
[INFO] Epoch: 11 , batch: 536 , training loss: 4.429502
[INFO] Epoch: 11 , batch: 537 , training loss: 4.435040
[INFO] Epoch: 11 , batch: 538 , training loss: 4.498346
[INFO] Epoch: 11 , batch: 539 , training loss: 4.624014
[INFO] Epoch: 11 , batch: 540 , training loss: 5.183915
[INFO] Epoch: 11 , batch: 541 , training loss: 5.111029
[INFO] Epoch: 11 , batch: 542 , training loss: 4.956791
[INFO] Epoch: 12 , batch: 0 , training loss: 4.016004
[INFO] Epoch: 12 , batch: 1 , training loss: 3.945616
[INFO] Epoch: 12 , batch: 2 , training loss: 3.905537
[INFO] Epoch: 12 , batch: 3 , training loss: 3.831384
[INFO] Epoch: 12 , batch: 4 , training loss: 4.183751
[INFO] Epoch: 12 , batch: 5 , training loss: 3.767931
[INFO] Epoch: 12 , batch: 6 , training loss: 4.244619
[INFO] Epoch: 12 , batch: 7 , training loss: 4.013704
[INFO] Epoch: 12 , batch: 8 , training loss: 3.757158
[INFO] Epoch: 12 , batch: 9 , training loss: 3.908840
[INFO] Epoch: 12 , batch: 10 , training loss: 3.877870
[INFO] Epoch: 12 , batch: 11 , training loss: 3.828809
[INFO] Epoch: 12 , batch: 12 , training loss: 3.782933
[INFO] Epoch: 12 , batch: 13 , training loss: 3.796694
[INFO] Epoch: 12 , batch: 14 , training loss: 3.597506
[INFO] Epoch: 12 , batch: 15 , training loss: 3.862543
[INFO] Epoch: 12 , batch: 16 , training loss: 3.692725
[INFO] Epoch: 12 , batch: 17 , training loss: 3.883817
[INFO] Epoch: 12 , batch: 18 , training loss: 3.851681
[INFO] Epoch: 12 , batch: 19 , training loss: 3.544713
[INFO] Epoch: 12 , batch: 20 , training loss: 3.543053
[INFO] Epoch: 12 , batch: 21 , training loss: 3.665956
[INFO] Epoch: 12 , batch: 22 , training loss: 3.648855
[INFO] Epoch: 12 , batch: 23 , training loss: 3.811861
[INFO] Epoch: 12 , batch: 24 , training loss: 3.637681
[INFO] Epoch: 12 , batch: 25 , training loss: 3.852468
[INFO] Epoch: 12 , batch: 26 , training loss: 3.641228
[INFO] Epoch: 12 , batch: 27 , training loss: 3.602031
[INFO] Epoch: 12 , batch: 28 , training loss: 3.807431
[INFO] Epoch: 12 , batch: 29 , training loss: 3.613228
[INFO] Epoch: 12 , batch: 30 , training loss: 3.653008
[INFO] Epoch: 12 , batch: 31 , training loss: 3.722310
[INFO] Epoch: 12 , batch: 32 , training loss: 3.731058
[INFO] Epoch: 12 , batch: 33 , training loss: 3.776661
[INFO] Epoch: 12 , batch: 34 , training loss: 3.759500
[INFO] Epoch: 12 , batch: 35 , training loss: 3.725378
[INFO] Epoch: 12 , batch: 36 , training loss: 3.755737
[INFO] Epoch: 12 , batch: 37 , training loss: 3.661665
[INFO] Epoch: 12 , batch: 38 , training loss: 3.719885
[INFO] Epoch: 12 , batch: 39 , training loss: 3.571605
[INFO] Epoch: 12 , batch: 40 , training loss: 3.752814
[INFO] Epoch: 12 , batch: 41 , training loss: 3.735212
[INFO] Epoch: 12 , batch: 42 , training loss: 4.224953
[INFO] Epoch: 12 , batch: 43 , training loss: 3.924306
[INFO] Epoch: 12 , batch: 44 , training loss: 4.193664
[INFO] Epoch: 12 , batch: 45 , training loss: 4.211324
[INFO] Epoch: 12 , batch: 46 , training loss: 4.267003
[INFO] Epoch: 12 , batch: 47 , training loss: 3.895156
[INFO] Epoch: 12 , batch: 48 , training loss: 3.899251
[INFO] Epoch: 12 , batch: 49 , training loss: 4.033835
[INFO] Epoch: 12 , batch: 50 , training loss: 3.801010
[INFO] Epoch: 12 , batch: 51 , training loss: 3.952289
[INFO] Epoch: 12 , batch: 52 , training loss: 3.788464
[INFO] Epoch: 12 , batch: 53 , training loss: 3.934078
[INFO] Epoch: 12 , batch: 54 , training loss: 3.969707
[INFO] Epoch: 12 , batch: 55 , training loss: 4.010837
[INFO] Epoch: 12 , batch: 56 , training loss: 3.913068
[INFO] Epoch: 12 , batch: 57 , training loss: 3.785284
[INFO] Epoch: 12 , batch: 58 , training loss: 3.859787
[INFO] Epoch: 12 , batch: 59 , training loss: 3.929914
[INFO] Epoch: 12 , batch: 60 , training loss: 3.875001
[INFO] Epoch: 12 , batch: 61 , training loss: 3.950524
[INFO] Epoch: 12 , batch: 62 , training loss: 3.824771
[INFO] Epoch: 12 , batch: 63 , training loss: 4.005581
[INFO] Epoch: 12 , batch: 64 , training loss: 4.191501
[INFO] Epoch: 12 , batch: 65 , training loss: 3.869075
[INFO] Epoch: 12 , batch: 66 , training loss: 3.735202
[INFO] Epoch: 12 , batch: 67 , training loss: 3.794850
[INFO] Epoch: 12 , batch: 68 , training loss: 3.988340
[INFO] Epoch: 12 , batch: 69 , training loss: 3.861603
[INFO] Epoch: 12 , batch: 70 , training loss: 4.110694
[INFO] Epoch: 12 , batch: 71 , training loss: 3.952797
[INFO] Epoch: 12 , batch: 72 , training loss: 3.965424
[INFO] Epoch: 12 , batch: 73 , training loss: 3.912669
[INFO] Epoch: 12 , batch: 74 , training loss: 4.062932
[INFO] Epoch: 12 , batch: 75 , training loss: 3.856604
[INFO] Epoch: 12 , batch: 76 , training loss: 3.988448
[INFO] Epoch: 12 , batch: 77 , training loss: 3.948964
[INFO] Epoch: 12 , batch: 78 , training loss: 4.034239
[INFO] Epoch: 12 , batch: 79 , training loss: 3.852883
[INFO] Epoch: 12 , batch: 80 , training loss: 4.095826
[INFO] Epoch: 12 , batch: 81 , training loss: 4.017117
[INFO] Epoch: 12 , batch: 82 , training loss: 3.991213
[INFO] Epoch: 12 , batch: 83 , training loss: 4.107680
[INFO] Epoch: 12 , batch: 84 , training loss: 4.025589
[INFO] Epoch: 12 , batch: 85 , training loss: 4.073444
[INFO] Epoch: 12 , batch: 86 , training loss: 4.052158
[INFO] Epoch: 12 , batch: 87 , training loss: 4.044468
[INFO] Epoch: 12 , batch: 88 , training loss: 4.177674
[INFO] Epoch: 12 , batch: 89 , training loss: 3.967044
[INFO] Epoch: 12 , batch: 90 , training loss: 4.017406
[INFO] Epoch: 12 , batch: 91 , training loss: 3.957874
[INFO] Epoch: 12 , batch: 92 , training loss: 3.950488
[INFO] Epoch: 12 , batch: 93 , training loss: 4.058985
[INFO] Epoch: 12 , batch: 94 , training loss: 4.245310
[INFO] Epoch: 12 , batch: 95 , training loss: 4.015785
[INFO] Epoch: 12 , batch: 96 , training loss: 3.988959
[INFO] Epoch: 12 , batch: 97 , training loss: 3.917984
[INFO] Epoch: 12 , batch: 98 , training loss: 3.879905
[INFO] Epoch: 12 , batch: 99 , training loss: 3.939917
[INFO] Epoch: 12 , batch: 100 , training loss: 3.868520
[INFO] Epoch: 12 , batch: 101 , training loss: 3.906949
[INFO] Epoch: 12 , batch: 102 , training loss: 4.027151
[INFO] Epoch: 12 , batch: 103 , training loss: 3.826994
[INFO] Epoch: 12 , batch: 104 , training loss: 3.794230
[INFO] Epoch: 12 , batch: 105 , training loss: 4.081025
[INFO] Epoch: 12 , batch: 106 , training loss: 4.042210
[INFO] Epoch: 12 , batch: 107 , training loss: 3.929678
[INFO] Epoch: 12 , batch: 108 , training loss: 3.841155
[INFO] Epoch: 12 , batch: 109 , training loss: 3.741723
[INFO] Epoch: 12 , batch: 110 , training loss: 3.941241
[INFO] Epoch: 12 , batch: 111 , training loss: 4.017304
[INFO] Epoch: 12 , batch: 112 , training loss: 3.931900
[INFO] Epoch: 12 , batch: 113 , training loss: 3.933876
[INFO] Epoch: 12 , batch: 114 , training loss: 4.008866
[INFO] Epoch: 12 , batch: 115 , training loss: 3.963434
[INFO] Epoch: 12 , batch: 116 , training loss: 3.883970
[INFO] Epoch: 12 , batch: 117 , training loss: 4.106201
[INFO] Epoch: 12 , batch: 118 , training loss: 4.059947
[INFO] Epoch: 12 , batch: 119 , training loss: 4.216769
[INFO] Epoch: 12 , batch: 120 , training loss: 4.203387
[INFO] Epoch: 12 , batch: 121 , training loss: 4.006835
[INFO] Epoch: 12 , batch: 122 , training loss: 3.920177
[INFO] Epoch: 12 , batch: 123 , training loss: 3.983215
[INFO] Epoch: 12 , batch: 124 , training loss: 4.122475
[INFO] Epoch: 12 , batch: 125 , training loss: 3.863820
[INFO] Epoch: 12 , batch: 126 , training loss: 3.873217
[INFO] Epoch: 12 , batch: 127 , training loss: 3.900873
[INFO] Epoch: 12 , batch: 128 , training loss: 4.045144
[INFO] Epoch: 12 , batch: 129 , training loss: 4.009752
[INFO] Epoch: 12 , batch: 130 , training loss: 3.991293
[INFO] Epoch: 12 , batch: 131 , training loss: 3.985764
[INFO] Epoch: 12 , batch: 132 , training loss: 4.006980
[INFO] Epoch: 12 , batch: 133 , training loss: 3.958879
[INFO] Epoch: 12 , batch: 134 , training loss: 3.773623
[INFO] Epoch: 12 , batch: 135 , training loss: 3.804925
[INFO] Epoch: 12 , batch: 136 , training loss: 4.106600
[INFO] Epoch: 12 , batch: 137 , training loss: 3.976982
[INFO] Epoch: 12 , batch: 138 , training loss: 4.087206
[INFO] Epoch: 12 , batch: 139 , training loss: 4.740709
[INFO] Epoch: 12 , batch: 140 , training loss: 4.480031
[INFO] Epoch: 12 , batch: 141 , training loss: 4.222517
[INFO] Epoch: 12 , batch: 142 , training loss: 3.952201
[INFO] Epoch: 12 , batch: 143 , training loss: 4.065115
[INFO] Epoch: 12 , batch: 144 , training loss: 3.856630
[INFO] Epoch: 12 , batch: 145 , training loss: 3.971945
[INFO] Epoch: 12 , batch: 146 , training loss: 4.173295
[INFO] Epoch: 12 , batch: 147 , training loss: 3.850432
[INFO] Epoch: 12 , batch: 148 , training loss: 3.810772
[INFO] Epoch: 12 , batch: 149 , training loss: 3.896441
[INFO] Epoch: 12 , batch: 150 , training loss: 4.125581
[INFO] Epoch: 12 , batch: 151 , training loss: 3.964480
[INFO] Epoch: 12 , batch: 152 , training loss: 4.004009
[INFO] Epoch: 12 , batch: 153 , training loss: 4.048943
[INFO] Epoch: 12 , batch: 154 , training loss: 4.135902
[INFO] Epoch: 12 , batch: 155 , training loss: 4.376856
[INFO] Epoch: 12 , batch: 156 , training loss: 4.052135
[INFO] Epoch: 12 , batch: 157 , training loss: 4.002223
[INFO] Epoch: 12 , batch: 158 , training loss: 4.231877
[INFO] Epoch: 12 , batch: 159 , training loss: 4.096025
[INFO] Epoch: 12 , batch: 160 , training loss: 4.471261
[INFO] Epoch: 12 , batch: 161 , training loss: 4.511750
[INFO] Epoch: 12 , batch: 162 , training loss: 4.473747
[INFO] Epoch: 12 , batch: 163 , training loss: 4.557939
[INFO] Epoch: 12 , batch: 164 , training loss: 4.534270
[INFO] Epoch: 12 , batch: 165 , training loss: 4.431703
[INFO] Epoch: 12 , batch: 166 , training loss: 4.403241
[INFO] Epoch: 12 , batch: 167 , training loss: 4.628077
[INFO] Epoch: 12 , batch: 168 , training loss: 4.367956
[INFO] Epoch: 12 , batch: 169 , training loss: 4.312771
[INFO] Epoch: 12 , batch: 170 , training loss: 4.402509
[INFO] Epoch: 12 , batch: 171 , training loss: 3.835079
[INFO] Epoch: 12 , batch: 172 , training loss: 4.051540
[INFO] Epoch: 12 , batch: 173 , training loss: 4.373268
[INFO] Epoch: 12 , batch: 174 , training loss: 4.671177
[INFO] Epoch: 12 , batch: 175 , training loss: 5.020417
[INFO] Epoch: 12 , batch: 176 , training loss: 4.749577
[INFO] Epoch: 12 , batch: 177 , training loss: 4.339560
[INFO] Epoch: 12 , batch: 178 , training loss: 4.338349
[INFO] Epoch: 12 , batch: 179 , training loss: 4.348255
[INFO] Epoch: 12 , batch: 180 , training loss: 4.288641
[INFO] Epoch: 12 , batch: 181 , training loss: 4.546100
[INFO] Epoch: 12 , batch: 182 , training loss: 4.462556
[INFO] Epoch: 12 , batch: 183 , training loss: 4.473907
[INFO] Epoch: 12 , batch: 184 , training loss: 4.332763
[INFO] Epoch: 12 , batch: 185 , training loss: 4.281198
[INFO] Epoch: 12 , batch: 186 , training loss: 4.438891
[INFO] Epoch: 12 , batch: 187 , training loss: 4.580931
[INFO] Epoch: 12 , batch: 188 , training loss: 4.539932
[INFO] Epoch: 12 , batch: 189 , training loss: 4.429891
[INFO] Epoch: 12 , batch: 190 , training loss: 4.422229
[INFO] Epoch: 12 , batch: 191 , training loss: 4.553441
[INFO] Epoch: 12 , batch: 192 , training loss: 4.357111
[INFO] Epoch: 12 , batch: 193 , training loss: 4.482948
[INFO] Epoch: 12 , batch: 194 , training loss: 4.390477
[INFO] Epoch: 12 , batch: 195 , training loss: 4.340292
[INFO] Epoch: 12 , batch: 196 , training loss: 4.182722
[INFO] Epoch: 12 , batch: 197 , training loss: 4.313574
[INFO] Epoch: 12 , batch: 198 , training loss: 4.217786
[INFO] Epoch: 12 , batch: 199 , training loss: 4.323896
[INFO] Epoch: 12 , batch: 200 , training loss: 4.238901
[INFO] Epoch: 12 , batch: 201 , training loss: 4.140446
[INFO] Epoch: 12 , batch: 202 , training loss: 4.142186
[INFO] Epoch: 12 , batch: 203 , training loss: 4.204023
[INFO] Epoch: 12 , batch: 204 , training loss: 4.367062
[INFO] Epoch: 12 , batch: 205 , training loss: 3.907865
[INFO] Epoch: 12 , batch: 206 , training loss: 3.866424
[INFO] Epoch: 12 , batch: 207 , training loss: 3.873827
[INFO] Epoch: 12 , batch: 208 , training loss: 4.217005
[INFO] Epoch: 12 , batch: 209 , training loss: 4.151976
[INFO] Epoch: 12 , batch: 210 , training loss: 4.172328
[INFO] Epoch: 12 , batch: 211 , training loss: 4.149238
[INFO] Epoch: 12 , batch: 212 , training loss: 4.286864
[INFO] Epoch: 12 , batch: 213 , training loss: 4.224919
[INFO] Epoch: 12 , batch: 214 , training loss: 4.317241
[INFO] Epoch: 12 , batch: 215 , training loss: 4.528055
[INFO] Epoch: 12 , batch: 216 , training loss: 4.237945
[INFO] Epoch: 12 , batch: 217 , training loss: 4.185863
[INFO] Epoch: 12 , batch: 218 , training loss: 4.160310
[INFO] Epoch: 12 , batch: 219 , training loss: 4.273886
[INFO] Epoch: 12 , batch: 220 , training loss: 4.081520
[INFO] Epoch: 12 , batch: 221 , training loss: 4.091053
[INFO] Epoch: 12 , batch: 222 , training loss: 4.231474
[INFO] Epoch: 12 , batch: 223 , training loss: 4.330398
[INFO] Epoch: 12 , batch: 224 , training loss: 4.363992
[INFO] Epoch: 12 , batch: 225 , training loss: 4.264031
[INFO] Epoch: 12 , batch: 226 , training loss: 4.390227
[INFO] Epoch: 12 , batch: 227 , training loss: 4.358827
[INFO] Epoch: 12 , batch: 228 , training loss: 4.400969
[INFO] Epoch: 12 , batch: 229 , training loss: 4.250745
[INFO] Epoch: 12 , batch: 230 , training loss: 4.100003
[INFO] Epoch: 12 , batch: 231 , training loss: 3.963713
[INFO] Epoch: 12 , batch: 232 , training loss: 4.130626
[INFO] Epoch: 12 , batch: 233 , training loss: 4.120104
[INFO] Epoch: 12 , batch: 234 , training loss: 3.801693
[INFO] Epoch: 12 , batch: 235 , training loss: 3.911335
[INFO] Epoch: 12 , batch: 236 , training loss: 4.072758
[INFO] Epoch: 12 , batch: 237 , training loss: 4.246447
[INFO] Epoch: 12 , batch: 238 , training loss: 3.989646
[INFO] Epoch: 12 , batch: 239 , training loss: 4.077259
[INFO] Epoch: 12 , batch: 240 , training loss: 4.127285
[INFO] Epoch: 12 , batch: 241 , training loss: 3.917569
[INFO] Epoch: 12 , batch: 242 , training loss: 3.910465
[INFO] Epoch: 12 , batch: 243 , training loss: 4.268500
[INFO] Epoch: 12 , batch: 244 , training loss: 4.168981
[INFO] Epoch: 12 , batch: 245 , training loss: 4.154611
[INFO] Epoch: 12 , batch: 246 , training loss: 3.833859
[INFO] Epoch: 12 , batch: 247 , training loss: 4.029623
[INFO] Epoch: 12 , batch: 248 , training loss: 4.084161
[INFO] Epoch: 12 , batch: 249 , training loss: 4.076307
[INFO] Epoch: 12 , batch: 250 , training loss: 3.830632
[INFO] Epoch: 12 , batch: 251 , training loss: 4.316852
[INFO] Epoch: 12 , batch: 252 , training loss: 4.022738
[INFO] Epoch: 12 , batch: 253 , training loss: 3.938942
[INFO] Epoch: 12 , batch: 254 , training loss: 4.235643
[INFO] Epoch: 12 , batch: 255 , training loss: 4.223002
[INFO] Epoch: 12 , batch: 256 , training loss: 4.223844
[INFO] Epoch: 12 , batch: 257 , training loss: 4.362847
[INFO] Epoch: 12 , batch: 258 , training loss: 4.406985
[INFO] Epoch: 12 , batch: 259 , training loss: 4.476545
[INFO] Epoch: 12 , batch: 260 , training loss: 4.166379
[INFO] Epoch: 12 , batch: 261 , training loss: 4.327271
[INFO] Epoch: 12 , batch: 262 , training loss: 4.541049
[INFO] Epoch: 12 , batch: 263 , training loss: 4.672309
[INFO] Epoch: 12 , batch: 264 , training loss: 4.005373
[INFO] Epoch: 12 , batch: 265 , training loss: 4.134385
[INFO] Epoch: 12 , batch: 266 , training loss: 4.576808
[INFO] Epoch: 12 , batch: 267 , training loss: 4.300304
[INFO] Epoch: 12 , batch: 268 , training loss: 4.228008
[INFO] Epoch: 12 , batch: 269 , training loss: 4.198262
[INFO] Epoch: 12 , batch: 270 , training loss: 4.216811
[INFO] Epoch: 12 , batch: 271 , training loss: 4.265962
[INFO] Epoch: 12 , batch: 272 , training loss: 4.244660
[INFO] Epoch: 12 , batch: 273 , training loss: 4.252275
[INFO] Epoch: 12 , batch: 274 , training loss: 4.352465
[INFO] Epoch: 12 , batch: 275 , training loss: 4.231045
[INFO] Epoch: 12 , batch: 276 , training loss: 4.288654
[INFO] Epoch: 12 , batch: 277 , training loss: 4.447137
[INFO] Epoch: 12 , batch: 278 , training loss: 4.078243
[INFO] Epoch: 12 , batch: 279 , training loss: 4.088205
[INFO] Epoch: 12 , batch: 280 , training loss: 4.036543
[INFO] Epoch: 12 , batch: 281 , training loss: 4.202395
[INFO] Epoch: 12 , batch: 282 , training loss: 4.101728
[INFO] Epoch: 12 , batch: 283 , training loss: 4.119750
[INFO] Epoch: 12 , batch: 284 , training loss: 4.151644
[INFO] Epoch: 12 , batch: 285 , training loss: 4.100219
[INFO] Epoch: 12 , batch: 286 , training loss: 4.093577
[INFO] Epoch: 12 , batch: 287 , training loss: 4.020995
[INFO] Epoch: 12 , batch: 288 , training loss: 4.039499
[INFO] Epoch: 12 , batch: 289 , training loss: 4.092190
[INFO] Epoch: 12 , batch: 290 , training loss: 3.873582
[INFO] Epoch: 12 , batch: 291 , training loss: 3.843927
[INFO] Epoch: 12 , batch: 292 , training loss: 3.956539
[INFO] Epoch: 12 , batch: 293 , training loss: 3.857905
[INFO] Epoch: 12 , batch: 294 , training loss: 4.556547
[INFO] Epoch: 12 , batch: 295 , training loss: 4.338786
[INFO] Epoch: 12 , batch: 296 , training loss: 4.250923
[INFO] Epoch: 12 , batch: 297 , training loss: 4.221675
[INFO] Epoch: 12 , batch: 298 , training loss: 4.046693
[INFO] Epoch: 12 , batch: 299 , training loss: 4.075629
[INFO] Epoch: 12 , batch: 300 , training loss: 4.040964
[INFO] Epoch: 12 , batch: 301 , training loss: 3.965208
[INFO] Epoch: 12 , batch: 302 , training loss: 4.136358
[INFO] Epoch: 12 , batch: 303 , training loss: 4.160668
[INFO] Epoch: 12 , batch: 304 , training loss: 4.333934
[INFO] Epoch: 12 , batch: 305 , training loss: 4.112564
[INFO] Epoch: 12 , batch: 306 , training loss: 4.242426
[INFO] Epoch: 12 , batch: 307 , training loss: 4.241426
[INFO] Epoch: 12 , batch: 308 , training loss: 4.087009
[INFO] Epoch: 12 , batch: 309 , training loss: 4.115694
[INFO] Epoch: 12 , batch: 310 , training loss: 3.965343
[INFO] Epoch: 12 , batch: 311 , training loss: 3.973695
[INFO] Epoch: 12 , batch: 312 , training loss: 3.877682
[INFO] Epoch: 12 , batch: 313 , training loss: 4.035747
[INFO] Epoch: 12 , batch: 314 , training loss: 4.076322
[INFO] Epoch: 12 , batch: 315 , training loss: 4.120679
[INFO] Epoch: 12 , batch: 316 , training loss: 4.396430
[INFO] Epoch: 12 , batch: 317 , training loss: 4.914284
[INFO] Epoch: 12 , batch: 318 , training loss: 5.028127
[INFO] Epoch: 12 , batch: 319 , training loss: 4.567388
[INFO] Epoch: 12 , batch: 320 , training loss: 4.118380
[INFO] Epoch: 12 , batch: 321 , training loss: 3.917549
[INFO] Epoch: 12 , batch: 322 , training loss: 4.043707
[INFO] Epoch: 12 , batch: 323 , training loss: 4.072316
[INFO] Epoch: 12 , batch: 324 , training loss: 4.038378
[INFO] Epoch: 12 , batch: 325 , training loss: 4.205888
[INFO] Epoch: 12 , batch: 326 , training loss: 4.255502
[INFO] Epoch: 12 , batch: 327 , training loss: 4.159370
[INFO] Epoch: 12 , batch: 328 , training loss: 4.140729
[INFO] Epoch: 12 , batch: 329 , training loss: 4.071974
[INFO] Epoch: 12 , batch: 330 , training loss: 4.054554
[INFO] Epoch: 12 , batch: 331 , training loss: 4.200355
[INFO] Epoch: 12 , batch: 332 , training loss: 4.014370
[INFO] Epoch: 12 , batch: 333 , training loss: 4.014791
[INFO] Epoch: 12 , batch: 334 , training loss: 4.052238
[INFO] Epoch: 12 , batch: 335 , training loss: 4.168300
[INFO] Epoch: 12 , batch: 336 , training loss: 4.189852
[INFO] Epoch: 12 , batch: 337 , training loss: 4.196301
[INFO] Epoch: 12 , batch: 338 , training loss: 4.433163
[INFO] Epoch: 12 , batch: 339 , training loss: 4.265098
[INFO] Epoch: 12 , batch: 340 , training loss: 4.439180
[INFO] Epoch: 12 , batch: 341 , training loss: 4.190665
[INFO] Epoch: 12 , batch: 342 , training loss: 3.976185
[INFO] Epoch: 12 , batch: 343 , training loss: 4.063032
[INFO] Epoch: 12 , batch: 344 , training loss: 3.926267
[INFO] Epoch: 12 , batch: 345 , training loss: 4.036237
[INFO] Epoch: 12 , batch: 346 , training loss: 4.120253
[INFO] Epoch: 12 , batch: 347 , training loss: 3.995058
[INFO] Epoch: 12 , batch: 348 , training loss: 4.152880
[INFO] Epoch: 12 , batch: 349 , training loss: 4.290730
[INFO] Epoch: 12 , batch: 350 , training loss: 4.033175
[INFO] Epoch: 12 , batch: 351 , training loss: 4.093514
[INFO] Epoch: 12 , batch: 352 , training loss: 4.133511
[INFO] Epoch: 12 , batch: 353 , training loss: 4.091460
[INFO] Epoch: 12 , batch: 354 , training loss: 4.212283
[INFO] Epoch: 12 , batch: 355 , training loss: 4.247031
[INFO] Epoch: 12 , batch: 356 , training loss: 4.073033
[INFO] Epoch: 12 , batch: 357 , training loss: 4.177196
[INFO] Epoch: 12 , batch: 358 , training loss: 4.103186
[INFO] Epoch: 12 , batch: 359 , training loss: 4.052277
[INFO] Epoch: 12 , batch: 360 , training loss: 4.127407
[INFO] Epoch: 12 , batch: 361 , training loss: 4.120665
[INFO] Epoch: 12 , batch: 362 , training loss: 4.208564
[INFO] Epoch: 12 , batch: 363 , training loss: 4.110544
[INFO] Epoch: 12 , batch: 364 , training loss: 4.123233
[INFO] Epoch: 12 , batch: 365 , training loss: 4.072537
[INFO] Epoch: 12 , batch: 366 , training loss: 4.198833
[INFO] Epoch: 12 , batch: 367 , training loss: 4.281782
[INFO] Epoch: 12 , batch: 368 , training loss: 4.769860
[INFO] Epoch: 12 , batch: 369 , training loss: 4.375658
[INFO] Epoch: 12 , batch: 370 , training loss: 4.137037
[INFO] Epoch: 12 , batch: 371 , training loss: 4.605743
[INFO] Epoch: 12 , batch: 372 , training loss: 4.891987
[INFO] Epoch: 12 , batch: 373 , training loss: 4.943936
[INFO] Epoch: 12 , batch: 374 , training loss: 5.006221
[INFO] Epoch: 12 , batch: 375 , training loss: 4.991995
[INFO] Epoch: 12 , batch: 376 , training loss: 4.909853
[INFO] Epoch: 12 , batch: 377 , training loss: 4.588410
[INFO] Epoch: 12 , batch: 378 , training loss: 4.713199
[INFO] Epoch: 12 , batch: 379 , training loss: 4.693158
[INFO] Epoch: 12 , batch: 380 , training loss: 4.836133
[INFO] Epoch: 12 , batch: 381 , training loss: 4.573164
[INFO] Epoch: 12 , batch: 382 , training loss: 4.811373
[INFO] Epoch: 12 , batch: 383 , training loss: 4.866043
[INFO] Epoch: 12 , batch: 384 , training loss: 4.902646
[INFO] Epoch: 12 , batch: 385 , training loss: 4.638544
[INFO] Epoch: 12 , batch: 386 , training loss: 4.821217
[INFO] Epoch: 12 , batch: 387 , training loss: 4.767963
[INFO] Epoch: 12 , batch: 388 , training loss: 4.563918
[INFO] Epoch: 12 , batch: 389 , training loss: 4.411530
[INFO] Epoch: 12 , batch: 390 , training loss: 4.384284
[INFO] Epoch: 12 , batch: 391 , training loss: 4.434472
[INFO] Epoch: 12 , batch: 392 , training loss: 4.790868
[INFO] Epoch: 12 , batch: 393 , training loss: 4.654749
[INFO] Epoch: 12 , batch: 394 , training loss: 4.722440
[INFO] Epoch: 12 , batch: 395 , training loss: 4.575546
[INFO] Epoch: 12 , batch: 396 , training loss: 4.336421
[INFO] Epoch: 12 , batch: 397 , training loss: 4.513100
[INFO] Epoch: 12 , batch: 398 , training loss: 4.354789
[INFO] Epoch: 12 , batch: 399 , training loss: 4.410608
[INFO] Epoch: 12 , batch: 400 , training loss: 4.375242
[INFO] Epoch: 12 , batch: 401 , training loss: 4.786504
[INFO] Epoch: 12 , batch: 402 , training loss: 4.523532
[INFO] Epoch: 12 , batch: 403 , training loss: 4.353822
[INFO] Epoch: 12 , batch: 404 , training loss: 4.530356
[INFO] Epoch: 12 , batch: 405 , training loss: 4.551669
[INFO] Epoch: 12 , batch: 406 , training loss: 4.466531
[INFO] Epoch: 12 , batch: 407 , training loss: 4.543803
[INFO] Epoch: 12 , batch: 408 , training loss: 4.483853
[INFO] Epoch: 12 , batch: 409 , training loss: 4.489620
[INFO] Epoch: 12 , batch: 410 , training loss: 4.520716
[INFO] Epoch: 12 , batch: 411 , training loss: 4.745786
[INFO] Epoch: 12 , batch: 412 , training loss: 4.554211
[INFO] Epoch: 12 , batch: 413 , training loss: 4.432587
[INFO] Epoch: 12 , batch: 414 , training loss: 4.444058
[INFO] Epoch: 12 , batch: 415 , training loss: 4.483193
[INFO] Epoch: 12 , batch: 416 , training loss: 4.565851
[INFO] Epoch: 12 , batch: 417 , training loss: 4.482409
[INFO] Epoch: 12 , batch: 418 , training loss: 4.505182
[INFO] Epoch: 12 , batch: 419 , training loss: 4.466778
[INFO] Epoch: 12 , batch: 420 , training loss: 4.440869
[INFO] Epoch: 12 , batch: 421 , training loss: 4.439490
[INFO] Epoch: 12 , batch: 422 , training loss: 4.316854
[INFO] Epoch: 12 , batch: 423 , training loss: 4.530982
[INFO] Epoch: 12 , batch: 424 , training loss: 4.697481
[INFO] Epoch: 12 , batch: 425 , training loss: 4.554593
[INFO] Epoch: 12 , batch: 426 , training loss: 4.287324
[INFO] Epoch: 12 , batch: 427 , training loss: 4.517481
[INFO] Epoch: 12 , batch: 428 , training loss: 4.418495
[INFO] Epoch: 12 , batch: 429 , training loss: 4.256098
[INFO] Epoch: 12 , batch: 430 , training loss: 4.527153
[INFO] Epoch: 12 , batch: 431 , training loss: 4.114747
[INFO] Epoch: 12 , batch: 432 , training loss: 4.192118
[INFO] Epoch: 12 , batch: 433 , training loss: 4.226593
[INFO] Epoch: 12 , batch: 434 , training loss: 4.077714
[INFO] Epoch: 12 , batch: 435 , training loss: 4.453841
[INFO] Epoch: 12 , batch: 436 , training loss: 4.538024
[INFO] Epoch: 12 , batch: 437 , training loss: 4.284744
[INFO] Epoch: 12 , batch: 438 , training loss: 4.126134
[INFO] Epoch: 12 , batch: 439 , training loss: 4.356499
[INFO] Epoch: 12 , batch: 440 , training loss: 4.496764
[INFO] Epoch: 12 , batch: 441 , training loss: 4.594064
[INFO] Epoch: 12 , batch: 442 , training loss: 4.369748
[INFO] Epoch: 12 , batch: 443 , training loss: 4.548383
[INFO] Epoch: 12 , batch: 444 , training loss: 4.163321
[INFO] Epoch: 12 , batch: 445 , training loss: 4.065849
[INFO] Epoch: 12 , batch: 446 , training loss: 3.978393
[INFO] Epoch: 12 , batch: 447 , training loss: 4.187019
[INFO] Epoch: 12 , batch: 448 , training loss: 4.310823
[INFO] Epoch: 12 , batch: 449 , training loss: 4.744649
[INFO] Epoch: 12 , batch: 450 , training loss: 4.759570
[INFO] Epoch: 12 , batch: 451 , training loss: 4.662255
[INFO] Epoch: 12 , batch: 452 , training loss: 4.444378
[INFO] Epoch: 12 , batch: 453 , training loss: 4.228084
[INFO] Epoch: 12 , batch: 454 , training loss: 4.371335
[INFO] Epoch: 12 , batch: 455 , training loss: 4.413593
[INFO] Epoch: 12 , batch: 456 , training loss: 4.405917
[INFO] Epoch: 12 , batch: 457 , training loss: 4.524569
[INFO] Epoch: 12 , batch: 458 , training loss: 4.239297
[INFO] Epoch: 12 , batch: 459 , training loss: 4.216612
[INFO] Epoch: 12 , batch: 460 , training loss: 4.340514
[INFO] Epoch: 12 , batch: 461 , training loss: 4.316259
[INFO] Epoch: 12 , batch: 462 , training loss: 4.367463
[INFO] Epoch: 12 , batch: 463 , training loss: 4.250496
[INFO] Epoch: 12 , batch: 464 , training loss: 4.474021
[INFO] Epoch: 12 , batch: 465 , training loss: 4.397831
[INFO] Epoch: 12 , batch: 466 , training loss: 4.464458
[INFO] Epoch: 12 , batch: 467 , training loss: 4.459652
[INFO] Epoch: 12 , batch: 468 , training loss: 4.416677
[INFO] Epoch: 12 , batch: 469 , training loss: 4.461526
[INFO] Epoch: 12 , batch: 470 , training loss: 4.253438
[INFO] Epoch: 12 , batch: 471 , training loss: 4.335760
[INFO] Epoch: 12 , batch: 472 , training loss: 4.383007
[INFO] Epoch: 12 , batch: 473 , training loss: 4.319808
[INFO] Epoch: 12 , batch: 474 , training loss: 4.111278
[INFO] Epoch: 12 , batch: 475 , training loss: 3.977893
[INFO] Epoch: 12 , batch: 476 , training loss: 4.384664
[INFO] Epoch: 12 , batch: 477 , training loss: 4.492864
[INFO] Epoch: 12 , batch: 478 , training loss: 4.548676
[INFO] Epoch: 12 , batch: 479 , training loss: 4.506266
[INFO] Epoch: 12 , batch: 480 , training loss: 4.620300
[INFO] Epoch: 12 , batch: 481 , training loss: 4.469316
[INFO] Epoch: 12 , batch: 482 , training loss: 4.608940
[INFO] Epoch: 12 , batch: 483 , training loss: 4.456716
[INFO] Epoch: 12 , batch: 484 , training loss: 4.248716
[INFO] Epoch: 12 , batch: 485 , training loss: 4.352183
[INFO] Epoch: 12 , batch: 486 , training loss: 4.221843
[INFO] Epoch: 12 , batch: 487 , training loss: 4.218790
[INFO] Epoch: 12 , batch: 488 , training loss: 4.435898
[INFO] Epoch: 12 , batch: 489 , training loss: 4.285067
[INFO] Epoch: 12 , batch: 490 , training loss: 4.338677
[INFO] Epoch: 12 , batch: 491 , training loss: 4.326994
[INFO] Epoch: 12 , batch: 492 , training loss: 4.253816
[INFO] Epoch: 12 , batch: 493 , training loss: 4.430596
[INFO] Epoch: 12 , batch: 494 , training loss: 4.375093
[INFO] Epoch: 12 , batch: 495 , training loss: 4.482318
[INFO] Epoch: 12 , batch: 496 , training loss: 4.351247
[INFO] Epoch: 12 , batch: 497 , training loss: 4.387796
[INFO] Epoch: 12 , batch: 498 , training loss: 4.406937
[INFO] Epoch: 12 , batch: 499 , training loss: 4.473289
[INFO] Epoch: 12 , batch: 500 , training loss: 4.644971
[INFO] Epoch: 12 , batch: 501 , training loss: 4.987691
[INFO] Epoch: 12 , batch: 502 , training loss: 5.112664
[INFO] Epoch: 12 , batch: 503 , training loss: 4.825451
[INFO] Epoch: 12 , batch: 504 , training loss: 4.935553
[INFO] Epoch: 12 , batch: 505 , training loss: 4.893881
[INFO] Epoch: 12 , batch: 506 , training loss: 4.824049
[INFO] Epoch: 12 , batch: 507 , training loss: 4.889375
[INFO] Epoch: 12 , batch: 508 , training loss: 4.845099
[INFO] Epoch: 12 , batch: 509 , training loss: 4.585992
[INFO] Epoch: 12 , batch: 510 , training loss: 4.638255
[INFO] Epoch: 12 , batch: 511 , training loss: 4.550126
[INFO] Epoch: 12 , batch: 512 , training loss: 4.637866
[INFO] Epoch: 12 , batch: 513 , training loss: 4.903872
[INFO] Epoch: 12 , batch: 514 , training loss: 4.526140
[INFO] Epoch: 12 , batch: 515 , training loss: 4.766899
[INFO] Epoch: 12 , batch: 516 , training loss: 4.578691
[INFO] Epoch: 12 , batch: 517 , training loss: 4.545184
[INFO] Epoch: 12 , batch: 518 , training loss: 4.515809
[INFO] Epoch: 12 , batch: 519 , training loss: 4.382652
[INFO] Epoch: 12 , batch: 520 , training loss: 4.600210
[INFO] Epoch: 12 , batch: 521 , training loss: 4.591254
[INFO] Epoch: 12 , batch: 522 , training loss: 4.635740
[INFO] Epoch: 12 , batch: 523 , training loss: 4.559541
[INFO] Epoch: 12 , batch: 524 , training loss: 4.831939
[INFO] Epoch: 12 , batch: 525 , training loss: 4.741551
[INFO] Epoch: 12 , batch: 526 , training loss: 4.506216
[INFO] Epoch: 12 , batch: 527 , training loss: 4.541772
[INFO] Epoch: 12 , batch: 528 , training loss: 4.597945
[INFO] Epoch: 12 , batch: 529 , training loss: 4.554771
[INFO] Epoch: 12 , batch: 530 , training loss: 4.412916
[INFO] Epoch: 12 , batch: 531 , training loss: 4.538397
[INFO] Epoch: 12 , batch: 532 , training loss: 4.434169
[INFO] Epoch: 12 , batch: 533 , training loss: 4.608299
[INFO] Epoch: 12 , batch: 534 , training loss: 4.566126
[INFO] Epoch: 12 , batch: 535 , training loss: 4.603174
[INFO] Epoch: 12 , batch: 536 , training loss: 4.411112
[INFO] Epoch: 12 , batch: 537 , training loss: 4.413604
[INFO] Epoch: 12 , batch: 538 , training loss: 4.485010
[INFO] Epoch: 12 , batch: 539 , training loss: 4.613357
[INFO] Epoch: 12 , batch: 540 , training loss: 5.165983
[INFO] Epoch: 12 , batch: 541 , training loss: 5.076162
[INFO] Epoch: 12 , batch: 542 , training loss: 4.939973
[INFO] Epoch: 13 , batch: 0 , training loss: 3.916077
[INFO] Epoch: 13 , batch: 1 , training loss: 3.893677
[INFO] Epoch: 13 , batch: 2 , training loss: 3.896959
[INFO] Epoch: 13 , batch: 3 , training loss: 3.783775
[INFO] Epoch: 13 , batch: 4 , training loss: 4.161719
[INFO] Epoch: 13 , batch: 5 , training loss: 3.739723
[INFO] Epoch: 13 , batch: 6 , training loss: 4.201886
[INFO] Epoch: 13 , batch: 7 , training loss: 3.997718
[INFO] Epoch: 13 , batch: 8 , training loss: 3.712606
[INFO] Epoch: 13 , batch: 9 , training loss: 3.884645
[INFO] Epoch: 13 , batch: 10 , training loss: 3.820848
[INFO] Epoch: 13 , batch: 11 , training loss: 3.799470
[INFO] Epoch: 13 , batch: 12 , training loss: 3.748674
[INFO] Epoch: 13 , batch: 13 , training loss: 3.750754
[INFO] Epoch: 13 , batch: 14 , training loss: 3.573713
[INFO] Epoch: 13 , batch: 15 , training loss: 3.826397
[INFO] Epoch: 13 , batch: 16 , training loss: 3.660188
[INFO] Epoch: 13 , batch: 17 , training loss: 3.857033
[INFO] Epoch: 13 , batch: 18 , training loss: 3.796059
[INFO] Epoch: 13 , batch: 19 , training loss: 3.512895
[INFO] Epoch: 13 , batch: 20 , training loss: 3.491711
[INFO] Epoch: 13 , batch: 21 , training loss: 3.637147
[INFO] Epoch: 13 , batch: 22 , training loss: 3.579844
[INFO] Epoch: 13 , batch: 23 , training loss: 3.780823
[INFO] Epoch: 13 , batch: 24 , training loss: 3.617549
[INFO] Epoch: 13 , batch: 25 , training loss: 3.803114
[INFO] Epoch: 13 , batch: 26 , training loss: 3.599555
[INFO] Epoch: 13 , batch: 27 , training loss: 3.577318
[INFO] Epoch: 13 , batch: 28 , training loss: 3.757318
[INFO] Epoch: 13 , batch: 29 , training loss: 3.580953
[INFO] Epoch: 13 , batch: 30 , training loss: 3.633276
[INFO] Epoch: 13 , batch: 31 , training loss: 3.692050
[INFO] Epoch: 13 , batch: 32 , training loss: 3.694296
[INFO] Epoch: 13 , batch: 33 , training loss: 3.720067
[INFO] Epoch: 13 , batch: 34 , training loss: 3.728306
[INFO] Epoch: 13 , batch: 35 , training loss: 3.688809
[INFO] Epoch: 13 , batch: 36 , training loss: 3.730288
[INFO] Epoch: 13 , batch: 37 , training loss: 3.615798
[INFO] Epoch: 13 , batch: 38 , training loss: 3.685868
[INFO] Epoch: 13 , batch: 39 , training loss: 3.539516
[INFO] Epoch: 13 , batch: 40 , training loss: 3.724380
[INFO] Epoch: 13 , batch: 41 , training loss: 3.698506
[INFO] Epoch: 13 , batch: 42 , training loss: 4.195900
[INFO] Epoch: 13 , batch: 43 , training loss: 3.835894
[INFO] Epoch: 13 , batch: 44 , training loss: 4.166491
[INFO] Epoch: 13 , batch: 45 , training loss: 4.102350
[INFO] Epoch: 13 , batch: 46 , training loss: 4.168914
[INFO] Epoch: 13 , batch: 47 , training loss: 3.871698
[INFO] Epoch: 13 , batch: 48 , training loss: 3.833789
[INFO] Epoch: 13 , batch: 49 , training loss: 3.978784
[INFO] Epoch: 13 , batch: 50 , training loss: 3.725602
[INFO] Epoch: 13 , batch: 51 , training loss: 3.922074
[INFO] Epoch: 13 , batch: 52 , training loss: 3.779952
[INFO] Epoch: 13 , batch: 53 , training loss: 3.909610
[INFO] Epoch: 13 , batch: 54 , training loss: 3.934458
[INFO] Epoch: 13 , batch: 55 , training loss: 3.987504
[INFO] Epoch: 13 , batch: 56 , training loss: 3.849429
[INFO] Epoch: 13 , batch: 57 , training loss: 3.765459
[INFO] Epoch: 13 , batch: 58 , training loss: 3.832553
[INFO] Epoch: 13 , batch: 59 , training loss: 3.887414
[INFO] Epoch: 13 , batch: 60 , training loss: 3.829666
[INFO] Epoch: 13 , batch: 61 , training loss: 3.936476
[INFO] Epoch: 13 , batch: 62 , training loss: 3.834989
[INFO] Epoch: 13 , batch: 63 , training loss: 4.020231
[INFO] Epoch: 13 , batch: 64 , training loss: 4.152611
[INFO] Epoch: 13 , batch: 65 , training loss: 3.863048
[INFO] Epoch: 13 , batch: 66 , training loss: 3.726148
[INFO] Epoch: 13 , batch: 67 , training loss: 3.799537
[INFO] Epoch: 13 , batch: 68 , training loss: 3.971974
[INFO] Epoch: 13 , batch: 69 , training loss: 3.824967
[INFO] Epoch: 13 , batch: 70 , training loss: 4.062765
[INFO] Epoch: 13 , batch: 71 , training loss: 3.920056
[INFO] Epoch: 13 , batch: 72 , training loss: 3.964568
[INFO] Epoch: 13 , batch: 73 , training loss: 3.894303
[INFO] Epoch: 13 , batch: 74 , training loss: 4.039288
[INFO] Epoch: 13 , batch: 75 , training loss: 3.826315
[INFO] Epoch: 13 , batch: 76 , training loss: 3.970029
[INFO] Epoch: 13 , batch: 77 , training loss: 3.938249
[INFO] Epoch: 13 , batch: 78 , training loss: 4.015430
[INFO] Epoch: 13 , batch: 79 , training loss: 3.828716
[INFO] Epoch: 13 , batch: 80 , training loss: 4.055798
[INFO] Epoch: 13 , batch: 81 , training loss: 3.998242
[INFO] Epoch: 13 , batch: 82 , training loss: 3.947226
[INFO] Epoch: 13 , batch: 83 , training loss: 4.068728
[INFO] Epoch: 13 , batch: 84 , training loss: 3.996022
[INFO] Epoch: 13 , batch: 85 , training loss: 4.058037
[INFO] Epoch: 13 , batch: 86 , training loss: 4.031832
[INFO] Epoch: 13 , batch: 87 , training loss: 4.035223
[INFO] Epoch: 13 , batch: 88 , training loss: 4.164915
[INFO] Epoch: 13 , batch: 89 , training loss: 3.950782
[INFO] Epoch: 13 , batch: 90 , training loss: 4.004632
[INFO] Epoch: 13 , batch: 91 , training loss: 3.947555
[INFO] Epoch: 13 , batch: 92 , training loss: 3.929228
[INFO] Epoch: 13 , batch: 93 , training loss: 4.068932
[INFO] Epoch: 13 , batch: 94 , training loss: 4.237205
[INFO] Epoch: 13 , batch: 95 , training loss: 3.980155
[INFO] Epoch: 13 , batch: 96 , training loss: 3.990137
[INFO] Epoch: 13 , batch: 97 , training loss: 3.909886
[INFO] Epoch: 13 , batch: 98 , training loss: 3.890527
[INFO] Epoch: 13 , batch: 99 , training loss: 3.940282
[INFO] Epoch: 13 , batch: 100 , training loss: 3.854024
[INFO] Epoch: 13 , batch: 101 , training loss: 3.903705
[INFO] Epoch: 13 , batch: 102 , training loss: 4.032226
[INFO] Epoch: 13 , batch: 103 , training loss: 3.806068
[INFO] Epoch: 13 , batch: 104 , training loss: 3.792843
[INFO] Epoch: 13 , batch: 105 , training loss: 4.058575
[INFO] Epoch: 13 , batch: 106 , training loss: 4.031463
[INFO] Epoch: 13 , batch: 107 , training loss: 3.914408
[INFO] Epoch: 13 , batch: 108 , training loss: 3.844820
[INFO] Epoch: 13 , batch: 109 , training loss: 3.733053
[INFO] Epoch: 13 , batch: 110 , training loss: 3.939376
[INFO] Epoch: 13 , batch: 111 , training loss: 4.008930
[INFO] Epoch: 13 , batch: 112 , training loss: 3.939603
[INFO] Epoch: 13 , batch: 113 , training loss: 3.922653
[INFO] Epoch: 13 , batch: 114 , training loss: 3.965996
[INFO] Epoch: 13 , batch: 115 , training loss: 3.920116
[INFO] Epoch: 13 , batch: 116 , training loss: 3.893183
[INFO] Epoch: 13 , batch: 117 , training loss: 4.113132
[INFO] Epoch: 13 , batch: 118 , training loss: 4.045643
[INFO] Epoch: 13 , batch: 119 , training loss: 4.211955
[INFO] Epoch: 13 , batch: 120 , training loss: 4.202355
[INFO] Epoch: 13 , batch: 121 , training loss: 4.005556
[INFO] Epoch: 13 , batch: 122 , training loss: 3.936513
[INFO] Epoch: 13 , batch: 123 , training loss: 3.950967
[INFO] Epoch: 13 , batch: 124 , training loss: 4.109737
[INFO] Epoch: 13 , batch: 125 , training loss: 3.850057
[INFO] Epoch: 13 , batch: 126 , training loss: 3.866518
[INFO] Epoch: 13 , batch: 127 , training loss: 3.873135
[INFO] Epoch: 13 , batch: 128 , training loss: 4.045281
[INFO] Epoch: 13 , batch: 129 , training loss: 3.978854
[INFO] Epoch: 13 , batch: 130 , training loss: 3.970331
[INFO] Epoch: 13 , batch: 131 , training loss: 3.969215
[INFO] Epoch: 13 , batch: 132 , training loss: 4.000571
[INFO] Epoch: 13 , batch: 133 , training loss: 3.926929
[INFO] Epoch: 13 , batch: 134 , training loss: 3.732133
[INFO] Epoch: 13 , batch: 135 , training loss: 3.783909
[INFO] Epoch: 13 , batch: 136 , training loss: 4.080631
[INFO] Epoch: 13 , batch: 137 , training loss: 3.991299
[INFO] Epoch: 13 , batch: 138 , training loss: 4.094725
[INFO] Epoch: 13 , batch: 139 , training loss: 4.704891
[INFO] Epoch: 13 , batch: 140 , training loss: 4.436062
[INFO] Epoch: 13 , batch: 141 , training loss: 4.214063
[INFO] Epoch: 13 , batch: 142 , training loss: 3.930218
[INFO] Epoch: 13 , batch: 143 , training loss: 4.020672
[INFO] Epoch: 13 , batch: 144 , training loss: 3.829845
[INFO] Epoch: 13 , batch: 145 , training loss: 3.957112
[INFO] Epoch: 13 , batch: 146 , training loss: 4.166302
[INFO] Epoch: 13 , batch: 147 , training loss: 3.838714
[INFO] Epoch: 13 , batch: 148 , training loss: 3.808789
[INFO] Epoch: 13 , batch: 149 , training loss: 3.870343
[INFO] Epoch: 13 , batch: 150 , training loss: 4.100557
[INFO] Epoch: 13 , batch: 151 , training loss: 3.953629
[INFO] Epoch: 13 , batch: 152 , training loss: 4.002443
[INFO] Epoch: 13 , batch: 153 , training loss: 4.023167
[INFO] Epoch: 13 , batch: 154 , training loss: 4.097204
[INFO] Epoch: 13 , batch: 155 , training loss: 4.333074
[INFO] Epoch: 13 , batch: 156 , training loss: 4.037836
[INFO] Epoch: 13 , batch: 157 , training loss: 4.004107
[INFO] Epoch: 13 , batch: 158 , training loss: 4.210413
[INFO] Epoch: 13 , batch: 159 , training loss: 4.061064
[INFO] Epoch: 13 , batch: 160 , training loss: 4.448064
[INFO] Epoch: 13 , batch: 161 , training loss: 4.469175
[INFO] Epoch: 13 , batch: 162 , training loss: 4.417452
[INFO] Epoch: 13 , batch: 163 , training loss: 4.513908
[INFO] Epoch: 13 , batch: 164 , training loss: 4.526045
[INFO] Epoch: 13 , batch: 165 , training loss: 4.448681
[INFO] Epoch: 13 , batch: 166 , training loss: 4.360019
[INFO] Epoch: 13 , batch: 167 , training loss: 4.582453
[INFO] Epoch: 13 , batch: 168 , training loss: 4.312428
[INFO] Epoch: 13 , batch: 169 , training loss: 4.244222
[INFO] Epoch: 13 , batch: 170 , training loss: 4.351918
[INFO] Epoch: 13 , batch: 171 , training loss: 3.798053
[INFO] Epoch: 13 , batch: 172 , training loss: 4.013447
[INFO] Epoch: 13 , batch: 173 , training loss: 4.328907
[INFO] Epoch: 13 , batch: 174 , training loss: 4.649579
[INFO] Epoch: 13 , batch: 175 , training loss: 5.003605
[INFO] Epoch: 13 , batch: 176 , training loss: 4.709800
[INFO] Epoch: 13 , batch: 177 , training loss: 4.277844
[INFO] Epoch: 13 , batch: 178 , training loss: 4.297455
[INFO] Epoch: 13 , batch: 179 , training loss: 4.356442
[INFO] Epoch: 13 , batch: 180 , training loss: 4.262687
[INFO] Epoch: 13 , batch: 181 , training loss: 4.541077
[INFO] Epoch: 13 , batch: 182 , training loss: 4.456885
[INFO] Epoch: 13 , batch: 183 , training loss: 4.447244
[INFO] Epoch: 13 , batch: 184 , training loss: 4.324225
[INFO] Epoch: 13 , batch: 185 , training loss: 4.260901
[INFO] Epoch: 13 , batch: 186 , training loss: 4.449436
[INFO] Epoch: 13 , batch: 187 , training loss: 4.520875
[INFO] Epoch: 13 , batch: 188 , training loss: 4.489335
[INFO] Epoch: 13 , batch: 189 , training loss: 4.431835
[INFO] Epoch: 13 , batch: 190 , training loss: 4.411396
[INFO] Epoch: 13 , batch: 191 , training loss: 4.531269
[INFO] Epoch: 13 , batch: 192 , training loss: 4.341583
[INFO] Epoch: 13 , batch: 193 , training loss: 4.450892
[INFO] Epoch: 13 , batch: 194 , training loss: 4.372000
[INFO] Epoch: 13 , batch: 195 , training loss: 4.304238
[INFO] Epoch: 13 , batch: 196 , training loss: 4.179016
[INFO] Epoch: 13 , batch: 197 , training loss: 4.290355
[INFO] Epoch: 13 , batch: 198 , training loss: 4.233167
[INFO] Epoch: 13 , batch: 199 , training loss: 4.309978
[INFO] Epoch: 13 , batch: 200 , training loss: 4.225301
[INFO] Epoch: 13 , batch: 201 , training loss: 4.121698
[INFO] Epoch: 13 , batch: 202 , training loss: 4.122455
[INFO] Epoch: 13 , batch: 203 , training loss: 4.207564
[INFO] Epoch: 13 , batch: 204 , training loss: 4.347555
[INFO] Epoch: 13 , batch: 205 , training loss: 3.911008
[INFO] Epoch: 13 , batch: 206 , training loss: 3.846140
[INFO] Epoch: 13 , batch: 207 , training loss: 3.862918
[INFO] Epoch: 13 , batch: 208 , training loss: 4.190429
[INFO] Epoch: 13 , batch: 209 , training loss: 4.135987
[INFO] Epoch: 13 , batch: 210 , training loss: 4.162004
[INFO] Epoch: 13 , batch: 211 , training loss: 4.135698
[INFO] Epoch: 13 , batch: 212 , training loss: 4.270719
[INFO] Epoch: 13 , batch: 213 , training loss: 4.227481
[INFO] Epoch: 13 , batch: 214 , training loss: 4.296187
[INFO] Epoch: 13 , batch: 215 , training loss: 4.512648
[INFO] Epoch: 13 , batch: 216 , training loss: 4.227014
[INFO] Epoch: 13 , batch: 217 , training loss: 4.159273
[INFO] Epoch: 13 , batch: 218 , training loss: 4.147407
[INFO] Epoch: 13 , batch: 219 , training loss: 4.261728
[INFO] Epoch: 13 , batch: 220 , training loss: 4.073682
[INFO] Epoch: 13 , batch: 221 , training loss: 4.077928
[INFO] Epoch: 13 , batch: 222 , training loss: 4.223916
[INFO] Epoch: 13 , batch: 223 , training loss: 4.319493
[INFO] Epoch: 13 , batch: 224 , training loss: 4.355773
[INFO] Epoch: 13 , batch: 225 , training loss: 4.243701
[INFO] Epoch: 13 , batch: 226 , training loss: 4.388150
[INFO] Epoch: 13 , batch: 227 , training loss: 4.335985
[INFO] Epoch: 13 , batch: 228 , training loss: 4.400316
[INFO] Epoch: 13 , batch: 229 , training loss: 4.233946
[INFO] Epoch: 13 , batch: 230 , training loss: 4.087795
[INFO] Epoch: 13 , batch: 231 , training loss: 3.947406
[INFO] Epoch: 13 , batch: 232 , training loss: 4.110065
[INFO] Epoch: 13 , batch: 233 , training loss: 4.113667
[INFO] Epoch: 13 , batch: 234 , training loss: 3.795454
[INFO] Epoch: 13 , batch: 235 , training loss: 3.904517
[INFO] Epoch: 13 , batch: 236 , training loss: 4.040330
[INFO] Epoch: 13 , batch: 237 , training loss: 4.230975
[INFO] Epoch: 13 , batch: 238 , training loss: 3.967830
[INFO] Epoch: 13 , batch: 239 , training loss: 4.059504
[INFO] Epoch: 13 , batch: 240 , training loss: 4.119487
[INFO] Epoch: 13 , batch: 241 , training loss: 3.908999
[INFO] Epoch: 13 , batch: 242 , training loss: 3.908906
[INFO] Epoch: 13 , batch: 243 , training loss: 4.247780
[INFO] Epoch: 13 , batch: 244 , training loss: 4.149000
[INFO] Epoch: 13 , batch: 245 , training loss: 4.126523
[INFO] Epoch: 13 , batch: 246 , training loss: 3.820551
[INFO] Epoch: 13 , batch: 247 , training loss: 4.008981
[INFO] Epoch: 13 , batch: 248 , training loss: 4.068584
[INFO] Epoch: 13 , batch: 249 , training loss: 4.070935
[INFO] Epoch: 13 , batch: 250 , training loss: 3.829762
[INFO] Epoch: 13 , batch: 251 , training loss: 4.311958
[INFO] Epoch: 13 , batch: 252 , training loss: 4.003531
[INFO] Epoch: 13 , batch: 253 , training loss: 3.930900
[INFO] Epoch: 13 , batch: 254 , training loss: 4.233605
[INFO] Epoch: 13 , batch: 255 , training loss: 4.206508
[INFO] Epoch: 13 , batch: 256 , training loss: 4.212035
[INFO] Epoch: 13 , batch: 257 , training loss: 4.360841
[INFO] Epoch: 13 , batch: 258 , training loss: 4.396831
[INFO] Epoch: 13 , batch: 259 , training loss: 4.464837
[INFO] Epoch: 13 , batch: 260 , training loss: 4.159778
[INFO] Epoch: 13 , batch: 261 , training loss: 4.297620
[INFO] Epoch: 13 , batch: 262 , training loss: 4.538286
[INFO] Epoch: 13 , batch: 263 , training loss: 4.675023
[INFO] Epoch: 13 , batch: 264 , training loss: 3.998682
[INFO] Epoch: 13 , batch: 265 , training loss: 4.119956
[INFO] Epoch: 13 , batch: 266 , training loss: 4.558676
[INFO] Epoch: 13 , batch: 267 , training loss: 4.312424
[INFO] Epoch: 13 , batch: 268 , training loss: 4.199754
[INFO] Epoch: 13 , batch: 269 , training loss: 4.193583
[INFO] Epoch: 13 , batch: 270 , training loss: 4.203268
[INFO] Epoch: 13 , batch: 271 , training loss: 4.250914
[INFO] Epoch: 13 , batch: 272 , training loss: 4.222971
[INFO] Epoch: 13 , batch: 273 , training loss: 4.252393
[INFO] Epoch: 13 , batch: 274 , training loss: 4.335834
[INFO] Epoch: 13 , batch: 275 , training loss: 4.207108
[INFO] Epoch: 13 , batch: 276 , training loss: 4.267532
[INFO] Epoch: 13 , batch: 277 , training loss: 4.436669
[INFO] Epoch: 13 , batch: 278 , training loss: 4.086519
[INFO] Epoch: 13 , batch: 279 , training loss: 4.087720
[INFO] Epoch: 13 , batch: 280 , training loss: 4.034658
[INFO] Epoch: 13 , batch: 281 , training loss: 4.185569
[INFO] Epoch: 13 , batch: 282 , training loss: 4.095406
[INFO] Epoch: 13 , batch: 283 , training loss: 4.114969
[INFO] Epoch: 13 , batch: 284 , training loss: 4.132761
[INFO] Epoch: 13 , batch: 285 , training loss: 4.094557
[INFO] Epoch: 13 , batch: 286 , training loss: 4.089021
[INFO] Epoch: 13 , batch: 287 , training loss: 4.006751
[INFO] Epoch: 13 , batch: 288 , training loss: 4.039570
[INFO] Epoch: 13 , batch: 289 , training loss: 4.087810
[INFO] Epoch: 13 , batch: 290 , training loss: 3.861767
[INFO] Epoch: 13 , batch: 291 , training loss: 3.836154
[INFO] Epoch: 13 , batch: 292 , training loss: 3.944231
[INFO] Epoch: 13 , batch: 293 , training loss: 3.845784
[INFO] Epoch: 13 , batch: 294 , training loss: 4.550497
[INFO] Epoch: 13 , batch: 295 , training loss: 4.321369
[INFO] Epoch: 13 , batch: 296 , training loss: 4.250939
[INFO] Epoch: 13 , batch: 297 , training loss: 4.196639
[INFO] Epoch: 13 , batch: 298 , training loss: 4.026093
[INFO] Epoch: 13 , batch: 299 , training loss: 4.061002
[INFO] Epoch: 13 , batch: 300 , training loss: 4.026540
[INFO] Epoch: 13 , batch: 301 , training loss: 3.951297
[INFO] Epoch: 13 , batch: 302 , training loss: 4.132035
[INFO] Epoch: 13 , batch: 303 , training loss: 4.150120
[INFO] Epoch: 13 , batch: 304 , training loss: 4.322848
[INFO] Epoch: 13 , batch: 305 , training loss: 4.107259
[INFO] Epoch: 13 , batch: 306 , training loss: 4.234919
[INFO] Epoch: 13 , batch: 307 , training loss: 4.232849
[INFO] Epoch: 13 , batch: 308 , training loss: 4.089542
[INFO] Epoch: 13 , batch: 309 , training loss: 4.100233
[INFO] Epoch: 13 , batch: 310 , training loss: 3.950085
[INFO] Epoch: 13 , batch: 311 , training loss: 3.965058
[INFO] Epoch: 13 , batch: 312 , training loss: 3.870296
[INFO] Epoch: 13 , batch: 313 , training loss: 4.013576
[INFO] Epoch: 13 , batch: 314 , training loss: 4.074475
[INFO] Epoch: 13 , batch: 315 , training loss: 4.123328
[INFO] Epoch: 13 , batch: 316 , training loss: 4.373193
[INFO] Epoch: 13 , batch: 317 , training loss: 4.867691
[INFO] Epoch: 13 , batch: 318 , training loss: 5.020577
[INFO] Epoch: 13 , batch: 319 , training loss: 4.558095
[INFO] Epoch: 13 , batch: 320 , training loss: 4.107047
[INFO] Epoch: 13 , batch: 321 , training loss: 3.925088
[INFO] Epoch: 13 , batch: 322 , training loss: 4.036130
[INFO] Epoch: 13 , batch: 323 , training loss: 4.059217
[INFO] Epoch: 13 , batch: 324 , training loss: 4.028532
[INFO] Epoch: 13 , batch: 325 , training loss: 4.194539
[INFO] Epoch: 13 , batch: 326 , training loss: 4.242553
[INFO] Epoch: 13 , batch: 327 , training loss: 4.150119
[INFO] Epoch: 13 , batch: 328 , training loss: 4.139487
[INFO] Epoch: 13 , batch: 329 , training loss: 4.051285
[INFO] Epoch: 13 , batch: 330 , training loss: 4.045262
[INFO] Epoch: 13 , batch: 331 , training loss: 4.189682
[INFO] Epoch: 13 , batch: 332 , training loss: 4.001921
[INFO] Epoch: 13 , batch: 333 , training loss: 4.013251
[INFO] Epoch: 13 , batch: 334 , training loss: 4.033978
[INFO] Epoch: 13 , batch: 335 , training loss: 4.154544
[INFO] Epoch: 13 , batch: 336 , training loss: 4.190598
[INFO] Epoch: 13 , batch: 337 , training loss: 4.195687
[INFO] Epoch: 13 , batch: 338 , training loss: 4.410161
[INFO] Epoch: 13 , batch: 339 , training loss: 4.258403
[INFO] Epoch: 13 , batch: 340 , training loss: 4.432756
[INFO] Epoch: 13 , batch: 341 , training loss: 4.191908
[INFO] Epoch: 13 , batch: 342 , training loss: 3.968827
[INFO] Epoch: 13 , batch: 343 , training loss: 4.047369
[INFO] Epoch: 13 , batch: 344 , training loss: 3.909925
[INFO] Epoch: 13 , batch: 345 , training loss: 4.027875
[INFO] Epoch: 13 , batch: 346 , training loss: 4.113220
[INFO] Epoch: 13 , batch: 347 , training loss: 3.999124
[INFO] Epoch: 13 , batch: 348 , training loss: 4.150262
[INFO] Epoch: 13 , batch: 349 , training loss: 4.274740
[INFO] Epoch: 13 , batch: 350 , training loss: 4.026855
[INFO] Epoch: 13 , batch: 351 , training loss: 4.092149
[INFO] Epoch: 13 , batch: 352 , training loss: 4.121441
[INFO] Epoch: 13 , batch: 353 , training loss: 4.090419
[INFO] Epoch: 13 , batch: 354 , training loss: 4.204173
[INFO] Epoch: 13 , batch: 355 , training loss: 4.246342
[INFO] Epoch: 13 , batch: 356 , training loss: 4.065959
[INFO] Epoch: 13 , batch: 357 , training loss: 4.174175
[INFO] Epoch: 13 , batch: 358 , training loss: 4.092385
[INFO] Epoch: 13 , batch: 359 , training loss: 4.051563
[INFO] Epoch: 13 , batch: 360 , training loss: 4.120187
[INFO] Epoch: 13 , batch: 361 , training loss: 4.112086
[INFO] Epoch: 13 , batch: 362 , training loss: 4.212923
[INFO] Epoch: 13 , batch: 363 , training loss: 4.096752
[INFO] Epoch: 13 , batch: 364 , training loss: 4.128399
[INFO] Epoch: 13 , batch: 365 , training loss: 4.056154
[INFO] Epoch: 13 , batch: 366 , training loss: 4.187763
[INFO] Epoch: 13 , batch: 367 , training loss: 4.271098
[INFO] Epoch: 13 , batch: 368 , training loss: 4.748691
[INFO] Epoch: 13 , batch: 369 , training loss: 4.371825
[INFO] Epoch: 13 , batch: 370 , training loss: 4.125019
[INFO] Epoch: 13 , batch: 371 , training loss: 4.543991
[INFO] Epoch: 13 , batch: 372 , training loss: 4.885315
[INFO] Epoch: 13 , batch: 373 , training loss: 4.917393
[INFO] Epoch: 13 , batch: 374 , training loss: 4.986950
[INFO] Epoch: 13 , batch: 375 , training loss: 4.976704
[INFO] Epoch: 13 , batch: 376 , training loss: 4.897837
[INFO] Epoch: 13 , batch: 377 , training loss: 4.591968
[INFO] Epoch: 13 , batch: 378 , training loss: 4.702730
[INFO] Epoch: 13 , batch: 379 , training loss: 4.687103
[INFO] Epoch: 13 , batch: 380 , training loss: 4.831747
[INFO] Epoch: 13 , batch: 381 , training loss: 4.568379
[INFO] Epoch: 13 , batch: 382 , training loss: 4.805202
[INFO] Epoch: 13 , batch: 383 , training loss: 4.830931
[INFO] Epoch: 13 , batch: 384 , training loss: 4.864000
[INFO] Epoch: 13 , batch: 385 , training loss: 4.615455
[INFO] Epoch: 13 , batch: 386 , training loss: 4.816335
[INFO] Epoch: 13 , batch: 387 , training loss: 4.741099
[INFO] Epoch: 13 , batch: 388 , training loss: 4.548784
[INFO] Epoch: 13 , batch: 389 , training loss: 4.405560
[INFO] Epoch: 13 , batch: 390 , training loss: 4.378921
[INFO] Epoch: 13 , batch: 391 , training loss: 4.436373
[INFO] Epoch: 13 , batch: 392 , training loss: 4.794092
[INFO] Epoch: 13 , batch: 393 , training loss: 4.659816
[INFO] Epoch: 13 , batch: 394 , training loss: 4.704136
[INFO] Epoch: 13 , batch: 395 , training loss: 4.574792
[INFO] Epoch: 13 , batch: 396 , training loss: 4.342786
[INFO] Epoch: 13 , batch: 397 , training loss: 4.494161
[INFO] Epoch: 13 , batch: 398 , training loss: 4.339683
[INFO] Epoch: 13 , batch: 399 , training loss: 4.409253
[INFO] Epoch: 13 , batch: 400 , training loss: 4.365633
[INFO] Epoch: 13 , batch: 401 , training loss: 4.792202
[INFO] Epoch: 13 , batch: 402 , training loss: 4.521299
[INFO] Epoch: 13 , batch: 403 , training loss: 4.340963
[INFO] Epoch: 13 , batch: 404 , training loss: 4.525292
[INFO] Epoch: 13 , batch: 405 , training loss: 4.552016
[INFO] Epoch: 13 , batch: 406 , training loss: 4.461979
[INFO] Epoch: 13 , batch: 407 , training loss: 4.538139
[INFO] Epoch: 13 , batch: 408 , training loss: 4.488731
[INFO] Epoch: 13 , batch: 409 , training loss: 4.485680
[INFO] Epoch: 13 , batch: 410 , training loss: 4.516569
[INFO] Epoch: 13 , batch: 411 , training loss: 4.729129
[INFO] Epoch: 13 , batch: 412 , training loss: 4.554069
[INFO] Epoch: 13 , batch: 413 , training loss: 4.430313
[INFO] Epoch: 13 , batch: 414 , training loss: 4.434228
[INFO] Epoch: 13 , batch: 415 , training loss: 4.463231
[INFO] Epoch: 13 , batch: 416 , training loss: 4.548302
[INFO] Epoch: 13 , batch: 417 , training loss: 4.478617
[INFO] Epoch: 13 , batch: 418 , training loss: 4.493883
[INFO] Epoch: 13 , batch: 419 , training loss: 4.456811
[INFO] Epoch: 13 , batch: 420 , training loss: 4.432902
[INFO] Epoch: 13 , batch: 421 , training loss: 4.439316
[INFO] Epoch: 13 , batch: 422 , training loss: 4.303500
[INFO] Epoch: 13 , batch: 423 , training loss: 4.527242
[INFO] Epoch: 13 , batch: 424 , training loss: 4.694084
[INFO] Epoch: 13 , batch: 425 , training loss: 4.541936
[INFO] Epoch: 13 , batch: 426 , training loss: 4.279880
[INFO] Epoch: 13 , batch: 427 , training loss: 4.526441
[INFO] Epoch: 13 , batch: 428 , training loss: 4.411357
[INFO] Epoch: 13 , batch: 429 , training loss: 4.253283
[INFO] Epoch: 13 , batch: 430 , training loss: 4.517206
[INFO] Epoch: 13 , batch: 431 , training loss: 4.107806
[INFO] Epoch: 13 , batch: 432 , training loss: 4.191593
[INFO] Epoch: 13 , batch: 433 , training loss: 4.222145
[INFO] Epoch: 13 , batch: 434 , training loss: 4.076144
[INFO] Epoch: 13 , batch: 435 , training loss: 4.451131
[INFO] Epoch: 13 , batch: 436 , training loss: 4.540803
[INFO] Epoch: 13 , batch: 437 , training loss: 4.279576
[INFO] Epoch: 13 , batch: 438 , training loss: 4.116503
[INFO] Epoch: 13 , batch: 439 , training loss: 4.343956
[INFO] Epoch: 13 , batch: 440 , training loss: 4.485919
[INFO] Epoch: 13 , batch: 441 , training loss: 4.590762
[INFO] Epoch: 13 , batch: 442 , training loss: 4.362199
[INFO] Epoch: 13 , batch: 443 , training loss: 4.547543
[INFO] Epoch: 13 , batch: 444 , training loss: 4.148194
[INFO] Epoch: 13 , batch: 445 , training loss: 4.048892
[INFO] Epoch: 13 , batch: 446 , training loss: 3.970727
[INFO] Epoch: 13 , batch: 447 , training loss: 4.172081
[INFO] Epoch: 13 , batch: 448 , training loss: 4.300460
[INFO] Epoch: 13 , batch: 449 , training loss: 4.723525
[INFO] Epoch: 13 , batch: 450 , training loss: 4.748987
[INFO] Epoch: 13 , batch: 451 , training loss: 4.658410
[INFO] Epoch: 13 , batch: 452 , training loss: 4.442902
[INFO] Epoch: 13 , batch: 453 , training loss: 4.214532
[INFO] Epoch: 13 , batch: 454 , training loss: 4.365380
[INFO] Epoch: 13 , batch: 455 , training loss: 4.402665
[INFO] Epoch: 13 , batch: 456 , training loss: 4.393957
[INFO] Epoch: 13 , batch: 457 , training loss: 4.512876
[INFO] Epoch: 13 , batch: 458 , training loss: 4.229784
[INFO] Epoch: 13 , batch: 459 , training loss: 4.207899
[INFO] Epoch: 13 , batch: 460 , training loss: 4.323822
[INFO] Epoch: 13 , batch: 461 , training loss: 4.295365
[INFO] Epoch: 13 , batch: 462 , training loss: 4.353824
[INFO] Epoch: 13 , batch: 463 , training loss: 4.250310
[INFO] Epoch: 13 , batch: 464 , training loss: 4.470235
[INFO] Epoch: 13 , batch: 465 , training loss: 4.391031
[INFO] Epoch: 13 , batch: 466 , training loss: 4.452262
[INFO] Epoch: 13 , batch: 467 , training loss: 4.454940
[INFO] Epoch: 13 , batch: 468 , training loss: 4.402802
[INFO] Epoch: 13 , batch: 469 , training loss: 4.460546
[INFO] Epoch: 13 , batch: 470 , training loss: 4.246670
[INFO] Epoch: 13 , batch: 471 , training loss: 4.339698
[INFO] Epoch: 13 , batch: 472 , training loss: 4.380519
[INFO] Epoch: 13 , batch: 473 , training loss: 4.317032
[INFO] Epoch: 13 , batch: 474 , training loss: 4.110043
[INFO] Epoch: 13 , batch: 475 , training loss: 3.979594
[INFO] Epoch: 13 , batch: 476 , training loss: 4.375468
[INFO] Epoch: 13 , batch: 477 , training loss: 4.489063
[INFO] Epoch: 13 , batch: 478 , training loss: 4.552015
[INFO] Epoch: 13 , batch: 479 , training loss: 4.491744
[INFO] Epoch: 13 , batch: 480 , training loss: 4.615532
[INFO] Epoch: 13 , batch: 481 , training loss: 4.456486
[INFO] Epoch: 13 , batch: 482 , training loss: 4.609201
[INFO] Epoch: 13 , batch: 483 , training loss: 4.446317
[INFO] Epoch: 13 , batch: 484 , training loss: 4.254676
[INFO] Epoch: 13 , batch: 485 , training loss: 4.338250
[INFO] Epoch: 13 , batch: 486 , training loss: 4.214058
[INFO] Epoch: 13 , batch: 487 , training loss: 4.210991
[INFO] Epoch: 13 , batch: 488 , training loss: 4.424865
[INFO] Epoch: 13 , batch: 489 , training loss: 4.276908
[INFO] Epoch: 13 , batch: 490 , training loss: 4.330284
[INFO] Epoch: 13 , batch: 491 , training loss: 4.312408
[INFO] Epoch: 13 , batch: 492 , training loss: 4.239651
[INFO] Epoch: 13 , batch: 493 , training loss: 4.430967
[INFO] Epoch: 13 , batch: 494 , training loss: 4.365692
[INFO] Epoch: 13 , batch: 495 , training loss: 4.470872
[INFO] Epoch: 13 , batch: 496 , training loss: 4.344338
[INFO] Epoch: 13 , batch: 497 , training loss: 4.377531
[INFO] Epoch: 13 , batch: 498 , training loss: 4.398049
[INFO] Epoch: 13 , batch: 499 , training loss: 4.467817
[INFO] Epoch: 13 , batch: 500 , training loss: 4.633904
[INFO] Epoch: 13 , batch: 501 , training loss: 4.968052
[INFO] Epoch: 13 , batch: 502 , training loss: 5.092368
[INFO] Epoch: 13 , batch: 503 , training loss: 4.813105
[INFO] Epoch: 13 , batch: 504 , training loss: 4.914241
[INFO] Epoch: 13 , batch: 505 , training loss: 4.862735
[INFO] Epoch: 13 , batch: 506 , training loss: 4.799856
[INFO] Epoch: 13 , batch: 507 , training loss: 4.864153
[INFO] Epoch: 13 , batch: 508 , training loss: 4.832398
[INFO] Epoch: 13 , batch: 509 , training loss: 4.564746
[INFO] Epoch: 13 , batch: 510 , training loss: 4.609424
[INFO] Epoch: 13 , batch: 511 , training loss: 4.519628
[INFO] Epoch: 13 , batch: 512 , training loss: 4.610909
[INFO] Epoch: 13 , batch: 513 , training loss: 4.875863
[INFO] Epoch: 13 , batch: 514 , training loss: 4.521410
[INFO] Epoch: 13 , batch: 515 , training loss: 4.757213
[INFO] Epoch: 13 , batch: 516 , training loss: 4.562551
[INFO] Epoch: 13 , batch: 517 , training loss: 4.524201
[INFO] Epoch: 13 , batch: 518 , training loss: 4.508428
[INFO] Epoch: 13 , batch: 519 , training loss: 4.358371
[INFO] Epoch: 13 , batch: 520 , training loss: 4.582796
[INFO] Epoch: 13 , batch: 521 , training loss: 4.567969
[INFO] Epoch: 13 , batch: 522 , training loss: 4.623963
[INFO] Epoch: 13 , batch: 523 , training loss: 4.543360
[INFO] Epoch: 13 , batch: 524 , training loss: 4.818972
[INFO] Epoch: 13 , batch: 525 , training loss: 4.715580
[INFO] Epoch: 13 , batch: 526 , training loss: 4.492377
[INFO] Epoch: 13 , batch: 527 , training loss: 4.526985
[INFO] Epoch: 13 , batch: 528 , training loss: 4.584081
[INFO] Epoch: 13 , batch: 529 , training loss: 4.532744
[INFO] Epoch: 13 , batch: 530 , training loss: 4.386942
[INFO] Epoch: 13 , batch: 531 , training loss: 4.526966
[INFO] Epoch: 13 , batch: 532 , training loss: 4.414452
[INFO] Epoch: 13 , batch: 533 , training loss: 4.589099
[INFO] Epoch: 13 , batch: 534 , training loss: 4.550283
[INFO] Epoch: 13 , batch: 535 , training loss: 4.586669
[INFO] Epoch: 13 , batch: 536 , training loss: 4.403278
[INFO] Epoch: 13 , batch: 537 , training loss: 4.414181
[INFO] Epoch: 13 , batch: 538 , training loss: 4.472002
[INFO] Epoch: 13 , batch: 539 , training loss: 4.596515
[INFO] Epoch: 13 , batch: 540 , training loss: 5.141674
[INFO] Epoch: 13 , batch: 541 , training loss: 5.055452
[INFO] Epoch: 13 , batch: 542 , training loss: 4.926156
[INFO] Epoch: 14 , batch: 0 , training loss: 3.923539
[INFO] Epoch: 14 , batch: 1 , training loss: 3.830721
[INFO] Epoch: 14 , batch: 2 , training loss: 3.891203
[INFO] Epoch: 14 , batch: 3 , training loss: 3.746686
[INFO] Epoch: 14 , batch: 4 , training loss: 4.140494
[INFO] Epoch: 14 , batch: 5 , training loss: 3.691822
[INFO] Epoch: 14 , batch: 6 , training loss: 4.185317
[INFO] Epoch: 14 , batch: 7 , training loss: 3.957708
[INFO] Epoch: 14 , batch: 8 , training loss: 3.696479
[INFO] Epoch: 14 , batch: 9 , training loss: 3.872429
[INFO] Epoch: 14 , batch: 10 , training loss: 3.826228
[INFO] Epoch: 14 , batch: 11 , training loss: 3.775432
[INFO] Epoch: 14 , batch: 12 , training loss: 3.717286
[INFO] Epoch: 14 , batch: 13 , training loss: 3.738835
[INFO] Epoch: 14 , batch: 14 , training loss: 3.567364
[INFO] Epoch: 14 , batch: 15 , training loss: 3.810254
[INFO] Epoch: 14 , batch: 16 , training loss: 3.635083
[INFO] Epoch: 14 , batch: 17 , training loss: 3.838407
[INFO] Epoch: 14 , batch: 18 , training loss: 3.815882
[INFO] Epoch: 14 , batch: 19 , training loss: 3.523845
[INFO] Epoch: 14 , batch: 20 , training loss: 3.485468
[INFO] Epoch: 14 , batch: 21 , training loss: 3.639000
[INFO] Epoch: 14 , batch: 22 , training loss: 3.564317
[INFO] Epoch: 14 , batch: 23 , training loss: 3.749123
[INFO] Epoch: 14 , batch: 24 , training loss: 3.568997
[INFO] Epoch: 14 , batch: 25 , training loss: 3.792706
[INFO] Epoch: 14 , batch: 26 , training loss: 3.593488
[INFO] Epoch: 14 , batch: 27 , training loss: 3.554244
[INFO] Epoch: 14 , batch: 28 , training loss: 3.746832
[INFO] Epoch: 14 , batch: 29 , training loss: 3.574969
[INFO] Epoch: 14 , batch: 30 , training loss: 3.608383
[INFO] Epoch: 14 , batch: 31 , training loss: 3.694813
[INFO] Epoch: 14 , batch: 32 , training loss: 3.673597
[INFO] Epoch: 14 , batch: 33 , training loss: 3.721658
[INFO] Epoch: 14 , batch: 34 , training loss: 3.717971
[INFO] Epoch: 14 , batch: 35 , training loss: 3.690394
[INFO] Epoch: 14 , batch: 36 , training loss: 3.705190
[INFO] Epoch: 14 , batch: 37 , training loss: 3.594771
[INFO] Epoch: 14 , batch: 38 , training loss: 3.664793
[INFO] Epoch: 14 , batch: 39 , training loss: 3.533154
[INFO] Epoch: 14 , batch: 40 , training loss: 3.722029
[INFO] Epoch: 14 , batch: 41 , training loss: 3.680282
[INFO] Epoch: 14 , batch: 42 , training loss: 4.172410
[INFO] Epoch: 14 , batch: 43 , training loss: 3.772317
[INFO] Epoch: 14 , batch: 44 , training loss: 4.169042
[INFO] Epoch: 14 , batch: 45 , training loss: 4.108816
[INFO] Epoch: 14 , batch: 46 , training loss: 4.187688
[INFO] Epoch: 14 , batch: 47 , training loss: 3.855044
[INFO] Epoch: 14 , batch: 48 , training loss: 3.812139
[INFO] Epoch: 14 , batch: 49 , training loss: 3.957908
[INFO] Epoch: 14 , batch: 50 , training loss: 3.710618
[INFO] Epoch: 14 , batch: 51 , training loss: 3.916971
[INFO] Epoch: 14 , batch: 52 , training loss: 3.759809
[INFO] Epoch: 14 , batch: 53 , training loss: 3.929307
[INFO] Epoch: 14 , batch: 54 , training loss: 3.946638
[INFO] Epoch: 14 , batch: 55 , training loss: 3.973197
[INFO] Epoch: 14 , batch: 56 , training loss: 3.849396
[INFO] Epoch: 14 , batch: 57 , training loss: 3.762410
[INFO] Epoch: 14 , batch: 58 , training loss: 3.835961
[INFO] Epoch: 14 , batch: 59 , training loss: 3.892374
[INFO] Epoch: 14 , batch: 60 , training loss: 3.838867
[INFO] Epoch: 14 , batch: 61 , training loss: 3.930190
[INFO] Epoch: 14 , batch: 62 , training loss: 3.823897
[INFO] Epoch: 14 , batch: 63 , training loss: 3.995062
[INFO] Epoch: 14 , batch: 64 , training loss: 4.144874
[INFO] Epoch: 14 , batch: 65 , training loss: 3.837683
[INFO] Epoch: 14 , batch: 66 , training loss: 3.730051
[INFO] Epoch: 14 , batch: 67 , training loss: 3.772009
[INFO] Epoch: 14 , batch: 68 , training loss: 3.975107
[INFO] Epoch: 14 , batch: 69 , training loss: 3.844760
[INFO] Epoch: 14 , batch: 70 , training loss: 4.060254
[INFO] Epoch: 14 , batch: 71 , training loss: 3.909151
[INFO] Epoch: 14 , batch: 72 , training loss: 3.959410
[INFO] Epoch: 14 , batch: 73 , training loss: 3.880817
[INFO] Epoch: 14 , batch: 74 , training loss: 4.046040
[INFO] Epoch: 14 , batch: 75 , training loss: 3.846019
[INFO] Epoch: 14 , batch: 76 , training loss: 3.964349
[INFO] Epoch: 14 , batch: 77 , training loss: 3.942416
[INFO] Epoch: 14 , batch: 78 , training loss: 4.050159
[INFO] Epoch: 14 , batch: 79 , training loss: 3.834676
[INFO] Epoch: 14 , batch: 80 , training loss: 4.052427
[INFO] Epoch: 14 , batch: 81 , training loss: 3.972494
[INFO] Epoch: 14 , batch: 82 , training loss: 3.953724
[INFO] Epoch: 14 , batch: 83 , training loss: 4.070323
[INFO] Epoch: 14 , batch: 84 , training loss: 4.003636
[INFO] Epoch: 14 , batch: 85 , training loss: 4.068127
[INFO] Epoch: 14 , batch: 86 , training loss: 4.007432
[INFO] Epoch: 14 , batch: 87 , training loss: 4.001419
[INFO] Epoch: 14 , batch: 88 , training loss: 4.146513
[INFO] Epoch: 14 , batch: 89 , training loss: 3.937207
[INFO] Epoch: 14 , batch: 90 , training loss: 3.991427
[INFO] Epoch: 14 , batch: 91 , training loss: 3.933493
[INFO] Epoch: 14 , batch: 92 , training loss: 3.922861
[INFO] Epoch: 14 , batch: 93 , training loss: 4.045027
[INFO] Epoch: 14 , batch: 94 , training loss: 4.239765
[INFO] Epoch: 14 , batch: 95 , training loss: 3.983416
[INFO] Epoch: 14 , batch: 96 , training loss: 3.976662
[INFO] Epoch: 14 , batch: 97 , training loss: 3.889210
[INFO] Epoch: 14 , batch: 98 , training loss: 3.869760
[INFO] Epoch: 14 , batch: 99 , training loss: 3.936882
[INFO] Epoch: 14 , batch: 100 , training loss: 3.852387
[INFO] Epoch: 14 , batch: 101 , training loss: 3.898137
[INFO] Epoch: 14 , batch: 102 , training loss: 4.016842
[INFO] Epoch: 14 , batch: 103 , training loss: 3.814032
[INFO] Epoch: 14 , batch: 104 , training loss: 3.763357
[INFO] Epoch: 14 , batch: 105 , training loss: 4.051758
[INFO] Epoch: 14 , batch: 106 , training loss: 4.021300
[INFO] Epoch: 14 , batch: 107 , training loss: 3.902586
[INFO] Epoch: 14 , batch: 108 , training loss: 3.819275
[INFO] Epoch: 14 , batch: 109 , training loss: 3.719612
[INFO] Epoch: 14 , batch: 110 , training loss: 3.935951
[INFO] Epoch: 14 , batch: 111 , training loss: 4.002611
[INFO] Epoch: 14 , batch: 112 , training loss: 3.935818
[INFO] Epoch: 14 , batch: 113 , training loss: 3.924289
[INFO] Epoch: 14 , batch: 114 , training loss: 3.965518
[INFO] Epoch: 14 , batch: 115 , training loss: 3.960087
[INFO] Epoch: 14 , batch: 116 , training loss: 3.854343
[INFO] Epoch: 14 , batch: 117 , training loss: 4.083576
[INFO] Epoch: 14 , batch: 118 , training loss: 4.019499
[INFO] Epoch: 14 , batch: 119 , training loss: 4.173618
[INFO] Epoch: 14 , batch: 120 , training loss: 4.175940
[INFO] Epoch: 14 , batch: 121 , training loss: 3.979033
[INFO] Epoch: 14 , batch: 122 , training loss: 3.906805
[INFO] Epoch: 14 , batch: 123 , training loss: 3.927999
[INFO] Epoch: 14 , batch: 124 , training loss: 4.090608
[INFO] Epoch: 14 , batch: 125 , training loss: 3.853579
[INFO] Epoch: 14 , batch: 126 , training loss: 3.851954
[INFO] Epoch: 14 , batch: 127 , training loss: 3.864679
[INFO] Epoch: 14 , batch: 128 , training loss: 4.020834
[INFO] Epoch: 14 , batch: 129 , training loss: 3.993605
[INFO] Epoch: 14 , batch: 130 , training loss: 3.946455
[INFO] Epoch: 14 , batch: 131 , training loss: 3.957988
[INFO] Epoch: 14 , batch: 132 , training loss: 3.983284
[INFO] Epoch: 14 , batch: 133 , training loss: 3.917206
[INFO] Epoch: 14 , batch: 134 , training loss: 3.750435
[INFO] Epoch: 14 , batch: 135 , training loss: 3.781638
[INFO] Epoch: 14 , batch: 136 , training loss: 4.080812
[INFO] Epoch: 14 , batch: 137 , training loss: 3.993082
[INFO] Epoch: 14 , batch: 138 , training loss: 4.058008
[INFO] Epoch: 14 , batch: 139 , training loss: 4.696097
[INFO] Epoch: 14 , batch: 140 , training loss: 4.422822
[INFO] Epoch: 14 , batch: 141 , training loss: 4.183359
[INFO] Epoch: 14 , batch: 142 , training loss: 3.913308
[INFO] Epoch: 14 , batch: 143 , training loss: 4.032543
[INFO] Epoch: 14 , batch: 144 , training loss: 3.841426
[INFO] Epoch: 14 , batch: 145 , training loss: 3.939461
[INFO] Epoch: 14 , batch: 146 , training loss: 4.138486
[INFO] Epoch: 14 , batch: 147 , training loss: 3.835801
[INFO] Epoch: 14 , batch: 148 , training loss: 3.787776
[INFO] Epoch: 14 , batch: 149 , training loss: 3.849617
[INFO] Epoch: 14 , batch: 150 , training loss: 4.112210
[INFO] Epoch: 14 , batch: 151 , training loss: 3.939139
[INFO] Epoch: 14 , batch: 152 , training loss: 3.994128
[INFO] Epoch: 14 , batch: 153 , training loss: 4.023281
[INFO] Epoch: 14 , batch: 154 , training loss: 4.104001
[INFO] Epoch: 14 , batch: 155 , training loss: 4.330297
[INFO] Epoch: 14 , batch: 156 , training loss: 4.016646
[INFO] Epoch: 14 , batch: 157 , training loss: 3.992083
[INFO] Epoch: 14 , batch: 158 , training loss: 4.192399
[INFO] Epoch: 14 , batch: 159 , training loss: 4.039495
[INFO] Epoch: 14 , batch: 160 , training loss: 4.450150
[INFO] Epoch: 14 , batch: 161 , training loss: 4.501621
[INFO] Epoch: 14 , batch: 162 , training loss: 4.440851
[INFO] Epoch: 14 , batch: 163 , training loss: 4.526032
[INFO] Epoch: 14 , batch: 164 , training loss: 4.540883
[INFO] Epoch: 14 , batch: 165 , training loss: 4.427887
[INFO] Epoch: 14 , batch: 166 , training loss: 4.374033
[INFO] Epoch: 14 , batch: 167 , training loss: 4.603015
[INFO] Epoch: 14 , batch: 168 , training loss: 4.314417
[INFO] Epoch: 14 , batch: 169 , training loss: 4.251101
[INFO] Epoch: 14 , batch: 170 , training loss: 4.381176
[INFO] Epoch: 14 , batch: 171 , training loss: 3.788782
[INFO] Epoch: 14 , batch: 172 , training loss: 4.000090
[INFO] Epoch: 14 , batch: 173 , training loss: 4.328847
[INFO] Epoch: 14 , batch: 174 , training loss: 4.610819
[INFO] Epoch: 14 , batch: 175 , training loss: 4.978810
[INFO] Epoch: 14 , batch: 176 , training loss: 4.689108
[INFO] Epoch: 14 , batch: 177 , training loss: 4.288476
[INFO] Epoch: 14 , batch: 178 , training loss: 4.315766
[INFO] Epoch: 14 , batch: 179 , training loss: 4.332790
[INFO] Epoch: 14 , batch: 180 , training loss: 4.271142
[INFO] Epoch: 14 , batch: 181 , training loss: 4.550537
[INFO] Epoch: 14 , batch: 182 , training loss: 4.457353
[INFO] Epoch: 14 , batch: 183 , training loss: 4.447084
[INFO] Epoch: 14 , batch: 184 , training loss: 4.313396
[INFO] Epoch: 14 , batch: 185 , training loss: 4.271191
[INFO] Epoch: 14 , batch: 186 , training loss: 4.430372
[INFO] Epoch: 14 , batch: 187 , training loss: 4.529134
[INFO] Epoch: 14 , batch: 188 , training loss: 4.526072
[INFO] Epoch: 14 , batch: 189 , training loss: 4.401309
[INFO] Epoch: 14 , batch: 190 , training loss: 4.401931
[INFO] Epoch: 14 , batch: 191 , training loss: 4.531847
[INFO] Epoch: 14 , batch: 192 , training loss: 4.328032
[INFO] Epoch: 14 , batch: 193 , training loss: 4.449686
[INFO] Epoch: 14 , batch: 194 , training loss: 4.388221
[INFO] Epoch: 14 , batch: 195 , training loss: 4.294457
[INFO] Epoch: 14 , batch: 196 , training loss: 4.165614
[INFO] Epoch: 14 , batch: 197 , training loss: 4.290229
[INFO] Epoch: 14 , batch: 198 , training loss: 4.218259
[INFO] Epoch: 14 , batch: 199 , training loss: 4.319565
[INFO] Epoch: 14 , batch: 200 , training loss: 4.241296
[INFO] Epoch: 14 , batch: 201 , training loss: 4.135602
[INFO] Epoch: 14 , batch: 202 , training loss: 4.124773
[INFO] Epoch: 14 , batch: 203 , training loss: 4.201915
[INFO] Epoch: 14 , batch: 204 , training loss: 4.357745
[INFO] Epoch: 14 , batch: 205 , training loss: 3.907737
[INFO] Epoch: 14 , batch: 206 , training loss: 3.858115
[INFO] Epoch: 14 , batch: 207 , training loss: 3.862902
[INFO] Epoch: 14 , batch: 208 , training loss: 4.187861
[INFO] Epoch: 14 , batch: 209 , training loss: 4.161795
[INFO] Epoch: 14 , batch: 210 , training loss: 4.146199
[INFO] Epoch: 14 , batch: 211 , training loss: 4.143063
[INFO] Epoch: 14 , batch: 212 , training loss: 4.261924
[INFO] Epoch: 14 , batch: 213 , training loss: 4.237922
[INFO] Epoch: 14 , batch: 214 , training loss: 4.304562
[INFO] Epoch: 14 , batch: 215 , training loss: 4.502963
[INFO] Epoch: 14 , batch: 216 , training loss: 4.213695
[INFO] Epoch: 14 , batch: 217 , training loss: 4.157346
[INFO] Epoch: 14 , batch: 218 , training loss: 4.151862
[INFO] Epoch: 14 , batch: 219 , training loss: 4.255602
[INFO] Epoch: 14 , batch: 220 , training loss: 4.068948
[INFO] Epoch: 14 , batch: 221 , training loss: 4.073134
[INFO] Epoch: 14 , batch: 222 , training loss: 4.241462
[INFO] Epoch: 14 , batch: 223 , training loss: 4.305653
[INFO] Epoch: 14 , batch: 224 , training loss: 4.351661
[INFO] Epoch: 14 , batch: 225 , training loss: 4.234533
[INFO] Epoch: 14 , batch: 226 , training loss: 4.388793
[INFO] Epoch: 14 , batch: 227 , training loss: 4.334197
[INFO] Epoch: 14 , batch: 228 , training loss: 4.386902
[INFO] Epoch: 14 , batch: 229 , training loss: 4.239687
[INFO] Epoch: 14 , batch: 230 , training loss: 4.086887
[INFO] Epoch: 14 , batch: 231 , training loss: 3.956127
[INFO] Epoch: 14 , batch: 232 , training loss: 4.101023
[INFO] Epoch: 14 , batch: 233 , training loss: 4.115394
[INFO] Epoch: 14 , batch: 234 , training loss: 3.800694
[INFO] Epoch: 14 , batch: 235 , training loss: 3.897765
[INFO] Epoch: 14 , batch: 236 , training loss: 4.044662
[INFO] Epoch: 14 , batch: 237 , training loss: 4.243536
[INFO] Epoch: 14 , batch: 238 , training loss: 3.980591
[INFO] Epoch: 14 , batch: 239 , training loss: 4.045640
[INFO] Epoch: 14 , batch: 240 , training loss: 4.111673
[INFO] Epoch: 14 , batch: 241 , training loss: 3.901478
[INFO] Epoch: 14 , batch: 242 , training loss: 3.928293
[INFO] Epoch: 14 , batch: 243 , training loss: 4.247579
[INFO] Epoch: 14 , batch: 244 , training loss: 4.146523
[INFO] Epoch: 14 , batch: 245 , training loss: 4.138648
[INFO] Epoch: 14 , batch: 246 , training loss: 3.822322
[INFO] Epoch: 14 , batch: 247 , training loss: 4.007835
[INFO] Epoch: 14 , batch: 248 , training loss: 4.078376
[INFO] Epoch: 14 , batch: 249 , training loss: 4.070949
[INFO] Epoch: 14 , batch: 250 , training loss: 3.830353
[INFO] Epoch: 14 , batch: 251 , training loss: 4.303474
[INFO] Epoch: 14 , batch: 252 , training loss: 4.015558
[INFO] Epoch: 14 , batch: 253 , training loss: 3.930715
[INFO] Epoch: 14 , batch: 254 , training loss: 4.224593
[INFO] Epoch: 14 , batch: 255 , training loss: 4.195747
[INFO] Epoch: 14 , batch: 256 , training loss: 4.208089
[INFO] Epoch: 14 , batch: 257 , training loss: 4.341426
[INFO] Epoch: 14 , batch: 258 , training loss: 4.392457
[INFO] Epoch: 14 , batch: 259 , training loss: 4.459650
[INFO] Epoch: 14 , batch: 260 , training loss: 4.151762
[INFO] Epoch: 14 , batch: 261 , training loss: 4.296659
[INFO] Epoch: 14 , batch: 262 , training loss: 4.521871
[INFO] Epoch: 14 , batch: 263 , training loss: 4.683037
[INFO] Epoch: 14 , batch: 264 , training loss: 3.982008
[INFO] Epoch: 14 , batch: 265 , training loss: 4.123238
[INFO] Epoch: 14 , batch: 266 , training loss: 4.555263
[INFO] Epoch: 14 , batch: 267 , training loss: 4.296001
[INFO] Epoch: 14 , batch: 268 , training loss: 4.197468
[INFO] Epoch: 14 , batch: 269 , training loss: 4.176155
[INFO] Epoch: 14 , batch: 270 , training loss: 4.201521
[INFO] Epoch: 14 , batch: 271 , training loss: 4.242821
[INFO] Epoch: 14 , batch: 272 , training loss: 4.226886
[INFO] Epoch: 14 , batch: 273 , training loss: 4.239432
[INFO] Epoch: 14 , batch: 274 , training loss: 4.328416
[INFO] Epoch: 14 , batch: 275 , training loss: 4.200266
[INFO] Epoch: 14 , batch: 276 , training loss: 4.258117
[INFO] Epoch: 14 , batch: 277 , training loss: 4.422452
[INFO] Epoch: 14 , batch: 278 , training loss: 4.078041
[INFO] Epoch: 14 , batch: 279 , training loss: 4.078498
[INFO] Epoch: 14 , batch: 280 , training loss: 4.036176
[INFO] Epoch: 14 , batch: 281 , training loss: 4.184746
[INFO] Epoch: 14 , batch: 282 , training loss: 4.075642
[INFO] Epoch: 14 , batch: 283 , training loss: 4.106827
[INFO] Epoch: 14 , batch: 284 , training loss: 4.139600
[INFO] Epoch: 14 , batch: 285 , training loss: 4.090940
[INFO] Epoch: 14 , batch: 286 , training loss: 4.080217
[INFO] Epoch: 14 , batch: 287 , training loss: 4.015158
[INFO] Epoch: 14 , batch: 288 , training loss: 4.034093
[INFO] Epoch: 14 , batch: 289 , training loss: 4.096402
[INFO] Epoch: 14 , batch: 290 , training loss: 3.855885
[INFO] Epoch: 14 , batch: 291 , training loss: 3.834062
[INFO] Epoch: 14 , batch: 292 , training loss: 3.940122
[INFO] Epoch: 14 , batch: 293 , training loss: 3.850567
[INFO] Epoch: 14 , batch: 294 , training loss: 4.542306
[INFO] Epoch: 14 , batch: 295 , training loss: 4.307876
[INFO] Epoch: 14 , batch: 296 , training loss: 4.241488
[INFO] Epoch: 14 , batch: 297 , training loss: 4.195238
[INFO] Epoch: 14 , batch: 298 , training loss: 4.037231
[INFO] Epoch: 14 , batch: 299 , training loss: 4.048553
[INFO] Epoch: 14 , batch: 300 , training loss: 4.019188
[INFO] Epoch: 14 , batch: 301 , training loss: 3.947753
[INFO] Epoch: 14 , batch: 302 , training loss: 4.125819
[INFO] Epoch: 14 , batch: 303 , training loss: 4.151699
[INFO] Epoch: 14 , batch: 304 , training loss: 4.324754
[INFO] Epoch: 14 , batch: 305 , training loss: 4.091879
[INFO] Epoch: 14 , batch: 306 , training loss: 4.223334
[INFO] Epoch: 14 , batch: 307 , training loss: 4.230013
[INFO] Epoch: 14 , batch: 308 , training loss: 4.086690
[INFO] Epoch: 14 , batch: 309 , training loss: 4.099709
[INFO] Epoch: 14 , batch: 310 , training loss: 3.948420
[INFO] Epoch: 14 , batch: 311 , training loss: 3.974974
[INFO] Epoch: 14 , batch: 312 , training loss: 3.869565
[INFO] Epoch: 14 , batch: 313 , training loss: 4.007836
[INFO] Epoch: 14 , batch: 314 , training loss: 4.058300
[INFO] Epoch: 14 , batch: 315 , training loss: 4.111314
[INFO] Epoch: 14 , batch: 316 , training loss: 4.370773
[INFO] Epoch: 14 , batch: 317 , training loss: 4.872530
[INFO] Epoch: 14 , batch: 318 , training loss: 5.012271
[INFO] Epoch: 14 , batch: 319 , training loss: 4.545035
[INFO] Epoch: 14 , batch: 320 , training loss: 4.091078
[INFO] Epoch: 14 , batch: 321 , training loss: 3.915250
[INFO] Epoch: 14 , batch: 322 , training loss: 4.038978
[INFO] Epoch: 14 , batch: 323 , training loss: 4.051597
[INFO] Epoch: 14 , batch: 324 , training loss: 4.017584
[INFO] Epoch: 14 , batch: 325 , training loss: 4.185665
[INFO] Epoch: 14 , batch: 326 , training loss: 4.232085
[INFO] Epoch: 14 , batch: 327 , training loss: 4.143967
[INFO] Epoch: 14 , batch: 328 , training loss: 4.131351
[INFO] Epoch: 14 , batch: 329 , training loss: 4.054879
[INFO] Epoch: 14 , batch: 330 , training loss: 4.041140
[INFO] Epoch: 14 , batch: 331 , training loss: 4.192505
[INFO] Epoch: 14 , batch: 332 , training loss: 4.002328
[INFO] Epoch: 14 , batch: 333 , training loss: 4.009943
[INFO] Epoch: 14 , batch: 334 , training loss: 4.028854
[INFO] Epoch: 14 , batch: 335 , training loss: 4.159267
[INFO] Epoch: 14 , batch: 336 , training loss: 4.174838
[INFO] Epoch: 14 , batch: 337 , training loss: 4.187319
[INFO] Epoch: 14 , batch: 338 , training loss: 4.425708
[INFO] Epoch: 14 , batch: 339 , training loss: 4.249167
[INFO] Epoch: 14 , batch: 340 , training loss: 4.420702
[INFO] Epoch: 14 , batch: 341 , training loss: 4.182486
[INFO] Epoch: 14 , batch: 342 , training loss: 3.964196
[INFO] Epoch: 14 , batch: 343 , training loss: 4.041850
[INFO] Epoch: 14 , batch: 344 , training loss: 3.893216
[INFO] Epoch: 14 , batch: 345 , training loss: 4.031159
[INFO] Epoch: 14 , batch: 346 , training loss: 4.101379
[INFO] Epoch: 14 , batch: 347 , training loss: 3.987214
[INFO] Epoch: 14 , batch: 348 , training loss: 4.134030
[INFO] Epoch: 14 , batch: 349 , training loss: 4.262403
[INFO] Epoch: 14 , batch: 350 , training loss: 4.024613
[INFO] Epoch: 14 , batch: 351 , training loss: 4.079892
[INFO] Epoch: 14 , batch: 352 , training loss: 4.106231
[INFO] Epoch: 14 , batch: 353 , training loss: 4.076968
[INFO] Epoch: 14 , batch: 354 , training loss: 4.211168
[INFO] Epoch: 14 , batch: 355 , training loss: 4.240542
[INFO] Epoch: 14 , batch: 356 , training loss: 4.055868
[INFO] Epoch: 14 , batch: 357 , training loss: 4.163238
[INFO] Epoch: 14 , batch: 358 , training loss: 4.072506
[INFO] Epoch: 14 , batch: 359 , training loss: 4.046865
[INFO] Epoch: 14 , batch: 360 , training loss: 4.124570
[INFO] Epoch: 14 , batch: 361 , training loss: 4.100583
[INFO] Epoch: 14 , batch: 362 , training loss: 4.200607
[INFO] Epoch: 14 , batch: 363 , training loss: 4.087571
[INFO] Epoch: 14 , batch: 364 , training loss: 4.116834
[INFO] Epoch: 14 , batch: 365 , training loss: 4.055861
[INFO] Epoch: 14 , batch: 366 , training loss: 4.184646
[INFO] Epoch: 14 , batch: 367 , training loss: 4.284067
[INFO] Epoch: 14 , batch: 368 , training loss: 4.743907
[INFO] Epoch: 14 , batch: 369 , training loss: 4.366788
[INFO] Epoch: 14 , batch: 370 , training loss: 4.119763
[INFO] Epoch: 14 , batch: 371 , training loss: 4.553741
[INFO] Epoch: 14 , batch: 372 , training loss: 4.851391
[INFO] Epoch: 14 , batch: 373 , training loss: 4.898840
[INFO] Epoch: 14 , batch: 374 , training loss: 4.977272
[INFO] Epoch: 14 , batch: 375 , training loss: 4.947617
[INFO] Epoch: 14 , batch: 376 , training loss: 4.880954
[INFO] Epoch: 14 , batch: 377 , training loss: 4.580163
[INFO] Epoch: 14 , batch: 378 , training loss: 4.694124
[INFO] Epoch: 14 , batch: 379 , training loss: 4.682903
[INFO] Epoch: 14 , batch: 380 , training loss: 4.808809
[INFO] Epoch: 14 , batch: 381 , training loss: 4.553701
[INFO] Epoch: 14 , batch: 382 , training loss: 4.796794
[INFO] Epoch: 14 , batch: 383 , training loss: 4.831681
[INFO] Epoch: 14 , batch: 384 , training loss: 4.842475
[INFO] Epoch: 14 , batch: 385 , training loss: 4.606793
[INFO] Epoch: 14 , batch: 386 , training loss: 4.789844
[INFO] Epoch: 14 , batch: 387 , training loss: 4.739900
[INFO] Epoch: 14 , batch: 388 , training loss: 4.567347
[INFO] Epoch: 14 , batch: 389 , training loss: 4.401789
[INFO] Epoch: 14 , batch: 390 , training loss: 4.373024
[INFO] Epoch: 14 , batch: 391 , training loss: 4.418667
[INFO] Epoch: 14 , batch: 392 , training loss: 4.776527
[INFO] Epoch: 14 , batch: 393 , training loss: 4.640748
[INFO] Epoch: 14 , batch: 394 , training loss: 4.686195
[INFO] Epoch: 14 , batch: 395 , training loss: 4.551034
[INFO] Epoch: 14 , batch: 396 , training loss: 4.323556
[INFO] Epoch: 14 , batch: 397 , training loss: 4.490926
[INFO] Epoch: 14 , batch: 398 , training loss: 4.342587
[INFO] Epoch: 14 , batch: 399 , training loss: 4.409377
[INFO] Epoch: 14 , batch: 400 , training loss: 4.369731
[INFO] Epoch: 14 , batch: 401 , training loss: 4.787661
[INFO] Epoch: 14 , batch: 402 , training loss: 4.510438
[INFO] Epoch: 14 , batch: 403 , training loss: 4.325356
[INFO] Epoch: 14 , batch: 404 , training loss: 4.512626
[INFO] Epoch: 14 , batch: 405 , training loss: 4.546540
[INFO] Epoch: 14 , batch: 406 , training loss: 4.456888
[INFO] Epoch: 14 , batch: 407 , training loss: 4.525908
[INFO] Epoch: 14 , batch: 408 , training loss: 4.470623
[INFO] Epoch: 14 , batch: 409 , training loss: 4.482961
[INFO] Epoch: 14 , batch: 410 , training loss: 4.516047
[INFO] Epoch: 14 , batch: 411 , training loss: 4.712875
[INFO] Epoch: 14 , batch: 412 , training loss: 4.543675
[INFO] Epoch: 14 , batch: 413 , training loss: 4.415832
[INFO] Epoch: 14 , batch: 414 , training loss: 4.435876
[INFO] Epoch: 14 , batch: 415 , training loss: 4.441635
[INFO] Epoch: 14 , batch: 416 , training loss: 4.545464
[INFO] Epoch: 14 , batch: 417 , training loss: 4.469231
[INFO] Epoch: 14 , batch: 418 , training loss: 4.491368
[INFO] Epoch: 14 , batch: 419 , training loss: 4.457144
[INFO] Epoch: 14 , batch: 420 , training loss: 4.433018
[INFO] Epoch: 14 , batch: 421 , training loss: 4.426938
[INFO] Epoch: 14 , batch: 422 , training loss: 4.313984
[INFO] Epoch: 14 , batch: 423 , training loss: 4.529756
[INFO] Epoch: 14 , batch: 424 , training loss: 4.688774
[INFO] Epoch: 14 , batch: 425 , training loss: 4.545421
[INFO] Epoch: 14 , batch: 426 , training loss: 4.269752
[INFO] Epoch: 14 , batch: 427 , training loss: 4.516622
[INFO] Epoch: 14 , batch: 428 , training loss: 4.401110
[INFO] Epoch: 14 , batch: 429 , training loss: 4.242771
[INFO] Epoch: 14 , batch: 430 , training loss: 4.527561
[INFO] Epoch: 14 , batch: 431 , training loss: 4.106662
[INFO] Epoch: 14 , batch: 432 , training loss: 4.192111
[INFO] Epoch: 14 , batch: 433 , training loss: 4.210043
[INFO] Epoch: 14 , batch: 434 , training loss: 4.066256
[INFO] Epoch: 14 , batch: 435 , training loss: 4.439525
[INFO] Epoch: 14 , batch: 436 , training loss: 4.532951
[INFO] Epoch: 14 , batch: 437 , training loss: 4.270466
[INFO] Epoch: 14 , batch: 438 , training loss: 4.108287
[INFO] Epoch: 14 , batch: 439 , training loss: 4.346071
[INFO] Epoch: 14 , batch: 440 , training loss: 4.474032
[INFO] Epoch: 14 , batch: 441 , training loss: 4.584249
[INFO] Epoch: 14 , batch: 442 , training loss: 4.347891
[INFO] Epoch: 14 , batch: 443 , training loss: 4.539504
[INFO] Epoch: 14 , batch: 444 , training loss: 4.139258
[INFO] Epoch: 14 , batch: 445 , training loss: 4.045241
[INFO] Epoch: 14 , batch: 446 , training loss: 3.976450
[INFO] Epoch: 14 , batch: 447 , training loss: 4.163388
[INFO] Epoch: 14 , batch: 448 , training loss: 4.295895
[INFO] Epoch: 14 , batch: 449 , training loss: 4.723486
[INFO] Epoch: 14 , batch: 450 , training loss: 4.744370
[INFO] Epoch: 14 , batch: 451 , training loss: 4.656667
[INFO] Epoch: 14 , batch: 452 , training loss: 4.445854
[INFO] Epoch: 14 , batch: 453 , training loss: 4.217078
[INFO] Epoch: 14 , batch: 454 , training loss: 4.364107
[INFO] Epoch: 14 , batch: 455 , training loss: 4.399319
[INFO] Epoch: 14 , batch: 456 , training loss: 4.391312
[INFO] Epoch: 14 , batch: 457 , training loss: 4.505633
[INFO] Epoch: 14 , batch: 458 , training loss: 4.232907
[INFO] Epoch: 14 , batch: 459 , training loss: 4.200048
[INFO] Epoch: 14 , batch: 460 , training loss: 4.318698
[INFO] Epoch: 14 , batch: 461 , training loss: 4.289168
[INFO] Epoch: 14 , batch: 462 , training loss: 4.353337
[INFO] Epoch: 14 , batch: 463 , training loss: 4.242379
[INFO] Epoch: 14 , batch: 464 , training loss: 4.461193
[INFO] Epoch: 14 , batch: 465 , training loss: 4.371554
[INFO] Epoch: 14 , batch: 466 , training loss: 4.452430
[INFO] Epoch: 14 , batch: 467 , training loss: 4.443383
[INFO] Epoch: 14 , batch: 468 , training loss: 4.409903
[INFO] Epoch: 14 , batch: 469 , training loss: 4.441308
[INFO] Epoch: 14 , batch: 470 , training loss: 4.240079
[INFO] Epoch: 14 , batch: 471 , training loss: 4.329190
[INFO] Epoch: 14 , batch: 472 , training loss: 4.370447
[INFO] Epoch: 14 , batch: 473 , training loss: 4.304873
[INFO] Epoch: 14 , batch: 474 , training loss: 4.096227
[INFO] Epoch: 14 , batch: 475 , training loss: 3.986450
[INFO] Epoch: 14 , batch: 476 , training loss: 4.373695
[INFO] Epoch: 14 , batch: 477 , training loss: 4.484531
[INFO] Epoch: 14 , batch: 478 , training loss: 4.534325
[INFO] Epoch: 14 , batch: 479 , training loss: 4.487566
[INFO] Epoch: 14 , batch: 480 , training loss: 4.610324
[INFO] Epoch: 14 , batch: 481 , training loss: 4.451710
[INFO] Epoch: 14 , batch: 482 , training loss: 4.600693
[INFO] Epoch: 14 , batch: 483 , training loss: 4.434040
[INFO] Epoch: 14 , batch: 484 , training loss: 4.248352
[INFO] Epoch: 14 , batch: 485 , training loss: 4.334293
[INFO] Epoch: 14 , batch: 486 , training loss: 4.204261
[INFO] Epoch: 14 , batch: 487 , training loss: 4.192334
[INFO] Epoch: 14 , batch: 488 , training loss: 4.415947
[INFO] Epoch: 14 , batch: 489 , training loss: 4.278957
[INFO] Epoch: 14 , batch: 490 , training loss: 4.335755
[INFO] Epoch: 14 , batch: 491 , training loss: 4.302712
[INFO] Epoch: 14 , batch: 492 , training loss: 4.236446
[INFO] Epoch: 14 , batch: 493 , training loss: 4.426203
[INFO] Epoch: 14 , batch: 494 , training loss: 4.361807
[INFO] Epoch: 14 , batch: 495 , training loss: 4.465364
[INFO] Epoch: 14 , batch: 496 , training loss: 4.341022
[INFO] Epoch: 14 , batch: 497 , training loss: 4.384214
[INFO] Epoch: 14 , batch: 498 , training loss: 4.394507
[INFO] Epoch: 14 , batch: 499 , training loss: 4.452199
[INFO] Epoch: 14 , batch: 500 , training loss: 4.628718
[INFO] Epoch: 14 , batch: 501 , training loss: 4.955099
[INFO] Epoch: 14 , batch: 502 , training loss: 5.084444
[INFO] Epoch: 14 , batch: 503 , training loss: 4.800178
[INFO] Epoch: 14 , batch: 504 , training loss: 4.894726
[INFO] Epoch: 14 , batch: 505 , training loss: 4.847921
[INFO] Epoch: 14 , batch: 506 , training loss: 4.789636
[INFO] Epoch: 14 , batch: 507 , training loss: 4.853404
[INFO] Epoch: 14 , batch: 508 , training loss: 4.795325
[INFO] Epoch: 14 , batch: 509 , training loss: 4.552473
[INFO] Epoch: 14 , batch: 510 , training loss: 4.620206
[INFO] Epoch: 14 , batch: 511 , training loss: 4.520158
[INFO] Epoch: 14 , batch: 512 , training loss: 4.607363
[INFO] Epoch: 14 , batch: 513 , training loss: 4.871803
[INFO] Epoch: 14 , batch: 514 , training loss: 4.507154
[INFO] Epoch: 14 , batch: 515 , training loss: 4.753040
[INFO] Epoch: 14 , batch: 516 , training loss: 4.560289
[INFO] Epoch: 14 , batch: 517 , training loss: 4.518398
[INFO] Epoch: 14 , batch: 518 , training loss: 4.509706
[INFO] Epoch: 14 , batch: 519 , training loss: 4.356493
[INFO] Epoch: 14 , batch: 520 , training loss: 4.574973
[INFO] Epoch: 14 , batch: 521 , training loss: 4.557129
[INFO] Epoch: 14 , batch: 522 , training loss: 4.610522
[INFO] Epoch: 14 , batch: 523 , training loss: 4.536747
[INFO] Epoch: 14 , batch: 524 , training loss: 4.817387
[INFO] Epoch: 14 , batch: 525 , training loss: 4.728717
[INFO] Epoch: 14 , batch: 526 , training loss: 4.489275
[INFO] Epoch: 14 , batch: 527 , training loss: 4.518862
[INFO] Epoch: 14 , batch: 528 , training loss: 4.572611
[INFO] Epoch: 14 , batch: 529 , training loss: 4.531963
[INFO] Epoch: 14 , batch: 530 , training loss: 4.392419
[INFO] Epoch: 14 , batch: 531 , training loss: 4.528074
[INFO] Epoch: 14 , batch: 532 , training loss: 4.407301
[INFO] Epoch: 14 , batch: 533 , training loss: 4.580953
[INFO] Epoch: 14 , batch: 534 , training loss: 4.553262
[INFO] Epoch: 14 , batch: 535 , training loss: 4.586432
[INFO] Epoch: 14 , batch: 536 , training loss: 4.391304
[INFO] Epoch: 14 , batch: 537 , training loss: 4.409999
[INFO] Epoch: 14 , batch: 538 , training loss: 4.464089
[INFO] Epoch: 14 , batch: 539 , training loss: 4.597560
[INFO] Epoch: 14 , batch: 540 , training loss: 5.133442
[INFO] Epoch: 14 , batch: 541 , training loss: 5.029594
[INFO] Epoch: 14 , batch: 542 , training loss: 4.902523
[INFO] Epoch: 15 , batch: 0 , training loss: 3.851561
[INFO] Epoch: 15 , batch: 1 , training loss: 3.829216
[INFO] Epoch: 15 , batch: 2 , training loss: 3.848513
[INFO] Epoch: 15 , batch: 3 , training loss: 3.744470
[INFO] Epoch: 15 , batch: 4 , training loss: 4.097868
[INFO] Epoch: 15 , batch: 5 , training loss: 3.694839
[INFO] Epoch: 15 , batch: 6 , training loss: 4.133737
[INFO] Epoch: 15 , batch: 7 , training loss: 3.959170
[INFO] Epoch: 15 , batch: 8 , training loss: 3.686951
[INFO] Epoch: 15 , batch: 9 , training loss: 3.836212
[INFO] Epoch: 15 , batch: 10 , training loss: 3.800147
[INFO] Epoch: 15 , batch: 11 , training loss: 3.769680
[INFO] Epoch: 15 , batch: 12 , training loss: 3.690868
[INFO] Epoch: 15 , batch: 13 , training loss: 3.703166
[INFO] Epoch: 15 , batch: 14 , training loss: 3.549515
[INFO] Epoch: 15 , batch: 15 , training loss: 3.784469
[INFO] Epoch: 15 , batch: 16 , training loss: 3.635294
[INFO] Epoch: 15 , batch: 17 , training loss: 3.798455
[INFO] Epoch: 15 , batch: 18 , training loss: 3.777856
[INFO] Epoch: 15 , batch: 19 , training loss: 3.494676
[INFO] Epoch: 15 , batch: 20 , training loss: 3.456962
[INFO] Epoch: 15 , batch: 21 , training loss: 3.603537
[INFO] Epoch: 15 , batch: 22 , training loss: 3.568410
[INFO] Epoch: 15 , batch: 23 , training loss: 3.740606
[INFO] Epoch: 15 , batch: 24 , training loss: 3.565372
[INFO] Epoch: 15 , batch: 25 , training loss: 3.758864
[INFO] Epoch: 15 , batch: 26 , training loss: 3.584259
[INFO] Epoch: 15 , batch: 27 , training loss: 3.527883
[INFO] Epoch: 15 , batch: 28 , training loss: 3.731102
[INFO] Epoch: 15 , batch: 29 , training loss: 3.546874
[INFO] Epoch: 15 , batch: 30 , training loss: 3.597411
[INFO] Epoch: 15 , batch: 31 , training loss: 3.646613
[INFO] Epoch: 15 , batch: 32 , training loss: 3.649141
[INFO] Epoch: 15 , batch: 33 , training loss: 3.714553
[INFO] Epoch: 15 , batch: 34 , training loss: 3.681611
[INFO] Epoch: 15 , batch: 35 , training loss: 3.668064
[INFO] Epoch: 15 , batch: 36 , training loss: 3.668256
[INFO] Epoch: 15 , batch: 37 , training loss: 3.583448
[INFO] Epoch: 15 , batch: 38 , training loss: 3.650666
[INFO] Epoch: 15 , batch: 39 , training loss: 3.516989
[INFO] Epoch: 15 , batch: 40 , training loss: 3.706545
[INFO] Epoch: 15 , batch: 41 , training loss: 3.628146
[INFO] Epoch: 15 , batch: 42 , training loss: 4.101895
[INFO] Epoch: 15 , batch: 43 , training loss: 3.797445
[INFO] Epoch: 15 , batch: 44 , training loss: 4.140185
[INFO] Epoch: 15 , batch: 45 , training loss: 4.104434
[INFO] Epoch: 15 , batch: 46 , training loss: 4.128644
[INFO] Epoch: 15 , batch: 47 , training loss: 3.846556
[INFO] Epoch: 15 , batch: 48 , training loss: 3.798749
[INFO] Epoch: 15 , batch: 49 , training loss: 3.944494
[INFO] Epoch: 15 , batch: 50 , training loss: 3.719530
[INFO] Epoch: 15 , batch: 51 , training loss: 3.916744
[INFO] Epoch: 15 , batch: 52 , training loss: 3.744716
[INFO] Epoch: 15 , batch: 53 , training loss: 3.893212
[INFO] Epoch: 15 , batch: 54 , training loss: 3.912608
[INFO] Epoch: 15 , batch: 55 , training loss: 3.957801
[INFO] Epoch: 15 , batch: 56 , training loss: 3.850656
[INFO] Epoch: 15 , batch: 57 , training loss: 3.734787
[INFO] Epoch: 15 , batch: 58 , training loss: 3.815077
[INFO] Epoch: 15 , batch: 59 , training loss: 3.863793
[INFO] Epoch: 15 , batch: 60 , training loss: 3.806544
[INFO] Epoch: 15 , batch: 61 , training loss: 3.896109
[INFO] Epoch: 15 , batch: 62 , training loss: 3.786025
[INFO] Epoch: 15 , batch: 63 , training loss: 3.984321
[INFO] Epoch: 15 , batch: 64 , training loss: 4.129673
[INFO] Epoch: 15 , batch: 65 , training loss: 3.840127
[INFO] Epoch: 15 , batch: 66 , training loss: 3.726215
[INFO] Epoch: 15 , batch: 67 , training loss: 3.765407
[INFO] Epoch: 15 , batch: 68 , training loss: 3.928910
[INFO] Epoch: 15 , batch: 69 , training loss: 3.797280
[INFO] Epoch: 15 , batch: 70 , training loss: 4.070633
[INFO] Epoch: 15 , batch: 71 , training loss: 3.912541
[INFO] Epoch: 15 , batch: 72 , training loss: 3.947294
[INFO] Epoch: 15 , batch: 73 , training loss: 3.892987
[INFO] Epoch: 15 , batch: 74 , training loss: 4.008664
[INFO] Epoch: 15 , batch: 75 , training loss: 3.851467
[INFO] Epoch: 15 , batch: 76 , training loss: 3.942997
[INFO] Epoch: 15 , batch: 77 , training loss: 3.918430
[INFO] Epoch: 15 , batch: 78 , training loss: 4.006304
[INFO] Epoch: 15 , batch: 79 , training loss: 3.829339
[INFO] Epoch: 15 , batch: 80 , training loss: 4.058878
[INFO] Epoch: 15 , batch: 81 , training loss: 3.963095
[INFO] Epoch: 15 , batch: 82 , training loss: 3.942161
[INFO] Epoch: 15 , batch: 83 , training loss: 4.072053
[INFO] Epoch: 15 , batch: 84 , training loss: 3.997622
[INFO] Epoch: 15 , batch: 85 , training loss: 4.033290
[INFO] Epoch: 15 , batch: 86 , training loss: 4.016737
[INFO] Epoch: 15 , batch: 87 , training loss: 3.993028
[INFO] Epoch: 15 , batch: 88 , training loss: 4.124441
[INFO] Epoch: 15 , batch: 89 , training loss: 3.929576
[INFO] Epoch: 15 , batch: 90 , training loss: 3.971637
[INFO] Epoch: 15 , batch: 91 , training loss: 3.920042
[INFO] Epoch: 15 , batch: 92 , training loss: 3.916643
[INFO] Epoch: 15 , batch: 93 , training loss: 4.052352
[INFO] Epoch: 15 , batch: 94 , training loss: 4.219002
[INFO] Epoch: 15 , batch: 95 , training loss: 3.971631
[INFO] Epoch: 15 , batch: 96 , training loss: 3.953090
[INFO] Epoch: 15 , batch: 97 , training loss: 3.867963
[INFO] Epoch: 15 , batch: 98 , training loss: 3.881994
[INFO] Epoch: 15 , batch: 99 , training loss: 3.912895
[INFO] Epoch: 15 , batch: 100 , training loss: 3.858218
[INFO] Epoch: 15 , batch: 101 , training loss: 3.912583
[INFO] Epoch: 15 , batch: 102 , training loss: 4.016525
[INFO] Epoch: 15 , batch: 103 , training loss: 3.805731
[INFO] Epoch: 15 , batch: 104 , training loss: 3.764483
[INFO] Epoch: 15 , batch: 105 , training loss: 4.053066
[INFO] Epoch: 15 , batch: 106 , training loss: 4.019711
[INFO] Epoch: 15 , batch: 107 , training loss: 3.893537
[INFO] Epoch: 15 , batch: 108 , training loss: 3.808399
[INFO] Epoch: 15 , batch: 109 , training loss: 3.717762
[INFO] Epoch: 15 , batch: 110 , training loss: 3.922592
[INFO] Epoch: 15 , batch: 111 , training loss: 3.992113
[INFO] Epoch: 15 , batch: 112 , training loss: 3.934448
[INFO] Epoch: 15 , batch: 113 , training loss: 3.902074
[INFO] Epoch: 15 , batch: 114 , training loss: 3.955984
[INFO] Epoch: 15 , batch: 115 , training loss: 3.916380
[INFO] Epoch: 15 , batch: 116 , training loss: 3.844761
[INFO] Epoch: 15 , batch: 117 , training loss: 4.053643
[INFO] Epoch: 15 , batch: 118 , training loss: 4.023488
[INFO] Epoch: 15 , batch: 119 , training loss: 4.182346
[INFO] Epoch: 15 , batch: 120 , training loss: 4.158710
[INFO] Epoch: 15 , batch: 121 , training loss: 3.990946
[INFO] Epoch: 15 , batch: 122 , training loss: 3.898328
[INFO] Epoch: 15 , batch: 123 , training loss: 3.916356
[INFO] Epoch: 15 , batch: 124 , training loss: 4.091256
[INFO] Epoch: 15 , batch: 125 , training loss: 3.833857
[INFO] Epoch: 15 , batch: 126 , training loss: 3.845310
[INFO] Epoch: 15 , batch: 127 , training loss: 3.857599
[INFO] Epoch: 15 , batch: 128 , training loss: 3.990897
[INFO] Epoch: 15 , batch: 129 , training loss: 3.984005
[INFO] Epoch: 15 , batch: 130 , training loss: 3.923485
[INFO] Epoch: 15 , batch: 131 , training loss: 3.938882
[INFO] Epoch: 15 , batch: 132 , training loss: 3.967422
[INFO] Epoch: 15 , batch: 133 , training loss: 3.911227
[INFO] Epoch: 15 , batch: 134 , training loss: 3.735251
[INFO] Epoch: 15 , batch: 135 , training loss: 3.778274
[INFO] Epoch: 15 , batch: 136 , training loss: 4.063745
[INFO] Epoch: 15 , batch: 137 , training loss: 3.975993
[INFO] Epoch: 15 , batch: 138 , training loss: 4.059197
[INFO] Epoch: 15 , batch: 139 , training loss: 4.660087
[INFO] Epoch: 15 , batch: 140 , training loss: 4.430027
[INFO] Epoch: 15 , batch: 141 , training loss: 4.152569
[INFO] Epoch: 15 , batch: 142 , training loss: 3.915290
[INFO] Epoch: 15 , batch: 143 , training loss: 4.024177
[INFO] Epoch: 15 , batch: 144 , training loss: 3.807310
[INFO] Epoch: 15 , batch: 145 , training loss: 3.938590
[INFO] Epoch: 15 , batch: 146 , training loss: 4.111979
[INFO] Epoch: 15 , batch: 147 , training loss: 3.810084
[INFO] Epoch: 15 , batch: 148 , training loss: 3.780965
[INFO] Epoch: 15 , batch: 149 , training loss: 3.867812
[INFO] Epoch: 15 , batch: 150 , training loss: 4.088004
[INFO] Epoch: 15 , batch: 151 , training loss: 3.935816
[INFO] Epoch: 15 , batch: 152 , training loss: 3.973153
[INFO] Epoch: 15 , batch: 153 , training loss: 4.008190
[INFO] Epoch: 15 , batch: 154 , training loss: 4.078305
[INFO] Epoch: 15 , batch: 155 , training loss: 4.317494
[INFO] Epoch: 15 , batch: 156 , training loss: 4.012051
[INFO] Epoch: 15 , batch: 157 , training loss: 3.976391
[INFO] Epoch: 15 , batch: 158 , training loss: 4.176472
[INFO] Epoch: 15 , batch: 159 , training loss: 4.018373
[INFO] Epoch: 15 , batch: 160 , training loss: 4.395306
[INFO] Epoch: 15 , batch: 161 , training loss: 4.471748
[INFO] Epoch: 15 , batch: 162 , training loss: 4.419268
[INFO] Epoch: 15 , batch: 163 , training loss: 4.483334
[INFO] Epoch: 15 , batch: 164 , training loss: 4.505974
[INFO] Epoch: 15 , batch: 165 , training loss: 4.421553
[INFO] Epoch: 15 , batch: 166 , training loss: 4.348823
[INFO] Epoch: 15 , batch: 167 , training loss: 4.535382
[INFO] Epoch: 15 , batch: 168 , training loss: 4.259911
[INFO] Epoch: 15 , batch: 169 , training loss: 4.202339
[INFO] Epoch: 15 , batch: 170 , training loss: 4.342265
[INFO] Epoch: 15 , batch: 171 , training loss: 3.726247
[INFO] Epoch: 15 , batch: 172 , training loss: 3.948646
[INFO] Epoch: 15 , batch: 173 , training loss: 4.270757
[INFO] Epoch: 15 , batch: 174 , training loss: 4.593800
[INFO] Epoch: 15 , batch: 175 , training loss: 4.964415
[INFO] Epoch: 15 , batch: 176 , training loss: 4.653976
[INFO] Epoch: 15 , batch: 177 , training loss: 4.263721
[INFO] Epoch: 15 , batch: 178 , training loss: 4.288974
[INFO] Epoch: 15 , batch: 179 , training loss: 4.337222
[INFO] Epoch: 15 , batch: 180 , training loss: 4.251524
[INFO] Epoch: 15 , batch: 181 , training loss: 4.513608
[INFO] Epoch: 15 , batch: 182 , training loss: 4.440596
[INFO] Epoch: 15 , batch: 183 , training loss: 4.404520
[INFO] Epoch: 15 , batch: 184 , training loss: 4.296077
[INFO] Epoch: 15 , batch: 185 , training loss: 4.246691
[INFO] Epoch: 15 , batch: 186 , training loss: 4.408226
[INFO] Epoch: 15 , batch: 187 , training loss: 4.509190
[INFO] Epoch: 15 , batch: 188 , training loss: 4.498153
[INFO] Epoch: 15 , batch: 189 , training loss: 4.382011
[INFO] Epoch: 15 , batch: 190 , training loss: 4.399889
[INFO] Epoch: 15 , batch: 191 , training loss: 4.506021
[INFO] Epoch: 15 , batch: 192 , training loss: 4.324338
[INFO] Epoch: 15 , batch: 193 , training loss: 4.447743
[INFO] Epoch: 15 , batch: 194 , training loss: 4.362392
[INFO] Epoch: 15 , batch: 195 , training loss: 4.281013
[INFO] Epoch: 15 , batch: 196 , training loss: 4.154985
[INFO] Epoch: 15 , batch: 197 , training loss: 4.275479
[INFO] Epoch: 15 , batch: 198 , training loss: 4.206804
[INFO] Epoch: 15 , batch: 199 , training loss: 4.295791
[INFO] Epoch: 15 , batch: 200 , training loss: 4.218002
[INFO] Epoch: 15 , batch: 201 , training loss: 4.125717
[INFO] Epoch: 15 , batch: 202 , training loss: 4.120995
[INFO] Epoch: 15 , batch: 203 , training loss: 4.198496
[INFO] Epoch: 15 , batch: 204 , training loss: 4.338440
[INFO] Epoch: 15 , batch: 205 , training loss: 3.909993
[INFO] Epoch: 15 , batch: 206 , training loss: 3.857435
[INFO] Epoch: 15 , batch: 207 , training loss: 3.846380
[INFO] Epoch: 15 , batch: 208 , training loss: 4.198996
[INFO] Epoch: 15 , batch: 209 , training loss: 4.133209
[INFO] Epoch: 15 , batch: 210 , training loss: 4.148680
[INFO] Epoch: 15 , batch: 211 , training loss: 4.148623
[INFO] Epoch: 15 , batch: 212 , training loss: 4.267242
[INFO] Epoch: 15 , batch: 213 , training loss: 4.212594
[INFO] Epoch: 15 , batch: 214 , training loss: 4.297794
[INFO] Epoch: 15 , batch: 215 , training loss: 4.499380
[INFO] Epoch: 15 , batch: 216 , training loss: 4.215699
[INFO] Epoch: 15 , batch: 217 , training loss: 4.136267
[INFO] Epoch: 15 , batch: 218 , training loss: 4.120350
[INFO] Epoch: 15 , batch: 219 , training loss: 4.253350
[INFO] Epoch: 15 , batch: 220 , training loss: 4.065533
[INFO] Epoch: 15 , batch: 221 , training loss: 4.071166
[INFO] Epoch: 15 , batch: 222 , training loss: 4.224467
[INFO] Epoch: 15 , batch: 223 , training loss: 4.299694
[INFO] Epoch: 15 , batch: 224 , training loss: 4.336513
[INFO] Epoch: 15 , batch: 225 , training loss: 4.217352
[INFO] Epoch: 15 , batch: 226 , training loss: 4.373342
[INFO] Epoch: 15 , batch: 227 , training loss: 4.342690
[INFO] Epoch: 15 , batch: 228 , training loss: 4.390754
[INFO] Epoch: 15 , batch: 229 , training loss: 4.241550
[INFO] Epoch: 15 , batch: 230 , training loss: 4.085856
[INFO] Epoch: 15 , batch: 231 , training loss: 3.942175
[INFO] Epoch: 15 , batch: 232 , training loss: 4.101577
[INFO] Epoch: 15 , batch: 233 , training loss: 4.109473
[INFO] Epoch: 15 , batch: 234 , training loss: 3.791964
[INFO] Epoch: 15 , batch: 235 , training loss: 3.898457
[INFO] Epoch: 15 , batch: 236 , training loss: 4.029294
[INFO] Epoch: 15 , batch: 237 , training loss: 4.240040
[INFO] Epoch: 15 , batch: 238 , training loss: 3.983526
[INFO] Epoch: 15 , batch: 239 , training loss: 4.043944
[INFO] Epoch: 15 , batch: 240 , training loss: 4.107437
[INFO] Epoch: 15 , batch: 241 , training loss: 3.886085
[INFO] Epoch: 15 , batch: 242 , training loss: 3.913352
[INFO] Epoch: 15 , batch: 243 , training loss: 4.234227
[INFO] Epoch: 15 , batch: 244 , training loss: 4.145195
[INFO] Epoch: 15 , batch: 245 , training loss: 4.145117
[INFO] Epoch: 15 , batch: 246 , training loss: 3.823807
[INFO] Epoch: 15 , batch: 247 , training loss: 4.000390
[INFO] Epoch: 15 , batch: 248 , training loss: 4.087133
[INFO] Epoch: 15 , batch: 249 , training loss: 4.081089
[INFO] Epoch: 15 , batch: 250 , training loss: 3.819285
[INFO] Epoch: 15 , batch: 251 , training loss: 4.291274
[INFO] Epoch: 15 , batch: 252 , training loss: 3.993316
[INFO] Epoch: 15 , batch: 253 , training loss: 3.921341
[INFO] Epoch: 15 , batch: 254 , training loss: 4.217727
[INFO] Epoch: 15 , batch: 255 , training loss: 4.206448
[INFO] Epoch: 15 , batch: 256 , training loss: 4.182853
[INFO] Epoch: 15 , batch: 257 , training loss: 4.339037
[INFO] Epoch: 15 , batch: 258 , training loss: 4.396611
[INFO] Epoch: 15 , batch: 259 , training loss: 4.448451
[INFO] Epoch: 15 , batch: 260 , training loss: 4.135060
[INFO] Epoch: 15 , batch: 261 , training loss: 4.295352
[INFO] Epoch: 15 , batch: 262 , training loss: 4.499831
[INFO] Epoch: 15 , batch: 263 , training loss: 4.663599
[INFO] Epoch: 15 , batch: 264 , training loss: 3.962206
[INFO] Epoch: 15 , batch: 265 , training loss: 4.117852
[INFO] Epoch: 15 , batch: 266 , training loss: 4.558613
[INFO] Epoch: 15 , batch: 267 , training loss: 4.310706
[INFO] Epoch: 15 , batch: 268 , training loss: 4.194852
[INFO] Epoch: 15 , batch: 269 , training loss: 4.174442
[INFO] Epoch: 15 , batch: 270 , training loss: 4.196608
[INFO] Epoch: 15 , batch: 271 , training loss: 4.248240
[INFO] Epoch: 15 , batch: 272 , training loss: 4.210611
[INFO] Epoch: 15 , batch: 273 , training loss: 4.233183
[INFO] Epoch: 15 , batch: 274 , training loss: 4.343317
[INFO] Epoch: 15 , batch: 275 , training loss: 4.205978
[INFO] Epoch: 15 , batch: 276 , training loss: 4.248030
[INFO] Epoch: 15 , batch: 277 , training loss: 4.410936
[INFO] Epoch: 15 , batch: 278 , training loss: 4.073972
[INFO] Epoch: 15 , batch: 279 , training loss: 4.071164
[INFO] Epoch: 15 , batch: 280 , training loss: 4.032271
[INFO] Epoch: 15 , batch: 281 , training loss: 4.169076
[INFO] Epoch: 15 , batch: 282 , training loss: 4.077427
[INFO] Epoch: 15 , batch: 283 , training loss: 4.110884
[INFO] Epoch: 15 , batch: 284 , training loss: 4.133946
[INFO] Epoch: 15 , batch: 285 , training loss: 4.076559
[INFO] Epoch: 15 , batch: 286 , training loss: 4.068439
[INFO] Epoch: 15 , batch: 287 , training loss: 4.000699
[INFO] Epoch: 15 , batch: 288 , training loss: 4.027277
[INFO] Epoch: 15 , batch: 289 , training loss: 4.085692
[INFO] Epoch: 15 , batch: 290 , training loss: 3.845375
[INFO] Epoch: 15 , batch: 291 , training loss: 3.820563
[INFO] Epoch: 15 , batch: 292 , training loss: 3.939618
[INFO] Epoch: 15 , batch: 293 , training loss: 3.849267
[INFO] Epoch: 15 , batch: 294 , training loss: 4.531807
[INFO] Epoch: 15 , batch: 295 , training loss: 4.312526
[INFO] Epoch: 15 , batch: 296 , training loss: 4.223938
[INFO] Epoch: 15 , batch: 297 , training loss: 4.177533
[INFO] Epoch: 15 , batch: 298 , training loss: 4.010480
[INFO] Epoch: 15 , batch: 299 , training loss: 4.041874
[INFO] Epoch: 15 , batch: 300 , training loss: 4.012687
[INFO] Epoch: 15 , batch: 301 , training loss: 3.946879
[INFO] Epoch: 15 , batch: 302 , training loss: 4.114906
[INFO] Epoch: 15 , batch: 303 , training loss: 4.145854
[INFO] Epoch: 15 , batch: 304 , training loss: 4.319097
[INFO] Epoch: 15 , batch: 305 , training loss: 4.084121
[INFO] Epoch: 15 , batch: 306 , training loss: 4.230494
[INFO] Epoch: 15 , batch: 307 , training loss: 4.208770
[INFO] Epoch: 15 , batch: 308 , training loss: 4.049555
[INFO] Epoch: 15 , batch: 309 , training loss: 4.076494
[INFO] Epoch: 15 , batch: 310 , training loss: 3.945210
[INFO] Epoch: 15 , batch: 311 , training loss: 3.969074
[INFO] Epoch: 15 , batch: 312 , training loss: 3.868959
[INFO] Epoch: 15 , batch: 313 , training loss: 4.008315
[INFO] Epoch: 15 , batch: 314 , training loss: 4.054219
[INFO] Epoch: 15 , batch: 315 , training loss: 4.113541
[INFO] Epoch: 15 , batch: 316 , training loss: 4.372179
[INFO] Epoch: 15 , batch: 317 , training loss: 4.834940
[INFO] Epoch: 15 , batch: 318 , training loss: 4.980395
[INFO] Epoch: 15 , batch: 319 , training loss: 4.534286
[INFO] Epoch: 15 , batch: 320 , training loss: 4.083125
[INFO] Epoch: 15 , batch: 321 , training loss: 3.920896
[INFO] Epoch: 15 , batch: 322 , training loss: 4.025179
[INFO] Epoch: 15 , batch: 323 , training loss: 4.037211
[INFO] Epoch: 15 , batch: 324 , training loss: 4.016690
[INFO] Epoch: 15 , batch: 325 , training loss: 4.163759
[INFO] Epoch: 15 , batch: 326 , training loss: 4.230519
[INFO] Epoch: 15 , batch: 327 , training loss: 4.124998
[INFO] Epoch: 15 , batch: 328 , training loss: 4.116585
[INFO] Epoch: 15 , batch: 329 , training loss: 4.040998
[INFO] Epoch: 15 , batch: 330 , training loss: 4.029813
[INFO] Epoch: 15 , batch: 331 , training loss: 4.185957
[INFO] Epoch: 15 , batch: 332 , training loss: 3.987550
[INFO] Epoch: 15 , batch: 333 , training loss: 3.987094
[INFO] Epoch: 15 , batch: 334 , training loss: 4.017012
[INFO] Epoch: 15 , batch: 335 , training loss: 4.141255
[INFO] Epoch: 15 , batch: 336 , training loss: 4.175714
[INFO] Epoch: 15 , batch: 337 , training loss: 4.180352
[INFO] Epoch: 15 , batch: 338 , training loss: 4.396017
[INFO] Epoch: 15 , batch: 339 , training loss: 4.253151
[INFO] Epoch: 15 , batch: 340 , training loss: 4.403191
[INFO] Epoch: 15 , batch: 341 , training loss: 4.175017
[INFO] Epoch: 15 , batch: 342 , training loss: 3.947870
[INFO] Epoch: 15 , batch: 343 , training loss: 4.046565
[INFO] Epoch: 15 , batch: 344 , training loss: 3.889216
[INFO] Epoch: 15 , batch: 345 , training loss: 4.017633
[INFO] Epoch: 15 , batch: 346 , training loss: 4.089801
[INFO] Epoch: 15 , batch: 347 , training loss: 3.978462
[INFO] Epoch: 15 , batch: 348 , training loss: 4.117090
[INFO] Epoch: 15 , batch: 349 , training loss: 4.262473
[INFO] Epoch: 15 , batch: 350 , training loss: 4.012182
[INFO] Epoch: 15 , batch: 351 , training loss: 4.070068
[INFO] Epoch: 15 , batch: 352 , training loss: 4.106759
[INFO] Epoch: 15 , batch: 353 , training loss: 4.062704
[INFO] Epoch: 15 , batch: 354 , training loss: 4.182340
[INFO] Epoch: 15 , batch: 355 , training loss: 4.218822
[INFO] Epoch: 15 , batch: 356 , training loss: 4.046809
[INFO] Epoch: 15 , batch: 357 , training loss: 4.151484
[INFO] Epoch: 15 , batch: 358 , training loss: 4.055728
[INFO] Epoch: 15 , batch: 359 , training loss: 4.034513
[INFO] Epoch: 15 , batch: 360 , training loss: 4.124812
[INFO] Epoch: 15 , batch: 361 , training loss: 4.090366
[INFO] Epoch: 15 , batch: 362 , training loss: 4.194250
[INFO] Epoch: 15 , batch: 363 , training loss: 4.074646
[INFO] Epoch: 15 , batch: 364 , training loss: 4.118014
[INFO] Epoch: 15 , batch: 365 , training loss: 4.043573
[INFO] Epoch: 15 , batch: 366 , training loss: 4.181563
[INFO] Epoch: 15 , batch: 367 , training loss: 4.253652
[INFO] Epoch: 15 , batch: 368 , training loss: 4.707356
[INFO] Epoch: 15 , batch: 369 , training loss: 4.356106
[INFO] Epoch: 15 , batch: 370 , training loss: 4.102526
[INFO] Epoch: 15 , batch: 371 , training loss: 4.510054
[INFO] Epoch: 15 , batch: 372 , training loss: 4.840786
[INFO] Epoch: 15 , batch: 373 , training loss: 4.888878
[INFO] Epoch: 15 , batch: 374 , training loss: 4.979940
[INFO] Epoch: 15 , batch: 375 , training loss: 4.933341
[INFO] Epoch: 15 , batch: 376 , training loss: 4.870159
[INFO] Epoch: 15 , batch: 377 , training loss: 4.583781
[INFO] Epoch: 15 , batch: 378 , training loss: 4.686423
[INFO] Epoch: 15 , batch: 379 , training loss: 4.667325
[INFO] Epoch: 15 , batch: 380 , training loss: 4.797858
[INFO] Epoch: 15 , batch: 381 , training loss: 4.538840
[INFO] Epoch: 15 , batch: 382 , training loss: 4.785182
[INFO] Epoch: 15 , batch: 383 , training loss: 4.813094
[INFO] Epoch: 15 , batch: 384 , training loss: 4.825166
[INFO] Epoch: 15 , batch: 385 , training loss: 4.581356
[INFO] Epoch: 15 , batch: 386 , training loss: 4.796402
[INFO] Epoch: 15 , batch: 387 , training loss: 4.745413
[INFO] Epoch: 15 , batch: 388 , training loss: 4.566005
[INFO] Epoch: 15 , batch: 389 , training loss: 4.404569
[INFO] Epoch: 15 , batch: 390 , training loss: 4.374721
[INFO] Epoch: 15 , batch: 391 , training loss: 4.405320
[INFO] Epoch: 15 , batch: 392 , training loss: 4.764822
[INFO] Epoch: 15 , batch: 393 , training loss: 4.635356
[INFO] Epoch: 15 , batch: 394 , training loss: 4.699105
[INFO] Epoch: 15 , batch: 395 , training loss: 4.542233
[INFO] Epoch: 15 , batch: 396 , training loss: 4.319790
[INFO] Epoch: 15 , batch: 397 , training loss: 4.472832
[INFO] Epoch: 15 , batch: 398 , training loss: 4.329173
[INFO] Epoch: 15 , batch: 399 , training loss: 4.401002
[INFO] Epoch: 15 , batch: 400 , training loss: 4.352448
[INFO] Epoch: 15 , batch: 401 , training loss: 4.782606
[INFO] Epoch: 15 , batch: 402 , training loss: 4.502290
[INFO] Epoch: 15 , batch: 403 , training loss: 4.309778
[INFO] Epoch: 15 , batch: 404 , training loss: 4.514184
[INFO] Epoch: 15 , batch: 405 , training loss: 4.540977
[INFO] Epoch: 15 , batch: 406 , training loss: 4.452304
[INFO] Epoch: 15 , batch: 407 , training loss: 4.513381
[INFO] Epoch: 15 , batch: 408 , training loss: 4.463707
[INFO] Epoch: 15 , batch: 409 , training loss: 4.476353
[INFO] Epoch: 15 , batch: 410 , training loss: 4.505023
[INFO] Epoch: 15 , batch: 411 , training loss: 4.697283
[INFO] Epoch: 15 , batch: 412 , training loss: 4.549920
[INFO] Epoch: 15 , batch: 413 , training loss: 4.422482
[INFO] Epoch: 15 , batch: 414 , training loss: 4.443788
[INFO] Epoch: 15 , batch: 415 , training loss: 4.444232
[INFO] Epoch: 15 , batch: 416 , training loss: 4.541348
[INFO] Epoch: 15 , batch: 417 , training loss: 4.458002
[INFO] Epoch: 15 , batch: 418 , training loss: 4.482520
[INFO] Epoch: 15 , batch: 419 , training loss: 4.435035
[INFO] Epoch: 15 , batch: 420 , training loss: 4.422533
[INFO] Epoch: 15 , batch: 421 , training loss: 4.412585
[INFO] Epoch: 15 , batch: 422 , training loss: 4.298149
[INFO] Epoch: 15 , batch: 423 , training loss: 4.516006
[INFO] Epoch: 15 , batch: 424 , training loss: 4.680263
[INFO] Epoch: 15 , batch: 425 , training loss: 4.536705
[INFO] Epoch: 15 , batch: 426 , training loss: 4.255407
[INFO] Epoch: 15 , batch: 427 , training loss: 4.511186
[INFO] Epoch: 15 , batch: 428 , training loss: 4.395813
[INFO] Epoch: 15 , batch: 429 , training loss: 4.228500
[INFO] Epoch: 15 , batch: 430 , training loss: 4.516703
[INFO] Epoch: 15 , batch: 431 , training loss: 4.099365
[INFO] Epoch: 15 , batch: 432 , training loss: 4.188141
[INFO] Epoch: 15 , batch: 433 , training loss: 4.210784
[INFO] Epoch: 15 , batch: 434 , training loss: 4.067393
[INFO] Epoch: 15 , batch: 435 , training loss: 4.437018
[INFO] Epoch: 15 , batch: 436 , training loss: 4.533844
[INFO] Epoch: 15 , batch: 437 , training loss: 4.251342
[INFO] Epoch: 15 , batch: 438 , training loss: 4.106533
[INFO] Epoch: 15 , batch: 439 , training loss: 4.342914
[INFO] Epoch: 15 , batch: 440 , training loss: 4.468032
[INFO] Epoch: 15 , batch: 441 , training loss: 4.567574
[INFO] Epoch: 15 , batch: 442 , training loss: 4.345384
[INFO] Epoch: 15 , batch: 443 , training loss: 4.541169
[INFO] Epoch: 15 , batch: 444 , training loss: 4.137650
[INFO] Epoch: 15 , batch: 445 , training loss: 4.049919
[INFO] Epoch: 15 , batch: 446 , training loss: 3.968772
[INFO] Epoch: 15 , batch: 447 , training loss: 4.165649
[INFO] Epoch: 15 , batch: 448 , training loss: 4.277719
[INFO] Epoch: 15 , batch: 449 , training loss: 4.707375
[INFO] Epoch: 15 , batch: 450 , training loss: 4.725574
[INFO] Epoch: 15 , batch: 451 , training loss: 4.649468
[INFO] Epoch: 15 , batch: 452 , training loss: 4.437268
[INFO] Epoch: 15 , batch: 453 , training loss: 4.207675
[INFO] Epoch: 15 , batch: 454 , training loss: 4.345352
[INFO] Epoch: 15 , batch: 455 , training loss: 4.392703
[INFO] Epoch: 15 , batch: 456 , training loss: 4.380285
[INFO] Epoch: 15 , batch: 457 , training loss: 4.496350
[INFO] Epoch: 15 , batch: 458 , training loss: 4.224509
[INFO] Epoch: 15 , batch: 459 , training loss: 4.201334
[INFO] Epoch: 15 , batch: 460 , training loss: 4.307456
[INFO] Epoch: 15 , batch: 461 , training loss: 4.303392
[INFO] Epoch: 15 , batch: 462 , training loss: 4.352354
[INFO] Epoch: 15 , batch: 463 , training loss: 4.239013
[INFO] Epoch: 15 , batch: 464 , training loss: 4.459307
[INFO] Epoch: 15 , batch: 465 , training loss: 4.373929
[INFO] Epoch: 15 , batch: 466 , training loss: 4.453736
[INFO] Epoch: 15 , batch: 467 , training loss: 4.431494
[INFO] Epoch: 15 , batch: 468 , training loss: 4.399494
[INFO] Epoch: 15 , batch: 469 , training loss: 4.430290
[INFO] Epoch: 15 , batch: 470 , training loss: 4.235892
[INFO] Epoch: 15 , batch: 471 , training loss: 4.321130
[INFO] Epoch: 15 , batch: 472 , training loss: 4.371228
[INFO] Epoch: 15 , batch: 473 , training loss: 4.304853
[INFO] Epoch: 15 , batch: 474 , training loss: 4.117996
[INFO] Epoch: 15 , batch: 475 , training loss: 3.973434
[INFO] Epoch: 15 , batch: 476 , training loss: 4.364588
[INFO] Epoch: 15 , batch: 477 , training loss: 4.484328
[INFO] Epoch: 15 , batch: 478 , training loss: 4.529202
[INFO] Epoch: 15 , batch: 479 , training loss: 4.494301
[INFO] Epoch: 15 , batch: 480 , training loss: 4.602417
[INFO] Epoch: 15 , batch: 481 , training loss: 4.453216
[INFO] Epoch: 15 , batch: 482 , training loss: 4.591738
[INFO] Epoch: 15 , batch: 483 , training loss: 4.427014
[INFO] Epoch: 15 , batch: 484 , training loss: 4.232354
[INFO] Epoch: 15 , batch: 485 , training loss: 4.326021
[INFO] Epoch: 15 , batch: 486 , training loss: 4.199893
[INFO] Epoch: 15 , batch: 487 , training loss: 4.198851
[INFO] Epoch: 15 , batch: 488 , training loss: 4.387841
[INFO] Epoch: 15 , batch: 489 , training loss: 4.272069
[INFO] Epoch: 15 , batch: 490 , training loss: 4.326259
[INFO] Epoch: 15 , batch: 491 , training loss: 4.297499
[INFO] Epoch: 15 , batch: 492 , training loss: 4.217534
[INFO] Epoch: 15 , batch: 493 , training loss: 4.434030
[INFO] Epoch: 15 , batch: 494 , training loss: 4.351972
[INFO] Epoch: 15 , batch: 495 , training loss: 4.465581
[INFO] Epoch: 15 , batch: 496 , training loss: 4.328820
[INFO] Epoch: 15 , batch: 497 , training loss: 4.382235
[INFO] Epoch: 15 , batch: 498 , training loss: 4.387408
[INFO] Epoch: 15 , batch: 499 , training loss: 4.449854
[INFO] Epoch: 15 , batch: 500 , training loss: 4.626870
[INFO] Epoch: 15 , batch: 501 , training loss: 4.952787
[INFO] Epoch: 15 , batch: 502 , training loss: 5.069056
[INFO] Epoch: 15 , batch: 503 , training loss: 4.778410
[INFO] Epoch: 15 , batch: 504 , training loss: 4.875652
[INFO] Epoch: 15 , batch: 505 , training loss: 4.829997
[INFO] Epoch: 15 , batch: 506 , training loss: 4.788534
[INFO] Epoch: 15 , batch: 507 , training loss: 4.833094
[INFO] Epoch: 15 , batch: 508 , training loss: 4.798341
[INFO] Epoch: 15 , batch: 509 , training loss: 4.556624
[INFO] Epoch: 15 , batch: 510 , training loss: 4.612140
[INFO] Epoch: 15 , batch: 511 , training loss: 4.509390
[INFO] Epoch: 15 , batch: 512 , training loss: 4.598222
[INFO] Epoch: 15 , batch: 513 , training loss: 4.862864
[INFO] Epoch: 15 , batch: 514 , training loss: 4.495687
[INFO] Epoch: 15 , batch: 515 , training loss: 4.741131
[INFO] Epoch: 15 , batch: 516 , training loss: 4.547152
[INFO] Epoch: 15 , batch: 517 , training loss: 4.504684
[INFO] Epoch: 15 , batch: 518 , training loss: 4.491347
[INFO] Epoch: 15 , batch: 519 , training loss: 4.339666
[INFO] Epoch: 15 , batch: 520 , training loss: 4.555959
[INFO] Epoch: 15 , batch: 521 , training loss: 4.539272
[INFO] Epoch: 15 , batch: 522 , training loss: 4.607210
[INFO] Epoch: 15 , batch: 523 , training loss: 4.538250
[INFO] Epoch: 15 , batch: 524 , training loss: 4.804105
[INFO] Epoch: 15 , batch: 525 , training loss: 4.720241
[INFO] Epoch: 15 , batch: 526 , training loss: 4.474134
[INFO] Epoch: 15 , batch: 527 , training loss: 4.509249
[INFO] Epoch: 15 , batch: 528 , training loss: 4.557677
[INFO] Epoch: 15 , batch: 529 , training loss: 4.521774
[INFO] Epoch: 15 , batch: 530 , training loss: 4.375286
[INFO] Epoch: 15 , batch: 531 , training loss: 4.517728
[INFO] Epoch: 15 , batch: 532 , training loss: 4.394791
[INFO] Epoch: 15 , batch: 533 , training loss: 4.569125
[INFO] Epoch: 15 , batch: 534 , training loss: 4.538780
[INFO] Epoch: 15 , batch: 535 , training loss: 4.563972
[INFO] Epoch: 15 , batch: 536 , training loss: 4.384035
[INFO] Epoch: 15 , batch: 537 , training loss: 4.399603
[INFO] Epoch: 15 , batch: 538 , training loss: 4.457302
[INFO] Epoch: 15 , batch: 539 , training loss: 4.578363
[INFO] Epoch: 15 , batch: 540 , training loss: 5.121094
[INFO] Epoch: 15 , batch: 541 , training loss: 5.033044
[INFO] Epoch: 15 , batch: 542 , training loss: 4.903360
[INFO] Epoch: 16 , batch: 0 , training loss: 3.798293
[INFO] Epoch: 16 , batch: 1 , training loss: 3.782293
[INFO] Epoch: 16 , batch: 2 , training loss: 3.817709
[INFO] Epoch: 16 , batch: 3 , training loss: 3.714166
[INFO] Epoch: 16 , batch: 4 , training loss: 4.079628
[INFO] Epoch: 16 , batch: 5 , training loss: 3.669169
[INFO] Epoch: 16 , batch: 6 , training loss: 4.153028
[INFO] Epoch: 16 , batch: 7 , training loss: 3.932036
[INFO] Epoch: 16 , batch: 8 , training loss: 3.666567
[INFO] Epoch: 16 , batch: 9 , training loss: 3.840800
[INFO] Epoch: 16 , batch: 10 , training loss: 3.795349
[INFO] Epoch: 16 , batch: 11 , training loss: 3.743415
[INFO] Epoch: 16 , batch: 12 , training loss: 3.678826
[INFO] Epoch: 16 , batch: 13 , training loss: 3.716793
[INFO] Epoch: 16 , batch: 14 , training loss: 3.547496
[INFO] Epoch: 16 , batch: 15 , training loss: 3.786493
[INFO] Epoch: 16 , batch: 16 , training loss: 3.621481
[INFO] Epoch: 16 , batch: 17 , training loss: 3.810044
[INFO] Epoch: 16 , batch: 18 , training loss: 3.761615
[INFO] Epoch: 16 , batch: 19 , training loss: 3.469895
[INFO] Epoch: 16 , batch: 20 , training loss: 3.455484
[INFO] Epoch: 16 , batch: 21 , training loss: 3.602899
[INFO] Epoch: 16 , batch: 22 , training loss: 3.521362
[INFO] Epoch: 16 , batch: 23 , training loss: 3.753011
[INFO] Epoch: 16 , batch: 24 , training loss: 3.550519
[INFO] Epoch: 16 , batch: 25 , training loss: 3.744522
[INFO] Epoch: 16 , batch: 26 , training loss: 3.576128
[INFO] Epoch: 16 , batch: 27 , training loss: 3.518491
[INFO] Epoch: 16 , batch: 28 , training loss: 3.714322
[INFO] Epoch: 16 , batch: 29 , training loss: 3.529622
[INFO] Epoch: 16 , batch: 30 , training loss: 3.582301
[INFO] Epoch: 16 , batch: 31 , training loss: 3.653917
[INFO] Epoch: 16 , batch: 32 , training loss: 3.630524
[INFO] Epoch: 16 , batch: 33 , training loss: 3.710740
[INFO] Epoch: 16 , batch: 34 , training loss: 3.677121
[INFO] Epoch: 16 , batch: 35 , training loss: 3.646093
[INFO] Epoch: 16 , batch: 36 , training loss: 3.665290
[INFO] Epoch: 16 , batch: 37 , training loss: 3.568872
[INFO] Epoch: 16 , batch: 38 , training loss: 3.609704
[INFO] Epoch: 16 , batch: 39 , training loss: 3.500084
[INFO] Epoch: 16 , batch: 40 , training loss: 3.701899
[INFO] Epoch: 16 , batch: 41 , training loss: 3.637066
[INFO] Epoch: 16 , batch: 42 , training loss: 4.102129
[INFO] Epoch: 16 , batch: 43 , training loss: 3.782855
[INFO] Epoch: 16 , batch: 44 , training loss: 4.124402
[INFO] Epoch: 16 , batch: 45 , training loss: 4.097355
[INFO] Epoch: 16 , batch: 46 , training loss: 4.085116
[INFO] Epoch: 16 , batch: 47 , training loss: 3.771107
[INFO] Epoch: 16 , batch: 48 , training loss: 3.785140
[INFO] Epoch: 16 , batch: 49 , training loss: 3.916494
[INFO] Epoch: 16 , batch: 50 , training loss: 3.691825
[INFO] Epoch: 16 , batch: 51 , training loss: 3.883175
[INFO] Epoch: 16 , batch: 52 , training loss: 3.749973
[INFO] Epoch: 16 , batch: 53 , training loss: 3.897682
[INFO] Epoch: 16 , batch: 54 , training loss: 3.886788
[INFO] Epoch: 16 , batch: 55 , training loss: 3.930826
[INFO] Epoch: 16 , batch: 56 , training loss: 3.835467
[INFO] Epoch: 16 , batch: 57 , training loss: 3.706101
[INFO] Epoch: 16 , batch: 58 , training loss: 3.803807
[INFO] Epoch: 16 , batch: 59 , training loss: 3.862204
[INFO] Epoch: 16 , batch: 60 , training loss: 3.824862
[INFO] Epoch: 16 , batch: 61 , training loss: 3.892644
[INFO] Epoch: 16 , batch: 62 , training loss: 3.792659
[INFO] Epoch: 16 , batch: 63 , training loss: 3.979263
[INFO] Epoch: 16 , batch: 64 , training loss: 4.145533
[INFO] Epoch: 16 , batch: 65 , training loss: 3.857111
[INFO] Epoch: 16 , batch: 66 , training loss: 3.712883
[INFO] Epoch: 16 , batch: 67 , training loss: 3.772225
[INFO] Epoch: 16 , batch: 68 , training loss: 3.941505
[INFO] Epoch: 16 , batch: 69 , training loss: 3.807755
[INFO] Epoch: 16 , batch: 70 , training loss: 4.045550
[INFO] Epoch: 16 , batch: 71 , training loss: 3.879405
[INFO] Epoch: 16 , batch: 72 , training loss: 3.908869
[INFO] Epoch: 16 , batch: 73 , training loss: 3.887097
[INFO] Epoch: 16 , batch: 74 , training loss: 3.998322
[INFO] Epoch: 16 , batch: 75 , training loss: 3.822189
[INFO] Epoch: 16 , batch: 76 , training loss: 3.960130
[INFO] Epoch: 16 , batch: 77 , training loss: 3.897563
[INFO] Epoch: 16 , batch: 78 , training loss: 4.015193
[INFO] Epoch: 16 , batch: 79 , training loss: 3.829164
[INFO] Epoch: 16 , batch: 80 , training loss: 4.045971
[INFO] Epoch: 16 , batch: 81 , training loss: 3.962820
[INFO] Epoch: 16 , batch: 82 , training loss: 3.923606
[INFO] Epoch: 16 , batch: 83 , training loss: 4.061905
[INFO] Epoch: 16 , batch: 84 , training loss: 3.972325
[INFO] Epoch: 16 , batch: 85 , training loss: 4.049729
[INFO] Epoch: 16 , batch: 86 , training loss: 3.983533
[INFO] Epoch: 16 , batch: 87 , training loss: 3.982967
[INFO] Epoch: 16 , batch: 88 , training loss: 4.116701
[INFO] Epoch: 16 , batch: 89 , training loss: 3.913622
[INFO] Epoch: 16 , batch: 90 , training loss: 3.984456
[INFO] Epoch: 16 , batch: 91 , training loss: 3.915622
[INFO] Epoch: 16 , batch: 92 , training loss: 3.928888
[INFO] Epoch: 16 , batch: 93 , training loss: 4.030156
[INFO] Epoch: 16 , batch: 94 , training loss: 4.198441
[INFO] Epoch: 16 , batch: 95 , training loss: 3.976345
[INFO] Epoch: 16 , batch: 96 , training loss: 3.959634
[INFO] Epoch: 16 , batch: 97 , training loss: 3.841988
[INFO] Epoch: 16 , batch: 98 , training loss: 3.843934
[INFO] Epoch: 16 , batch: 99 , training loss: 3.915368
[INFO] Epoch: 16 , batch: 100 , training loss: 3.836034
[INFO] Epoch: 16 , batch: 101 , training loss: 3.890817
[INFO] Epoch: 16 , batch: 102 , training loss: 4.015232
[INFO] Epoch: 16 , batch: 103 , training loss: 3.797168
[INFO] Epoch: 16 , batch: 104 , training loss: 3.754402
[INFO] Epoch: 16 , batch: 105 , training loss: 4.032130
[INFO] Epoch: 16 , batch: 106 , training loss: 4.021542
[INFO] Epoch: 16 , batch: 107 , training loss: 3.882001
[INFO] Epoch: 16 , batch: 108 , training loss: 3.794533
[INFO] Epoch: 16 , batch: 109 , training loss: 3.714827
[INFO] Epoch: 16 , batch: 110 , training loss: 3.904147
[INFO] Epoch: 16 , batch: 111 , training loss: 4.006684
[INFO] Epoch: 16 , batch: 112 , training loss: 3.923647
[INFO] Epoch: 16 , batch: 113 , training loss: 3.898545
[INFO] Epoch: 16 , batch: 114 , training loss: 3.930977
[INFO] Epoch: 16 , batch: 115 , training loss: 3.924227
[INFO] Epoch: 16 , batch: 116 , training loss: 3.829461
[INFO] Epoch: 16 , batch: 117 , training loss: 4.064390
[INFO] Epoch: 16 , batch: 118 , training loss: 4.001128
[INFO] Epoch: 16 , batch: 119 , training loss: 4.173622
[INFO] Epoch: 16 , batch: 120 , training loss: 4.159791
[INFO] Epoch: 16 , batch: 121 , training loss: 3.980080
[INFO] Epoch: 16 , batch: 122 , training loss: 3.889199
[INFO] Epoch: 16 , batch: 123 , training loss: 3.929430
[INFO] Epoch: 16 , batch: 124 , training loss: 4.048038
[INFO] Epoch: 16 , batch: 125 , training loss: 3.826536
[INFO] Epoch: 16 , batch: 126 , training loss: 3.846578
[INFO] Epoch: 16 , batch: 127 , training loss: 3.843763
[INFO] Epoch: 16 , batch: 128 , training loss: 3.991966
[INFO] Epoch: 16 , batch: 129 , training loss: 3.996074
[INFO] Epoch: 16 , batch: 130 , training loss: 3.925776
[INFO] Epoch: 16 , batch: 131 , training loss: 3.919978
[INFO] Epoch: 16 , batch: 132 , training loss: 3.960536
[INFO] Epoch: 16 , batch: 133 , training loss: 3.907099
[INFO] Epoch: 16 , batch: 134 , training loss: 3.717315
[INFO] Epoch: 16 , batch: 135 , training loss: 3.758913
[INFO] Epoch: 16 , batch: 136 , training loss: 4.034133
[INFO] Epoch: 16 , batch: 137 , training loss: 3.959791
[INFO] Epoch: 16 , batch: 138 , training loss: 4.032475
[INFO] Epoch: 16 , batch: 139 , training loss: 4.642733
[INFO] Epoch: 16 , batch: 140 , training loss: 4.411312
[INFO] Epoch: 16 , batch: 141 , training loss: 4.146554
[INFO] Epoch: 16 , batch: 142 , training loss: 3.895395
[INFO] Epoch: 16 , batch: 143 , training loss: 4.012819
[INFO] Epoch: 16 , batch: 144 , training loss: 3.795505
[INFO] Epoch: 16 , batch: 145 , training loss: 3.937185
[INFO] Epoch: 16 , batch: 146 , training loss: 4.103699
[INFO] Epoch: 16 , batch: 147 , training loss: 3.776718
[INFO] Epoch: 16 , batch: 148 , training loss: 3.768636
[INFO] Epoch: 16 , batch: 149 , training loss: 3.854471
[INFO] Epoch: 16 , batch: 150 , training loss: 4.061742
[INFO] Epoch: 16 , batch: 151 , training loss: 3.932029
[INFO] Epoch: 16 , batch: 152 , training loss: 3.967254
[INFO] Epoch: 16 , batch: 153 , training loss: 3.991710
[INFO] Epoch: 16 , batch: 154 , training loss: 4.078056
[INFO] Epoch: 16 , batch: 155 , training loss: 4.298710
[INFO] Epoch: 16 , batch: 156 , training loss: 3.983659
[INFO] Epoch: 16 , batch: 157 , training loss: 3.964234
[INFO] Epoch: 16 , batch: 158 , training loss: 4.156637
[INFO] Epoch: 16 , batch: 159 , training loss: 4.023410
[INFO] Epoch: 16 , batch: 160 , training loss: 4.377306
[INFO] Epoch: 16 , batch: 161 , training loss: 4.453097
[INFO] Epoch: 16 , batch: 162 , training loss: 4.380579
[INFO] Epoch: 16 , batch: 163 , training loss: 4.483730
[INFO] Epoch: 16 , batch: 164 , training loss: 4.502120
[INFO] Epoch: 16 , batch: 165 , training loss: 4.396577
[INFO] Epoch: 16 , batch: 166 , training loss: 4.316593
[INFO] Epoch: 16 , batch: 167 , training loss: 4.473348
[INFO] Epoch: 16 , batch: 168 , training loss: 4.174332
[INFO] Epoch: 16 , batch: 169 , training loss: 4.204263
[INFO] Epoch: 16 , batch: 170 , training loss: 4.310931
[INFO] Epoch: 16 , batch: 171 , training loss: 3.704256
[INFO] Epoch: 16 , batch: 172 , training loss: 3.931734
[INFO] Epoch: 16 , batch: 173 , training loss: 4.256444
[INFO] Epoch: 16 , batch: 174 , training loss: 4.581775
[INFO] Epoch: 16 , batch: 175 , training loss: 4.930362
[INFO] Epoch: 16 , batch: 176 , training loss: 4.654492
[INFO] Epoch: 16 , batch: 177 , training loss: 4.220983
[INFO] Epoch: 16 , batch: 178 , training loss: 4.265033
[INFO] Epoch: 16 , batch: 179 , training loss: 4.308347
[INFO] Epoch: 16 , batch: 180 , training loss: 4.241363
[INFO] Epoch: 16 , batch: 181 , training loss: 4.507912
[INFO] Epoch: 16 , batch: 182 , training loss: 4.423518
[INFO] Epoch: 16 , batch: 183 , training loss: 4.390056
[INFO] Epoch: 16 , batch: 184 , training loss: 4.292013
[INFO] Epoch: 16 , batch: 185 , training loss: 4.233071
[INFO] Epoch: 16 , batch: 186 , training loss: 4.397268
[INFO] Epoch: 16 , batch: 187 , training loss: 4.502991
[INFO] Epoch: 16 , batch: 188 , training loss: 4.475596
[INFO] Epoch: 16 , batch: 189 , training loss: 4.369525
[INFO] Epoch: 16 , batch: 190 , training loss: 4.420531
[INFO] Epoch: 16 , batch: 191 , training loss: 4.499609
[INFO] Epoch: 16 , batch: 192 , training loss: 4.320329
[INFO] Epoch: 16 , batch: 193 , training loss: 4.447547
[INFO] Epoch: 16 , batch: 194 , training loss: 4.362709
[INFO] Epoch: 16 , batch: 195 , training loss: 4.276391
[INFO] Epoch: 16 , batch: 196 , training loss: 4.159667
[INFO] Epoch: 16 , batch: 197 , training loss: 4.270661
[INFO] Epoch: 16 , batch: 198 , training loss: 4.193159
[INFO] Epoch: 16 , batch: 199 , training loss: 4.289335
[INFO] Epoch: 16 , batch: 200 , training loss: 4.198638
[INFO] Epoch: 16 , batch: 201 , training loss: 4.111148
[INFO] Epoch: 16 , batch: 202 , training loss: 4.096313
[INFO] Epoch: 16 , batch: 203 , training loss: 4.190585
[INFO] Epoch: 16 , batch: 204 , training loss: 4.321066
[INFO] Epoch: 16 , batch: 205 , training loss: 3.898956
[INFO] Epoch: 16 , batch: 206 , training loss: 3.835045
[INFO] Epoch: 16 , batch: 207 , training loss: 3.842330
[INFO] Epoch: 16 , batch: 208 , training loss: 4.164966
[INFO] Epoch: 16 , batch: 209 , training loss: 4.115895
[INFO] Epoch: 16 , batch: 210 , training loss: 4.134890
[INFO] Epoch: 16 , batch: 211 , training loss: 4.124026
[INFO] Epoch: 16 , batch: 212 , training loss: 4.243686
[INFO] Epoch: 16 , batch: 213 , training loss: 4.196253
[INFO] Epoch: 16 , batch: 214 , training loss: 4.278135
[INFO] Epoch: 16 , batch: 215 , training loss: 4.479396
[INFO] Epoch: 16 , batch: 216 , training loss: 4.201907
[INFO] Epoch: 16 , batch: 217 , training loss: 4.127753
[INFO] Epoch: 16 , batch: 218 , training loss: 4.115283
[INFO] Epoch: 16 , batch: 219 , training loss: 4.247628
[INFO] Epoch: 16 , batch: 220 , training loss: 4.047142
[INFO] Epoch: 16 , batch: 221 , training loss: 4.056589
[INFO] Epoch: 16 , batch: 222 , training loss: 4.211318
[INFO] Epoch: 16 , batch: 223 , training loss: 4.282435
[INFO] Epoch: 16 , batch: 224 , training loss: 4.335732
[INFO] Epoch: 16 , batch: 225 , training loss: 4.235256
[INFO] Epoch: 16 , batch: 226 , training loss: 4.362164
[INFO] Epoch: 16 , batch: 227 , training loss: 4.323924
[INFO] Epoch: 16 , batch: 228 , training loss: 4.368716
[INFO] Epoch: 16 , batch: 229 , training loss: 4.224415
[INFO] Epoch: 16 , batch: 230 , training loss: 4.080152
[INFO] Epoch: 16 , batch: 231 , training loss: 3.942054
[INFO] Epoch: 16 , batch: 232 , training loss: 4.103931
[INFO] Epoch: 16 , batch: 233 , training loss: 4.103517
[INFO] Epoch: 16 , batch: 234 , training loss: 3.782801
[INFO] Epoch: 16 , batch: 235 , training loss: 3.882671
[INFO] Epoch: 16 , batch: 236 , training loss: 4.024210
[INFO] Epoch: 16 , batch: 237 , training loss: 4.240581
[INFO] Epoch: 16 , batch: 238 , training loss: 3.960103
[INFO] Epoch: 16 , batch: 239 , training loss: 4.034544
[INFO] Epoch: 16 , batch: 240 , training loss: 4.097276
[INFO] Epoch: 16 , batch: 241 , training loss: 3.884952
[INFO] Epoch: 16 , batch: 242 , training loss: 3.920981
[INFO] Epoch: 16 , batch: 243 , training loss: 4.237361
[INFO] Epoch: 16 , batch: 244 , training loss: 4.144363
[INFO] Epoch: 16 , batch: 245 , training loss: 4.125445
[INFO] Epoch: 16 , batch: 246 , training loss: 3.811557
[INFO] Epoch: 16 , batch: 247 , training loss: 4.005707
[INFO] Epoch: 16 , batch: 248 , training loss: 4.077306
[INFO] Epoch: 16 , batch: 249 , training loss: 4.066378
[INFO] Epoch: 16 , batch: 250 , training loss: 3.828405
[INFO] Epoch: 16 , batch: 251 , training loss: 4.289769
[INFO] Epoch: 16 , batch: 252 , training loss: 3.998092
[INFO] Epoch: 16 , batch: 253 , training loss: 3.911280
[INFO] Epoch: 16 , batch: 254 , training loss: 4.209844
[INFO] Epoch: 16 , batch: 255 , training loss: 4.175102
[INFO] Epoch: 16 , batch: 256 , training loss: 4.192967
[INFO] Epoch: 16 , batch: 257 , training loss: 4.330384
[INFO] Epoch: 16 , batch: 258 , training loss: 4.390868
[INFO] Epoch: 16 , batch: 259 , training loss: 4.432518
[INFO] Epoch: 16 , batch: 260 , training loss: 4.136433
[INFO] Epoch: 16 , batch: 261 , training loss: 4.286527
[INFO] Epoch: 16 , batch: 262 , training loss: 4.476856
[INFO] Epoch: 16 , batch: 263 , training loss: 4.648221
[INFO] Epoch: 16 , batch: 264 , training loss: 3.955335
[INFO] Epoch: 16 , batch: 265 , training loss: 4.105597
[INFO] Epoch: 16 , batch: 266 , training loss: 4.530176
[INFO] Epoch: 16 , batch: 267 , training loss: 4.290793
[INFO] Epoch: 16 , batch: 268 , training loss: 4.193633
[INFO] Epoch: 16 , batch: 269 , training loss: 4.159952
[INFO] Epoch: 16 , batch: 270 , training loss: 4.195239
[INFO] Epoch: 16 , batch: 271 , training loss: 4.236324
[INFO] Epoch: 16 , batch: 272 , training loss: 4.208212
[INFO] Epoch: 16 , batch: 273 , training loss: 4.234236
[INFO] Epoch: 16 , batch: 274 , training loss: 4.323660
[INFO] Epoch: 16 , batch: 275 , training loss: 4.200155
[INFO] Epoch: 16 , batch: 276 , training loss: 4.252550
[INFO] Epoch: 16 , batch: 277 , training loss: 4.391594
[INFO] Epoch: 16 , batch: 278 , training loss: 4.060174
[INFO] Epoch: 16 , batch: 279 , training loss: 4.067756
[INFO] Epoch: 16 , batch: 280 , training loss: 4.025473
[INFO] Epoch: 16 , batch: 281 , training loss: 4.170714
[INFO] Epoch: 16 , batch: 282 , training loss: 4.070529
[INFO] Epoch: 16 , batch: 283 , training loss: 4.102888
[INFO] Epoch: 16 , batch: 284 , training loss: 4.130992
[INFO] Epoch: 16 , batch: 285 , training loss: 4.076141
[INFO] Epoch: 16 , batch: 286 , training loss: 4.052896
[INFO] Epoch: 16 , batch: 287 , training loss: 4.001010
[INFO] Epoch: 16 , batch: 288 , training loss: 4.024260
[INFO] Epoch: 16 , batch: 289 , training loss: 4.072617
[INFO] Epoch: 16 , batch: 290 , training loss: 3.834329
[INFO] Epoch: 16 , batch: 291 , training loss: 3.806763
[INFO] Epoch: 16 , batch: 292 , training loss: 3.943946
[INFO] Epoch: 16 , batch: 293 , training loss: 3.828750
[INFO] Epoch: 16 , batch: 294 , training loss: 4.526068
[INFO] Epoch: 16 , batch: 295 , training loss: 4.296736
[INFO] Epoch: 16 , batch: 296 , training loss: 4.216339
[INFO] Epoch: 16 , batch: 297 , training loss: 4.173217
[INFO] Epoch: 16 , batch: 298 , training loss: 4.012217
[INFO] Epoch: 16 , batch: 299 , training loss: 4.045508
[INFO] Epoch: 16 , batch: 300 , training loss: 4.002071
[INFO] Epoch: 16 , batch: 301 , training loss: 3.942371
[INFO] Epoch: 16 , batch: 302 , training loss: 4.100687
[INFO] Epoch: 16 , batch: 303 , training loss: 4.119119
[INFO] Epoch: 16 , batch: 304 , training loss: 4.298347
[INFO] Epoch: 16 , batch: 305 , training loss: 4.072876
[INFO] Epoch: 16 , batch: 306 , training loss: 4.214390
[INFO] Epoch: 16 , batch: 307 , training loss: 4.213734
[INFO] Epoch: 16 , batch: 308 , training loss: 4.053465
[INFO] Epoch: 16 , batch: 309 , training loss: 4.088894
[INFO] Epoch: 16 , batch: 310 , training loss: 3.927919
[INFO] Epoch: 16 , batch: 311 , training loss: 3.962475
[INFO] Epoch: 16 , batch: 312 , training loss: 3.860063
[INFO] Epoch: 16 , batch: 313 , training loss: 3.984897
[INFO] Epoch: 16 , batch: 314 , training loss: 4.054291
[INFO] Epoch: 16 , batch: 315 , training loss: 4.096436
[INFO] Epoch: 16 , batch: 316 , training loss: 4.359219
[INFO] Epoch: 16 , batch: 317 , training loss: 4.835395
[INFO] Epoch: 16 , batch: 318 , training loss: 4.990039
[INFO] Epoch: 16 , batch: 319 , training loss: 4.527783
[INFO] Epoch: 16 , batch: 320 , training loss: 4.091369
[INFO] Epoch: 16 , batch: 321 , training loss: 3.902520
[INFO] Epoch: 16 , batch: 322 , training loss: 4.013396
[INFO] Epoch: 16 , batch: 323 , training loss: 4.041053
[INFO] Epoch: 16 , batch: 324 , training loss: 4.010121
[INFO] Epoch: 16 , batch: 325 , training loss: 4.163295
[INFO] Epoch: 16 , batch: 326 , training loss: 4.221164
[INFO] Epoch: 16 , batch: 327 , training loss: 4.115935
[INFO] Epoch: 16 , batch: 328 , training loss: 4.107584
[INFO] Epoch: 16 , batch: 329 , training loss: 4.034114
[INFO] Epoch: 16 , batch: 330 , training loss: 4.025053
[INFO] Epoch: 16 , batch: 331 , training loss: 4.177959
[INFO] Epoch: 16 , batch: 332 , training loss: 3.984191
[INFO] Epoch: 16 , batch: 333 , training loss: 3.992703
[INFO] Epoch: 16 , batch: 334 , training loss: 4.004552
[INFO] Epoch: 16 , batch: 335 , training loss: 4.136351
[INFO] Epoch: 16 , batch: 336 , training loss: 4.169691
[INFO] Epoch: 16 , batch: 337 , training loss: 4.174540
[INFO] Epoch: 16 , batch: 338 , training loss: 4.372694
[INFO] Epoch: 16 , batch: 339 , training loss: 4.244620
[INFO] Epoch: 16 , batch: 340 , training loss: 4.404839
[INFO] Epoch: 16 , batch: 341 , training loss: 4.164848
[INFO] Epoch: 16 , batch: 342 , training loss: 3.940731
[INFO] Epoch: 16 , batch: 343 , training loss: 4.023194
[INFO] Epoch: 16 , batch: 344 , training loss: 3.878433
[INFO] Epoch: 16 , batch: 345 , training loss: 4.007554
[INFO] Epoch: 16 , batch: 346 , training loss: 4.071895
[INFO] Epoch: 16 , batch: 347 , training loss: 3.969175
[INFO] Epoch: 16 , batch: 348 , training loss: 4.105051
[INFO] Epoch: 16 , batch: 349 , training loss: 4.247183
[INFO] Epoch: 16 , batch: 350 , training loss: 4.002001
[INFO] Epoch: 16 , batch: 351 , training loss: 4.079213
[INFO] Epoch: 16 , batch: 352 , training loss: 4.089490
[INFO] Epoch: 16 , batch: 353 , training loss: 4.065368
[INFO] Epoch: 16 , batch: 354 , training loss: 4.188226
[INFO] Epoch: 16 , batch: 355 , training loss: 4.215480
[INFO] Epoch: 16 , batch: 356 , training loss: 4.040077
[INFO] Epoch: 16 , batch: 357 , training loss: 4.138027
[INFO] Epoch: 16 , batch: 358 , training loss: 4.041372
[INFO] Epoch: 16 , batch: 359 , training loss: 4.017534
[INFO] Epoch: 16 , batch: 360 , training loss: 4.112839
[INFO] Epoch: 16 , batch: 361 , training loss: 4.074133
[INFO] Epoch: 16 , batch: 362 , training loss: 4.186326
[INFO] Epoch: 16 , batch: 363 , training loss: 4.070916
[INFO] Epoch: 16 , batch: 364 , training loss: 4.105748
[INFO] Epoch: 16 , batch: 365 , training loss: 4.035071
[INFO] Epoch: 16 , batch: 366 , training loss: 4.169686
[INFO] Epoch: 16 , batch: 367 , training loss: 4.250120
[INFO] Epoch: 16 , batch: 368 , training loss: 4.707544
[INFO] Epoch: 16 , batch: 369 , training loss: 4.328248
[INFO] Epoch: 16 , batch: 370 , training loss: 4.093457
[INFO] Epoch: 16 , batch: 371 , training loss: 4.508009
[INFO] Epoch: 16 , batch: 372 , training loss: 4.797343
[INFO] Epoch: 16 , batch: 373 , training loss: 4.884298
[INFO] Epoch: 16 , batch: 374 , training loss: 4.954354
[INFO] Epoch: 16 , batch: 375 , training loss: 4.931207
[INFO] Epoch: 16 , batch: 376 , training loss: 4.851346
[INFO] Epoch: 16 , batch: 377 , training loss: 4.583459
[INFO] Epoch: 16 , batch: 378 , training loss: 4.676705
[INFO] Epoch: 16 , batch: 379 , training loss: 4.659288
[INFO] Epoch: 16 , batch: 380 , training loss: 4.785910
[INFO] Epoch: 16 , batch: 381 , training loss: 4.530440
[INFO] Epoch: 16 , batch: 382 , training loss: 4.769902
[INFO] Epoch: 16 , batch: 383 , training loss: 4.792864
[INFO] Epoch: 16 , batch: 384 , training loss: 4.825068
[INFO] Epoch: 16 , batch: 385 , training loss: 4.549986
[INFO] Epoch: 16 , batch: 386 , training loss: 4.774035
[INFO] Epoch: 16 , batch: 387 , training loss: 4.723888
[INFO] Epoch: 16 , batch: 388 , training loss: 4.531246
[INFO] Epoch: 16 , batch: 389 , training loss: 4.389859
[INFO] Epoch: 16 , batch: 390 , training loss: 4.360439
[INFO] Epoch: 16 , batch: 391 , training loss: 4.401496
[INFO] Epoch: 16 , batch: 392 , training loss: 4.764147
[INFO] Epoch: 16 , batch: 393 , training loss: 4.632963
[INFO] Epoch: 16 , batch: 394 , training loss: 4.674454
[INFO] Epoch: 16 , batch: 395 , training loss: 4.539934
[INFO] Epoch: 16 , batch: 396 , training loss: 4.307641
[INFO] Epoch: 16 , batch: 397 , training loss: 4.464843
[INFO] Epoch: 16 , batch: 398 , training loss: 4.335328
[INFO] Epoch: 16 , batch: 399 , training loss: 4.393713
[INFO] Epoch: 16 , batch: 400 , training loss: 4.361422
[INFO] Epoch: 16 , batch: 401 , training loss: 4.780557
[INFO] Epoch: 16 , batch: 402 , training loss: 4.497235
[INFO] Epoch: 16 , batch: 403 , training loss: 4.302893
[INFO] Epoch: 16 , batch: 404 , training loss: 4.496023
[INFO] Epoch: 16 , batch: 405 , training loss: 4.539772
[INFO] Epoch: 16 , batch: 406 , training loss: 4.453393
[INFO] Epoch: 16 , batch: 407 , training loss: 4.518002
[INFO] Epoch: 16 , batch: 408 , training loss: 4.451545
[INFO] Epoch: 16 , batch: 409 , training loss: 4.466205
[INFO] Epoch: 16 , batch: 410 , training loss: 4.490365
[INFO] Epoch: 16 , batch: 411 , training loss: 4.690239
[INFO] Epoch: 16 , batch: 412 , training loss: 4.533353
[INFO] Epoch: 16 , batch: 413 , training loss: 4.408192
[INFO] Epoch: 16 , batch: 414 , training loss: 4.437602
[INFO] Epoch: 16 , batch: 415 , training loss: 4.450516
[INFO] Epoch: 16 , batch: 416 , training loss: 4.541843
[INFO] Epoch: 16 , batch: 417 , training loss: 4.456688
[INFO] Epoch: 16 , batch: 418 , training loss: 4.468123
[INFO] Epoch: 16 , batch: 419 , training loss: 4.444191
[INFO] Epoch: 16 , batch: 420 , training loss: 4.400605
[INFO] Epoch: 16 , batch: 421 , training loss: 4.404479
[INFO] Epoch: 16 , batch: 422 , training loss: 4.298984
[INFO] Epoch: 16 , batch: 423 , training loss: 4.507381
[INFO] Epoch: 16 , batch: 424 , training loss: 4.673467
[INFO] Epoch: 16 , batch: 425 , training loss: 4.531990
[INFO] Epoch: 16 , batch: 426 , training loss: 4.262703
[INFO] Epoch: 16 , batch: 427 , training loss: 4.503671
[INFO] Epoch: 16 , batch: 428 , training loss: 4.372594
[INFO] Epoch: 16 , batch: 429 , training loss: 4.237407
[INFO] Epoch: 16 , batch: 430 , training loss: 4.509464
[INFO] Epoch: 16 , batch: 431 , training loss: 4.098284
[INFO] Epoch: 16 , batch: 432 , training loss: 4.183882
[INFO] Epoch: 16 , batch: 433 , training loss: 4.215092
[INFO] Epoch: 16 , batch: 434 , training loss: 4.069174
[INFO] Epoch: 16 , batch: 435 , training loss: 4.445524
[INFO] Epoch: 16 , batch: 436 , training loss: 4.514968
[INFO] Epoch: 16 , batch: 437 , training loss: 4.251478
[INFO] Epoch: 16 , batch: 438 , training loss: 4.109657
[INFO] Epoch: 16 , batch: 439 , training loss: 4.348921
[INFO] Epoch: 16 , batch: 440 , training loss: 4.449568
[INFO] Epoch: 16 , batch: 441 , training loss: 4.568794
[INFO] Epoch: 16 , batch: 442 , training loss: 4.337087
[INFO] Epoch: 16 , batch: 443 , training loss: 4.535097
[INFO] Epoch: 16 , batch: 444 , training loss: 4.137274
[INFO] Epoch: 16 , batch: 445 , training loss: 4.036749
[INFO] Epoch: 16 , batch: 446 , training loss: 3.971571
[INFO] Epoch: 16 , batch: 447 , training loss: 4.160202
[INFO] Epoch: 16 , batch: 448 , training loss: 4.282856
[INFO] Epoch: 16 , batch: 449 , training loss: 4.701439
[INFO] Epoch: 16 , batch: 450 , training loss: 4.730522
[INFO] Epoch: 16 , batch: 451 , training loss: 4.640458
[INFO] Epoch: 16 , batch: 452 , training loss: 4.435431
[INFO] Epoch: 16 , batch: 453 , training loss: 4.199037
[INFO] Epoch: 16 , batch: 454 , training loss: 4.348610
[INFO] Epoch: 16 , batch: 455 , training loss: 4.389321
[INFO] Epoch: 16 , batch: 456 , training loss: 4.379633
[INFO] Epoch: 16 , batch: 457 , training loss: 4.502447
[INFO] Epoch: 16 , batch: 458 , training loss: 4.220939
[INFO] Epoch: 16 , batch: 459 , training loss: 4.194764
[INFO] Epoch: 16 , batch: 460 , training loss: 4.291972
[INFO] Epoch: 16 , batch: 461 , training loss: 4.284321
[INFO] Epoch: 16 , batch: 462 , training loss: 4.353565
[INFO] Epoch: 16 , batch: 463 , training loss: 4.227358
[INFO] Epoch: 16 , batch: 464 , training loss: 4.447676
[INFO] Epoch: 16 , batch: 465 , training loss: 4.370138
[INFO] Epoch: 16 , batch: 466 , training loss: 4.446172
[INFO] Epoch: 16 , batch: 467 , training loss: 4.432968
[INFO] Epoch: 16 , batch: 468 , training loss: 4.402094
[INFO] Epoch: 16 , batch: 469 , training loss: 4.427177
[INFO] Epoch: 16 , batch: 470 , training loss: 4.233997
[INFO] Epoch: 16 , batch: 471 , training loss: 4.322979
[INFO] Epoch: 16 , batch: 472 , training loss: 4.366130
[INFO] Epoch: 16 , batch: 473 , training loss: 4.298074
[INFO] Epoch: 16 , batch: 474 , training loss: 4.092094
[INFO] Epoch: 16 , batch: 475 , training loss: 3.976012
[INFO] Epoch: 16 , batch: 476 , training loss: 4.353533
[INFO] Epoch: 16 , batch: 477 , training loss: 4.480114
[INFO] Epoch: 16 , batch: 478 , training loss: 4.531312
[INFO] Epoch: 16 , batch: 479 , training loss: 4.493366
[INFO] Epoch: 16 , batch: 480 , training loss: 4.589943
[INFO] Epoch: 16 , batch: 481 , training loss: 4.445476
[INFO] Epoch: 16 , batch: 482 , training loss: 4.592218
[INFO] Epoch: 16 , batch: 483 , training loss: 4.421967
[INFO] Epoch: 16 , batch: 484 , training loss: 4.230653
[INFO] Epoch: 16 , batch: 485 , training loss: 4.323183
[INFO] Epoch: 16 , batch: 486 , training loss: 4.190484
[INFO] Epoch: 16 , batch: 487 , training loss: 4.178776
[INFO] Epoch: 16 , batch: 488 , training loss: 4.393636
[INFO] Epoch: 16 , batch: 489 , training loss: 4.269651
[INFO] Epoch: 16 , batch: 490 , training loss: 4.333545
[INFO] Epoch: 16 , batch: 491 , training loss: 4.300804
[INFO] Epoch: 16 , batch: 492 , training loss: 4.207011
[INFO] Epoch: 16 , batch: 493 , training loss: 4.415902
[INFO] Epoch: 16 , batch: 494 , training loss: 4.339286
[INFO] Epoch: 16 , batch: 495 , training loss: 4.457018
[INFO] Epoch: 16 , batch: 496 , training loss: 4.317366
[INFO] Epoch: 16 , batch: 497 , training loss: 4.367988
[INFO] Epoch: 16 , batch: 498 , training loss: 4.380454
[INFO] Epoch: 16 , batch: 499 , training loss: 4.462108
[INFO] Epoch: 16 , batch: 500 , training loss: 4.616982
[INFO] Epoch: 16 , batch: 501 , training loss: 4.945940
[INFO] Epoch: 16 , batch: 502 , training loss: 5.051465
[INFO] Epoch: 16 , batch: 503 , training loss: 4.775609
[INFO] Epoch: 16 , batch: 504 , training loss: 4.862230
[INFO] Epoch: 16 , batch: 505 , training loss: 4.812883
[INFO] Epoch: 16 , batch: 506 , training loss: 4.774065
[INFO] Epoch: 16 , batch: 507 , training loss: 4.821480
[INFO] Epoch: 16 , batch: 508 , training loss: 4.794079
[INFO] Epoch: 16 , batch: 509 , training loss: 4.550761
[INFO] Epoch: 16 , batch: 510 , training loss: 4.593288
[INFO] Epoch: 16 , batch: 511 , training loss: 4.502393
[INFO] Epoch: 16 , batch: 512 , training loss: 4.583794
[INFO] Epoch: 16 , batch: 513 , training loss: 4.854967
[INFO] Epoch: 16 , batch: 514 , training loss: 4.499305
[INFO] Epoch: 16 , batch: 515 , training loss: 4.724589
[INFO] Epoch: 16 , batch: 516 , training loss: 4.537490
[INFO] Epoch: 16 , batch: 517 , training loss: 4.499342
[INFO] Epoch: 16 , batch: 518 , training loss: 4.481188
[INFO] Epoch: 16 , batch: 519 , training loss: 4.333287
[INFO] Epoch: 16 , batch: 520 , training loss: 4.551237
[INFO] Epoch: 16 , batch: 521 , training loss: 4.530481
[INFO] Epoch: 16 , batch: 522 , training loss: 4.599881
[INFO] Epoch: 16 , batch: 523 , training loss: 4.527832
[INFO] Epoch: 16 , batch: 524 , training loss: 4.793033
[INFO] Epoch: 16 , batch: 525 , training loss: 4.690429
[INFO] Epoch: 16 , batch: 526 , training loss: 4.478484
[INFO] Epoch: 16 , batch: 527 , training loss: 4.507850
[INFO] Epoch: 16 , batch: 528 , training loss: 4.546675
[INFO] Epoch: 16 , batch: 529 , training loss: 4.519055
[INFO] Epoch: 16 , batch: 530 , training loss: 4.367825
[INFO] Epoch: 16 , batch: 531 , training loss: 4.507494
[INFO] Epoch: 16 , batch: 532 , training loss: 4.385362
[INFO] Epoch: 16 , batch: 533 , training loss: 4.553394
[INFO] Epoch: 16 , batch: 534 , training loss: 4.535017
[INFO] Epoch: 16 , batch: 535 , training loss: 4.542862
[INFO] Epoch: 16 , batch: 536 , training loss: 4.381017
[INFO] Epoch: 16 , batch: 537 , training loss: 4.395115
[INFO] Epoch: 16 , batch: 538 , training loss: 4.456251
[INFO] Epoch: 16 , batch: 539 , training loss: 4.570617
[INFO] Epoch: 16 , batch: 540 , training loss: 5.099464
[INFO] Epoch: 16 , batch: 541 , training loss: 5.028599
[INFO] Epoch: 16 , batch: 542 , training loss: 4.889814
[INFO] Epoch: 17 , batch: 0 , training loss: 3.759413
[INFO] Epoch: 17 , batch: 1 , training loss: 3.772457
[INFO] Epoch: 17 , batch: 2 , training loss: 3.822495
[INFO] Epoch: 17 , batch: 3 , training loss: 3.663861
[INFO] Epoch: 17 , batch: 4 , training loss: 4.032909
[INFO] Epoch: 17 , batch: 5 , training loss: 3.633035
[INFO] Epoch: 17 , batch: 6 , training loss: 4.085332
[INFO] Epoch: 17 , batch: 7 , training loss: 3.899008
[INFO] Epoch: 17 , batch: 8 , training loss: 3.656761
[INFO] Epoch: 17 , batch: 9 , training loss: 3.837954
[INFO] Epoch: 17 , batch: 10 , training loss: 3.795434
[INFO] Epoch: 17 , batch: 11 , training loss: 3.750551
[INFO] Epoch: 17 , batch: 12 , training loss: 3.683245
[INFO] Epoch: 17 , batch: 13 , training loss: 3.701805
[INFO] Epoch: 17 , batch: 14 , training loss: 3.551556
[INFO] Epoch: 17 , batch: 15 , training loss: 3.753054
[INFO] Epoch: 17 , batch: 16 , training loss: 3.624578
[INFO] Epoch: 17 , batch: 17 , training loss: 3.804231
[INFO] Epoch: 17 , batch: 18 , training loss: 3.736854
[INFO] Epoch: 17 , batch: 19 , training loss: 3.463444
[INFO] Epoch: 17 , batch: 20 , training loss: 3.426019
[INFO] Epoch: 17 , batch: 21 , training loss: 3.587397
[INFO] Epoch: 17 , batch: 22 , training loss: 3.510616
[INFO] Epoch: 17 , batch: 23 , training loss: 3.716497
[INFO] Epoch: 17 , batch: 24 , training loss: 3.535059
[INFO] Epoch: 17 , batch: 25 , training loss: 3.707704
[INFO] Epoch: 17 , batch: 26 , training loss: 3.557977
[INFO] Epoch: 17 , batch: 27 , training loss: 3.527475
[INFO] Epoch: 17 , batch: 28 , training loss: 3.691607
[INFO] Epoch: 17 , batch: 29 , training loss: 3.519757
[INFO] Epoch: 17 , batch: 30 , training loss: 3.587795
[INFO] Epoch: 17 , batch: 31 , training loss: 3.628510
[INFO] Epoch: 17 , batch: 32 , training loss: 3.624285
[INFO] Epoch: 17 , batch: 33 , training loss: 3.685265
[INFO] Epoch: 17 , batch: 34 , training loss: 3.656927
[INFO] Epoch: 17 , batch: 35 , training loss: 3.635240
[INFO] Epoch: 17 , batch: 36 , training loss: 3.663049
[INFO] Epoch: 17 , batch: 37 , training loss: 3.596932
[INFO] Epoch: 17 , batch: 38 , training loss: 3.599818
[INFO] Epoch: 17 , batch: 39 , training loss: 3.480089
[INFO] Epoch: 17 , batch: 40 , training loss: 3.687035
[INFO] Epoch: 17 , batch: 41 , training loss: 3.603615
[INFO] Epoch: 17 , batch: 42 , training loss: 4.084694
[INFO] Epoch: 17 , batch: 43 , training loss: 3.759516
[INFO] Epoch: 17 , batch: 44 , training loss: 4.070821
[INFO] Epoch: 17 , batch: 45 , training loss: 4.020851
[INFO] Epoch: 17 , batch: 46 , training loss: 4.027798
[INFO] Epoch: 17 , batch: 47 , training loss: 3.752079
[INFO] Epoch: 17 , batch: 48 , training loss: 3.753889
[INFO] Epoch: 17 , batch: 49 , training loss: 3.903874
[INFO] Epoch: 17 , batch: 50 , training loss: 3.679690
[INFO] Epoch: 17 , batch: 51 , training loss: 3.869951
[INFO] Epoch: 17 , batch: 52 , training loss: 3.710357
[INFO] Epoch: 17 , batch: 53 , training loss: 3.887473
[INFO] Epoch: 17 , batch: 54 , training loss: 3.893440
[INFO] Epoch: 17 , batch: 55 , training loss: 3.947987
[INFO] Epoch: 17 , batch: 56 , training loss: 3.832718
[INFO] Epoch: 17 , batch: 57 , training loss: 3.717119
[INFO] Epoch: 17 , batch: 58 , training loss: 3.768409
[INFO] Epoch: 17 , batch: 59 , training loss: 3.821656
[INFO] Epoch: 17 , batch: 60 , training loss: 3.805261
[INFO] Epoch: 17 , batch: 61 , training loss: 3.879613
[INFO] Epoch: 17 , batch: 62 , training loss: 3.779336
[INFO] Epoch: 17 , batch: 63 , training loss: 3.974160
[INFO] Epoch: 17 , batch: 64 , training loss: 4.107888
[INFO] Epoch: 17 , batch: 65 , training loss: 3.814762
[INFO] Epoch: 17 , batch: 66 , training loss: 3.693241
[INFO] Epoch: 17 , batch: 67 , training loss: 3.758622
[INFO] Epoch: 17 , batch: 68 , training loss: 3.911061
[INFO] Epoch: 17 , batch: 69 , training loss: 3.781211
[INFO] Epoch: 17 , batch: 70 , training loss: 4.036734
[INFO] Epoch: 17 , batch: 71 , training loss: 3.882776
[INFO] Epoch: 17 , batch: 72 , training loss: 3.924224
[INFO] Epoch: 17 , batch: 73 , training loss: 3.857496
[INFO] Epoch: 17 , batch: 74 , training loss: 3.994294
[INFO] Epoch: 17 , batch: 75 , training loss: 3.813860
[INFO] Epoch: 17 , batch: 76 , training loss: 3.939428
[INFO] Epoch: 17 , batch: 77 , training loss: 3.901242
[INFO] Epoch: 17 , batch: 78 , training loss: 3.996129
[INFO] Epoch: 17 , batch: 79 , training loss: 3.842598
[INFO] Epoch: 17 , batch: 80 , training loss: 4.020617
[INFO] Epoch: 17 , batch: 81 , training loss: 3.974848
[INFO] Epoch: 17 , batch: 82 , training loss: 3.931766
[INFO] Epoch: 17 , batch: 83 , training loss: 4.073213
[INFO] Epoch: 17 , batch: 84 , training loss: 3.961419
[INFO] Epoch: 17 , batch: 85 , training loss: 4.028283
[INFO] Epoch: 17 , batch: 86 , training loss: 3.998410
[INFO] Epoch: 17 , batch: 87 , training loss: 3.956740
[INFO] Epoch: 17 , batch: 88 , training loss: 4.118217
[INFO] Epoch: 17 , batch: 89 , training loss: 3.914725
[INFO] Epoch: 17 , batch: 90 , training loss: 3.968618
[INFO] Epoch: 17 , batch: 91 , training loss: 3.923612
[INFO] Epoch: 17 , batch: 92 , training loss: 3.911423
[INFO] Epoch: 17 , batch: 93 , training loss: 4.018850
[INFO] Epoch: 17 , batch: 94 , training loss: 4.184278
[INFO] Epoch: 17 , batch: 95 , training loss: 3.945287
[INFO] Epoch: 17 , batch: 96 , training loss: 3.936970
[INFO] Epoch: 17 , batch: 97 , training loss: 3.849441
[INFO] Epoch: 17 , batch: 98 , training loss: 3.845041
[INFO] Epoch: 17 , batch: 99 , training loss: 3.914030
[INFO] Epoch: 17 , batch: 100 , training loss: 3.812848
[INFO] Epoch: 17 , batch: 101 , training loss: 3.868454
[INFO] Epoch: 17 , batch: 102 , training loss: 3.995329
[INFO] Epoch: 17 , batch: 103 , training loss: 3.787709
[INFO] Epoch: 17 , batch: 104 , training loss: 3.751829
[INFO] Epoch: 17 , batch: 105 , training loss: 4.030376
[INFO] Epoch: 17 , batch: 106 , training loss: 4.023512
[INFO] Epoch: 17 , batch: 107 , training loss: 3.887472
[INFO] Epoch: 17 , batch: 108 , training loss: 3.808244
[INFO] Epoch: 17 , batch: 109 , training loss: 3.727143
[INFO] Epoch: 17 , batch: 110 , training loss: 3.898577
[INFO] Epoch: 17 , batch: 111 , training loss: 3.990823
[INFO] Epoch: 17 , batch: 112 , training loss: 3.905407
[INFO] Epoch: 17 , batch: 113 , training loss: 3.895013
[INFO] Epoch: 17 , batch: 114 , training loss: 3.939944
[INFO] Epoch: 17 , batch: 115 , training loss: 3.909554
[INFO] Epoch: 17 , batch: 116 , training loss: 3.845671
[INFO] Epoch: 17 , batch: 117 , training loss: 4.073543
[INFO] Epoch: 17 , batch: 118 , training loss: 4.010090
[INFO] Epoch: 17 , batch: 119 , training loss: 4.168461
[INFO] Epoch: 17 , batch: 120 , training loss: 4.133898
[INFO] Epoch: 17 , batch: 121 , training loss: 3.967270
[INFO] Epoch: 17 , batch: 122 , training loss: 3.884289
[INFO] Epoch: 17 , batch: 123 , training loss: 3.912755
[INFO] Epoch: 17 , batch: 124 , training loss: 4.031336
[INFO] Epoch: 17 , batch: 125 , training loss: 3.820752
[INFO] Epoch: 17 , batch: 126 , training loss: 3.832114
[INFO] Epoch: 17 , batch: 127 , training loss: 3.847538
[INFO] Epoch: 17 , batch: 128 , training loss: 3.986753
[INFO] Epoch: 17 , batch: 129 , training loss: 3.977053
[INFO] Epoch: 17 , batch: 130 , training loss: 3.924145
[INFO] Epoch: 17 , batch: 131 , training loss: 3.928330
[INFO] Epoch: 17 , batch: 132 , training loss: 3.930028
[INFO] Epoch: 17 , batch: 133 , training loss: 3.902048
[INFO] Epoch: 17 , batch: 134 , training loss: 3.733340
[INFO] Epoch: 17 , batch: 135 , training loss: 3.759083
[INFO] Epoch: 17 , batch: 136 , training loss: 4.044776
[INFO] Epoch: 17 , batch: 137 , training loss: 3.949735
[INFO] Epoch: 17 , batch: 138 , training loss: 4.032105
[INFO] Epoch: 17 , batch: 139 , training loss: 4.632454
[INFO] Epoch: 17 , batch: 140 , training loss: 4.399197
[INFO] Epoch: 17 , batch: 141 , training loss: 4.148332
[INFO] Epoch: 17 , batch: 142 , training loss: 3.883234
[INFO] Epoch: 17 , batch: 143 , training loss: 4.011688
[INFO] Epoch: 17 , batch: 144 , training loss: 3.787792
[INFO] Epoch: 17 , batch: 145 , training loss: 3.906976
[INFO] Epoch: 17 , batch: 146 , training loss: 4.092129
[INFO] Epoch: 17 , batch: 147 , training loss: 3.761686
[INFO] Epoch: 17 , batch: 148 , training loss: 3.767188
[INFO] Epoch: 17 , batch: 149 , training loss: 3.841777
[INFO] Epoch: 17 , batch: 150 , training loss: 4.075385
[INFO] Epoch: 17 , batch: 151 , training loss: 3.935536
[INFO] Epoch: 17 , batch: 152 , training loss: 3.978170
[INFO] Epoch: 17 , batch: 153 , training loss: 3.993202
[INFO] Epoch: 17 , batch: 154 , training loss: 4.061118
[INFO] Epoch: 17 , batch: 155 , training loss: 4.308527
[INFO] Epoch: 17 , batch: 156 , training loss: 3.996746
[INFO] Epoch: 17 , batch: 157 , training loss: 3.964246
[INFO] Epoch: 17 , batch: 158 , training loss: 4.152413
[INFO] Epoch: 17 , batch: 159 , training loss: 4.047487
[INFO] Epoch: 17 , batch: 160 , training loss: 4.343125
[INFO] Epoch: 17 , batch: 161 , training loss: 4.443001
[INFO] Epoch: 17 , batch: 162 , training loss: 4.361578
[INFO] Epoch: 17 , batch: 163 , training loss: 4.494603
[INFO] Epoch: 17 , batch: 164 , training loss: 4.471885
[INFO] Epoch: 17 , batch: 165 , training loss: 4.397039
[INFO] Epoch: 17 , batch: 166 , training loss: 4.296838
[INFO] Epoch: 17 , batch: 167 , training loss: 4.458039
[INFO] Epoch: 17 , batch: 168 , training loss: 4.165194
[INFO] Epoch: 17 , batch: 169 , training loss: 4.123760
[INFO] Epoch: 17 , batch: 170 , training loss: 4.296484
[INFO] Epoch: 17 , batch: 171 , training loss: 3.705530
[INFO] Epoch: 17 , batch: 172 , training loss: 3.910054
[INFO] Epoch: 17 , batch: 173 , training loss: 4.237147
[INFO] Epoch: 17 , batch: 174 , training loss: 4.564535
[INFO] Epoch: 17 , batch: 175 , training loss: 4.915560
[INFO] Epoch: 17 , batch: 176 , training loss: 4.568643
[INFO] Epoch: 17 , batch: 177 , training loss: 4.203557
[INFO] Epoch: 17 , batch: 178 , training loss: 4.249424
[INFO] Epoch: 17 , batch: 179 , training loss: 4.279879
[INFO] Epoch: 17 , batch: 180 , training loss: 4.231484
[INFO] Epoch: 17 , batch: 181 , training loss: 4.482011
[INFO] Epoch: 17 , batch: 182 , training loss: 4.414774
[INFO] Epoch: 17 , batch: 183 , training loss: 4.404659
[INFO] Epoch: 17 , batch: 184 , training loss: 4.299662
[INFO] Epoch: 17 , batch: 185 , training loss: 4.235064
[INFO] Epoch: 17 , batch: 186 , training loss: 4.398906
[INFO] Epoch: 17 , batch: 187 , training loss: 4.478508
[INFO] Epoch: 17 , batch: 188 , training loss: 4.447689
[INFO] Epoch: 17 , batch: 189 , training loss: 4.363815
[INFO] Epoch: 17 , batch: 190 , training loss: 4.389274
[INFO] Epoch: 17 , batch: 191 , training loss: 4.495992
[INFO] Epoch: 17 , batch: 192 , training loss: 4.301622
[INFO] Epoch: 17 , batch: 193 , training loss: 4.421981
[INFO] Epoch: 17 , batch: 194 , training loss: 4.366388
[INFO] Epoch: 17 , batch: 195 , training loss: 4.276020
[INFO] Epoch: 17 , batch: 196 , training loss: 4.137360
[INFO] Epoch: 17 , batch: 197 , training loss: 4.253585
[INFO] Epoch: 17 , batch: 198 , training loss: 4.158850
[INFO] Epoch: 17 , batch: 199 , training loss: 4.279306
[INFO] Epoch: 17 , batch: 200 , training loss: 4.202434
[INFO] Epoch: 17 , batch: 201 , training loss: 4.098306
[INFO] Epoch: 17 , batch: 202 , training loss: 4.107032
[INFO] Epoch: 17 , batch: 203 , training loss: 4.170096
[INFO] Epoch: 17 , batch: 204 , training loss: 4.328409
[INFO] Epoch: 17 , batch: 205 , training loss: 3.878965
[INFO] Epoch: 17 , batch: 206 , training loss: 3.825963
[INFO] Epoch: 17 , batch: 207 , training loss: 3.834987
[INFO] Epoch: 17 , batch: 208 , training loss: 4.174866
[INFO] Epoch: 17 , batch: 209 , training loss: 4.122701
[INFO] Epoch: 17 , batch: 210 , training loss: 4.113569
[INFO] Epoch: 17 , batch: 211 , training loss: 4.120568
[INFO] Epoch: 17 , batch: 212 , training loss: 4.230819
[INFO] Epoch: 17 , batch: 213 , training loss: 4.182507
[INFO] Epoch: 17 , batch: 214 , training loss: 4.290372
[INFO] Epoch: 17 , batch: 215 , training loss: 4.470935
[INFO] Epoch: 17 , batch: 216 , training loss: 4.198614
[INFO] Epoch: 17 , batch: 217 , training loss: 4.117064
[INFO] Epoch: 17 , batch: 218 , training loss: 4.098608
[INFO] Epoch: 17 , batch: 219 , training loss: 4.227438
[INFO] Epoch: 17 , batch: 220 , training loss: 4.023671
[INFO] Epoch: 17 , batch: 221 , training loss: 4.053389
[INFO] Epoch: 17 , batch: 222 , training loss: 4.202635
[INFO] Epoch: 17 , batch: 223 , training loss: 4.296021
[INFO] Epoch: 17 , batch: 224 , training loss: 4.341434
[INFO] Epoch: 17 , batch: 225 , training loss: 4.232956
[INFO] Epoch: 17 , batch: 226 , training loss: 4.359452
[INFO] Epoch: 17 , batch: 227 , training loss: 4.308577
[INFO] Epoch: 17 , batch: 228 , training loss: 4.357045
[INFO] Epoch: 17 , batch: 229 , training loss: 4.218510
[INFO] Epoch: 17 , batch: 230 , training loss: 4.081561
[INFO] Epoch: 17 , batch: 231 , training loss: 3.936362
[INFO] Epoch: 17 , batch: 232 , training loss: 4.089209
[INFO] Epoch: 17 , batch: 233 , training loss: 4.083877
[INFO] Epoch: 17 , batch: 234 , training loss: 3.785426
[INFO] Epoch: 17 , batch: 235 , training loss: 3.894540
[INFO] Epoch: 17 , batch: 236 , training loss: 4.022574
[INFO] Epoch: 17 , batch: 237 , training loss: 4.226711
[INFO] Epoch: 17 , batch: 238 , training loss: 3.967605
[INFO] Epoch: 17 , batch: 239 , training loss: 4.032362
[INFO] Epoch: 17 , batch: 240 , training loss: 4.092394
[INFO] Epoch: 17 , batch: 241 , training loss: 3.882427
[INFO] Epoch: 17 , batch: 242 , training loss: 3.914248
[INFO] Epoch: 17 , batch: 243 , training loss: 4.224586
[INFO] Epoch: 17 , batch: 244 , training loss: 4.128737
[INFO] Epoch: 17 , batch: 245 , training loss: 4.133551
[INFO] Epoch: 17 , batch: 246 , training loss: 3.809155
[INFO] Epoch: 17 , batch: 247 , training loss: 3.994246
[INFO] Epoch: 17 , batch: 248 , training loss: 4.084238
[INFO] Epoch: 17 , batch: 249 , training loss: 4.081625
[INFO] Epoch: 17 , batch: 250 , training loss: 3.824233
[INFO] Epoch: 17 , batch: 251 , training loss: 4.290614
[INFO] Epoch: 17 , batch: 252 , training loss: 3.993721
[INFO] Epoch: 17 , batch: 253 , training loss: 3.898027
[INFO] Epoch: 17 , batch: 254 , training loss: 4.195852
[INFO] Epoch: 17 , batch: 255 , training loss: 4.178903
[INFO] Epoch: 17 , batch: 256 , training loss: 4.190777
[INFO] Epoch: 17 , batch: 257 , training loss: 4.320244
[INFO] Epoch: 17 , batch: 258 , training loss: 4.373480
[INFO] Epoch: 17 , batch: 259 , training loss: 4.420364
[INFO] Epoch: 17 , batch: 260 , training loss: 4.139815
[INFO] Epoch: 17 , batch: 261 , training loss: 4.278618
[INFO] Epoch: 17 , batch: 262 , training loss: 4.483700
[INFO] Epoch: 17 , batch: 263 , training loss: 4.643769
[INFO] Epoch: 17 , batch: 264 , training loss: 3.953902
[INFO] Epoch: 17 , batch: 265 , training loss: 4.101595
[INFO] Epoch: 17 , batch: 266 , training loss: 4.523326
[INFO] Epoch: 17 , batch: 267 , training loss: 4.257949
[INFO] Epoch: 17 , batch: 268 , training loss: 4.185001
[INFO] Epoch: 17 , batch: 269 , training loss: 4.161634
[INFO] Epoch: 17 , batch: 270 , training loss: 4.175960
[INFO] Epoch: 17 , batch: 271 , training loss: 4.228057
[INFO] Epoch: 17 , batch: 272 , training loss: 4.201988
[INFO] Epoch: 17 , batch: 273 , training loss: 4.228955
[INFO] Epoch: 17 , batch: 274 , training loss: 4.318040
[INFO] Epoch: 17 , batch: 275 , training loss: 4.197490
[INFO] Epoch: 17 , batch: 276 , training loss: 4.229918
[INFO] Epoch: 17 , batch: 277 , training loss: 4.389947
[INFO] Epoch: 17 , batch: 278 , training loss: 4.048822
[INFO] Epoch: 17 , batch: 279 , training loss: 4.072832
[INFO] Epoch: 17 , batch: 280 , training loss: 4.025130
[INFO] Epoch: 17 , batch: 281 , training loss: 4.159493
[INFO] Epoch: 17 , batch: 282 , training loss: 4.065751
[INFO] Epoch: 17 , batch: 283 , training loss: 4.110018
[INFO] Epoch: 17 , batch: 284 , training loss: 4.111424
[INFO] Epoch: 17 , batch: 285 , training loss: 4.061113
[INFO] Epoch: 17 , batch: 286 , training loss: 4.058181
[INFO] Epoch: 17 , batch: 287 , training loss: 3.997396
[INFO] Epoch: 17 , batch: 288 , training loss: 4.008946
[INFO] Epoch: 17 , batch: 289 , training loss: 4.065645
[INFO] Epoch: 17 , batch: 290 , training loss: 3.835329
[INFO] Epoch: 17 , batch: 291 , training loss: 3.825217
[INFO] Epoch: 17 , batch: 292 , training loss: 3.934695
[INFO] Epoch: 17 , batch: 293 , training loss: 3.830930
[INFO] Epoch: 17 , batch: 294 , training loss: 4.503169
[INFO] Epoch: 17 , batch: 295 , training loss: 4.291925
[INFO] Epoch: 17 , batch: 296 , training loss: 4.221168
[INFO] Epoch: 17 , batch: 297 , training loss: 4.167177
[INFO] Epoch: 17 , batch: 298 , training loss: 4.012345
[INFO] Epoch: 17 , batch: 299 , training loss: 4.035685
[INFO] Epoch: 17 , batch: 300 , training loss: 4.001072
[INFO] Epoch: 17 , batch: 301 , training loss: 3.949858
[INFO] Epoch: 17 , batch: 302 , training loss: 4.091776
[INFO] Epoch: 17 , batch: 303 , training loss: 4.122042
[INFO] Epoch: 17 , batch: 304 , training loss: 4.301102
[INFO] Epoch: 17 , batch: 305 , training loss: 4.072406
[INFO] Epoch: 17 , batch: 306 , training loss: 4.217902
[INFO] Epoch: 17 , batch: 307 , training loss: 4.203688
[INFO] Epoch: 17 , batch: 308 , training loss: 4.055251
[INFO] Epoch: 17 , batch: 309 , training loss: 4.063107
[INFO] Epoch: 17 , batch: 310 , training loss: 3.933673
[INFO] Epoch: 17 , batch: 311 , training loss: 3.955693
[INFO] Epoch: 17 , batch: 312 , training loss: 3.867742
[INFO] Epoch: 17 , batch: 313 , training loss: 3.982754
[INFO] Epoch: 17 , batch: 314 , training loss: 4.050066
[INFO] Epoch: 17 , batch: 315 , training loss: 4.097478
[INFO] Epoch: 17 , batch: 316 , training loss: 4.344493
[INFO] Epoch: 17 , batch: 317 , training loss: 4.804010
[INFO] Epoch: 17 , batch: 318 , training loss: 4.965747
[INFO] Epoch: 17 , batch: 319 , training loss: 4.520162
[INFO] Epoch: 17 , batch: 320 , training loss: 4.076101
[INFO] Epoch: 17 , batch: 321 , training loss: 3.909264
[INFO] Epoch: 17 , batch: 322 , training loss: 4.028431
[INFO] Epoch: 17 , batch: 323 , training loss: 4.032251
[INFO] Epoch: 17 , batch: 324 , training loss: 4.005034
[INFO] Epoch: 17 , batch: 325 , training loss: 4.152922
[INFO] Epoch: 17 , batch: 326 , training loss: 4.218511
[INFO] Epoch: 17 , batch: 327 , training loss: 4.115230
[INFO] Epoch: 17 , batch: 328 , training loss: 4.104761
[INFO] Epoch: 17 , batch: 329 , training loss: 4.025618
[INFO] Epoch: 17 , batch: 330 , training loss: 4.029859
[INFO] Epoch: 17 , batch: 331 , training loss: 4.164073
[INFO] Epoch: 17 , batch: 332 , training loss: 3.973215
[INFO] Epoch: 17 , batch: 333 , training loss: 3.986996
[INFO] Epoch: 17 , batch: 334 , training loss: 4.014147
[INFO] Epoch: 17 , batch: 335 , training loss: 4.135468
[INFO] Epoch: 17 , batch: 336 , training loss: 4.157165
[INFO] Epoch: 17 , batch: 337 , training loss: 4.175923
[INFO] Epoch: 17 , batch: 338 , training loss: 4.378528
[INFO] Epoch: 17 , batch: 339 , training loss: 4.235806
[INFO] Epoch: 17 , batch: 340 , training loss: 4.408192
[INFO] Epoch: 17 , batch: 341 , training loss: 4.165893
[INFO] Epoch: 17 , batch: 342 , training loss: 3.948785
[INFO] Epoch: 17 , batch: 343 , training loss: 4.031727
[INFO] Epoch: 17 , batch: 344 , training loss: 3.875750
[INFO] Epoch: 17 , batch: 345 , training loss: 3.990586
[INFO] Epoch: 17 , batch: 346 , training loss: 4.075007
[INFO] Epoch: 17 , batch: 347 , training loss: 3.957285
[INFO] Epoch: 17 , batch: 348 , training loss: 4.110869
[INFO] Epoch: 17 , batch: 349 , training loss: 4.230013
[INFO] Epoch: 17 , batch: 350 , training loss: 4.007652
[INFO] Epoch: 17 , batch: 351 , training loss: 4.067285
[INFO] Epoch: 17 , batch: 352 , training loss: 4.099734
[INFO] Epoch: 17 , batch: 353 , training loss: 4.058315
[INFO] Epoch: 17 , batch: 354 , training loss: 4.173816
[INFO] Epoch: 17 , batch: 355 , training loss: 4.211954
[INFO] Epoch: 17 , batch: 356 , training loss: 4.035440
[INFO] Epoch: 17 , batch: 357 , training loss: 4.115412
[INFO] Epoch: 17 , batch: 358 , training loss: 4.036673
[INFO] Epoch: 17 , batch: 359 , training loss: 4.024580
[INFO] Epoch: 17 , batch: 360 , training loss: 4.109089
[INFO] Epoch: 17 , batch: 361 , training loss: 4.069450
[INFO] Epoch: 17 , batch: 362 , training loss: 4.191588
[INFO] Epoch: 17 , batch: 363 , training loss: 4.063928
[INFO] Epoch: 17 , batch: 364 , training loss: 4.106616
[INFO] Epoch: 17 , batch: 365 , training loss: 4.038507
[INFO] Epoch: 17 , batch: 366 , training loss: 4.167997
[INFO] Epoch: 17 , batch: 367 , training loss: 4.254239
[INFO] Epoch: 17 , batch: 368 , training loss: 4.698581
[INFO] Epoch: 17 , batch: 369 , training loss: 4.313855
[INFO] Epoch: 17 , batch: 370 , training loss: 4.077070
[INFO] Epoch: 17 , batch: 371 , training loss: 4.486275
[INFO] Epoch: 17 , batch: 372 , training loss: 4.774380
[INFO] Epoch: 17 , batch: 373 , training loss: 4.894611
[INFO] Epoch: 17 , batch: 374 , training loss: 4.950093
[INFO] Epoch: 17 , batch: 375 , training loss: 4.912286
[INFO] Epoch: 17 , batch: 376 , training loss: 4.840209
[INFO] Epoch: 17 , batch: 377 , training loss: 4.571391
[INFO] Epoch: 17 , batch: 378 , training loss: 4.668503
[INFO] Epoch: 17 , batch: 379 , training loss: 4.658307
[INFO] Epoch: 17 , batch: 380 , training loss: 4.787588
[INFO] Epoch: 17 , batch: 381 , training loss: 4.517549
[INFO] Epoch: 17 , batch: 382 , training loss: 4.771709
[INFO] Epoch: 17 , batch: 383 , training loss: 4.769978
[INFO] Epoch: 17 , batch: 384 , training loss: 4.795179
[INFO] Epoch: 17 , batch: 385 , training loss: 4.530584
[INFO] Epoch: 17 , batch: 386 , training loss: 4.754232
[INFO] Epoch: 17 , batch: 387 , training loss: 4.710187
[INFO] Epoch: 17 , batch: 388 , training loss: 4.536013
[INFO] Epoch: 17 , batch: 389 , training loss: 4.369330
[INFO] Epoch: 17 , batch: 390 , training loss: 4.357556
[INFO] Epoch: 17 , batch: 391 , training loss: 4.387720
[INFO] Epoch: 17 , batch: 392 , training loss: 4.761037
[INFO] Epoch: 17 , batch: 393 , training loss: 4.630867
[INFO] Epoch: 17 , batch: 394 , training loss: 4.674676
[INFO] Epoch: 17 , batch: 395 , training loss: 4.529948
[INFO] Epoch: 17 , batch: 396 , training loss: 4.302118
[INFO] Epoch: 17 , batch: 397 , training loss: 4.467900
[INFO] Epoch: 17 , batch: 398 , training loss: 4.327132
[INFO] Epoch: 17 , batch: 399 , training loss: 4.383254
[INFO] Epoch: 17 , batch: 400 , training loss: 4.370359
[INFO] Epoch: 17 , batch: 401 , training loss: 4.773547
[INFO] Epoch: 17 , batch: 402 , training loss: 4.487193
[INFO] Epoch: 17 , batch: 403 , training loss: 4.286939
[INFO] Epoch: 17 , batch: 404 , training loss: 4.497053
[INFO] Epoch: 17 , batch: 405 , training loss: 4.526280
[INFO] Epoch: 17 , batch: 406 , training loss: 4.433340
[INFO] Epoch: 17 , batch: 407 , training loss: 4.506329
[INFO] Epoch: 17 , batch: 408 , training loss: 4.450885
[INFO] Epoch: 17 , batch: 409 , training loss: 4.453243
[INFO] Epoch: 17 , batch: 410 , training loss: 4.476824
[INFO] Epoch: 17 , batch: 411 , training loss: 4.681846
[INFO] Epoch: 17 , batch: 412 , training loss: 4.528924
[INFO] Epoch: 17 , batch: 413 , training loss: 4.399959
[INFO] Epoch: 17 , batch: 414 , training loss: 4.422075
[INFO] Epoch: 17 , batch: 415 , training loss: 4.440123
[INFO] Epoch: 17 , batch: 416 , training loss: 4.524910
[INFO] Epoch: 17 , batch: 417 , training loss: 4.452017
[INFO] Epoch: 17 , batch: 418 , training loss: 4.474963
[INFO] Epoch: 17 , batch: 419 , training loss: 4.422511
[INFO] Epoch: 17 , batch: 420 , training loss: 4.394128
[INFO] Epoch: 17 , batch: 421 , training loss: 4.405101
[INFO] Epoch: 17 , batch: 422 , training loss: 4.282462
[INFO] Epoch: 17 , batch: 423 , training loss: 4.502113
[INFO] Epoch: 17 , batch: 424 , training loss: 4.666723
[INFO] Epoch: 17 , batch: 425 , training loss: 4.527401
[INFO] Epoch: 17 , batch: 426 , training loss: 4.242554
[INFO] Epoch: 17 , batch: 427 , training loss: 4.486220
[INFO] Epoch: 17 , batch: 428 , training loss: 4.366330
[INFO] Epoch: 17 , batch: 429 , training loss: 4.232823
[INFO] Epoch: 17 , batch: 430 , training loss: 4.512117
[INFO] Epoch: 17 , batch: 431 , training loss: 4.083472
[INFO] Epoch: 17 , batch: 432 , training loss: 4.179965
[INFO] Epoch: 17 , batch: 433 , training loss: 4.188673
[INFO] Epoch: 17 , batch: 434 , training loss: 4.055474
[INFO] Epoch: 17 , batch: 435 , training loss: 4.436049
[INFO] Epoch: 17 , batch: 436 , training loss: 4.515879
[INFO] Epoch: 17 , batch: 437 , training loss: 4.245092
[INFO] Epoch: 17 , batch: 438 , training loss: 4.102829
[INFO] Epoch: 17 , batch: 439 , training loss: 4.335442
[INFO] Epoch: 17 , batch: 440 , training loss: 4.447122
[INFO] Epoch: 17 , batch: 441 , training loss: 4.559862
[INFO] Epoch: 17 , batch: 442 , training loss: 4.329428
[INFO] Epoch: 17 , batch: 443 , training loss: 4.519793
[INFO] Epoch: 17 , batch: 444 , training loss: 4.139429
[INFO] Epoch: 17 , batch: 445 , training loss: 4.032433
[INFO] Epoch: 17 , batch: 446 , training loss: 3.960917
[INFO] Epoch: 17 , batch: 447 , training loss: 4.144505
[INFO] Epoch: 17 , batch: 448 , training loss: 4.264412
[INFO] Epoch: 17 , batch: 449 , training loss: 4.701813
[INFO] Epoch: 17 , batch: 450 , training loss: 4.736989
[INFO] Epoch: 17 , batch: 451 , training loss: 4.646715
[INFO] Epoch: 17 , batch: 452 , training loss: 4.427340
[INFO] Epoch: 17 , batch: 453 , training loss: 4.195284
[INFO] Epoch: 17 , batch: 454 , training loss: 4.342053
[INFO] Epoch: 17 , batch: 455 , training loss: 4.384837
[INFO] Epoch: 17 , batch: 456 , training loss: 4.386224
[INFO] Epoch: 17 , batch: 457 , training loss: 4.497170
[INFO] Epoch: 17 , batch: 458 , training loss: 4.214887
[INFO] Epoch: 17 , batch: 459 , training loss: 4.181379
[INFO] Epoch: 17 , batch: 460 , training loss: 4.291641
[INFO] Epoch: 17 , batch: 461 , training loss: 4.287437
[INFO] Epoch: 17 , batch: 462 , training loss: 4.340257
[INFO] Epoch: 17 , batch: 463 , training loss: 4.216352
[INFO] Epoch: 17 , batch: 464 , training loss: 4.444282
[INFO] Epoch: 17 , batch: 465 , training loss: 4.355744
[INFO] Epoch: 17 , batch: 466 , training loss: 4.436047
[INFO] Epoch: 17 , batch: 467 , training loss: 4.432827
[INFO] Epoch: 17 , batch: 468 , training loss: 4.400684
[INFO] Epoch: 17 , batch: 469 , training loss: 4.424332
[INFO] Epoch: 17 , batch: 470 , training loss: 4.227652
[INFO] Epoch: 17 , batch: 471 , training loss: 4.307613
[INFO] Epoch: 17 , batch: 472 , training loss: 4.370349
[INFO] Epoch: 17 , batch: 473 , training loss: 4.300989
[INFO] Epoch: 17 , batch: 474 , training loss: 4.090500
[INFO] Epoch: 17 , batch: 475 , training loss: 3.961969
[INFO] Epoch: 17 , batch: 476 , training loss: 4.341835
[INFO] Epoch: 17 , batch: 477 , training loss: 4.482076
[INFO] Epoch: 17 , batch: 478 , training loss: 4.506381
[INFO] Epoch: 17 , batch: 479 , training loss: 4.478469
[INFO] Epoch: 17 , batch: 480 , training loss: 4.580583
[INFO] Epoch: 17 , batch: 481 , training loss: 4.441897
[INFO] Epoch: 17 , batch: 482 , training loss: 4.586904
[INFO] Epoch: 17 , batch: 483 , training loss: 4.406850
[INFO] Epoch: 17 , batch: 484 , training loss: 4.225051
[INFO] Epoch: 17 , batch: 485 , training loss: 4.320912
[INFO] Epoch: 17 , batch: 486 , training loss: 4.188498
[INFO] Epoch: 17 , batch: 487 , training loss: 4.173934
[INFO] Epoch: 17 , batch: 488 , training loss: 4.388920
[INFO] Epoch: 17 , batch: 489 , training loss: 4.276698
[INFO] Epoch: 17 , batch: 490 , training loss: 4.324576
[INFO] Epoch: 17 , batch: 491 , training loss: 4.287967
[INFO] Epoch: 17 , batch: 492 , training loss: 4.210793
[INFO] Epoch: 17 , batch: 493 , training loss: 4.408412
[INFO] Epoch: 17 , batch: 494 , training loss: 4.335670
[INFO] Epoch: 17 , batch: 495 , training loss: 4.449756
[INFO] Epoch: 17 , batch: 496 , training loss: 4.315788
[INFO] Epoch: 17 , batch: 497 , training loss: 4.366996
[INFO] Epoch: 17 , batch: 498 , training loss: 4.379228
[INFO] Epoch: 17 , batch: 499 , training loss: 4.447182
[INFO] Epoch: 17 , batch: 500 , training loss: 4.600963
[INFO] Epoch: 17 , batch: 501 , training loss: 4.918869
[INFO] Epoch: 17 , batch: 502 , training loss: 5.023453
[INFO] Epoch: 17 , batch: 503 , training loss: 4.749371
[INFO] Epoch: 17 , batch: 504 , training loss: 4.844905
[INFO] Epoch: 17 , batch: 505 , training loss: 4.806087
[INFO] Epoch: 17 , batch: 506 , training loss: 4.758977
[INFO] Epoch: 17 , batch: 507 , training loss: 4.814736
[INFO] Epoch: 17 , batch: 508 , training loss: 4.775160
[INFO] Epoch: 17 , batch: 509 , training loss: 4.533040
[INFO] Epoch: 17 , batch: 510 , training loss: 4.586237
[INFO] Epoch: 17 , batch: 511 , training loss: 4.498986
[INFO] Epoch: 17 , batch: 512 , training loss: 4.576101
[INFO] Epoch: 17 , batch: 513 , training loss: 4.844429
[INFO] Epoch: 17 , batch: 514 , training loss: 4.480846
[INFO] Epoch: 17 , batch: 515 , training loss: 4.725838
[INFO] Epoch: 17 , batch: 516 , training loss: 4.534298
[INFO] Epoch: 17 , batch: 517 , training loss: 4.486330
[INFO] Epoch: 17 , batch: 518 , training loss: 4.464915
[INFO] Epoch: 17 , batch: 519 , training loss: 4.315615
[INFO] Epoch: 17 , batch: 520 , training loss: 4.544397
[INFO] Epoch: 17 , batch: 521 , training loss: 4.532496
[INFO] Epoch: 17 , batch: 522 , training loss: 4.584401
[INFO] Epoch: 17 , batch: 523 , training loss: 4.511038
[INFO] Epoch: 17 , batch: 524 , training loss: 4.795738
[INFO] Epoch: 17 , batch: 525 , training loss: 4.693000
[INFO] Epoch: 17 , batch: 526 , training loss: 4.466940
[INFO] Epoch: 17 , batch: 527 , training loss: 4.500594
[INFO] Epoch: 17 , batch: 528 , training loss: 4.531167
[INFO] Epoch: 17 , batch: 529 , training loss: 4.523036
[INFO] Epoch: 17 , batch: 530 , training loss: 4.357336
[INFO] Epoch: 17 , batch: 531 , training loss: 4.496279
[INFO] Epoch: 17 , batch: 532 , training loss: 4.377521
[INFO] Epoch: 17 , batch: 533 , training loss: 4.556691
[INFO] Epoch: 17 , batch: 534 , training loss: 4.531282
[INFO] Epoch: 17 , batch: 535 , training loss: 4.541306
[INFO] Epoch: 17 , batch: 536 , training loss: 4.370979
[INFO] Epoch: 17 , batch: 537 , training loss: 4.386724
[INFO] Epoch: 17 , batch: 538 , training loss: 4.448205
[INFO] Epoch: 17 , batch: 539 , training loss: 4.557846
[INFO] Epoch: 17 , batch: 540 , training loss: 5.092471
[INFO] Epoch: 17 , batch: 541 , training loss: 5.014460
[INFO] Epoch: 17 , batch: 542 , training loss: 4.881209
[INFO] Epoch: 18 , batch: 0 , training loss: 3.667725
[INFO] Epoch: 18 , batch: 1 , training loss: 3.754548
[INFO] Epoch: 18 , batch: 2 , training loss: 3.792699
[INFO] Epoch: 18 , batch: 3 , training loss: 3.627376
[INFO] Epoch: 18 , batch: 4 , training loss: 4.025831
[INFO] Epoch: 18 , batch: 5 , training loss: 3.649138
[INFO] Epoch: 18 , batch: 6 , training loss: 4.063509
[INFO] Epoch: 18 , batch: 7 , training loss: 3.881791
[INFO] Epoch: 18 , batch: 8 , training loss: 3.641594
[INFO] Epoch: 18 , batch: 9 , training loss: 3.813338
[INFO] Epoch: 18 , batch: 10 , training loss: 3.783108
[INFO] Epoch: 18 , batch: 11 , training loss: 3.726194
[INFO] Epoch: 18 , batch: 12 , training loss: 3.653669
[INFO] Epoch: 18 , batch: 13 , training loss: 3.681848
[INFO] Epoch: 18 , batch: 14 , training loss: 3.535955
[INFO] Epoch: 18 , batch: 15 , training loss: 3.742391
[INFO] Epoch: 18 , batch: 16 , training loss: 3.590266
[INFO] Epoch: 18 , batch: 17 , training loss: 3.786751
[INFO] Epoch: 18 , batch: 18 , training loss: 3.721707
[INFO] Epoch: 18 , batch: 19 , training loss: 3.427217
[INFO] Epoch: 18 , batch: 20 , training loss: 3.433101
[INFO] Epoch: 18 , batch: 21 , training loss: 3.557419
[INFO] Epoch: 18 , batch: 22 , training loss: 3.478276
[INFO] Epoch: 18 , batch: 23 , training loss: 3.680454
[INFO] Epoch: 18 , batch: 24 , training loss: 3.516093
[INFO] Epoch: 18 , batch: 25 , training loss: 3.721060
[INFO] Epoch: 18 , batch: 26 , training loss: 3.526873
[INFO] Epoch: 18 , batch: 27 , training loss: 3.484464
[INFO] Epoch: 18 , batch: 28 , training loss: 3.642424
[INFO] Epoch: 18 , batch: 29 , training loss: 3.503189
[INFO] Epoch: 18 , batch: 30 , training loss: 3.545652
[INFO] Epoch: 18 , batch: 31 , training loss: 3.613289
[INFO] Epoch: 18 , batch: 32 , training loss: 3.600471
[INFO] Epoch: 18 , batch: 33 , training loss: 3.644689
[INFO] Epoch: 18 , batch: 34 , training loss: 3.622436
[INFO] Epoch: 18 , batch: 35 , training loss: 3.615954
[INFO] Epoch: 18 , batch: 36 , training loss: 3.618520
[INFO] Epoch: 18 , batch: 37 , training loss: 3.556176
[INFO] Epoch: 18 , batch: 38 , training loss: 3.576387
[INFO] Epoch: 18 , batch: 39 , training loss: 3.455880
[INFO] Epoch: 18 , batch: 40 , training loss: 3.671240
[INFO] Epoch: 18 , batch: 41 , training loss: 3.604869
[INFO] Epoch: 18 , batch: 42 , training loss: 4.042079
[INFO] Epoch: 18 , batch: 43 , training loss: 3.675375
[INFO] Epoch: 18 , batch: 44 , training loss: 4.085357
[INFO] Epoch: 18 , batch: 45 , training loss: 4.000685
[INFO] Epoch: 18 , batch: 46 , training loss: 3.985769
[INFO] Epoch: 18 , batch: 47 , training loss: 3.765899
[INFO] Epoch: 18 , batch: 48 , training loss: 3.749516
[INFO] Epoch: 18 , batch: 49 , training loss: 3.876002
[INFO] Epoch: 18 , batch: 50 , training loss: 3.650921
[INFO] Epoch: 18 , batch: 51 , training loss: 3.865122
[INFO] Epoch: 18 , batch: 52 , training loss: 3.704571
[INFO] Epoch: 18 , batch: 53 , training loss: 3.883617
[INFO] Epoch: 18 , batch: 54 , training loss: 3.892776
[INFO] Epoch: 18 , batch: 55 , training loss: 3.954572
[INFO] Epoch: 18 , batch: 56 , training loss: 3.816499
[INFO] Epoch: 18 , batch: 57 , training loss: 3.696510
[INFO] Epoch: 18 , batch: 58 , training loss: 3.768025
[INFO] Epoch: 18 , batch: 59 , training loss: 3.824585
[INFO] Epoch: 18 , batch: 60 , training loss: 3.782251
[INFO] Epoch: 18 , batch: 61 , training loss: 3.865874
[INFO] Epoch: 18 , batch: 62 , training loss: 3.765816
[INFO] Epoch: 18 , batch: 63 , training loss: 3.967360
[INFO] Epoch: 18 , batch: 64 , training loss: 4.093497
[INFO] Epoch: 18 , batch: 65 , training loss: 3.810515
[INFO] Epoch: 18 , batch: 66 , training loss: 3.664867
[INFO] Epoch: 18 , batch: 67 , training loss: 3.742963
[INFO] Epoch: 18 , batch: 68 , training loss: 3.890692
[INFO] Epoch: 18 , batch: 69 , training loss: 3.790099
[INFO] Epoch: 18 , batch: 70 , training loss: 4.012436
[INFO] Epoch: 18 , batch: 71 , training loss: 3.866134
[INFO] Epoch: 18 , batch: 72 , training loss: 3.911966
[INFO] Epoch: 18 , batch: 73 , training loss: 3.850362
[INFO] Epoch: 18 , batch: 74 , training loss: 3.960303
[INFO] Epoch: 18 , batch: 75 , training loss: 3.810417
[INFO] Epoch: 18 , batch: 76 , training loss: 3.922270
[INFO] Epoch: 18 , batch: 77 , training loss: 3.886454
[INFO] Epoch: 18 , batch: 78 , training loss: 3.993388
[INFO] Epoch: 18 , batch: 79 , training loss: 3.818073
[INFO] Epoch: 18 , batch: 80 , training loss: 4.006444
[INFO] Epoch: 18 , batch: 81 , training loss: 3.960795
[INFO] Epoch: 18 , batch: 82 , training loss: 3.929569
[INFO] Epoch: 18 , batch: 83 , training loss: 4.037417
[INFO] Epoch: 18 , batch: 84 , training loss: 3.952967
[INFO] Epoch: 18 , batch: 85 , training loss: 4.018381
[INFO] Epoch: 18 , batch: 86 , training loss: 3.966389
[INFO] Epoch: 18 , batch: 87 , training loss: 3.960879
[INFO] Epoch: 18 , batch: 88 , training loss: 4.093030
[INFO] Epoch: 18 , batch: 89 , training loss: 3.887959
[INFO] Epoch: 18 , batch: 90 , training loss: 3.946064
[INFO] Epoch: 18 , batch: 91 , training loss: 3.902448
[INFO] Epoch: 18 , batch: 92 , training loss: 3.915964
[INFO] Epoch: 18 , batch: 93 , training loss: 3.999638
[INFO] Epoch: 18 , batch: 94 , training loss: 4.197832
[INFO] Epoch: 18 , batch: 95 , training loss: 3.940519
[INFO] Epoch: 18 , batch: 96 , training loss: 3.914178
[INFO] Epoch: 18 , batch: 97 , training loss: 3.842752
[INFO] Epoch: 18 , batch: 98 , training loss: 3.813410
[INFO] Epoch: 18 , batch: 99 , training loss: 3.896721
[INFO] Epoch: 18 , batch: 100 , training loss: 3.823130
[INFO] Epoch: 18 , batch: 101 , training loss: 3.877134
[INFO] Epoch: 18 , batch: 102 , training loss: 3.999802
[INFO] Epoch: 18 , batch: 103 , training loss: 3.781329
[INFO] Epoch: 18 , batch: 104 , training loss: 3.740259
[INFO] Epoch: 18 , batch: 105 , training loss: 4.004530
[INFO] Epoch: 18 , batch: 106 , training loss: 4.001900
[INFO] Epoch: 18 , batch: 107 , training loss: 3.857397
[INFO] Epoch: 18 , batch: 108 , training loss: 3.790643
[INFO] Epoch: 18 , batch: 109 , training loss: 3.701539
[INFO] Epoch: 18 , batch: 110 , training loss: 3.890385
[INFO] Epoch: 18 , batch: 111 , training loss: 3.978678
[INFO] Epoch: 18 , batch: 112 , training loss: 3.895476
[INFO] Epoch: 18 , batch: 113 , training loss: 3.894918
[INFO] Epoch: 18 , batch: 114 , training loss: 3.895574
[INFO] Epoch: 18 , batch: 115 , training loss: 3.893423
[INFO] Epoch: 18 , batch: 116 , training loss: 3.803660
[INFO] Epoch: 18 , batch: 117 , training loss: 4.032374
[INFO] Epoch: 18 , batch: 118 , training loss: 3.981797
[INFO] Epoch: 18 , batch: 119 , training loss: 4.146804
[INFO] Epoch: 18 , batch: 120 , training loss: 4.143893
[INFO] Epoch: 18 , batch: 121 , training loss: 3.957643
[INFO] Epoch: 18 , batch: 122 , training loss: 3.859871
[INFO] Epoch: 18 , batch: 123 , training loss: 3.888637
[INFO] Epoch: 18 , batch: 124 , training loss: 4.055315
[INFO] Epoch: 18 , batch: 125 , training loss: 3.831235
[INFO] Epoch: 18 , batch: 126 , training loss: 3.838118
[INFO] Epoch: 18 , batch: 127 , training loss: 3.817853
[INFO] Epoch: 18 , batch: 128 , training loss: 3.979315
[INFO] Epoch: 18 , batch: 129 , training loss: 3.963093
[INFO] Epoch: 18 , batch: 130 , training loss: 3.918939
[INFO] Epoch: 18 , batch: 131 , training loss: 3.919588
[INFO] Epoch: 18 , batch: 132 , training loss: 3.941651
[INFO] Epoch: 18 , batch: 133 , training loss: 3.892082
[INFO] Epoch: 18 , batch: 134 , training loss: 3.702036
[INFO] Epoch: 18 , batch: 135 , training loss: 3.738055
[INFO] Epoch: 18 , batch: 136 , training loss: 4.037817
[INFO] Epoch: 18 , batch: 137 , training loss: 3.927704
[INFO] Epoch: 18 , batch: 138 , training loss: 4.016133
[INFO] Epoch: 18 , batch: 139 , training loss: 4.613554
[INFO] Epoch: 18 , batch: 140 , training loss: 4.372489
[INFO] Epoch: 18 , batch: 141 , training loss: 4.108699
[INFO] Epoch: 18 , batch: 142 , training loss: 3.865061
[INFO] Epoch: 18 , batch: 143 , training loss: 4.000379
[INFO] Epoch: 18 , batch: 144 , training loss: 3.771473
[INFO] Epoch: 18 , batch: 145 , training loss: 3.904164
[INFO] Epoch: 18 , batch: 146 , training loss: 4.093050
[INFO] Epoch: 18 , batch: 147 , training loss: 3.753757
[INFO] Epoch: 18 , batch: 148 , training loss: 3.755971
[INFO] Epoch: 18 , batch: 149 , training loss: 3.820784
[INFO] Epoch: 18 , batch: 150 , training loss: 4.045746
[INFO] Epoch: 18 , batch: 151 , training loss: 3.924179
[INFO] Epoch: 18 , batch: 152 , training loss: 3.964304
[INFO] Epoch: 18 , batch: 153 , training loss: 3.957510
[INFO] Epoch: 18 , batch: 154 , training loss: 4.045837
[INFO] Epoch: 18 , batch: 155 , training loss: 4.278213
[INFO] Epoch: 18 , batch: 156 , training loss: 3.971047
[INFO] Epoch: 18 , batch: 157 , training loss: 3.955519
[INFO] Epoch: 18 , batch: 158 , training loss: 4.137317
[INFO] Epoch: 18 , batch: 159 , training loss: 4.012737
[INFO] Epoch: 18 , batch: 160 , training loss: 4.339033
[INFO] Epoch: 18 , batch: 161 , training loss: 4.445898
[INFO] Epoch: 18 , batch: 162 , training loss: 4.378288
[INFO] Epoch: 18 , batch: 163 , training loss: 4.485546
[INFO] Epoch: 18 , batch: 164 , training loss: 4.473026
[INFO] Epoch: 18 , batch: 165 , training loss: 4.369036
[INFO] Epoch: 18 , batch: 166 , training loss: 4.294083
[INFO] Epoch: 18 , batch: 167 , training loss: 4.436511
[INFO] Epoch: 18 , batch: 168 , training loss: 4.131377
[INFO] Epoch: 18 , batch: 169 , training loss: 4.103724
[INFO] Epoch: 18 , batch: 170 , training loss: 4.264353
[INFO] Epoch: 18 , batch: 171 , training loss: 3.658596
[INFO] Epoch: 18 , batch: 172 , training loss: 3.884892
[INFO] Epoch: 18 , batch: 173 , training loss: 4.195526
[INFO] Epoch: 18 , batch: 174 , training loss: 4.589296
[INFO] Epoch: 18 , batch: 175 , training loss: 4.919144
[INFO] Epoch: 18 , batch: 176 , training loss: 4.578303
[INFO] Epoch: 18 , batch: 177 , training loss: 4.222281
[INFO] Epoch: 18 , batch: 178 , training loss: 4.227201
[INFO] Epoch: 18 , batch: 179 , training loss: 4.278489
[INFO] Epoch: 18 , batch: 180 , training loss: 4.176668
[INFO] Epoch: 18 , batch: 181 , training loss: 4.471241
[INFO] Epoch: 18 , batch: 182 , training loss: 4.393755
[INFO] Epoch: 18 , batch: 183 , training loss: 4.360107
[INFO] Epoch: 18 , batch: 184 , training loss: 4.261148
[INFO] Epoch: 18 , batch: 185 , training loss: 4.227060
[INFO] Epoch: 18 , batch: 186 , training loss: 4.376800
[INFO] Epoch: 18 , batch: 187 , training loss: 4.470934
[INFO] Epoch: 18 , batch: 188 , training loss: 4.439080
[INFO] Epoch: 18 , batch: 189 , training loss: 4.344547
[INFO] Epoch: 18 , batch: 190 , training loss: 4.364907
[INFO] Epoch: 18 , batch: 191 , training loss: 4.489128
[INFO] Epoch: 18 , batch: 192 , training loss: 4.278891
[INFO] Epoch: 18 , batch: 193 , training loss: 4.405793
[INFO] Epoch: 18 , batch: 194 , training loss: 4.344890
[INFO] Epoch: 18 , batch: 195 , training loss: 4.262359
[INFO] Epoch: 18 , batch: 196 , training loss: 4.141419
[INFO] Epoch: 18 , batch: 197 , training loss: 4.253716
[INFO] Epoch: 18 , batch: 198 , training loss: 4.180165
[INFO] Epoch: 18 , batch: 199 , training loss: 4.294261
[INFO] Epoch: 18 , batch: 200 , training loss: 4.186115
[INFO] Epoch: 18 , batch: 201 , training loss: 4.082630
[INFO] Epoch: 18 , batch: 202 , training loss: 4.083867
[INFO] Epoch: 18 , batch: 203 , training loss: 4.163875
[INFO] Epoch: 18 , batch: 204 , training loss: 4.323470
[INFO] Epoch: 18 , batch: 205 , training loss: 3.881116
[INFO] Epoch: 18 , batch: 206 , training loss: 3.842439
[INFO] Epoch: 18 , batch: 207 , training loss: 3.828410
[INFO] Epoch: 18 , batch: 208 , training loss: 4.149431
[INFO] Epoch: 18 , batch: 209 , training loss: 4.126718
[INFO] Epoch: 18 , batch: 210 , training loss: 4.107442
[INFO] Epoch: 18 , batch: 211 , training loss: 4.120024
[INFO] Epoch: 18 , batch: 212 , training loss: 4.234547
[INFO] Epoch: 18 , batch: 213 , training loss: 4.176587
[INFO] Epoch: 18 , batch: 214 , training loss: 4.276814
[INFO] Epoch: 18 , batch: 215 , training loss: 4.493441
[INFO] Epoch: 18 , batch: 216 , training loss: 4.206026
[INFO] Epoch: 18 , batch: 217 , training loss: 4.114066
[INFO] Epoch: 18 , batch: 218 , training loss: 4.108406
[INFO] Epoch: 18 , batch: 219 , training loss: 4.226770
[INFO] Epoch: 18 , batch: 220 , training loss: 4.018766
[INFO] Epoch: 18 , batch: 221 , training loss: 4.055338
[INFO] Epoch: 18 , batch: 222 , training loss: 4.194187
[INFO] Epoch: 18 , batch: 223 , training loss: 4.285064
[INFO] Epoch: 18 , batch: 224 , training loss: 4.330195
[INFO] Epoch: 18 , batch: 225 , training loss: 4.224840
[INFO] Epoch: 18 , batch: 226 , training loss: 4.368017
[INFO] Epoch: 18 , batch: 227 , training loss: 4.315646
[INFO] Epoch: 18 , batch: 228 , training loss: 4.367511
[INFO] Epoch: 18 , batch: 229 , training loss: 4.217132
[INFO] Epoch: 18 , batch: 230 , training loss: 4.086844
[INFO] Epoch: 18 , batch: 231 , training loss: 3.925439
[INFO] Epoch: 18 , batch: 232 , training loss: 4.085231
[INFO] Epoch: 18 , batch: 233 , training loss: 4.090945
[INFO] Epoch: 18 , batch: 234 , training loss: 3.789237
[INFO] Epoch: 18 , batch: 235 , training loss: 3.885354
[INFO] Epoch: 18 , batch: 236 , training loss: 4.012463
[INFO] Epoch: 18 , batch: 237 , training loss: 4.214232
[INFO] Epoch: 18 , batch: 238 , training loss: 3.957373
[INFO] Epoch: 18 , batch: 239 , training loss: 4.039205
[INFO] Epoch: 18 , batch: 240 , training loss: 4.094382
[INFO] Epoch: 18 , batch: 241 , training loss: 3.882659
[INFO] Epoch: 18 , batch: 242 , training loss: 3.909431
[INFO] Epoch: 18 , batch: 243 , training loss: 4.229669
[INFO] Epoch: 18 , batch: 244 , training loss: 4.133340
[INFO] Epoch: 18 , batch: 245 , training loss: 4.133421
[INFO] Epoch: 18 , batch: 246 , training loss: 3.801340
[INFO] Epoch: 18 , batch: 247 , training loss: 3.987543
[INFO] Epoch: 18 , batch: 248 , training loss: 4.071796
[INFO] Epoch: 18 , batch: 249 , training loss: 4.076699
[INFO] Epoch: 18 , batch: 250 , training loss: 3.820612
[INFO] Epoch: 18 , batch: 251 , training loss: 4.300581
[INFO] Epoch: 18 , batch: 252 , training loss: 3.980402
[INFO] Epoch: 18 , batch: 253 , training loss: 3.908023
[INFO] Epoch: 18 , batch: 254 , training loss: 4.207428
[INFO] Epoch: 18 , batch: 255 , training loss: 4.163296
[INFO] Epoch: 18 , batch: 256 , training loss: 4.179755
[INFO] Epoch: 18 , batch: 257 , training loss: 4.312643
[INFO] Epoch: 18 , batch: 258 , training loss: 4.375539
[INFO] Epoch: 18 , batch: 259 , training loss: 4.412498
[INFO] Epoch: 18 , batch: 260 , training loss: 4.140959
[INFO] Epoch: 18 , batch: 261 , training loss: 4.279845
[INFO] Epoch: 18 , batch: 262 , training loss: 4.448244
[INFO] Epoch: 18 , batch: 263 , training loss: 4.638530
[INFO] Epoch: 18 , batch: 264 , training loss: 3.943405
[INFO] Epoch: 18 , batch: 265 , training loss: 4.098260
[INFO] Epoch: 18 , batch: 266 , training loss: 4.518304
[INFO] Epoch: 18 , batch: 267 , training loss: 4.263756
[INFO] Epoch: 18 , batch: 268 , training loss: 4.191436
[INFO] Epoch: 18 , batch: 269 , training loss: 4.144323
[INFO] Epoch: 18 , batch: 270 , training loss: 4.188053
[INFO] Epoch: 18 , batch: 271 , training loss: 4.214815
[INFO] Epoch: 18 , batch: 272 , training loss: 4.195827
[INFO] Epoch: 18 , batch: 273 , training loss: 4.214735
[INFO] Epoch: 18 , batch: 274 , training loss: 4.291414
[INFO] Epoch: 18 , batch: 275 , training loss: 4.191682
[INFO] Epoch: 18 , batch: 276 , training loss: 4.228154
[INFO] Epoch: 18 , batch: 277 , training loss: 4.387046
[INFO] Epoch: 18 , batch: 278 , training loss: 4.055710
[INFO] Epoch: 18 , batch: 279 , training loss: 4.046911
[INFO] Epoch: 18 , batch: 280 , training loss: 4.009897
[INFO] Epoch: 18 , batch: 281 , training loss: 4.153139
[INFO] Epoch: 18 , batch: 282 , training loss: 4.059153
[INFO] Epoch: 18 , batch: 283 , training loss: 4.091429
[INFO] Epoch: 18 , batch: 284 , training loss: 4.107504
[INFO] Epoch: 18 , batch: 285 , training loss: 4.063329
[INFO] Epoch: 18 , batch: 286 , training loss: 4.040915
[INFO] Epoch: 18 , batch: 287 , training loss: 3.987102
[INFO] Epoch: 18 , batch: 288 , training loss: 3.998056
[INFO] Epoch: 18 , batch: 289 , training loss: 4.057433
[INFO] Epoch: 18 , batch: 290 , training loss: 3.823981
[INFO] Epoch: 18 , batch: 291 , training loss: 3.806840
[INFO] Epoch: 18 , batch: 292 , training loss: 3.935653
[INFO] Epoch: 18 , batch: 293 , training loss: 3.834082
[INFO] Epoch: 18 , batch: 294 , training loss: 4.507221
[INFO] Epoch: 18 , batch: 295 , training loss: 4.274948
[INFO] Epoch: 18 , batch: 296 , training loss: 4.207273
[INFO] Epoch: 18 , batch: 297 , training loss: 4.168328
[INFO] Epoch: 18 , batch: 298 , training loss: 4.004869
[INFO] Epoch: 18 , batch: 299 , training loss: 4.033683
[INFO] Epoch: 18 , batch: 300 , training loss: 3.998484
[INFO] Epoch: 18 , batch: 301 , training loss: 3.924824
[INFO] Epoch: 18 , batch: 302 , training loss: 4.105132
[INFO] Epoch: 18 , batch: 303 , training loss: 4.116781
[INFO] Epoch: 18 , batch: 304 , training loss: 4.288625
[INFO] Epoch: 18 , batch: 305 , training loss: 4.067653
[INFO] Epoch: 18 , batch: 306 , training loss: 4.209443
[INFO] Epoch: 18 , batch: 307 , training loss: 4.196295
[INFO] Epoch: 18 , batch: 308 , training loss: 4.053414
[INFO] Epoch: 18 , batch: 309 , training loss: 4.060913
[INFO] Epoch: 18 , batch: 310 , training loss: 3.921837
[INFO] Epoch: 18 , batch: 311 , training loss: 3.949039
[INFO] Epoch: 18 , batch: 312 , training loss: 3.876184
[INFO] Epoch: 18 , batch: 313 , training loss: 3.983987
[INFO] Epoch: 18 , batch: 314 , training loss: 4.050325
[INFO] Epoch: 18 , batch: 315 , training loss: 4.096952
[INFO] Epoch: 18 , batch: 316 , training loss: 4.330915
[INFO] Epoch: 18 , batch: 317 , training loss: 4.816385
[INFO] Epoch: 18 , batch: 318 , training loss: 4.962051
[INFO] Epoch: 18 , batch: 319 , training loss: 4.499949
[INFO] Epoch: 18 , batch: 320 , training loss: 4.074904
[INFO] Epoch: 18 , batch: 321 , training loss: 3.896876
[INFO] Epoch: 18 , batch: 322 , training loss: 4.012737
[INFO] Epoch: 18 , batch: 323 , training loss: 4.025513
[INFO] Epoch: 18 , batch: 324 , training loss: 3.999891
[INFO] Epoch: 18 , batch: 325 , training loss: 4.153537
[INFO] Epoch: 18 , batch: 326 , training loss: 4.206548
[INFO] Epoch: 18 , batch: 327 , training loss: 4.116861
[INFO] Epoch: 18 , batch: 328 , training loss: 4.094542
[INFO] Epoch: 18 , batch: 329 , training loss: 4.016295
[INFO] Epoch: 18 , batch: 330 , training loss: 4.024849
[INFO] Epoch: 18 , batch: 331 , training loss: 4.158114
[INFO] Epoch: 18 , batch: 332 , training loss: 3.977833
[INFO] Epoch: 18 , batch: 333 , training loss: 3.977872
[INFO] Epoch: 18 , batch: 334 , training loss: 4.002996
[INFO] Epoch: 18 , batch: 335 , training loss: 4.133887
[INFO] Epoch: 18 , batch: 336 , training loss: 4.164538
[INFO] Epoch: 18 , batch: 337 , training loss: 4.161951
[INFO] Epoch: 18 , batch: 338 , training loss: 4.371037
[INFO] Epoch: 18 , batch: 339 , training loss: 4.221797
[INFO] Epoch: 18 , batch: 340 , training loss: 4.402520
[INFO] Epoch: 18 , batch: 341 , training loss: 4.164644
[INFO] Epoch: 18 , batch: 342 , training loss: 3.939417
[INFO] Epoch: 18 , batch: 343 , training loss: 4.014611
[INFO] Epoch: 18 , batch: 344 , training loss: 3.858669
[INFO] Epoch: 18 , batch: 345 , training loss: 3.998204
[INFO] Epoch: 18 , batch: 346 , training loss: 4.051176
[INFO] Epoch: 18 , batch: 347 , training loss: 3.960717
[INFO] Epoch: 18 , batch: 348 , training loss: 4.102596
[INFO] Epoch: 18 , batch: 349 , training loss: 4.224303
[INFO] Epoch: 18 , batch: 350 , training loss: 4.003829
[INFO] Epoch: 18 , batch: 351 , training loss: 4.058452
[INFO] Epoch: 18 , batch: 352 , training loss: 4.094132
[INFO] Epoch: 18 , batch: 353 , training loss: 4.052593
[INFO] Epoch: 18 , batch: 354 , training loss: 4.170382
[INFO] Epoch: 18 , batch: 355 , training loss: 4.209230
[INFO] Epoch: 18 , batch: 356 , training loss: 4.027936
[INFO] Epoch: 18 , batch: 357 , training loss: 4.114673
[INFO] Epoch: 18 , batch: 358 , training loss: 4.019354
[INFO] Epoch: 18 , batch: 359 , training loss: 4.015966
[INFO] Epoch: 18 , batch: 360 , training loss: 4.115586
[INFO] Epoch: 18 , batch: 361 , training loss: 4.075465
[INFO] Epoch: 18 , batch: 362 , training loss: 4.165709
[INFO] Epoch: 18 , batch: 363 , training loss: 4.073892
[INFO] Epoch: 18 , batch: 364 , training loss: 4.091141
[INFO] Epoch: 18 , batch: 365 , training loss: 4.038457
[INFO] Epoch: 18 , batch: 366 , training loss: 4.167436
[INFO] Epoch: 18 , batch: 367 , training loss: 4.247904
[INFO] Epoch: 18 , batch: 368 , training loss: 4.679937
[INFO] Epoch: 18 , batch: 369 , training loss: 4.308628
[INFO] Epoch: 18 , batch: 370 , training loss: 4.073525
[INFO] Epoch: 18 , batch: 371 , training loss: 4.478715
[INFO] Epoch: 18 , batch: 372 , training loss: 4.794563
[INFO] Epoch: 18 , batch: 373 , training loss: 4.883611
[INFO] Epoch: 18 , batch: 374 , training loss: 4.918206
[INFO] Epoch: 18 , batch: 375 , training loss: 4.915011
[INFO] Epoch: 18 , batch: 376 , training loss: 4.837707
[INFO] Epoch: 18 , batch: 377 , training loss: 4.570800
[INFO] Epoch: 18 , batch: 378 , training loss: 4.659409
[INFO] Epoch: 18 , batch: 379 , training loss: 4.663096
[INFO] Epoch: 18 , batch: 380 , training loss: 4.791692
[INFO] Epoch: 18 , batch: 381 , training loss: 4.515863
[INFO] Epoch: 18 , batch: 382 , training loss: 4.746691
[INFO] Epoch: 18 , batch: 383 , training loss: 4.765762
[INFO] Epoch: 18 , batch: 384 , training loss: 4.784595
[INFO] Epoch: 18 , batch: 385 , training loss: 4.518850
[INFO] Epoch: 18 , batch: 386 , training loss: 4.748064
[INFO] Epoch: 18 , batch: 387 , training loss: 4.695634
[INFO] Epoch: 18 , batch: 388 , training loss: 4.526904
[INFO] Epoch: 18 , batch: 389 , training loss: 4.377295
[INFO] Epoch: 18 , batch: 390 , training loss: 4.348173
[INFO] Epoch: 18 , batch: 391 , training loss: 4.383145
[INFO] Epoch: 18 , batch: 392 , training loss: 4.762646
[INFO] Epoch: 18 , batch: 393 , training loss: 4.623976
[INFO] Epoch: 18 , batch: 394 , training loss: 4.668791
[INFO] Epoch: 18 , batch: 395 , training loss: 4.525712
[INFO] Epoch: 18 , batch: 396 , training loss: 4.293526
[INFO] Epoch: 18 , batch: 397 , training loss: 4.456861
[INFO] Epoch: 18 , batch: 398 , training loss: 4.323280
[INFO] Epoch: 18 , batch: 399 , training loss: 4.382378
[INFO] Epoch: 18 , batch: 400 , training loss: 4.354766
[INFO] Epoch: 18 , batch: 401 , training loss: 4.777701
[INFO] Epoch: 18 , batch: 402 , training loss: 4.487507
[INFO] Epoch: 18 , batch: 403 , training loss: 4.285733
[INFO] Epoch: 18 , batch: 404 , training loss: 4.492394
[INFO] Epoch: 18 , batch: 405 , training loss: 4.525771
[INFO] Epoch: 18 , batch: 406 , training loss: 4.437557
[INFO] Epoch: 18 , batch: 407 , training loss: 4.498365
[INFO] Epoch: 18 , batch: 408 , training loss: 4.450710
[INFO] Epoch: 18 , batch: 409 , training loss: 4.447495
[INFO] Epoch: 18 , batch: 410 , training loss: 4.479780
[INFO] Epoch: 18 , batch: 411 , training loss: 4.682559
[INFO] Epoch: 18 , batch: 412 , training loss: 4.525911
[INFO] Epoch: 18 , batch: 413 , training loss: 4.406498
[INFO] Epoch: 18 , batch: 414 , training loss: 4.416089
[INFO] Epoch: 18 , batch: 415 , training loss: 4.438459
[INFO] Epoch: 18 , batch: 416 , training loss: 4.520404
[INFO] Epoch: 18 , batch: 417 , training loss: 4.450913
[INFO] Epoch: 18 , batch: 418 , training loss: 4.462264
[INFO] Epoch: 18 , batch: 419 , training loss: 4.429278
[INFO] Epoch: 18 , batch: 420 , training loss: 4.390534
[INFO] Epoch: 18 , batch: 421 , training loss: 4.391507
[INFO] Epoch: 18 , batch: 422 , training loss: 4.278851
[INFO] Epoch: 18 , batch: 423 , training loss: 4.502402
[INFO] Epoch: 18 , batch: 424 , training loss: 4.663227
[INFO] Epoch: 18 , batch: 425 , training loss: 4.518795
[INFO] Epoch: 18 , batch: 426 , training loss: 4.243980
[INFO] Epoch: 18 , batch: 427 , training loss: 4.481748
[INFO] Epoch: 18 , batch: 428 , training loss: 4.356449
[INFO] Epoch: 18 , batch: 429 , training loss: 4.221799
[INFO] Epoch: 18 , batch: 430 , training loss: 4.494816
[INFO] Epoch: 18 , batch: 431 , training loss: 4.084882
[INFO] Epoch: 18 , batch: 432 , training loss: 4.163044
[INFO] Epoch: 18 , batch: 433 , training loss: 4.193953
[INFO] Epoch: 18 , batch: 434 , training loss: 4.061411
[INFO] Epoch: 18 , batch: 435 , training loss: 4.430063
[INFO] Epoch: 18 , batch: 436 , training loss: 4.501091
[INFO] Epoch: 18 , batch: 437 , training loss: 4.239024
[INFO] Epoch: 18 , batch: 438 , training loss: 4.086999
[INFO] Epoch: 18 , batch: 439 , training loss: 4.322786
[INFO] Epoch: 18 , batch: 440 , training loss: 4.442297
[INFO] Epoch: 18 , batch: 441 , training loss: 4.547081
[INFO] Epoch: 18 , batch: 442 , training loss: 4.322078
[INFO] Epoch: 18 , batch: 443 , training loss: 4.514452
[INFO] Epoch: 18 , batch: 444 , training loss: 4.123663
[INFO] Epoch: 18 , batch: 445 , training loss: 4.028828
[INFO] Epoch: 18 , batch: 446 , training loss: 3.962633
[INFO] Epoch: 18 , batch: 447 , training loss: 4.150347
[INFO] Epoch: 18 , batch: 448 , training loss: 4.253817
[INFO] Epoch: 18 , batch: 449 , training loss: 4.688655
[INFO] Epoch: 18 , batch: 450 , training loss: 4.728588
[INFO] Epoch: 18 , batch: 451 , training loss: 4.624358
[INFO] Epoch: 18 , batch: 452 , training loss: 4.418988
[INFO] Epoch: 18 , batch: 453 , training loss: 4.200794
[INFO] Epoch: 18 , batch: 454 , training loss: 4.343594
[INFO] Epoch: 18 , batch: 455 , training loss: 4.376788
[INFO] Epoch: 18 , batch: 456 , training loss: 4.375328
[INFO] Epoch: 18 , batch: 457 , training loss: 4.476438
[INFO] Epoch: 18 , batch: 458 , training loss: 4.203730
[INFO] Epoch: 18 , batch: 459 , training loss: 4.181529
[INFO] Epoch: 18 , batch: 460 , training loss: 4.279905
[INFO] Epoch: 18 , batch: 461 , training loss: 4.285746
[INFO] Epoch: 18 , batch: 462 , training loss: 4.329130
[INFO] Epoch: 18 , batch: 463 , training loss: 4.227554
[INFO] Epoch: 18 , batch: 464 , training loss: 4.441487
[INFO] Epoch: 18 , batch: 465 , training loss: 4.351643
[INFO] Epoch: 18 , batch: 466 , training loss: 4.444138
[INFO] Epoch: 18 , batch: 467 , training loss: 4.424278
[INFO] Epoch: 18 , batch: 468 , training loss: 4.386464
[INFO] Epoch: 18 , batch: 469 , training loss: 4.415453
[INFO] Epoch: 18 , batch: 470 , training loss: 4.223367
[INFO] Epoch: 18 , batch: 471 , training loss: 4.309121
[INFO] Epoch: 18 , batch: 472 , training loss: 4.359345
[INFO] Epoch: 18 , batch: 473 , training loss: 4.288391
[INFO] Epoch: 18 , batch: 474 , training loss: 4.081583
[INFO] Epoch: 18 , batch: 475 , training loss: 3.959774
[INFO] Epoch: 18 , batch: 476 , training loss: 4.335118
[INFO] Epoch: 18 , batch: 477 , training loss: 4.463498
[INFO] Epoch: 18 , batch: 478 , training loss: 4.505302
[INFO] Epoch: 18 , batch: 479 , training loss: 4.472982
[INFO] Epoch: 18 , batch: 480 , training loss: 4.578161
[INFO] Epoch: 18 , batch: 481 , training loss: 4.437982
[INFO] Epoch: 18 , batch: 482 , training loss: 4.578042
[INFO] Epoch: 18 , batch: 483 , training loss: 4.403365
[INFO] Epoch: 18 , batch: 484 , training loss: 4.218745
[INFO] Epoch: 18 , batch: 485 , training loss: 4.314153
[INFO] Epoch: 18 , batch: 486 , training loss: 4.193342
[INFO] Epoch: 18 , batch: 487 , training loss: 4.171678
[INFO] Epoch: 18 , batch: 488 , training loss: 4.372022
[INFO] Epoch: 18 , batch: 489 , training loss: 4.260633
[INFO] Epoch: 18 , batch: 490 , training loss: 4.311201
[INFO] Epoch: 18 , batch: 491 , training loss: 4.274249
[INFO] Epoch: 18 , batch: 492 , training loss: 4.211845
[INFO] Epoch: 18 , batch: 493 , training loss: 4.407519
[INFO] Epoch: 18 , batch: 494 , training loss: 4.328075
[INFO] Epoch: 18 , batch: 495 , training loss: 4.447250
[INFO] Epoch: 18 , batch: 496 , training loss: 4.308986
[INFO] Epoch: 18 , batch: 497 , training loss: 4.365753
[INFO] Epoch: 18 , batch: 498 , training loss: 4.371408
[INFO] Epoch: 18 , batch: 499 , training loss: 4.438761
[INFO] Epoch: 18 , batch: 500 , training loss: 4.597709
[INFO] Epoch: 18 , batch: 501 , training loss: 4.931948
[INFO] Epoch: 18 , batch: 502 , training loss: 5.006765
[INFO] Epoch: 18 , batch: 503 , training loss: 4.731039
[INFO] Epoch: 18 , batch: 504 , training loss: 4.822164
[INFO] Epoch: 18 , batch: 505 , training loss: 4.797090
[INFO] Epoch: 18 , batch: 506 , training loss: 4.750048
[INFO] Epoch: 18 , batch: 507 , training loss: 4.808430
[INFO] Epoch: 18 , batch: 508 , training loss: 4.762363
[INFO] Epoch: 18 , batch: 509 , training loss: 4.529553
[INFO] Epoch: 18 , batch: 510 , training loss: 4.595030
[INFO] Epoch: 18 , batch: 511 , training loss: 4.499353
[INFO] Epoch: 18 , batch: 512 , training loss: 4.593247
[INFO] Epoch: 18 , batch: 513 , training loss: 4.840785
[INFO] Epoch: 18 , batch: 514 , training loss: 4.480452
[INFO] Epoch: 18 , batch: 515 , training loss: 4.717219
[INFO] Epoch: 18 , batch: 516 , training loss: 4.531310
[INFO] Epoch: 18 , batch: 517 , training loss: 4.483783
[INFO] Epoch: 18 , batch: 518 , training loss: 4.466002
[INFO] Epoch: 18 , batch: 519 , training loss: 4.328240
[INFO] Epoch: 18 , batch: 520 , training loss: 4.549527
[INFO] Epoch: 18 , batch: 521 , training loss: 4.528262
[INFO] Epoch: 18 , batch: 522 , training loss: 4.573597
[INFO] Epoch: 18 , batch: 523 , training loss: 4.507122
[INFO] Epoch: 18 , batch: 524 , training loss: 4.782937
[INFO] Epoch: 18 , batch: 525 , training loss: 4.696136
[INFO] Epoch: 18 , batch: 526 , training loss: 4.463339
[INFO] Epoch: 18 , batch: 527 , training loss: 4.493464
[INFO] Epoch: 18 , batch: 528 , training loss: 4.519793
[INFO] Epoch: 18 , batch: 529 , training loss: 4.516560
[INFO] Epoch: 18 , batch: 530 , training loss: 4.347126
[INFO] Epoch: 18 , batch: 531 , training loss: 4.498828
[INFO] Epoch: 18 , batch: 532 , training loss: 4.369851
[INFO] Epoch: 18 , batch: 533 , training loss: 4.543122
[INFO] Epoch: 18 , batch: 534 , training loss: 4.529567
[INFO] Epoch: 18 , batch: 535 , training loss: 4.530705
[INFO] Epoch: 18 , batch: 536 , training loss: 4.359569
[INFO] Epoch: 18 , batch: 537 , training loss: 4.367645
[INFO] Epoch: 18 , batch: 538 , training loss: 4.438394
[INFO] Epoch: 18 , batch: 539 , training loss: 4.557856
[INFO] Epoch: 18 , batch: 540 , training loss: 5.094794
[INFO] Epoch: 18 , batch: 541 , training loss: 4.995423
[INFO] Epoch: 18 , batch: 542 , training loss: 4.863229
[INFO] Epoch: 19 , batch: 0 , training loss: 3.673934
[INFO] Epoch: 19 , batch: 1 , training loss: 3.723329
[INFO] Epoch: 19 , batch: 2 , training loss: 3.763947
[INFO] Epoch: 19 , batch: 3 , training loss: 3.596816
[INFO] Epoch: 19 , batch: 4 , training loss: 3.969367
[INFO] Epoch: 19 , batch: 5 , training loss: 3.605244
[INFO] Epoch: 19 , batch: 6 , training loss: 4.026682
[INFO] Epoch: 19 , batch: 7 , training loss: 3.873617
[INFO] Epoch: 19 , batch: 8 , training loss: 3.607969
[INFO] Epoch: 19 , batch: 9 , training loss: 3.777166
[INFO] Epoch: 19 , batch: 10 , training loss: 3.775074
[INFO] Epoch: 19 , batch: 11 , training loss: 3.722459
[INFO] Epoch: 19 , batch: 12 , training loss: 3.639505
[INFO] Epoch: 19 , batch: 13 , training loss: 3.664478
[INFO] Epoch: 19 , batch: 14 , training loss: 3.538698
[INFO] Epoch: 19 , batch: 15 , training loss: 3.726540
[INFO] Epoch: 19 , batch: 16 , training loss: 3.583591
[INFO] Epoch: 19 , batch: 17 , training loss: 3.770777
[INFO] Epoch: 19 , batch: 18 , training loss: 3.718991
[INFO] Epoch: 19 , batch: 19 , training loss: 3.440481
[INFO] Epoch: 19 , batch: 20 , training loss: 3.398757
[INFO] Epoch: 19 , batch: 21 , training loss: 3.552423
[INFO] Epoch: 19 , batch: 22 , training loss: 3.458848
[INFO] Epoch: 19 , batch: 23 , training loss: 3.679556
[INFO] Epoch: 19 , batch: 24 , training loss: 3.487547
[INFO] Epoch: 19 , batch: 25 , training loss: 3.683686
[INFO] Epoch: 19 , batch: 26 , training loss: 3.535818
[INFO] Epoch: 19 , batch: 27 , training loss: 3.464767
[INFO] Epoch: 19 , batch: 28 , training loss: 3.639065
[INFO] Epoch: 19 , batch: 29 , training loss: 3.486992
[INFO] Epoch: 19 , batch: 30 , training loss: 3.537298
[INFO] Epoch: 19 , batch: 31 , training loss: 3.573030
[INFO] Epoch: 19 , batch: 32 , training loss: 3.570612
[INFO] Epoch: 19 , batch: 33 , training loss: 3.615510
[INFO] Epoch: 19 , batch: 34 , training loss: 3.593680
[INFO] Epoch: 19 , batch: 35 , training loss: 3.600544
[INFO] Epoch: 19 , batch: 36 , training loss: 3.623269
[INFO] Epoch: 19 , batch: 37 , training loss: 3.543915
[INFO] Epoch: 19 , batch: 38 , training loss: 3.557637
[INFO] Epoch: 19 , batch: 39 , training loss: 3.442246
[INFO] Epoch: 19 , batch: 40 , training loss: 3.670708
[INFO] Epoch: 19 , batch: 41 , training loss: 3.557166
[INFO] Epoch: 19 , batch: 42 , training loss: 4.028488
[INFO] Epoch: 19 , batch: 43 , training loss: 3.742625
[INFO] Epoch: 19 , batch: 44 , training loss: 4.064860
[INFO] Epoch: 19 , batch: 45 , training loss: 4.058661
[INFO] Epoch: 19 , batch: 46 , training loss: 4.000989
[INFO] Epoch: 19 , batch: 47 , training loss: 3.735658
[INFO] Epoch: 19 , batch: 48 , training loss: 3.749413
[INFO] Epoch: 19 , batch: 49 , training loss: 3.889705
[INFO] Epoch: 19 , batch: 50 , training loss: 3.646512
[INFO] Epoch: 19 , batch: 51 , training loss: 3.871660
[INFO] Epoch: 19 , batch: 52 , training loss: 3.690343
[INFO] Epoch: 19 , batch: 53 , training loss: 3.854109
[INFO] Epoch: 19 , batch: 54 , training loss: 3.863249
[INFO] Epoch: 19 , batch: 55 , training loss: 3.944460
[INFO] Epoch: 19 , batch: 56 , training loss: 3.802674
[INFO] Epoch: 19 , batch: 57 , training loss: 3.685026
[INFO] Epoch: 19 , batch: 58 , training loss: 3.784870
[INFO] Epoch: 19 , batch: 59 , training loss: 3.834631
[INFO] Epoch: 19 , batch: 60 , training loss: 3.774312
[INFO] Epoch: 19 , batch: 61 , training loss: 3.849231
[INFO] Epoch: 19 , batch: 62 , training loss: 3.762172
[INFO] Epoch: 19 , batch: 63 , training loss: 3.973966
[INFO] Epoch: 19 , batch: 64 , training loss: 4.082564
[INFO] Epoch: 19 , batch: 65 , training loss: 3.812226
[INFO] Epoch: 19 , batch: 66 , training loss: 3.699018
[INFO] Epoch: 19 , batch: 67 , training loss: 3.737041
[INFO] Epoch: 19 , batch: 68 , training loss: 3.899135
[INFO] Epoch: 19 , batch: 69 , training loss: 3.787877
[INFO] Epoch: 19 , batch: 70 , training loss: 4.030668
[INFO] Epoch: 19 , batch: 71 , training loss: 3.879117
[INFO] Epoch: 19 , batch: 72 , training loss: 3.912572
[INFO] Epoch: 19 , batch: 73 , training loss: 3.858765
[INFO] Epoch: 19 , batch: 74 , training loss: 3.981341
[INFO] Epoch: 19 , batch: 75 , training loss: 3.819616
[INFO] Epoch: 19 , batch: 76 , training loss: 3.941581
[INFO] Epoch: 19 , batch: 77 , training loss: 3.914758
[INFO] Epoch: 19 , batch: 78 , training loss: 3.983251
[INFO] Epoch: 19 , batch: 79 , training loss: 3.825610
[INFO] Epoch: 19 , batch: 80 , training loss: 4.012223
[INFO] Epoch: 19 , batch: 81 , training loss: 3.944656
[INFO] Epoch: 19 , batch: 82 , training loss: 3.932710
[INFO] Epoch: 19 , batch: 83 , training loss: 4.030758
[INFO] Epoch: 19 , batch: 84 , training loss: 3.970734
[INFO] Epoch: 19 , batch: 85 , training loss: 4.004799
[INFO] Epoch: 19 , batch: 86 , training loss: 3.988165
[INFO] Epoch: 19 , batch: 87 , training loss: 3.963966
[INFO] Epoch: 19 , batch: 88 , training loss: 4.092798
[INFO] Epoch: 19 , batch: 89 , training loss: 3.907771
[INFO] Epoch: 19 , batch: 90 , training loss: 3.970795
[INFO] Epoch: 19 , batch: 91 , training loss: 3.898979
[INFO] Epoch: 19 , batch: 92 , training loss: 3.902008
[INFO] Epoch: 19 , batch: 93 , training loss: 4.028149
[INFO] Epoch: 19 , batch: 94 , training loss: 4.205979
[INFO] Epoch: 19 , batch: 95 , training loss: 3.944853
[INFO] Epoch: 19 , batch: 96 , training loss: 3.933670
[INFO] Epoch: 19 , batch: 97 , training loss: 3.866160
[INFO] Epoch: 19 , batch: 98 , training loss: 3.823863
[INFO] Epoch: 19 , batch: 99 , training loss: 3.899950
[INFO] Epoch: 19 , batch: 100 , training loss: 3.822461
[INFO] Epoch: 19 , batch: 101 , training loss: 3.865268
[INFO] Epoch: 19 , batch: 102 , training loss: 3.985530
[INFO] Epoch: 19 , batch: 103 , training loss: 3.780645
[INFO] Epoch: 19 , batch: 104 , training loss: 3.736795
[INFO] Epoch: 19 , batch: 105 , training loss: 4.008811
[INFO] Epoch: 19 , batch: 106 , training loss: 3.987599
[INFO] Epoch: 19 , batch: 107 , training loss: 3.870951
[INFO] Epoch: 19 , batch: 108 , training loss: 3.784220
[INFO] Epoch: 19 , batch: 109 , training loss: 3.702099
[INFO] Epoch: 19 , batch: 110 , training loss: 3.909235
[INFO] Epoch: 19 , batch: 111 , training loss: 3.973127
[INFO] Epoch: 19 , batch: 112 , training loss: 3.898372
[INFO] Epoch: 19 , batch: 113 , training loss: 3.905791
[INFO] Epoch: 19 , batch: 114 , training loss: 3.908129
[INFO] Epoch: 19 , batch: 115 , training loss: 3.901155
[INFO] Epoch: 19 , batch: 116 , training loss: 3.800329
[INFO] Epoch: 19 , batch: 117 , training loss: 4.025418
[INFO] Epoch: 19 , batch: 118 , training loss: 4.021250
[INFO] Epoch: 19 , batch: 119 , training loss: 4.152588
[INFO] Epoch: 19 , batch: 120 , training loss: 4.115884
[INFO] Epoch: 19 , batch: 121 , training loss: 3.956235
[INFO] Epoch: 19 , batch: 122 , training loss: 3.862239
[INFO] Epoch: 19 , batch: 123 , training loss: 3.889815
[INFO] Epoch: 19 , batch: 124 , training loss: 4.039481
[INFO] Epoch: 19 , batch: 125 , training loss: 3.802817
[INFO] Epoch: 19 , batch: 126 , training loss: 3.830801
[INFO] Epoch: 19 , batch: 127 , training loss: 3.820395
[INFO] Epoch: 19 , batch: 128 , training loss: 3.979343
[INFO] Epoch: 19 , batch: 129 , training loss: 3.952114
[INFO] Epoch: 19 , batch: 130 , training loss: 3.932074
[INFO] Epoch: 19 , batch: 131 , training loss: 3.911808
[INFO] Epoch: 19 , batch: 132 , training loss: 3.923842
[INFO] Epoch: 19 , batch: 133 , training loss: 3.896973
[INFO] Epoch: 19 , batch: 134 , training loss: 3.693904
[INFO] Epoch: 19 , batch: 135 , training loss: 3.721277
[INFO] Epoch: 19 , batch: 136 , training loss: 4.032018
[INFO] Epoch: 19 , batch: 137 , training loss: 3.941994
[INFO] Epoch: 19 , batch: 138 , training loss: 3.998802
[INFO] Epoch: 19 , batch: 139 , training loss: 4.618574
[INFO] Epoch: 19 , batch: 140 , training loss: 4.350114
[INFO] Epoch: 19 , batch: 141 , training loss: 4.095679
[INFO] Epoch: 19 , batch: 142 , training loss: 3.877977
[INFO] Epoch: 19 , batch: 143 , training loss: 4.001934
[INFO] Epoch: 19 , batch: 144 , training loss: 3.763816
[INFO] Epoch: 19 , batch: 145 , training loss: 3.893197
[INFO] Epoch: 19 , batch: 146 , training loss: 4.091517
[INFO] Epoch: 19 , batch: 147 , training loss: 3.732678
[INFO] Epoch: 19 , batch: 148 , training loss: 3.747697
[INFO] Epoch: 19 , batch: 149 , training loss: 3.840641
[INFO] Epoch: 19 , batch: 150 , training loss: 4.053385
[INFO] Epoch: 19 , batch: 151 , training loss: 3.929587
[INFO] Epoch: 19 , batch: 152 , training loss: 3.967101
[INFO] Epoch: 19 , batch: 153 , training loss: 3.961692
[INFO] Epoch: 19 , batch: 154 , training loss: 4.066196
[INFO] Epoch: 19 , batch: 155 , training loss: 4.283778
[INFO] Epoch: 19 , batch: 156 , training loss: 3.990606
[INFO] Epoch: 19 , batch: 157 , training loss: 3.943687
[INFO] Epoch: 19 , batch: 158 , training loss: 4.128567
[INFO] Epoch: 19 , batch: 159 , training loss: 4.048330
[INFO] Epoch: 19 , batch: 160 , training loss: 4.335205
[INFO] Epoch: 19 , batch: 161 , training loss: 4.402235
[INFO] Epoch: 19 , batch: 162 , training loss: 4.375299
[INFO] Epoch: 19 , batch: 163 , training loss: 4.449896
[INFO] Epoch: 19 , batch: 164 , training loss: 4.465741
[INFO] Epoch: 19 , batch: 165 , training loss: 4.382684
[INFO] Epoch: 19 , batch: 166 , training loss: 4.293814
[INFO] Epoch: 19 , batch: 167 , training loss: 4.458841
[INFO] Epoch: 19 , batch: 168 , training loss: 4.156687
[INFO] Epoch: 19 , batch: 169 , training loss: 4.083337
[INFO] Epoch: 19 , batch: 170 , training loss: 4.255881
[INFO] Epoch: 19 , batch: 171 , training loss: 3.661575
[INFO] Epoch: 19 , batch: 172 , training loss: 3.883744
[INFO] Epoch: 19 , batch: 173 , training loss: 4.217963
[INFO] Epoch: 19 , batch: 174 , training loss: 4.559231
[INFO] Epoch: 19 , batch: 175 , training loss: 4.907923
[INFO] Epoch: 19 , batch: 176 , training loss: 4.578261
[INFO] Epoch: 19 , batch: 177 , training loss: 4.251526
[INFO] Epoch: 19 , batch: 178 , training loss: 4.237821
[INFO] Epoch: 19 , batch: 179 , training loss: 4.283252
[INFO] Epoch: 19 , batch: 180 , training loss: 4.200655
[INFO] Epoch: 19 , batch: 181 , training loss: 4.467783
[INFO] Epoch: 19 , batch: 182 , training loss: 4.399537
[INFO] Epoch: 19 , batch: 183 , training loss: 4.368772
[INFO] Epoch: 19 , batch: 184 , training loss: 4.288682
[INFO] Epoch: 19 , batch: 185 , training loss: 4.229849
[INFO] Epoch: 19 , batch: 186 , training loss: 4.382858
[INFO] Epoch: 19 , batch: 187 , training loss: 4.486834
[INFO] Epoch: 19 , batch: 188 , training loss: 4.450303
[INFO] Epoch: 19 , batch: 189 , training loss: 4.342720
[INFO] Epoch: 19 , batch: 190 , training loss: 4.376529
[INFO] Epoch: 19 , batch: 191 , training loss: 4.473946
[INFO] Epoch: 19 , batch: 192 , training loss: 4.299319
[INFO] Epoch: 19 , batch: 193 , training loss: 4.407279
[INFO] Epoch: 19 , batch: 194 , training loss: 4.358752
[INFO] Epoch: 19 , batch: 195 , training loss: 4.257257
[INFO] Epoch: 19 , batch: 196 , training loss: 4.142442
[INFO] Epoch: 19 , batch: 197 , training loss: 4.234808
[INFO] Epoch: 19 , batch: 198 , training loss: 4.161920
[INFO] Epoch: 19 , batch: 199 , training loss: 4.285351
[INFO] Epoch: 19 , batch: 200 , training loss: 4.188372
[INFO] Epoch: 19 , batch: 201 , training loss: 4.098089
[INFO] Epoch: 19 , batch: 202 , training loss: 4.071716
[INFO] Epoch: 19 , batch: 203 , training loss: 4.172416
[INFO] Epoch: 19 , batch: 204 , training loss: 4.317523
[INFO] Epoch: 19 , batch: 205 , training loss: 3.886049
[INFO] Epoch: 19 , batch: 206 , training loss: 3.835963
[INFO] Epoch: 19 , batch: 207 , training loss: 3.831968
[INFO] Epoch: 19 , batch: 208 , training loss: 4.145507
[INFO] Epoch: 19 , batch: 209 , training loss: 4.107297
[INFO] Epoch: 19 , batch: 210 , training loss: 4.114652
[INFO] Epoch: 19 , batch: 211 , training loss: 4.122887
[INFO] Epoch: 19 , batch: 212 , training loss: 4.223528
[INFO] Epoch: 19 , batch: 213 , training loss: 4.172500
[INFO] Epoch: 19 , batch: 214 , training loss: 4.264604
[INFO] Epoch: 19 , batch: 215 , training loss: 4.481314
[INFO] Epoch: 19 , batch: 216 , training loss: 4.194127
[INFO] Epoch: 19 , batch: 217 , training loss: 4.125427
[INFO] Epoch: 19 , batch: 218 , training loss: 4.102312
[INFO] Epoch: 19 , batch: 219 , training loss: 4.225780
[INFO] Epoch: 19 , batch: 220 , training loss: 4.027371
[INFO] Epoch: 19 , batch: 221 , training loss: 4.054350
[INFO] Epoch: 19 , batch: 222 , training loss: 4.196506
[INFO] Epoch: 19 , batch: 223 , training loss: 4.273927
[INFO] Epoch: 19 , batch: 224 , training loss: 4.326918
[INFO] Epoch: 19 , batch: 225 , training loss: 4.219141
[INFO] Epoch: 19 , batch: 226 , training loss: 4.355834
[INFO] Epoch: 19 , batch: 227 , training loss: 4.289723
[INFO] Epoch: 19 , batch: 228 , training loss: 4.346428
[INFO] Epoch: 19 , batch: 229 , training loss: 4.221964
[INFO] Epoch: 19 , batch: 230 , training loss: 4.071912
[INFO] Epoch: 19 , batch: 231 , training loss: 3.921945
[INFO] Epoch: 19 , batch: 232 , training loss: 4.074838
[INFO] Epoch: 19 , batch: 233 , training loss: 4.086060
[INFO] Epoch: 19 , batch: 234 , training loss: 3.776431
[INFO] Epoch: 19 , batch: 235 , training loss: 3.888745
[INFO] Epoch: 19 , batch: 236 , training loss: 4.017062
[INFO] Epoch: 19 , batch: 237 , training loss: 4.222500
[INFO] Epoch: 19 , batch: 238 , training loss: 3.962077
[INFO] Epoch: 19 , batch: 239 , training loss: 4.028487
[INFO] Epoch: 19 , batch: 240 , training loss: 4.086787
[INFO] Epoch: 19 , batch: 241 , training loss: 3.871950
[INFO] Epoch: 19 , batch: 242 , training loss: 3.912210
[INFO] Epoch: 19 , batch: 243 , training loss: 4.216726
[INFO] Epoch: 19 , batch: 244 , training loss: 4.127546
[INFO] Epoch: 19 , batch: 245 , training loss: 4.131917
[INFO] Epoch: 19 , batch: 246 , training loss: 3.812006
[INFO] Epoch: 19 , batch: 247 , training loss: 3.995428
[INFO] Epoch: 19 , batch: 248 , training loss: 4.075120
[INFO] Epoch: 19 , batch: 249 , training loss: 4.062220
[INFO] Epoch: 19 , batch: 250 , training loss: 3.816581
[INFO] Epoch: 19 , batch: 251 , training loss: 4.285521
[INFO] Epoch: 19 , batch: 252 , training loss: 3.982674
[INFO] Epoch: 19 , batch: 253 , training loss: 3.915751
[INFO] Epoch: 19 , batch: 254 , training loss: 4.197172
[INFO] Epoch: 19 , batch: 255 , training loss: 4.159789
[INFO] Epoch: 19 , batch: 256 , training loss: 4.176835
[INFO] Epoch: 19 , batch: 257 , training loss: 4.298026
[INFO] Epoch: 19 , batch: 258 , training loss: 4.372629
[INFO] Epoch: 19 , batch: 259 , training loss: 4.413167
[INFO] Epoch: 19 , batch: 260 , training loss: 4.132150
[INFO] Epoch: 19 , batch: 261 , training loss: 4.287530
[INFO] Epoch: 19 , batch: 262 , training loss: 4.470350
[INFO] Epoch: 19 , batch: 263 , training loss: 4.623255
[INFO] Epoch: 19 , batch: 264 , training loss: 3.942292
[INFO] Epoch: 19 , batch: 265 , training loss: 4.094713
[INFO] Epoch: 19 , batch: 266 , training loss: 4.507236
[INFO] Epoch: 19 , batch: 267 , training loss: 4.251504
[INFO] Epoch: 19 , batch: 268 , training loss: 4.176404
[INFO] Epoch: 19 , batch: 269 , training loss: 4.139750
[INFO] Epoch: 19 , batch: 270 , training loss: 4.188460
[INFO] Epoch: 19 , batch: 271 , training loss: 4.207529
[INFO] Epoch: 19 , batch: 272 , training loss: 4.186603
[INFO] Epoch: 19 , batch: 273 , training loss: 4.229575
[INFO] Epoch: 19 , batch: 274 , training loss: 4.303769
[INFO] Epoch: 19 , batch: 275 , training loss: 4.183484
[INFO] Epoch: 19 , batch: 276 , training loss: 4.219453
[INFO] Epoch: 19 , batch: 277 , training loss: 4.370857
[INFO] Epoch: 19 , batch: 278 , training loss: 4.054461
[INFO] Epoch: 19 , batch: 279 , training loss: 4.045754
[INFO] Epoch: 19 , batch: 280 , training loss: 4.015037
[INFO] Epoch: 19 , batch: 281 , training loss: 4.155922
[INFO] Epoch: 19 , batch: 282 , training loss: 4.056066
[INFO] Epoch: 19 , batch: 283 , training loss: 4.092645
[INFO] Epoch: 19 , batch: 284 , training loss: 4.101720
[INFO] Epoch: 19 , batch: 285 , training loss: 4.057531
[INFO] Epoch: 19 , batch: 286 , training loss: 4.048130
[INFO] Epoch: 19 , batch: 287 , training loss: 3.986812
[INFO] Epoch: 19 , batch: 288 , training loss: 3.996800
[INFO] Epoch: 19 , batch: 289 , training loss: 4.049462
[INFO] Epoch: 19 , batch: 290 , training loss: 3.814777
[INFO] Epoch: 19 , batch: 291 , training loss: 3.800867
[INFO] Epoch: 19 , batch: 292 , training loss: 3.928749
[INFO] Epoch: 19 , batch: 293 , training loss: 3.816731
[INFO] Epoch: 19 , batch: 294 , training loss: 4.516911
[INFO] Epoch: 19 , batch: 295 , training loss: 4.266283
[INFO] Epoch: 19 , batch: 296 , training loss: 4.211922
[INFO] Epoch: 19 , batch: 297 , training loss: 4.154046
[INFO] Epoch: 19 , batch: 298 , training loss: 4.003963
[INFO] Epoch: 19 , batch: 299 , training loss: 4.018631
[INFO] Epoch: 19 , batch: 300 , training loss: 3.980694
[INFO] Epoch: 19 , batch: 301 , training loss: 3.924984
[INFO] Epoch: 19 , batch: 302 , training loss: 4.101218
[INFO] Epoch: 19 , batch: 303 , training loss: 4.104772
[INFO] Epoch: 19 , batch: 304 , training loss: 4.289336
[INFO] Epoch: 19 , batch: 305 , training loss: 4.080254
[INFO] Epoch: 19 , batch: 306 , training loss: 4.207710
[INFO] Epoch: 19 , batch: 307 , training loss: 4.197017
[INFO] Epoch: 19 , batch: 308 , training loss: 4.038663
[INFO] Epoch: 19 , batch: 309 , training loss: 4.068890
[INFO] Epoch: 19 , batch: 310 , training loss: 3.930552
[INFO] Epoch: 19 , batch: 311 , training loss: 3.947701
[INFO] Epoch: 19 , batch: 312 , training loss: 3.860721
[INFO] Epoch: 19 , batch: 313 , training loss: 3.981812
[INFO] Epoch: 19 , batch: 314 , training loss: 4.046455
[INFO] Epoch: 19 , batch: 315 , training loss: 4.097108
[INFO] Epoch: 19 , batch: 316 , training loss: 4.335346
[INFO] Epoch: 19 , batch: 317 , training loss: 4.781110
[INFO] Epoch: 19 , batch: 318 , training loss: 4.927812
[INFO] Epoch: 19 , batch: 319 , training loss: 4.499473
[INFO] Epoch: 19 , batch: 320 , training loss: 4.063704
[INFO] Epoch: 19 , batch: 321 , training loss: 3.902246
[INFO] Epoch: 19 , batch: 322 , training loss: 4.006913
[INFO] Epoch: 19 , batch: 323 , training loss: 4.034548
[INFO] Epoch: 19 , batch: 324 , training loss: 3.995847
[INFO] Epoch: 19 , batch: 325 , training loss: 4.146081
[INFO] Epoch: 19 , batch: 326 , training loss: 4.201245
[INFO] Epoch: 19 , batch: 327 , training loss: 4.106303
[INFO] Epoch: 19 , batch: 328 , training loss: 4.085103
[INFO] Epoch: 19 , batch: 329 , training loss: 4.014706
[INFO] Epoch: 19 , batch: 330 , training loss: 4.024367
[INFO] Epoch: 19 , batch: 331 , training loss: 4.164903
[INFO] Epoch: 19 , batch: 332 , training loss: 3.960402
[INFO] Epoch: 19 , batch: 333 , training loss: 3.970058
[INFO] Epoch: 19 , batch: 334 , training loss: 3.995900
[INFO] Epoch: 19 , batch: 335 , training loss: 4.125552
[INFO] Epoch: 19 , batch: 336 , training loss: 4.144325
[INFO] Epoch: 19 , batch: 337 , training loss: 4.161485
[INFO] Epoch: 19 , batch: 338 , training loss: 4.365446
[INFO] Epoch: 19 , batch: 339 , training loss: 4.228865
[INFO] Epoch: 19 , batch: 340 , training loss: 4.394792
[INFO] Epoch: 19 , batch: 341 , training loss: 4.154820
[INFO] Epoch: 19 , batch: 342 , training loss: 3.930814
[INFO] Epoch: 19 , batch: 343 , training loss: 3.995496
[INFO] Epoch: 19 , batch: 344 , training loss: 3.861311
[INFO] Epoch: 19 , batch: 345 , training loss: 3.988021
[INFO] Epoch: 19 , batch: 346 , training loss: 4.053486
[INFO] Epoch: 19 , batch: 347 , training loss: 3.957090
[INFO] Epoch: 19 , batch: 348 , training loss: 4.078994
[INFO] Epoch: 19 , batch: 349 , training loss: 4.218809
[INFO] Epoch: 19 , batch: 350 , training loss: 3.990560
[INFO] Epoch: 19 , batch: 351 , training loss: 4.074177
[INFO] Epoch: 19 , batch: 352 , training loss: 4.102015
[INFO] Epoch: 19 , batch: 353 , training loss: 4.046948
[INFO] Epoch: 19 , batch: 354 , training loss: 4.176014
[INFO] Epoch: 19 , batch: 355 , training loss: 4.206279
[INFO] Epoch: 19 , batch: 356 , training loss: 4.037115
[INFO] Epoch: 19 , batch: 357 , training loss: 4.114639
[INFO] Epoch: 19 , batch: 358 , training loss: 4.022374
[INFO] Epoch: 19 , batch: 359 , training loss: 4.017221
[INFO] Epoch: 19 , batch: 360 , training loss: 4.118227
[INFO] Epoch: 19 , batch: 361 , training loss: 4.058049
[INFO] Epoch: 19 , batch: 362 , training loss: 4.163196
[INFO] Epoch: 19 , batch: 363 , training loss: 4.043285
[INFO] Epoch: 19 , batch: 364 , training loss: 4.093705
[INFO] Epoch: 19 , batch: 365 , training loss: 4.041247
[INFO] Epoch: 19 , batch: 366 , training loss: 4.168634
[INFO] Epoch: 19 , batch: 367 , training loss: 4.241231
[INFO] Epoch: 19 , batch: 368 , training loss: 4.686194
[INFO] Epoch: 19 , batch: 369 , training loss: 4.299597
[INFO] Epoch: 19 , batch: 370 , training loss: 4.075654
[INFO] Epoch: 19 , batch: 371 , training loss: 4.458808
[INFO] Epoch: 19 , batch: 372 , training loss: 4.766706
[INFO] Epoch: 19 , batch: 373 , training loss: 4.853906
[INFO] Epoch: 19 , batch: 374 , training loss: 4.928441
[INFO] Epoch: 19 , batch: 375 , training loss: 4.904088
[INFO] Epoch: 19 , batch: 376 , training loss: 4.842876
[INFO] Epoch: 19 , batch: 377 , training loss: 4.558774
[INFO] Epoch: 19 , batch: 378 , training loss: 4.678375
[INFO] Epoch: 19 , batch: 379 , training loss: 4.651967
[INFO] Epoch: 19 , batch: 380 , training loss: 4.788195
[INFO] Epoch: 19 , batch: 381 , training loss: 4.500832
[INFO] Epoch: 19 , batch: 382 , training loss: 4.747958
[INFO] Epoch: 19 , batch: 383 , training loss: 4.770497
[INFO] Epoch: 19 , batch: 384 , training loss: 4.781559
[INFO] Epoch: 19 , batch: 385 , training loss: 4.513462
[INFO] Epoch: 19 , batch: 386 , training loss: 4.751683
[INFO] Epoch: 19 , batch: 387 , training loss: 4.699366
[INFO] Epoch: 19 , batch: 388 , training loss: 4.504976
[INFO] Epoch: 19 , batch: 389 , training loss: 4.360145
[INFO] Epoch: 19 , batch: 390 , training loss: 4.324837
[INFO] Epoch: 19 , batch: 391 , training loss: 4.387729
[INFO] Epoch: 19 , batch: 392 , training loss: 4.748547
[INFO] Epoch: 19 , batch: 393 , training loss: 4.610806
[INFO] Epoch: 19 , batch: 394 , training loss: 4.647219
[INFO] Epoch: 19 , batch: 395 , training loss: 4.529711
[INFO] Epoch: 19 , batch: 396 , training loss: 4.291863
[INFO] Epoch: 19 , batch: 397 , training loss: 4.467190
[INFO] Epoch: 19 , batch: 398 , training loss: 4.316536
[INFO] Epoch: 19 , batch: 399 , training loss: 4.378812
[INFO] Epoch: 19 , batch: 400 , training loss: 4.360679
[INFO] Epoch: 19 , batch: 401 , training loss: 4.769903
[INFO] Epoch: 19 , batch: 402 , training loss: 4.485071
[INFO] Epoch: 19 , batch: 403 , training loss: 4.282831
[INFO] Epoch: 19 , batch: 404 , training loss: 4.494911
[INFO] Epoch: 19 , batch: 405 , training loss: 4.521911
[INFO] Epoch: 19 , batch: 406 , training loss: 4.429017
[INFO] Epoch: 19 , batch: 407 , training loss: 4.490726
[INFO] Epoch: 19 , batch: 408 , training loss: 4.450783
[INFO] Epoch: 19 , batch: 409 , training loss: 4.437468
[INFO] Epoch: 19 , batch: 410 , training loss: 4.487873
[INFO] Epoch: 19 , batch: 411 , training loss: 4.678545
[INFO] Epoch: 19 , batch: 412 , training loss: 4.520719
[INFO] Epoch: 19 , batch: 413 , training loss: 4.397058
[INFO] Epoch: 19 , batch: 414 , training loss: 4.415794
[INFO] Epoch: 19 , batch: 415 , training loss: 4.443250
[INFO] Epoch: 19 , batch: 416 , training loss: 4.528280
[INFO] Epoch: 19 , batch: 417 , training loss: 4.441166
[INFO] Epoch: 19 , batch: 418 , training loss: 4.455240
[INFO] Epoch: 19 , batch: 419 , training loss: 4.414398
[INFO] Epoch: 19 , batch: 420 , training loss: 4.386550
[INFO] Epoch: 19 , batch: 421 , training loss: 4.396208
[INFO] Epoch: 19 , batch: 422 , training loss: 4.277196
[INFO] Epoch: 19 , batch: 423 , training loss: 4.482454
[INFO] Epoch: 19 , batch: 424 , training loss: 4.670033
[INFO] Epoch: 19 , batch: 425 , training loss: 4.515809
[INFO] Epoch: 19 , batch: 426 , training loss: 4.234631
[INFO] Epoch: 19 , batch: 427 , training loss: 4.489159
[INFO] Epoch: 19 , batch: 428 , training loss: 4.361632
[INFO] Epoch: 19 , batch: 429 , training loss: 4.228495
[INFO] Epoch: 19 , batch: 430 , training loss: 4.496835
[INFO] Epoch: 19 , batch: 431 , training loss: 4.086704
[INFO] Epoch: 19 , batch: 432 , training loss: 4.155465
[INFO] Epoch: 19 , batch: 433 , training loss: 4.197303
[INFO] Epoch: 19 , batch: 434 , training loss: 4.050579
[INFO] Epoch: 19 , batch: 435 , training loss: 4.426611
[INFO] Epoch: 19 , batch: 436 , training loss: 4.503321
[INFO] Epoch: 19 , batch: 437 , training loss: 4.245606
[INFO] Epoch: 19 , batch: 438 , training loss: 4.098088
[INFO] Epoch: 19 , batch: 439 , training loss: 4.315398
[INFO] Epoch: 19 , batch: 440 , training loss: 4.443575
[INFO] Epoch: 19 , batch: 441 , training loss: 4.549334
[INFO] Epoch: 19 , batch: 442 , training loss: 4.324706
[INFO] Epoch: 19 , batch: 443 , training loss: 4.517415
[INFO] Epoch: 19 , batch: 444 , training loss: 4.116976
[INFO] Epoch: 19 , batch: 445 , training loss: 4.025956
[INFO] Epoch: 19 , batch: 446 , training loss: 3.963696
[INFO] Epoch: 19 , batch: 447 , training loss: 4.160257
[INFO] Epoch: 19 , batch: 448 , training loss: 4.256711
[INFO] Epoch: 19 , batch: 449 , training loss: 4.694019
[INFO] Epoch: 19 , batch: 450 , training loss: 4.728809
[INFO] Epoch: 19 , batch: 451 , training loss: 4.621019
[INFO] Epoch: 19 , batch: 452 , training loss: 4.421962
[INFO] Epoch: 19 , batch: 453 , training loss: 4.196518
[INFO] Epoch: 19 , batch: 454 , training loss: 4.333621
[INFO] Epoch: 19 , batch: 455 , training loss: 4.373273
[INFO] Epoch: 19 , batch: 456 , training loss: 4.382683
[INFO] Epoch: 19 , batch: 457 , training loss: 4.486720
[INFO] Epoch: 19 , batch: 458 , training loss: 4.198309
[INFO] Epoch: 19 , batch: 459 , training loss: 4.181185
[INFO] Epoch: 19 , batch: 460 , training loss: 4.298843
[INFO] Epoch: 19 , batch: 461 , training loss: 4.278610
[INFO] Epoch: 19 , batch: 462 , training loss: 4.332883
[INFO] Epoch: 19 , batch: 463 , training loss: 4.223120
[INFO] Epoch: 19 , batch: 464 , training loss: 4.440845
[INFO] Epoch: 19 , batch: 465 , training loss: 4.355392
[INFO] Epoch: 19 , batch: 466 , training loss: 4.436722
[INFO] Epoch: 19 , batch: 467 , training loss: 4.419913
[INFO] Epoch: 19 , batch: 468 , training loss: 4.384434
[INFO] Epoch: 19 , batch: 469 , training loss: 4.407259
[INFO] Epoch: 19 , batch: 470 , training loss: 4.228356
[INFO] Epoch: 19 , batch: 471 , training loss: 4.302734
[INFO] Epoch: 19 , batch: 472 , training loss: 4.352879
[INFO] Epoch: 19 , batch: 473 , training loss: 4.290642
[INFO] Epoch: 19 , batch: 474 , training loss: 4.084219
[INFO] Epoch: 19 , batch: 475 , training loss: 3.948782
[INFO] Epoch: 19 , batch: 476 , training loss: 4.329220
[INFO] Epoch: 19 , batch: 477 , training loss: 4.474015
[INFO] Epoch: 19 , batch: 478 , training loss: 4.517664
[INFO] Epoch: 19 , batch: 479 , training loss: 4.465355
[INFO] Epoch: 19 , batch: 480 , training loss: 4.580155
[INFO] Epoch: 19 , batch: 481 , training loss: 4.420469
[INFO] Epoch: 19 , batch: 482 , training loss: 4.575913
[INFO] Epoch: 19 , batch: 483 , training loss: 4.401289
[INFO] Epoch: 19 , batch: 484 , training loss: 4.218333
[INFO] Epoch: 19 , batch: 485 , training loss: 4.310694
[INFO] Epoch: 19 , batch: 486 , training loss: 4.189344
[INFO] Epoch: 19 , batch: 487 , training loss: 4.173043
[INFO] Epoch: 19 , batch: 488 , training loss: 4.371791
[INFO] Epoch: 19 , batch: 489 , training loss: 4.258243
[INFO] Epoch: 19 , batch: 490 , training loss: 4.311341
[INFO] Epoch: 19 , batch: 491 , training loss: 4.278367
[INFO] Epoch: 19 , batch: 492 , training loss: 4.206649
[INFO] Epoch: 19 , batch: 493 , training loss: 4.410850
[INFO] Epoch: 19 , batch: 494 , training loss: 4.331089
[INFO] Epoch: 19 , batch: 495 , training loss: 4.446119
[INFO] Epoch: 19 , batch: 496 , training loss: 4.309428
[INFO] Epoch: 19 , batch: 497 , training loss: 4.354681
[INFO] Epoch: 19 , batch: 498 , training loss: 4.359118
[INFO] Epoch: 19 , batch: 499 , training loss: 4.436868
[INFO] Epoch: 19 , batch: 500 , training loss: 4.600214
[INFO] Epoch: 19 , batch: 501 , training loss: 4.908780
[INFO] Epoch: 19 , batch: 502 , training loss: 4.985113
[INFO] Epoch: 19 , batch: 503 , training loss: 4.718637
[INFO] Epoch: 19 , batch: 504 , training loss: 4.838517
[INFO] Epoch: 19 , batch: 505 , training loss: 4.780284
[INFO] Epoch: 19 , batch: 506 , training loss: 4.745928
[INFO] Epoch: 19 , batch: 507 , training loss: 4.807094
[INFO] Epoch: 19 , batch: 508 , training loss: 4.744270
[INFO] Epoch: 19 , batch: 509 , training loss: 4.518919
[INFO] Epoch: 19 , batch: 510 , training loss: 4.580567
[INFO] Epoch: 19 , batch: 511 , training loss: 4.498238
[INFO] Epoch: 19 , batch: 512 , training loss: 4.591441
[INFO] Epoch: 19 , batch: 513 , training loss: 4.831966
[INFO] Epoch: 19 , batch: 514 , training loss: 4.493808
[INFO] Epoch: 19 , batch: 515 , training loss: 4.714019
[INFO] Epoch: 19 , batch: 516 , training loss: 4.528220
[INFO] Epoch: 19 , batch: 517 , training loss: 4.481570
[INFO] Epoch: 19 , batch: 518 , training loss: 4.468699
[INFO] Epoch: 19 , batch: 519 , training loss: 4.320703
[INFO] Epoch: 19 , batch: 520 , training loss: 4.547435
[INFO] Epoch: 19 , batch: 521 , training loss: 4.531583
[INFO] Epoch: 19 , batch: 522 , training loss: 4.567367
[INFO] Epoch: 19 , batch: 523 , training loss: 4.490590
[INFO] Epoch: 19 , batch: 524 , training loss: 4.781414
[INFO] Epoch: 19 , batch: 525 , training loss: 4.684119
[INFO] Epoch: 19 , batch: 526 , training loss: 4.484016
[INFO] Epoch: 19 , batch: 527 , training loss: 4.495497
[INFO] Epoch: 19 , batch: 528 , training loss: 4.529226
[INFO] Epoch: 19 , batch: 529 , training loss: 4.513272
[INFO] Epoch: 19 , batch: 530 , training loss: 4.351123
[INFO] Epoch: 19 , batch: 531 , training loss: 4.476302
[INFO] Epoch: 19 , batch: 532 , training loss: 4.371382
[INFO] Epoch: 19 , batch: 533 , training loss: 4.546177
[INFO] Epoch: 19 , batch: 534 , training loss: 4.530511
[INFO] Epoch: 19 , batch: 535 , training loss: 4.535553
[INFO] Epoch: 19 , batch: 536 , training loss: 4.361635
[INFO] Epoch: 19 , batch: 537 , training loss: 4.376804
[INFO] Epoch: 19 , batch: 538 , training loss: 4.437207
[INFO] Epoch: 19 , batch: 539 , training loss: 4.551097
[INFO] Epoch: 19 , batch: 540 , training loss: 5.090762
[INFO] Epoch: 19 , batch: 541 , training loss: 5.002989
[INFO] Epoch: 19 , batch: 542 , training loss: 4.873984
[INFO] Epoch: 20 , batch: 0 , training loss: 3.611445
[INFO] Epoch: 20 , batch: 1 , training loss: 3.666747
[INFO] Epoch: 20 , batch: 2 , training loss: 3.729719
[INFO] Epoch: 20 , batch: 3 , training loss: 3.581025
[INFO] Epoch: 20 , batch: 4 , training loss: 3.942281
[INFO] Epoch: 20 , batch: 5 , training loss: 3.583684
[INFO] Epoch: 20 , batch: 6 , training loss: 3.997532
[INFO] Epoch: 20 , batch: 7 , training loss: 3.849072
[INFO] Epoch: 20 , batch: 8 , training loss: 3.605006
[INFO] Epoch: 20 , batch: 9 , training loss: 3.802338
[INFO] Epoch: 20 , batch: 10 , training loss: 3.785167
[INFO] Epoch: 20 , batch: 11 , training loss: 3.729874
[INFO] Epoch: 20 , batch: 12 , training loss: 3.633965
[INFO] Epoch: 20 , batch: 13 , training loss: 3.677358
[INFO] Epoch: 20 , batch: 14 , training loss: 3.506413
[INFO] Epoch: 20 , batch: 15 , training loss: 3.701045
[INFO] Epoch: 20 , batch: 16 , training loss: 3.561938
[INFO] Epoch: 20 , batch: 17 , training loss: 3.749549
[INFO] Epoch: 20 , batch: 18 , training loss: 3.712471
[INFO] Epoch: 20 , batch: 19 , training loss: 3.409575
[INFO] Epoch: 20 , batch: 20 , training loss: 3.399555
[INFO] Epoch: 20 , batch: 21 , training loss: 3.524149
[INFO] Epoch: 20 , batch: 22 , training loss: 3.475046
[INFO] Epoch: 20 , batch: 23 , training loss: 3.637902
[INFO] Epoch: 20 , batch: 24 , training loss: 3.479041
[INFO] Epoch: 20 , batch: 25 , training loss: 3.667145
[INFO] Epoch: 20 , batch: 26 , training loss: 3.512433
[INFO] Epoch: 20 , batch: 27 , training loss: 3.477492
[INFO] Epoch: 20 , batch: 28 , training loss: 3.594405
[INFO] Epoch: 20 , batch: 29 , training loss: 3.466997
[INFO] Epoch: 20 , batch: 30 , training loss: 3.553062
[INFO] Epoch: 20 , batch: 31 , training loss: 3.570464
[INFO] Epoch: 20 , batch: 32 , training loss: 3.573359
[INFO] Epoch: 20 , batch: 33 , training loss: 3.597360
[INFO] Epoch: 20 , batch: 34 , training loss: 3.597455
[INFO] Epoch: 20 , batch: 35 , training loss: 3.586382
[INFO] Epoch: 20 , batch: 36 , training loss: 3.609274
[INFO] Epoch: 20 , batch: 37 , training loss: 3.538418
[INFO] Epoch: 20 , batch: 38 , training loss: 3.545558
[INFO] Epoch: 20 , batch: 39 , training loss: 3.429783
[INFO] Epoch: 20 , batch: 40 , training loss: 3.656411
[INFO] Epoch: 20 , batch: 41 , training loss: 3.572896
[INFO] Epoch: 20 , batch: 42 , training loss: 3.973547
[INFO] Epoch: 20 , batch: 43 , training loss: 3.743922
[INFO] Epoch: 20 , batch: 44 , training loss: 4.071341
[INFO] Epoch: 20 , batch: 45 , training loss: 4.037994
[INFO] Epoch: 20 , batch: 46 , training loss: 3.948849
[INFO] Epoch: 20 , batch: 47 , training loss: 3.674124
[INFO] Epoch: 20 , batch: 48 , training loss: 3.692746
[INFO] Epoch: 20 , batch: 49 , training loss: 3.885741
[INFO] Epoch: 20 , batch: 50 , training loss: 3.640774
[INFO] Epoch: 20 , batch: 51 , training loss: 3.836550
[INFO] Epoch: 20 , batch: 52 , training loss: 3.675386
[INFO] Epoch: 20 , batch: 53 , training loss: 3.851484
[INFO] Epoch: 20 , batch: 54 , training loss: 3.850552
[INFO] Epoch: 20 , batch: 55 , training loss: 3.913689
[INFO] Epoch: 20 , batch: 56 , training loss: 3.781308
[INFO] Epoch: 20 , batch: 57 , training loss: 3.669300
[INFO] Epoch: 20 , batch: 58 , training loss: 3.753821
[INFO] Epoch: 20 , batch: 59 , training loss: 3.786793
[INFO] Epoch: 20 , batch: 60 , training loss: 3.762090
[INFO] Epoch: 20 , batch: 61 , training loss: 3.834097
[INFO] Epoch: 20 , batch: 62 , training loss: 3.757158
[INFO] Epoch: 20 , batch: 63 , training loss: 3.941096
[INFO] Epoch: 20 , batch: 64 , training loss: 4.091343
[INFO] Epoch: 20 , batch: 65 , training loss: 3.805053
[INFO] Epoch: 20 , batch: 66 , training loss: 3.652306
[INFO] Epoch: 20 , batch: 67 , training loss: 3.741709
[INFO] Epoch: 20 , batch: 68 , training loss: 3.892726
[INFO] Epoch: 20 , batch: 69 , training loss: 3.750937
[INFO] Epoch: 20 , batch: 70 , training loss: 4.019337
[INFO] Epoch: 20 , batch: 71 , training loss: 3.862091
[INFO] Epoch: 20 , batch: 72 , training loss: 3.881099
[INFO] Epoch: 20 , batch: 73 , training loss: 3.835922
[INFO] Epoch: 20 , batch: 74 , training loss: 3.973519
[INFO] Epoch: 20 , batch: 75 , training loss: 3.819256
[INFO] Epoch: 20 , batch: 76 , training loss: 3.915196
[INFO] Epoch: 20 , batch: 77 , training loss: 3.870806
[INFO] Epoch: 20 , batch: 78 , training loss: 3.995128
[INFO] Epoch: 20 , batch: 79 , training loss: 3.808075
[INFO] Epoch: 20 , batch: 80 , training loss: 4.014957
[INFO] Epoch: 20 , batch: 81 , training loss: 3.951508
[INFO] Epoch: 20 , batch: 82 , training loss: 3.898469
[INFO] Epoch: 20 , batch: 83 , training loss: 4.029394
[INFO] Epoch: 20 , batch: 84 , training loss: 3.937582
[INFO] Epoch: 20 , batch: 85 , training loss: 4.030998
[INFO] Epoch: 20 , batch: 86 , training loss: 3.977831
[INFO] Epoch: 20 , batch: 87 , training loss: 3.959583
[INFO] Epoch: 20 , batch: 88 , training loss: 4.087700
[INFO] Epoch: 20 , batch: 89 , training loss: 3.889894
[INFO] Epoch: 20 , batch: 90 , training loss: 3.948062
[INFO] Epoch: 20 , batch: 91 , training loss: 3.889203
[INFO] Epoch: 20 , batch: 92 , training loss: 3.889252
[INFO] Epoch: 20 , batch: 93 , training loss: 4.009832
[INFO] Epoch: 20 , batch: 94 , training loss: 4.159937
[INFO] Epoch: 20 , batch: 95 , training loss: 3.899934
[INFO] Epoch: 20 , batch: 96 , training loss: 3.923585
[INFO] Epoch: 20 , batch: 97 , training loss: 3.844098
[INFO] Epoch: 20 , batch: 98 , training loss: 3.797929
[INFO] Epoch: 20 , batch: 99 , training loss: 3.904142
[INFO] Epoch: 20 , batch: 100 , training loss: 3.818974
[INFO] Epoch: 20 , batch: 101 , training loss: 3.859537
[INFO] Epoch: 20 , batch: 102 , training loss: 3.961046
[INFO] Epoch: 20 , batch: 103 , training loss: 3.781623
[INFO] Epoch: 20 , batch: 104 , training loss: 3.741904
[INFO] Epoch: 20 , batch: 105 , training loss: 4.011910
[INFO] Epoch: 20 , batch: 106 , training loss: 3.987302
[INFO] Epoch: 20 , batch: 107 , training loss: 3.839612
[INFO] Epoch: 20 , batch: 108 , training loss: 3.765661
[INFO] Epoch: 20 , batch: 109 , training loss: 3.675959
[INFO] Epoch: 20 , batch: 110 , training loss: 3.876132
[INFO] Epoch: 20 , batch: 111 , training loss: 3.975322
[INFO] Epoch: 20 , batch: 112 , training loss: 3.902508
[INFO] Epoch: 20 , batch: 113 , training loss: 3.871656
[INFO] Epoch: 20 , batch: 114 , training loss: 3.899160
[INFO] Epoch: 20 , batch: 115 , training loss: 3.882694
[INFO] Epoch: 20 , batch: 116 , training loss: 3.788546
[INFO] Epoch: 20 , batch: 117 , training loss: 4.032631
[INFO] Epoch: 20 , batch: 118 , training loss: 3.981823
[INFO] Epoch: 20 , batch: 119 , training loss: 4.112798
[INFO] Epoch: 20 , batch: 120 , training loss: 4.082819
[INFO] Epoch: 20 , batch: 121 , training loss: 3.954943
[INFO] Epoch: 20 , batch: 122 , training loss: 3.838000
[INFO] Epoch: 20 , batch: 123 , training loss: 3.908136
[INFO] Epoch: 20 , batch: 124 , training loss: 4.033544
[INFO] Epoch: 20 , batch: 125 , training loss: 3.805510
[INFO] Epoch: 20 , batch: 126 , training loss: 3.820454
[INFO] Epoch: 20 , batch: 127 , training loss: 3.793067
[INFO] Epoch: 20 , batch: 128 , training loss: 3.948869
[INFO] Epoch: 20 , batch: 129 , training loss: 3.958135
[INFO] Epoch: 20 , batch: 130 , training loss: 3.929328
[INFO] Epoch: 20 , batch: 131 , training loss: 3.882967
[INFO] Epoch: 20 , batch: 132 , training loss: 3.891458
[INFO] Epoch: 20 , batch: 133 , training loss: 3.873969
[INFO] Epoch: 20 , batch: 134 , training loss: 3.683939
[INFO] Epoch: 20 , batch: 135 , training loss: 3.749048
[INFO] Epoch: 20 , batch: 136 , training loss: 4.046099
[INFO] Epoch: 20 , batch: 137 , training loss: 3.937487
[INFO] Epoch: 20 , batch: 138 , training loss: 3.979748
[INFO] Epoch: 20 , batch: 139 , training loss: 4.600054
[INFO] Epoch: 20 , batch: 140 , training loss: 4.328465
[INFO] Epoch: 20 , batch: 141 , training loss: 4.101502
[INFO] Epoch: 20 , batch: 142 , training loss: 3.839321
[INFO] Epoch: 20 , batch: 143 , training loss: 3.987510
[INFO] Epoch: 20 , batch: 144 , training loss: 3.762568
[INFO] Epoch: 20 , batch: 145 , training loss: 3.884882
[INFO] Epoch: 20 , batch: 146 , training loss: 4.057768
[INFO] Epoch: 20 , batch: 147 , training loss: 3.754743
[INFO] Epoch: 20 , batch: 148 , training loss: 3.739149
[INFO] Epoch: 20 , batch: 149 , training loss: 3.823792
[INFO] Epoch: 20 , batch: 150 , training loss: 4.024517
[INFO] Epoch: 20 , batch: 151 , training loss: 3.906916
[INFO] Epoch: 20 , batch: 152 , training loss: 3.991608
[INFO] Epoch: 20 , batch: 153 , training loss: 3.950433
[INFO] Epoch: 20 , batch: 154 , training loss: 4.030047
[INFO] Epoch: 20 , batch: 155 , training loss: 4.283130
[INFO] Epoch: 20 , batch: 156 , training loss: 3.978933
[INFO] Epoch: 20 , batch: 157 , training loss: 3.946586
[INFO] Epoch: 20 , batch: 158 , training loss: 4.125537
[INFO] Epoch: 20 , batch: 159 , training loss: 3.962754
[INFO] Epoch: 20 , batch: 160 , training loss: 4.281315
[INFO] Epoch: 20 , batch: 161 , training loss: 4.405523
[INFO] Epoch: 20 , batch: 162 , training loss: 4.357899
[INFO] Epoch: 20 , batch: 163 , training loss: 4.459952
[INFO] Epoch: 20 , batch: 164 , training loss: 4.467429
[INFO] Epoch: 20 , batch: 165 , training loss: 4.367667
[INFO] Epoch: 20 , batch: 166 , training loss: 4.274315
[INFO] Epoch: 20 , batch: 167 , training loss: 4.377320
[INFO] Epoch: 20 , batch: 168 , training loss: 4.144464
[INFO] Epoch: 20 , batch: 169 , training loss: 4.061362
[INFO] Epoch: 20 , batch: 170 , training loss: 4.258710
[INFO] Epoch: 20 , batch: 171 , training loss: 3.636580
[INFO] Epoch: 20 , batch: 172 , training loss: 3.854023
[INFO] Epoch: 20 , batch: 173 , training loss: 4.170191
[INFO] Epoch: 20 , batch: 174 , training loss: 4.528325
[INFO] Epoch: 20 , batch: 175 , training loss: 4.929561
[INFO] Epoch: 20 , batch: 176 , training loss: 4.580534
[INFO] Epoch: 20 , batch: 177 , training loss: 4.221085
[INFO] Epoch: 20 , batch: 178 , training loss: 4.199603
[INFO] Epoch: 20 , batch: 179 , training loss: 4.251891
[INFO] Epoch: 20 , batch: 180 , training loss: 4.178905
[INFO] Epoch: 20 , batch: 181 , training loss: 4.464244
[INFO] Epoch: 20 , batch: 182 , training loss: 4.405421
[INFO] Epoch: 20 , batch: 183 , training loss: 4.370959
[INFO] Epoch: 20 , batch: 184 , training loss: 4.276980
[INFO] Epoch: 20 , batch: 185 , training loss: 4.206065
[INFO] Epoch: 20 , batch: 186 , training loss: 4.365328
[INFO] Epoch: 20 , batch: 187 , training loss: 4.456752
[INFO] Epoch: 20 , batch: 188 , training loss: 4.443135
[INFO] Epoch: 20 , batch: 189 , training loss: 4.348208
[INFO] Epoch: 20 , batch: 190 , training loss: 4.387498
[INFO] Epoch: 20 , batch: 191 , training loss: 4.480068
[INFO] Epoch: 20 , batch: 192 , training loss: 4.282976
[INFO] Epoch: 20 , batch: 193 , training loss: 4.405273
[INFO] Epoch: 20 , batch: 194 , training loss: 4.344999
[INFO] Epoch: 20 , batch: 195 , training loss: 4.240833
[INFO] Epoch: 20 , batch: 196 , training loss: 4.127092
[INFO] Epoch: 20 , batch: 197 , training loss: 4.224173
[INFO] Epoch: 20 , batch: 198 , training loss: 4.145011
[INFO] Epoch: 20 , batch: 199 , training loss: 4.296855
[INFO] Epoch: 20 , batch: 200 , training loss: 4.181968
[INFO] Epoch: 20 , batch: 201 , training loss: 4.078523
[INFO] Epoch: 20 , batch: 202 , training loss: 4.050398
[INFO] Epoch: 20 , batch: 203 , training loss: 4.165853
[INFO] Epoch: 20 , batch: 204 , training loss: 4.297501
[INFO] Epoch: 20 , batch: 205 , training loss: 3.862542
[INFO] Epoch: 20 , batch: 206 , training loss: 3.810194
[INFO] Epoch: 20 , batch: 207 , training loss: 3.824521
[INFO] Epoch: 20 , batch: 208 , training loss: 4.133982
[INFO] Epoch: 20 , batch: 209 , training loss: 4.098855
[INFO] Epoch: 20 , batch: 210 , training loss: 4.080211
[INFO] Epoch: 20 , batch: 211 , training loss: 4.107799
[INFO] Epoch: 20 , batch: 212 , training loss: 4.208170
[INFO] Epoch: 20 , batch: 213 , training loss: 4.165375
[INFO] Epoch: 20 , batch: 214 , training loss: 4.270263
[INFO] Epoch: 20 , batch: 215 , training loss: 4.468947
[INFO] Epoch: 20 , batch: 216 , training loss: 4.168298
[INFO] Epoch: 20 , batch: 217 , training loss: 4.117470
[INFO] Epoch: 20 , batch: 218 , training loss: 4.109552
[INFO] Epoch: 20 , batch: 219 , training loss: 4.220541
[INFO] Epoch: 20 , batch: 220 , training loss: 4.012258
[INFO] Epoch: 20 , batch: 221 , training loss: 4.037954
[INFO] Epoch: 20 , batch: 222 , training loss: 4.191535
[INFO] Epoch: 20 , batch: 223 , training loss: 4.264744
[INFO] Epoch: 20 , batch: 224 , training loss: 4.299451
[INFO] Epoch: 20 , batch: 225 , training loss: 4.205277
[INFO] Epoch: 20 , batch: 226 , training loss: 4.332892
[INFO] Epoch: 20 , batch: 227 , training loss: 4.296655
[INFO] Epoch: 20 , batch: 228 , training loss: 4.337237
[INFO] Epoch: 20 , batch: 229 , training loss: 4.205219
[INFO] Epoch: 20 , batch: 230 , training loss: 4.060116
[INFO] Epoch: 20 , batch: 231 , training loss: 3.927028
[INFO] Epoch: 20 , batch: 232 , training loss: 4.079311
[INFO] Epoch: 20 , batch: 233 , training loss: 4.081105
[INFO] Epoch: 20 , batch: 234 , training loss: 3.769661
[INFO] Epoch: 20 , batch: 235 , training loss: 3.876379
[INFO] Epoch: 20 , batch: 236 , training loss: 3.997240
[INFO] Epoch: 20 , batch: 237 , training loss: 4.215045
[INFO] Epoch: 20 , batch: 238 , training loss: 3.951264
[INFO] Epoch: 20 , batch: 239 , training loss: 4.034587
[INFO] Epoch: 20 , batch: 240 , training loss: 4.066954
[INFO] Epoch: 20 , batch: 241 , training loss: 3.859967
[INFO] Epoch: 20 , batch: 242 , training loss: 3.894701
[INFO] Epoch: 20 , batch: 243 , training loss: 4.199173
[INFO] Epoch: 20 , batch: 244 , training loss: 4.122028
[INFO] Epoch: 20 , batch: 245 , training loss: 4.115146
[INFO] Epoch: 20 , batch: 246 , training loss: 3.798174
[INFO] Epoch: 20 , batch: 247 , training loss: 3.975423
[INFO] Epoch: 20 , batch: 248 , training loss: 4.067307
[INFO] Epoch: 20 , batch: 249 , training loss: 4.066738
[INFO] Epoch: 20 , batch: 250 , training loss: 3.820989
[INFO] Epoch: 20 , batch: 251 , training loss: 4.279468
[INFO] Epoch: 20 , batch: 252 , training loss: 3.955048
[INFO] Epoch: 20 , batch: 253 , training loss: 3.907985
[INFO] Epoch: 20 , batch: 254 , training loss: 4.182068
[INFO] Epoch: 20 , batch: 255 , training loss: 4.129265
[INFO] Epoch: 20 , batch: 256 , training loss: 4.156495
[INFO] Epoch: 20 , batch: 257 , training loss: 4.321176
[INFO] Epoch: 20 , batch: 258 , training loss: 4.349251
[INFO] Epoch: 20 , batch: 259 , training loss: 4.414144
[INFO] Epoch: 20 , batch: 260 , training loss: 4.119915
[INFO] Epoch: 20 , batch: 261 , training loss: 4.274184
[INFO] Epoch: 20 , batch: 262 , training loss: 4.454439
[INFO] Epoch: 20 , batch: 263 , training loss: 4.618176
[INFO] Epoch: 20 , batch: 264 , training loss: 3.942800
[INFO] Epoch: 20 , batch: 265 , training loss: 4.072184
[INFO] Epoch: 20 , batch: 266 , training loss: 4.507648
[INFO] Epoch: 20 , batch: 267 , training loss: 4.256806
[INFO] Epoch: 20 , batch: 268 , training loss: 4.159030
[INFO] Epoch: 20 , batch: 269 , training loss: 4.130973
[INFO] Epoch: 20 , batch: 270 , training loss: 4.185169
[INFO] Epoch: 20 , batch: 271 , training loss: 4.205403
[INFO] Epoch: 20 , batch: 272 , training loss: 4.192825
[INFO] Epoch: 20 , batch: 273 , training loss: 4.205474
[INFO] Epoch: 20 , batch: 274 , training loss: 4.280886
[INFO] Epoch: 20 , batch: 275 , training loss: 4.180692
[INFO] Epoch: 20 , batch: 276 , training loss: 4.223779
[INFO] Epoch: 20 , batch: 277 , training loss: 4.384063
[INFO] Epoch: 20 , batch: 278 , training loss: 4.031373
[INFO] Epoch: 20 , batch: 279 , training loss: 4.038382
[INFO] Epoch: 20 , batch: 280 , training loss: 4.009823
[INFO] Epoch: 20 , batch: 281 , training loss: 4.127532
[INFO] Epoch: 20 , batch: 282 , training loss: 4.064656
[INFO] Epoch: 20 , batch: 283 , training loss: 4.075850
[INFO] Epoch: 20 , batch: 284 , training loss: 4.092046
[INFO] Epoch: 20 , batch: 285 , training loss: 4.044053
[INFO] Epoch: 20 , batch: 286 , training loss: 4.043215
[INFO] Epoch: 20 , batch: 287 , training loss: 3.969919
[INFO] Epoch: 20 , batch: 288 , training loss: 3.978756
[INFO] Epoch: 20 , batch: 289 , training loss: 4.047322
[INFO] Epoch: 20 , batch: 290 , training loss: 3.813011
[INFO] Epoch: 20 , batch: 291 , training loss: 3.802672
[INFO] Epoch: 20 , batch: 292 , training loss: 3.907410
[INFO] Epoch: 20 , batch: 293 , training loss: 3.820867
[INFO] Epoch: 20 , batch: 294 , training loss: 4.501113
[INFO] Epoch: 20 , batch: 295 , training loss: 4.265638
[INFO] Epoch: 20 , batch: 296 , training loss: 4.194699
[INFO] Epoch: 20 , batch: 297 , training loss: 4.146420
[INFO] Epoch: 20 , batch: 298 , training loss: 3.997239
[INFO] Epoch: 20 , batch: 299 , training loss: 4.029722
[INFO] Epoch: 20 , batch: 300 , training loss: 3.981973
[INFO] Epoch: 20 , batch: 301 , training loss: 3.931971
[INFO] Epoch: 20 , batch: 302 , training loss: 4.102809
[INFO] Epoch: 20 , batch: 303 , training loss: 4.115808
[INFO] Epoch: 20 , batch: 304 , training loss: 4.289918
[INFO] Epoch: 20 , batch: 305 , training loss: 4.066896
[INFO] Epoch: 20 , batch: 306 , training loss: 4.205610
[INFO] Epoch: 20 , batch: 307 , training loss: 4.200206
[INFO] Epoch: 20 , batch: 308 , training loss: 4.032505
[INFO] Epoch: 20 , batch: 309 , training loss: 4.056967
[INFO] Epoch: 20 , batch: 310 , training loss: 3.914151
[INFO] Epoch: 20 , batch: 311 , training loss: 3.946570
[INFO] Epoch: 20 , batch: 312 , training loss: 3.852652
[INFO] Epoch: 20 , batch: 313 , training loss: 3.981770
[INFO] Epoch: 20 , batch: 314 , training loss: 4.036493
[INFO] Epoch: 20 , batch: 315 , training loss: 4.076728
[INFO] Epoch: 20 , batch: 316 , training loss: 4.326737
[INFO] Epoch: 20 , batch: 317 , training loss: 4.771053
[INFO] Epoch: 20 , batch: 318 , training loss: 4.955970
[INFO] Epoch: 20 , batch: 319 , training loss: 4.493187
[INFO] Epoch: 20 , batch: 320 , training loss: 4.066273
[INFO] Epoch: 20 , batch: 321 , training loss: 3.895098
[INFO] Epoch: 20 , batch: 322 , training loss: 4.004172
[INFO] Epoch: 20 , batch: 323 , training loss: 4.027330
[INFO] Epoch: 20 , batch: 324 , training loss: 3.981397
[INFO] Epoch: 20 , batch: 325 , training loss: 4.149049
[INFO] Epoch: 20 , batch: 326 , training loss: 4.189354
[INFO] Epoch: 20 , batch: 327 , training loss: 4.104749
[INFO] Epoch: 20 , batch: 328 , training loss: 4.087467
[INFO] Epoch: 20 , batch: 329 , training loss: 4.011669
[INFO] Epoch: 20 , batch: 330 , training loss: 4.018845
[INFO] Epoch: 20 , batch: 331 , training loss: 4.164036
[INFO] Epoch: 20 , batch: 332 , training loss: 3.963987
[INFO] Epoch: 20 , batch: 333 , training loss: 3.981518
[INFO] Epoch: 20 , batch: 334 , training loss: 3.987653
[INFO] Epoch: 20 , batch: 335 , training loss: 4.121392
[INFO] Epoch: 20 , batch: 336 , training loss: 4.150027
[INFO] Epoch: 20 , batch: 337 , training loss: 4.167472
[INFO] Epoch: 20 , batch: 338 , training loss: 4.370650
[INFO] Epoch: 20 , batch: 339 , training loss: 4.222577
[INFO] Epoch: 20 , batch: 340 , training loss: 4.395840
[INFO] Epoch: 20 , batch: 341 , training loss: 4.145287
[INFO] Epoch: 20 , batch: 342 , training loss: 3.924543
[INFO] Epoch: 20 , batch: 343 , training loss: 4.001487
[INFO] Epoch: 20 , batch: 344 , training loss: 3.850481
[INFO] Epoch: 20 , batch: 345 , training loss: 3.985320
[INFO] Epoch: 20 , batch: 346 , training loss: 4.044576
[INFO] Epoch: 20 , batch: 347 , training loss: 3.944561
[INFO] Epoch: 20 , batch: 348 , training loss: 4.071329
[INFO] Epoch: 20 , batch: 349 , training loss: 4.200983
[INFO] Epoch: 20 , batch: 350 , training loss: 3.990181
[INFO] Epoch: 20 , batch: 351 , training loss: 4.060817
[INFO] Epoch: 20 , batch: 352 , training loss: 4.082013
[INFO] Epoch: 20 , batch: 353 , training loss: 4.040998
[INFO] Epoch: 20 , batch: 354 , training loss: 4.154709
[INFO] Epoch: 20 , batch: 355 , training loss: 4.199581
[INFO] Epoch: 20 , batch: 356 , training loss: 4.030937
[INFO] Epoch: 20 , batch: 357 , training loss: 4.116828
[INFO] Epoch: 20 , batch: 358 , training loss: 4.024353
[INFO] Epoch: 20 , batch: 359 , training loss: 4.006936
[INFO] Epoch: 20 , batch: 360 , training loss: 4.102542
[INFO] Epoch: 20 , batch: 361 , training loss: 4.053034
[INFO] Epoch: 20 , batch: 362 , training loss: 4.160586
[INFO] Epoch: 20 , batch: 363 , training loss: 4.046774
[INFO] Epoch: 20 , batch: 364 , training loss: 4.091247
[INFO] Epoch: 20 , batch: 365 , training loss: 4.019851
[INFO] Epoch: 20 , batch: 366 , training loss: 4.160129
[INFO] Epoch: 20 , batch: 367 , training loss: 4.220306
[INFO] Epoch: 20 , batch: 368 , training loss: 4.658281
[INFO] Epoch: 20 , batch: 369 , training loss: 4.303316
[INFO] Epoch: 20 , batch: 370 , training loss: 4.069699
[INFO] Epoch: 20 , batch: 371 , training loss: 4.447999
[INFO] Epoch: 20 , batch: 372 , training loss: 4.747440
[INFO] Epoch: 20 , batch: 373 , training loss: 4.839023
[INFO] Epoch: 20 , batch: 374 , training loss: 4.912314
[INFO] Epoch: 20 , batch: 375 , training loss: 4.887934
[INFO] Epoch: 20 , batch: 376 , training loss: 4.820240
[INFO] Epoch: 20 , batch: 377 , training loss: 4.554363
[INFO] Epoch: 20 , batch: 378 , training loss: 4.663483
[INFO] Epoch: 20 , batch: 379 , training loss: 4.640155
[INFO] Epoch: 20 , batch: 380 , training loss: 4.769298
[INFO] Epoch: 20 , batch: 381 , training loss: 4.503676
[INFO] Epoch: 20 , batch: 382 , training loss: 4.750527
[INFO] Epoch: 20 , batch: 383 , training loss: 4.746934
[INFO] Epoch: 20 , batch: 384 , training loss: 4.771835
[INFO] Epoch: 20 , batch: 385 , training loss: 4.489321
[INFO] Epoch: 20 , batch: 386 , training loss: 4.749415
[INFO] Epoch: 20 , batch: 387 , training loss: 4.706360
[INFO] Epoch: 20 , batch: 388 , training loss: 4.509632
[INFO] Epoch: 20 , batch: 389 , training loss: 4.363369
[INFO] Epoch: 20 , batch: 390 , training loss: 4.332517
[INFO] Epoch: 20 , batch: 391 , training loss: 4.369431
[INFO] Epoch: 20 , batch: 392 , training loss: 4.745319
[INFO] Epoch: 20 , batch: 393 , training loss: 4.617938
[INFO] Epoch: 20 , batch: 394 , training loss: 4.643136
[INFO] Epoch: 20 , batch: 395 , training loss: 4.519104
[INFO] Epoch: 20 , batch: 396 , training loss: 4.286723
[INFO] Epoch: 20 , batch: 397 , training loss: 4.455118
[INFO] Epoch: 20 , batch: 398 , training loss: 4.319704
[INFO] Epoch: 20 , batch: 399 , training loss: 4.369559
[INFO] Epoch: 20 , batch: 400 , training loss: 4.345995
[INFO] Epoch: 20 , batch: 401 , training loss: 4.772905
[INFO] Epoch: 20 , batch: 402 , training loss: 4.481756
[INFO] Epoch: 20 , batch: 403 , training loss: 4.281913
[INFO] Epoch: 20 , batch: 404 , training loss: 4.483095
[INFO] Epoch: 20 , batch: 405 , training loss: 4.525378
[INFO] Epoch: 20 , batch: 406 , training loss: 4.428795
[INFO] Epoch: 20 , batch: 407 , training loss: 4.474627
[INFO] Epoch: 20 , batch: 408 , training loss: 4.442906
[INFO] Epoch: 20 , batch: 409 , training loss: 4.423753
[INFO] Epoch: 20 , batch: 410 , training loss: 4.473002
[INFO] Epoch: 20 , batch: 411 , training loss: 4.664637
[INFO] Epoch: 20 , batch: 412 , training loss: 4.522952
[INFO] Epoch: 20 , batch: 413 , training loss: 4.389884
[INFO] Epoch: 20 , batch: 414 , training loss: 4.414672
[INFO] Epoch: 20 , batch: 415 , training loss: 4.427287
[INFO] Epoch: 20 , batch: 416 , training loss: 4.531296
[INFO] Epoch: 20 , batch: 417 , training loss: 4.436676
[INFO] Epoch: 20 , batch: 418 , training loss: 4.458330
[INFO] Epoch: 20 , batch: 419 , training loss: 4.407192
[INFO] Epoch: 20 , batch: 420 , training loss: 4.395354
[INFO] Epoch: 20 , batch: 421 , training loss: 4.393516
[INFO] Epoch: 20 , batch: 422 , training loss: 4.271040
[INFO] Epoch: 20 , batch: 423 , training loss: 4.464202
[INFO] Epoch: 20 , batch: 424 , training loss: 4.660101
[INFO] Epoch: 20 , batch: 425 , training loss: 4.507863
[INFO] Epoch: 20 , batch: 426 , training loss: 4.238352
[INFO] Epoch: 20 , batch: 427 , training loss: 4.477477
[INFO] Epoch: 20 , batch: 428 , training loss: 4.349755
[INFO] Epoch: 20 , batch: 429 , training loss: 4.214958
[INFO] Epoch: 20 , batch: 430 , training loss: 4.486446
[INFO] Epoch: 20 , batch: 431 , training loss: 4.080157
[INFO] Epoch: 20 , batch: 432 , training loss: 4.159271
[INFO] Epoch: 20 , batch: 433 , training loss: 4.186975
[INFO] Epoch: 20 , batch: 434 , training loss: 4.048890
[INFO] Epoch: 20 , batch: 435 , training loss: 4.410717
[INFO] Epoch: 20 , batch: 436 , training loss: 4.502751
[INFO] Epoch: 20 , batch: 437 , training loss: 4.249379
[INFO] Epoch: 20 , batch: 438 , training loss: 4.090946
[INFO] Epoch: 20 , batch: 439 , training loss: 4.308188
[INFO] Epoch: 20 , batch: 440 , training loss: 4.437618
[INFO] Epoch: 20 , batch: 441 , training loss: 4.542965
[INFO] Epoch: 20 , batch: 442 , training loss: 4.315999
[INFO] Epoch: 20 , batch: 443 , training loss: 4.502433
[INFO] Epoch: 20 , batch: 444 , training loss: 4.109730
[INFO] Epoch: 20 , batch: 445 , training loss: 4.015261
[INFO] Epoch: 20 , batch: 446 , training loss: 3.961688
[INFO] Epoch: 20 , batch: 447 , training loss: 4.140724
[INFO] Epoch: 20 , batch: 448 , training loss: 4.242228
[INFO] Epoch: 20 , batch: 449 , training loss: 4.659717
[INFO] Epoch: 20 , batch: 450 , training loss: 4.713553
[INFO] Epoch: 20 , batch: 451 , training loss: 4.615752
[INFO] Epoch: 20 , batch: 452 , training loss: 4.416679
[INFO] Epoch: 20 , batch: 453 , training loss: 4.197563
[INFO] Epoch: 20 , batch: 454 , training loss: 4.327021
[INFO] Epoch: 20 , batch: 455 , training loss: 4.366370
[INFO] Epoch: 20 , batch: 456 , training loss: 4.386615
[INFO] Epoch: 20 , batch: 457 , training loss: 4.479346
[INFO] Epoch: 20 , batch: 458 , training loss: 4.203720
[INFO] Epoch: 20 , batch: 459 , training loss: 4.175304
[INFO] Epoch: 20 , batch: 460 , training loss: 4.279654
[INFO] Epoch: 20 , batch: 461 , training loss: 4.271936
[INFO] Epoch: 20 , batch: 462 , training loss: 4.325175
[INFO] Epoch: 20 , batch: 463 , training loss: 4.228508
[INFO] Epoch: 20 , batch: 464 , training loss: 4.420695
[INFO] Epoch: 20 , batch: 465 , training loss: 4.343923
[INFO] Epoch: 20 , batch: 466 , training loss: 4.433906
[INFO] Epoch: 20 , batch: 467 , training loss: 4.419284
[INFO] Epoch: 20 , batch: 468 , training loss: 4.375399
[INFO] Epoch: 20 , batch: 469 , training loss: 4.397076
[INFO] Epoch: 20 , batch: 470 , training loss: 4.222060
[INFO] Epoch: 20 , batch: 471 , training loss: 4.293746
[INFO] Epoch: 20 , batch: 472 , training loss: 4.345798
[INFO] Epoch: 20 , batch: 473 , training loss: 4.274442
[INFO] Epoch: 20 , batch: 474 , training loss: 4.084296
[INFO] Epoch: 20 , batch: 475 , training loss: 3.953034
[INFO] Epoch: 20 , batch: 476 , training loss: 4.320462
[INFO] Epoch: 20 , batch: 477 , training loss: 4.457834
[INFO] Epoch: 20 , batch: 478 , training loss: 4.504800
[INFO] Epoch: 20 , batch: 479 , training loss: 4.462989
[INFO] Epoch: 20 , batch: 480 , training loss: 4.563664
[INFO] Epoch: 20 , batch: 481 , training loss: 4.420916
[INFO] Epoch: 20 , batch: 482 , training loss: 4.560895
[INFO] Epoch: 20 , batch: 483 , training loss: 4.394423
[INFO] Epoch: 20 , batch: 484 , training loss: 4.207052
[INFO] Epoch: 20 , batch: 485 , training loss: 4.294877
[INFO] Epoch: 20 , batch: 486 , training loss: 4.185335
[INFO] Epoch: 20 , batch: 487 , training loss: 4.167830
[INFO] Epoch: 20 , batch: 488 , training loss: 4.378984
[INFO] Epoch: 20 , batch: 489 , training loss: 4.253038
[INFO] Epoch: 20 , batch: 490 , training loss: 4.298863
[INFO] Epoch: 20 , batch: 491 , training loss: 4.270405
[INFO] Epoch: 20 , batch: 492 , training loss: 4.202918
[INFO] Epoch: 20 , batch: 493 , training loss: 4.408662
[INFO] Epoch: 20 , batch: 494 , training loss: 4.320694
[INFO] Epoch: 20 , batch: 495 , training loss: 4.430307
[INFO] Epoch: 20 , batch: 496 , training loss: 4.315059
[INFO] Epoch: 20 , batch: 497 , training loss: 4.343732
[INFO] Epoch: 20 , batch: 498 , training loss: 4.363980
[INFO] Epoch: 20 , batch: 499 , training loss: 4.429913
[INFO] Epoch: 20 , batch: 500 , training loss: 4.607521
[INFO] Epoch: 20 , batch: 501 , training loss: 4.891886
[INFO] Epoch: 20 , batch: 502 , training loss: 4.971492
[INFO] Epoch: 20 , batch: 503 , training loss: 4.705643
[INFO] Epoch: 20 , batch: 504 , training loss: 4.812702
[INFO] Epoch: 20 , batch: 505 , training loss: 4.774783
[INFO] Epoch: 20 , batch: 506 , training loss: 4.740107
[INFO] Epoch: 20 , batch: 507 , training loss: 4.798734
[INFO] Epoch: 20 , batch: 508 , training loss: 4.741277
[INFO] Epoch: 20 , batch: 509 , training loss: 4.509036
[INFO] Epoch: 20 , batch: 510 , training loss: 4.572875
[INFO] Epoch: 20 , batch: 511 , training loss: 4.487876
[INFO] Epoch: 20 , batch: 512 , training loss: 4.581926
[INFO] Epoch: 20 , batch: 513 , training loss: 4.821178
[INFO] Epoch: 20 , batch: 514 , training loss: 4.485283
[INFO] Epoch: 20 , batch: 515 , training loss: 4.697431
[INFO] Epoch: 20 , batch: 516 , training loss: 4.512238
[INFO] Epoch: 20 , batch: 517 , training loss: 4.470210
[INFO] Epoch: 20 , batch: 518 , training loss: 4.450256
[INFO] Epoch: 20 , batch: 519 , training loss: 4.302235
[INFO] Epoch: 20 , batch: 520 , training loss: 4.540638
[INFO] Epoch: 20 , batch: 521 , training loss: 4.523232
[INFO] Epoch: 20 , batch: 522 , training loss: 4.562273
[INFO] Epoch: 20 , batch: 523 , training loss: 4.497241
[INFO] Epoch: 20 , batch: 524 , training loss: 4.764080
[INFO] Epoch: 20 , batch: 525 , training loss: 4.683504
[INFO] Epoch: 20 , batch: 526 , training loss: 4.463341
[INFO] Epoch: 20 , batch: 527 , training loss: 4.482647
[INFO] Epoch: 20 , batch: 528 , training loss: 4.528258
[INFO] Epoch: 20 , batch: 529 , training loss: 4.494938
[INFO] Epoch: 20 , batch: 530 , training loss: 4.339988
[INFO] Epoch: 20 , batch: 531 , training loss: 4.476837
[INFO] Epoch: 20 , batch: 532 , training loss: 4.372380
[INFO] Epoch: 20 , batch: 533 , training loss: 4.541996
[INFO] Epoch: 20 , batch: 534 , training loss: 4.508711
[INFO] Epoch: 20 , batch: 535 , training loss: 4.513334
[INFO] Epoch: 20 , batch: 536 , training loss: 4.350873
[INFO] Epoch: 20 , batch: 537 , training loss: 4.378739
[INFO] Epoch: 20 , batch: 538 , training loss: 4.425854
[INFO] Epoch: 20 , batch: 539 , training loss: 4.546717
[INFO] Epoch: 20 , batch: 540 , training loss: 5.079625
[INFO] Epoch: 20 , batch: 541 , training loss: 4.968657
[INFO] Epoch: 20 , batch: 542 , training loss: 4.856705
[INFO] Epoch: 21 , batch: 0 , training loss: 3.564961
[INFO] Epoch: 21 , batch: 1 , training loss: 3.664587
[INFO] Epoch: 21 , batch: 2 , training loss: 3.733908
[INFO] Epoch: 21 , batch: 3 , training loss: 3.526154
[INFO] Epoch: 21 , batch: 4 , training loss: 3.926744
[INFO] Epoch: 21 , batch: 5 , training loss: 3.543218
[INFO] Epoch: 21 , batch: 6 , training loss: 3.996014
[INFO] Epoch: 21 , batch: 7 , training loss: 3.852143
[INFO] Epoch: 21 , batch: 8 , training loss: 3.584667
[INFO] Epoch: 21 , batch: 9 , training loss: 3.792938
[INFO] Epoch: 21 , batch: 10 , training loss: 3.778087
[INFO] Epoch: 21 , batch: 11 , training loss: 3.734620
[INFO] Epoch: 21 , batch: 12 , training loss: 3.636843
[INFO] Epoch: 21 , batch: 13 , training loss: 3.689405
[INFO] Epoch: 21 , batch: 14 , training loss: 3.513127
[INFO] Epoch: 21 , batch: 15 , training loss: 3.707875
[INFO] Epoch: 21 , batch: 16 , training loss: 3.553753
[INFO] Epoch: 21 , batch: 17 , training loss: 3.734432
[INFO] Epoch: 21 , batch: 18 , training loss: 3.713092
[INFO] Epoch: 21 , batch: 19 , training loss: 3.427623
[INFO] Epoch: 21 , batch: 20 , training loss: 3.387607
[INFO] Epoch: 21 , batch: 21 , training loss: 3.549456
[INFO] Epoch: 21 , batch: 22 , training loss: 3.453245
[INFO] Epoch: 21 , batch: 23 , training loss: 3.643417
[INFO] Epoch: 21 , batch: 24 , training loss: 3.453600
[INFO] Epoch: 21 , batch: 25 , training loss: 3.665618
[INFO] Epoch: 21 , batch: 26 , training loss: 3.493337
[INFO] Epoch: 21 , batch: 27 , training loss: 3.463223
[INFO] Epoch: 21 , batch: 28 , training loss: 3.614401
[INFO] Epoch: 21 , batch: 29 , training loss: 3.458575
[INFO] Epoch: 21 , batch: 30 , training loss: 3.536587
[INFO] Epoch: 21 , batch: 31 , training loss: 3.572558
[INFO] Epoch: 21 , batch: 32 , training loss: 3.570057
[INFO] Epoch: 21 , batch: 33 , training loss: 3.596486
[INFO] Epoch: 21 , batch: 34 , training loss: 3.560858
[INFO] Epoch: 21 , batch: 35 , training loss: 3.554630
[INFO] Epoch: 21 , batch: 36 , training loss: 3.596000
[INFO] Epoch: 21 , batch: 37 , training loss: 3.507117
[INFO] Epoch: 21 , batch: 38 , training loss: 3.524596
[INFO] Epoch: 21 , batch: 39 , training loss: 3.401200
[INFO] Epoch: 21 , batch: 40 , training loss: 3.630344
[INFO] Epoch: 21 , batch: 41 , training loss: 3.546570
[INFO] Epoch: 21 , batch: 42 , training loss: 3.954116
[INFO] Epoch: 21 , batch: 43 , training loss: 3.698833
[INFO] Epoch: 21 , batch: 44 , training loss: 3.981123
[INFO] Epoch: 21 , batch: 45 , training loss: 4.005112
[INFO] Epoch: 21 , batch: 46 , training loss: 3.934662
[INFO] Epoch: 21 , batch: 47 , training loss: 3.669458
[INFO] Epoch: 21 , batch: 48 , training loss: 3.670160
[INFO] Epoch: 21 , batch: 49 , training loss: 3.841365
[INFO] Epoch: 21 , batch: 50 , training loss: 3.627907
[INFO] Epoch: 21 , batch: 51 , training loss: 3.812565
[INFO] Epoch: 21 , batch: 52 , training loss: 3.657916
[INFO] Epoch: 21 , batch: 53 , training loss: 3.844366
[INFO] Epoch: 21 , batch: 54 , training loss: 3.837164
[INFO] Epoch: 21 , batch: 55 , training loss: 3.885810
[INFO] Epoch: 21 , batch: 56 , training loss: 3.787732
[INFO] Epoch: 21 , batch: 57 , training loss: 3.643525
[INFO] Epoch: 21 , batch: 58 , training loss: 3.738396
[INFO] Epoch: 21 , batch: 59 , training loss: 3.793058
[INFO] Epoch: 21 , batch: 60 , training loss: 3.740602
[INFO] Epoch: 21 , batch: 61 , training loss: 3.829500
[INFO] Epoch: 21 , batch: 62 , training loss: 3.745159
[INFO] Epoch: 21 , batch: 63 , training loss: 3.941026
[INFO] Epoch: 21 , batch: 64 , training loss: 4.070721
[INFO] Epoch: 21 , batch: 65 , training loss: 3.793361
[INFO] Epoch: 21 , batch: 66 , training loss: 3.645714
[INFO] Epoch: 21 , batch: 67 , training loss: 3.723194
[INFO] Epoch: 21 , batch: 68 , training loss: 3.858021
[INFO] Epoch: 21 , batch: 69 , training loss: 3.745924
[INFO] Epoch: 21 , batch: 70 , training loss: 3.988351
[INFO] Epoch: 21 , batch: 71 , training loss: 3.868059
[INFO] Epoch: 21 , batch: 72 , training loss: 3.901593
[INFO] Epoch: 21 , batch: 73 , training loss: 3.848583
[INFO] Epoch: 21 , batch: 74 , training loss: 3.962099
[INFO] Epoch: 21 , batch: 75 , training loss: 3.807935
[INFO] Epoch: 21 , batch: 76 , training loss: 3.910749
[INFO] Epoch: 21 , batch: 77 , training loss: 3.871746
[INFO] Epoch: 21 , batch: 78 , training loss: 3.988303
[INFO] Epoch: 21 , batch: 79 , training loss: 3.820425
[INFO] Epoch: 21 , batch: 80 , training loss: 3.993664
[INFO] Epoch: 21 , batch: 81 , training loss: 3.937822
[INFO] Epoch: 21 , batch: 82 , training loss: 3.887374
[INFO] Epoch: 21 , batch: 83 , training loss: 4.040733
[INFO] Epoch: 21 , batch: 84 , training loss: 3.946990
[INFO] Epoch: 21 , batch: 85 , training loss: 4.024113
[INFO] Epoch: 21 , batch: 86 , training loss: 3.990026
[INFO] Epoch: 21 , batch: 87 , training loss: 3.956179
[INFO] Epoch: 21 , batch: 88 , training loss: 4.099289
[INFO] Epoch: 21 , batch: 89 , training loss: 3.883457
[INFO] Epoch: 21 , batch: 90 , training loss: 3.954100
[INFO] Epoch: 21 , batch: 91 , training loss: 3.892747
[INFO] Epoch: 21 , batch: 92 , training loss: 3.892740
[INFO] Epoch: 21 , batch: 93 , training loss: 3.994540
[INFO] Epoch: 21 , batch: 94 , training loss: 4.158706
[INFO] Epoch: 21 , batch: 95 , training loss: 3.896897
[INFO] Epoch: 21 , batch: 96 , training loss: 3.932142
[INFO] Epoch: 21 , batch: 97 , training loss: 3.847185
[INFO] Epoch: 21 , batch: 98 , training loss: 3.795200
[INFO] Epoch: 21 , batch: 99 , training loss: 3.881380
[INFO] Epoch: 21 , batch: 100 , training loss: 3.812487
[INFO] Epoch: 21 , batch: 101 , training loss: 3.862801
[INFO] Epoch: 21 , batch: 102 , training loss: 3.990096
[INFO] Epoch: 21 , batch: 103 , training loss: 3.754895
[INFO] Epoch: 21 , batch: 104 , training loss: 3.746561
[INFO] Epoch: 21 , batch: 105 , training loss: 4.019691
[INFO] Epoch: 21 , batch: 106 , training loss: 4.001374
[INFO] Epoch: 21 , batch: 107 , training loss: 3.864413
[INFO] Epoch: 21 , batch: 108 , training loss: 3.773861
[INFO] Epoch: 21 , batch: 109 , training loss: 3.691694
[INFO] Epoch: 21 , batch: 110 , training loss: 3.898644
[INFO] Epoch: 21 , batch: 111 , training loss: 3.962526
[INFO] Epoch: 21 , batch: 112 , training loss: 3.898669
[INFO] Epoch: 21 , batch: 113 , training loss: 3.884784
[INFO] Epoch: 21 , batch: 114 , training loss: 3.873809
[INFO] Epoch: 21 , batch: 115 , training loss: 3.868160
[INFO] Epoch: 21 , batch: 116 , training loss: 3.788604
[INFO] Epoch: 21 , batch: 117 , training loss: 4.051687
[INFO] Epoch: 21 , batch: 118 , training loss: 3.995043
[INFO] Epoch: 21 , batch: 119 , training loss: 4.123081
[INFO] Epoch: 21 , batch: 120 , training loss: 4.099805
[INFO] Epoch: 21 , batch: 121 , training loss: 3.949181
[INFO] Epoch: 21 , batch: 122 , training loss: 3.845552
[INFO] Epoch: 21 , batch: 123 , training loss: 3.904060
[INFO] Epoch: 21 , batch: 124 , training loss: 4.005383
[INFO] Epoch: 21 , batch: 125 , training loss: 3.818244
[INFO] Epoch: 21 , batch: 126 , training loss: 3.795287
[INFO] Epoch: 21 , batch: 127 , training loss: 3.808784
[INFO] Epoch: 21 , batch: 128 , training loss: 3.959108
[INFO] Epoch: 21 , batch: 129 , training loss: 3.936936
[INFO] Epoch: 21 , batch: 130 , training loss: 3.924562
[INFO] Epoch: 21 , batch: 131 , training loss: 3.896570
[INFO] Epoch: 21 , batch: 132 , training loss: 3.899996
[INFO] Epoch: 21 , batch: 133 , training loss: 3.884113
[INFO] Epoch: 21 , batch: 134 , training loss: 3.682306
[INFO] Epoch: 21 , batch: 135 , training loss: 3.736537
[INFO] Epoch: 21 , batch: 136 , training loss: 4.023045
[INFO] Epoch: 21 , batch: 137 , training loss: 3.932842
[INFO] Epoch: 21 , batch: 138 , training loss: 3.989973
[INFO] Epoch: 21 , batch: 139 , training loss: 4.594226
[INFO] Epoch: 21 , batch: 140 , training loss: 4.338052
[INFO] Epoch: 21 , batch: 141 , training loss: 4.087090
[INFO] Epoch: 21 , batch: 142 , training loss: 3.846022
[INFO] Epoch: 21 , batch: 143 , training loss: 3.972780
[INFO] Epoch: 21 , batch: 144 , training loss: 3.776323
[INFO] Epoch: 21 , batch: 145 , training loss: 3.886343
[INFO] Epoch: 21 , batch: 146 , training loss: 4.054256
[INFO] Epoch: 21 , batch: 147 , training loss: 3.737870
[INFO] Epoch: 21 , batch: 148 , training loss: 3.709464
[INFO] Epoch: 21 , batch: 149 , training loss: 3.826622
[INFO] Epoch: 21 , batch: 150 , training loss: 4.025379
[INFO] Epoch: 21 , batch: 151 , training loss: 3.899092
[INFO] Epoch: 21 , batch: 152 , training loss: 3.977673
[INFO] Epoch: 21 , batch: 153 , training loss: 3.945393
[INFO] Epoch: 21 , batch: 154 , training loss: 4.036205
[INFO] Epoch: 21 , batch: 155 , training loss: 4.251968
[INFO] Epoch: 21 , batch: 156 , training loss: 3.968710
[INFO] Epoch: 21 , batch: 157 , training loss: 3.922002
[INFO] Epoch: 21 , batch: 158 , training loss: 4.141693
[INFO] Epoch: 21 , batch: 159 , training loss: 3.996234
[INFO] Epoch: 21 , batch: 160 , training loss: 4.307238
[INFO] Epoch: 21 , batch: 161 , training loss: 4.413614
[INFO] Epoch: 21 , batch: 162 , training loss: 4.358893
[INFO] Epoch: 21 , batch: 163 , training loss: 4.464657
[INFO] Epoch: 21 , batch: 164 , training loss: 4.472752
[INFO] Epoch: 21 , batch: 165 , training loss: 4.354359
[INFO] Epoch: 21 , batch: 166 , training loss: 4.296101
[INFO] Epoch: 21 , batch: 167 , training loss: 4.389735
[INFO] Epoch: 21 , batch: 168 , training loss: 4.107100
[INFO] Epoch: 21 , batch: 169 , training loss: 4.078295
[INFO] Epoch: 21 , batch: 170 , training loss: 4.247964
[INFO] Epoch: 21 , batch: 171 , training loss: 3.631328
[INFO] Epoch: 21 , batch: 172 , training loss: 3.869851
[INFO] Epoch: 21 , batch: 173 , training loss: 4.167280
[INFO] Epoch: 21 , batch: 174 , training loss: 4.542192
[INFO] Epoch: 21 , batch: 175 , training loss: 4.913853
[INFO] Epoch: 21 , batch: 176 , training loss: 4.543413
[INFO] Epoch: 21 , batch: 177 , training loss: 4.207215
[INFO] Epoch: 21 , batch: 178 , training loss: 4.174797
[INFO] Epoch: 21 , batch: 179 , training loss: 4.238713
[INFO] Epoch: 21 , batch: 180 , training loss: 4.176736
[INFO] Epoch: 21 , batch: 181 , training loss: 4.475331
[INFO] Epoch: 21 , batch: 182 , training loss: 4.392928
[INFO] Epoch: 21 , batch: 183 , training loss: 4.382982
[INFO] Epoch: 21 , batch: 184 , training loss: 4.260057
[INFO] Epoch: 21 , batch: 185 , training loss: 4.208189
[INFO] Epoch: 21 , batch: 186 , training loss: 4.379794
[INFO] Epoch: 21 , batch: 187 , training loss: 4.456019
[INFO] Epoch: 21 , batch: 188 , training loss: 4.451551
[INFO] Epoch: 21 , batch: 189 , training loss: 4.341049
[INFO] Epoch: 21 , batch: 190 , training loss: 4.377523
[INFO] Epoch: 21 , batch: 191 , training loss: 4.472334
[INFO] Epoch: 21 , batch: 192 , training loss: 4.293992
[INFO] Epoch: 21 , batch: 193 , training loss: 4.409966
[INFO] Epoch: 21 , batch: 194 , training loss: 4.340722
[INFO] Epoch: 21 , batch: 195 , training loss: 4.250365
[INFO] Epoch: 21 , batch: 196 , training loss: 4.128350
[INFO] Epoch: 21 , batch: 197 , training loss: 4.230627
[INFO] Epoch: 21 , batch: 198 , training loss: 4.145422
[INFO] Epoch: 21 , batch: 199 , training loss: 4.288103
[INFO] Epoch: 21 , batch: 200 , training loss: 4.175076
[INFO] Epoch: 21 , batch: 201 , training loss: 4.071051
[INFO] Epoch: 21 , batch: 202 , training loss: 4.071849
[INFO] Epoch: 21 , batch: 203 , training loss: 4.165902
[INFO] Epoch: 21 , batch: 204 , training loss: 4.308331
[INFO] Epoch: 21 , batch: 205 , training loss: 3.882291
[INFO] Epoch: 21 , batch: 206 , training loss: 3.827239
[INFO] Epoch: 21 , batch: 207 , training loss: 3.820589
[INFO] Epoch: 21 , batch: 208 , training loss: 4.133940
[INFO] Epoch: 21 , batch: 209 , training loss: 4.103433
[INFO] Epoch: 21 , batch: 210 , training loss: 4.101708
[INFO] Epoch: 21 , batch: 211 , training loss: 4.088074
[INFO] Epoch: 21 , batch: 212 , training loss: 4.208927
[INFO] Epoch: 21 , batch: 213 , training loss: 4.155230
[INFO] Epoch: 21 , batch: 214 , training loss: 4.243280
[INFO] Epoch: 21 , batch: 215 , training loss: 4.467635
[INFO] Epoch: 21 , batch: 216 , training loss: 4.172619
[INFO] Epoch: 21 , batch: 217 , training loss: 4.116918
[INFO] Epoch: 21 , batch: 218 , training loss: 4.103601
[INFO] Epoch: 21 , batch: 219 , training loss: 4.213307
[INFO] Epoch: 21 , batch: 220 , training loss: 4.000461
[INFO] Epoch: 21 , batch: 221 , training loss: 4.037372
[INFO] Epoch: 21 , batch: 222 , training loss: 4.175150
[INFO] Epoch: 21 , batch: 223 , training loss: 4.247929
[INFO] Epoch: 21 , batch: 224 , training loss: 4.315097
[INFO] Epoch: 21 , batch: 225 , training loss: 4.196043
[INFO] Epoch: 21 , batch: 226 , training loss: 4.336553
[INFO] Epoch: 21 , batch: 227 , training loss: 4.298072
[INFO] Epoch: 21 , batch: 228 , training loss: 4.338099
[INFO] Epoch: 21 , batch: 229 , training loss: 4.205880
[INFO] Epoch: 21 , batch: 230 , training loss: 4.068762
[INFO] Epoch: 21 , batch: 231 , training loss: 3.918404
[INFO] Epoch: 21 , batch: 232 , training loss: 4.054327
[INFO] Epoch: 21 , batch: 233 , training loss: 4.073554
[INFO] Epoch: 21 , batch: 234 , training loss: 3.780326
[INFO] Epoch: 21 , batch: 235 , training loss: 3.879450
[INFO] Epoch: 21 , batch: 236 , training loss: 3.990032
[INFO] Epoch: 21 , batch: 237 , training loss: 4.201535
[INFO] Epoch: 21 , batch: 238 , training loss: 3.961581
[INFO] Epoch: 21 , batch: 239 , training loss: 4.020206
[INFO] Epoch: 21 , batch: 240 , training loss: 4.074600
[INFO] Epoch: 21 , batch: 241 , training loss: 3.868703
[INFO] Epoch: 21 , batch: 242 , training loss: 3.898322
[INFO] Epoch: 21 , batch: 243 , training loss: 4.188526
[INFO] Epoch: 21 , batch: 244 , training loss: 4.117350
[INFO] Epoch: 21 , batch: 245 , training loss: 4.100668
[INFO] Epoch: 21 , batch: 246 , training loss: 3.791064
[INFO] Epoch: 21 , batch: 247 , training loss: 3.987953
[INFO] Epoch: 21 , batch: 248 , training loss: 4.062496
[INFO] Epoch: 21 , batch: 249 , training loss: 4.052557
[INFO] Epoch: 21 , batch: 250 , training loss: 3.809941
[INFO] Epoch: 21 , batch: 251 , training loss: 4.267052
[INFO] Epoch: 21 , batch: 252 , training loss: 3.971146
[INFO] Epoch: 21 , batch: 253 , training loss: 3.909092
[INFO] Epoch: 21 , batch: 254 , training loss: 4.172650
[INFO] Epoch: 21 , batch: 255 , training loss: 4.145935
[INFO] Epoch: 21 , batch: 256 , training loss: 4.157458
[INFO] Epoch: 21 , batch: 257 , training loss: 4.307929
[INFO] Epoch: 21 , batch: 258 , training loss: 4.365846
[INFO] Epoch: 21 , batch: 259 , training loss: 4.396381
[INFO] Epoch: 21 , batch: 260 , training loss: 4.104067
[INFO] Epoch: 21 , batch: 261 , training loss: 4.266519
[INFO] Epoch: 21 , batch: 262 , training loss: 4.432322
[INFO] Epoch: 21 , batch: 263 , training loss: 4.622446
[INFO] Epoch: 21 , batch: 264 , training loss: 3.931688
[INFO] Epoch: 21 , batch: 265 , training loss: 4.084745
[INFO] Epoch: 21 , batch: 266 , training loss: 4.487614
[INFO] Epoch: 21 , batch: 267 , training loss: 4.233056
[INFO] Epoch: 21 , batch: 268 , training loss: 4.166169
[INFO] Epoch: 21 , batch: 269 , training loss: 4.135843
[INFO] Epoch: 21 , batch: 270 , training loss: 4.181027
[INFO] Epoch: 21 , batch: 271 , training loss: 4.202644
[INFO] Epoch: 21 , batch: 272 , training loss: 4.187727
[INFO] Epoch: 21 , batch: 273 , training loss: 4.195662
[INFO] Epoch: 21 , batch: 274 , training loss: 4.277968
[INFO] Epoch: 21 , batch: 275 , training loss: 4.179708
[INFO] Epoch: 21 , batch: 276 , training loss: 4.209689
[INFO] Epoch: 21 , batch: 277 , training loss: 4.374633
[INFO] Epoch: 21 , batch: 278 , training loss: 4.031440
[INFO] Epoch: 21 , batch: 279 , training loss: 4.043320
[INFO] Epoch: 21 , batch: 280 , training loss: 4.008859
[INFO] Epoch: 21 , batch: 281 , training loss: 4.142775
[INFO] Epoch: 21 , batch: 282 , training loss: 4.048149
[INFO] Epoch: 21 , batch: 283 , training loss: 4.078117
[INFO] Epoch: 21 , batch: 284 , training loss: 4.094670
[INFO] Epoch: 21 , batch: 285 , training loss: 4.041924
[INFO] Epoch: 21 , batch: 286 , training loss: 4.026448
[INFO] Epoch: 21 , batch: 287 , training loss: 3.980623
[INFO] Epoch: 21 , batch: 288 , training loss: 3.987492
[INFO] Epoch: 21 , batch: 289 , training loss: 4.030599
[INFO] Epoch: 21 , batch: 290 , training loss: 3.815425
[INFO] Epoch: 21 , batch: 291 , training loss: 3.795527
[INFO] Epoch: 21 , batch: 292 , training loss: 3.910318
[INFO] Epoch: 21 , batch: 293 , training loss: 3.823318
[INFO] Epoch: 21 , batch: 294 , training loss: 4.507959
[INFO] Epoch: 21 , batch: 295 , training loss: 4.269245
[INFO] Epoch: 21 , batch: 296 , training loss: 4.194418
[INFO] Epoch: 21 , batch: 297 , training loss: 4.143965
[INFO] Epoch: 21 , batch: 298 , training loss: 4.000189
[INFO] Epoch: 21 , batch: 299 , training loss: 4.007908
[INFO] Epoch: 21 , batch: 300 , training loss: 3.975826
[INFO] Epoch: 21 , batch: 301 , training loss: 3.922456
[INFO] Epoch: 21 , batch: 302 , training loss: 4.077406
[INFO] Epoch: 21 , batch: 303 , training loss: 4.112299
[INFO] Epoch: 21 , batch: 304 , training loss: 4.284167
[INFO] Epoch: 21 , batch: 305 , training loss: 4.055647
[INFO] Epoch: 21 , batch: 306 , training loss: 4.193421
[INFO] Epoch: 21 , batch: 307 , training loss: 4.197435
[INFO] Epoch: 21 , batch: 308 , training loss: 4.013061
[INFO] Epoch: 21 , batch: 309 , training loss: 4.052105
[INFO] Epoch: 21 , batch: 310 , training loss: 3.916426
[INFO] Epoch: 21 , batch: 311 , training loss: 3.942796
[INFO] Epoch: 21 , batch: 312 , training loss: 3.861668
[INFO] Epoch: 21 , batch: 313 , training loss: 3.974500
[INFO] Epoch: 21 , batch: 314 , training loss: 4.025865
[INFO] Epoch: 21 , batch: 315 , training loss: 4.083867
[INFO] Epoch: 21 , batch: 316 , training loss: 4.321593
[INFO] Epoch: 21 , batch: 317 , training loss: 4.779130
[INFO] Epoch: 21 , batch: 318 , training loss: 4.925847
[INFO] Epoch: 21 , batch: 319 , training loss: 4.503321
[INFO] Epoch: 21 , batch: 320 , training loss: 4.066036
[INFO] Epoch: 21 , batch: 321 , training loss: 3.879374
[INFO] Epoch: 21 , batch: 322 , training loss: 4.008042
[INFO] Epoch: 21 , batch: 323 , training loss: 4.011306
[INFO] Epoch: 21 , batch: 324 , training loss: 3.982394
[INFO] Epoch: 21 , batch: 325 , training loss: 4.142330
[INFO] Epoch: 21 , batch: 326 , training loss: 4.193482
[INFO] Epoch: 21 , batch: 327 , training loss: 4.097634
[INFO] Epoch: 21 , batch: 328 , training loss: 4.085210
[INFO] Epoch: 21 , batch: 329 , training loss: 4.012002
[INFO] Epoch: 21 , batch: 330 , training loss: 4.013625
[INFO] Epoch: 21 , batch: 331 , training loss: 4.151917
[INFO] Epoch: 21 , batch: 332 , training loss: 3.967145
[INFO] Epoch: 21 , batch: 333 , training loss: 3.979507
[INFO] Epoch: 21 , batch: 334 , training loss: 3.984647
[INFO] Epoch: 21 , batch: 335 , training loss: 4.116448
[INFO] Epoch: 21 , batch: 336 , training loss: 4.130685
[INFO] Epoch: 21 , batch: 337 , training loss: 4.152088
[INFO] Epoch: 21 , batch: 338 , training loss: 4.362065
[INFO] Epoch: 21 , batch: 339 , training loss: 4.230636
[INFO] Epoch: 21 , batch: 340 , training loss: 4.375457
[INFO] Epoch: 21 , batch: 341 , training loss: 4.157756
[INFO] Epoch: 21 , batch: 342 , training loss: 3.916977
[INFO] Epoch: 21 , batch: 343 , training loss: 3.986642
[INFO] Epoch: 21 , batch: 344 , training loss: 3.846251
[INFO] Epoch: 21 , batch: 345 , training loss: 3.988712
[INFO] Epoch: 21 , batch: 346 , training loss: 4.046158
[INFO] Epoch: 21 , batch: 347 , training loss: 3.946366
[INFO] Epoch: 21 , batch: 348 , training loss: 4.055413
[INFO] Epoch: 21 , batch: 349 , training loss: 4.195768
[INFO] Epoch: 21 , batch: 350 , training loss: 3.984582
[INFO] Epoch: 21 , batch: 351 , training loss: 4.059765
[INFO] Epoch: 21 , batch: 352 , training loss: 4.063951
[INFO] Epoch: 21 , batch: 353 , training loss: 4.041593
[INFO] Epoch: 21 , batch: 354 , training loss: 4.149955
[INFO] Epoch: 21 , batch: 355 , training loss: 4.203589
[INFO] Epoch: 21 , batch: 356 , training loss: 4.017780
[INFO] Epoch: 21 , batch: 357 , training loss: 4.107430
[INFO] Epoch: 21 , batch: 358 , training loss: 4.007365
[INFO] Epoch: 21 , batch: 359 , training loss: 4.004094
[INFO] Epoch: 21 , batch: 360 , training loss: 4.088216
[INFO] Epoch: 21 , batch: 361 , training loss: 4.044808
[INFO] Epoch: 21 , batch: 362 , training loss: 4.165299
[INFO] Epoch: 21 , batch: 363 , training loss: 4.032277
[INFO] Epoch: 21 , batch: 364 , training loss: 4.083723
[INFO] Epoch: 21 , batch: 365 , training loss: 4.020995
[INFO] Epoch: 21 , batch: 366 , training loss: 4.155909
[INFO] Epoch: 21 , batch: 367 , training loss: 4.240087
[INFO] Epoch: 21 , batch: 368 , training loss: 4.637283
[INFO] Epoch: 21 , batch: 369 , training loss: 4.288937
[INFO] Epoch: 21 , batch: 370 , training loss: 4.071351
[INFO] Epoch: 21 , batch: 371 , training loss: 4.462461
[INFO] Epoch: 21 , batch: 372 , training loss: 4.754023
[INFO] Epoch: 21 , batch: 373 , training loss: 4.825744
[INFO] Epoch: 21 , batch: 374 , training loss: 4.905713
[INFO] Epoch: 21 , batch: 375 , training loss: 4.883753
[INFO] Epoch: 21 , batch: 376 , training loss: 4.825027
[INFO] Epoch: 21 , batch: 377 , training loss: 4.565665
[INFO] Epoch: 21 , batch: 378 , training loss: 4.661265
[INFO] Epoch: 21 , batch: 379 , training loss: 4.630850
[INFO] Epoch: 21 , batch: 380 , training loss: 4.773114
[INFO] Epoch: 21 , batch: 381 , training loss: 4.493300
[INFO] Epoch: 21 , batch: 382 , training loss: 4.717177
[INFO] Epoch: 21 , batch: 383 , training loss: 4.743215
[INFO] Epoch: 21 , batch: 384 , training loss: 4.754733
[INFO] Epoch: 21 , batch: 385 , training loss: 4.470826
[INFO] Epoch: 21 , batch: 386 , training loss: 4.724936
[INFO] Epoch: 21 , batch: 387 , training loss: 4.691611
[INFO] Epoch: 21 , batch: 388 , training loss: 4.490699
[INFO] Epoch: 21 , batch: 389 , training loss: 4.346677
[INFO] Epoch: 21 , batch: 390 , training loss: 4.311607
[INFO] Epoch: 21 , batch: 391 , training loss: 4.365412
[INFO] Epoch: 21 , batch: 392 , training loss: 4.728388
[INFO] Epoch: 21 , batch: 393 , training loss: 4.601591
[INFO] Epoch: 21 , batch: 394 , training loss: 4.638964
[INFO] Epoch: 21 , batch: 395 , training loss: 4.510967
[INFO] Epoch: 21 , batch: 396 , training loss: 4.272614
[INFO] Epoch: 21 , batch: 397 , training loss: 4.439544
[INFO] Epoch: 21 , batch: 398 , training loss: 4.299321
[INFO] Epoch: 21 , batch: 399 , training loss: 4.363719
[INFO] Epoch: 21 , batch: 400 , training loss: 4.333543
[INFO] Epoch: 21 , batch: 401 , training loss: 4.766376
[INFO] Epoch: 21 , batch: 402 , training loss: 4.487513
[INFO] Epoch: 21 , batch: 403 , training loss: 4.272196
[INFO] Epoch: 21 , batch: 404 , training loss: 4.484131
[INFO] Epoch: 21 , batch: 405 , training loss: 4.510603
[INFO] Epoch: 21 , batch: 406 , training loss: 4.427966
[INFO] Epoch: 21 , batch: 407 , training loss: 4.478936
[INFO] Epoch: 21 , batch: 408 , training loss: 4.435651
[INFO] Epoch: 21 , batch: 409 , training loss: 4.432421
[INFO] Epoch: 21 , batch: 410 , training loss: 4.474147
[INFO] Epoch: 21 , batch: 411 , training loss: 4.661832
[INFO] Epoch: 21 , batch: 412 , training loss: 4.510817
[INFO] Epoch: 21 , batch: 413 , training loss: 4.387386
[INFO] Epoch: 21 , batch: 414 , training loss: 4.410381
[INFO] Epoch: 21 , batch: 415 , training loss: 4.428499
[INFO] Epoch: 21 , batch: 416 , training loss: 4.498869
[INFO] Epoch: 21 , batch: 417 , training loss: 4.415676
[INFO] Epoch: 21 , batch: 418 , training loss: 4.453515
[INFO] Epoch: 21 , batch: 419 , training loss: 4.401412
[INFO] Epoch: 21 , batch: 420 , training loss: 4.388873
[INFO] Epoch: 21 , batch: 421 , training loss: 4.383702
[INFO] Epoch: 21 , batch: 422 , training loss: 4.264233
[INFO] Epoch: 21 , batch: 423 , training loss: 4.461586
[INFO] Epoch: 21 , batch: 424 , training loss: 4.635501
[INFO] Epoch: 21 , batch: 425 , training loss: 4.500637
[INFO] Epoch: 21 , batch: 426 , training loss: 4.237509
[INFO] Epoch: 21 , batch: 427 , training loss: 4.464531
[INFO] Epoch: 21 , batch: 428 , training loss: 4.336526
[INFO] Epoch: 21 , batch: 429 , training loss: 4.205107
[INFO] Epoch: 21 , batch: 430 , training loss: 4.472878
[INFO] Epoch: 21 , batch: 431 , training loss: 4.069824
[INFO] Epoch: 21 , batch: 432 , training loss: 4.134290
[INFO] Epoch: 21 , batch: 433 , training loss: 4.187522
[INFO] Epoch: 21 , batch: 434 , training loss: 4.032940
[INFO] Epoch: 21 , batch: 435 , training loss: 4.412217
[INFO] Epoch: 21 , batch: 436 , training loss: 4.496150
[INFO] Epoch: 21 , batch: 437 , training loss: 4.243423
[INFO] Epoch: 21 , batch: 438 , training loss: 4.087793
[INFO] Epoch: 21 , batch: 439 , training loss: 4.302248
[INFO] Epoch: 21 , batch: 440 , training loss: 4.418783
[INFO] Epoch: 21 , batch: 441 , training loss: 4.532932
[INFO] Epoch: 21 , batch: 442 , training loss: 4.309349
[INFO] Epoch: 21 , batch: 443 , training loss: 4.495269
[INFO] Epoch: 21 , batch: 444 , training loss: 4.111377
[INFO] Epoch: 21 , batch: 445 , training loss: 4.004957
[INFO] Epoch: 21 , batch: 446 , training loss: 3.960261
[INFO] Epoch: 21 , batch: 447 , training loss: 4.134013
[INFO] Epoch: 21 , batch: 448 , training loss: 4.233999
[INFO] Epoch: 21 , batch: 449 , training loss: 4.650484
[INFO] Epoch: 21 , batch: 450 , training loss: 4.697770
[INFO] Epoch: 21 , batch: 451 , training loss: 4.614723
[INFO] Epoch: 21 , batch: 452 , training loss: 4.417906
[INFO] Epoch: 21 , batch: 453 , training loss: 4.182513
[INFO] Epoch: 21 , batch: 454 , training loss: 4.319686
[INFO] Epoch: 21 , batch: 455 , training loss: 4.360762
[INFO] Epoch: 21 , batch: 456 , training loss: 4.379021
[INFO] Epoch: 21 , batch: 457 , training loss: 4.470202
[INFO] Epoch: 21 , batch: 458 , training loss: 4.191393
[INFO] Epoch: 21 , batch: 459 , training loss: 4.167739
[INFO] Epoch: 21 , batch: 460 , training loss: 4.271896
[INFO] Epoch: 21 , batch: 461 , training loss: 4.266945
[INFO] Epoch: 21 , batch: 462 , training loss: 4.320466
[INFO] Epoch: 21 , batch: 463 , training loss: 4.217659
[INFO] Epoch: 21 , batch: 464 , training loss: 4.417319
[INFO] Epoch: 21 , batch: 465 , training loss: 4.344467
[INFO] Epoch: 21 , batch: 466 , training loss: 4.427572
[INFO] Epoch: 21 , batch: 467 , training loss: 4.404681
[INFO] Epoch: 21 , batch: 468 , training loss: 4.374911
[INFO] Epoch: 21 , batch: 469 , training loss: 4.397364
[INFO] Epoch: 21 , batch: 470 , training loss: 4.215427
[INFO] Epoch: 21 , batch: 471 , training loss: 4.291199
[INFO] Epoch: 21 , batch: 472 , training loss: 4.341075
[INFO] Epoch: 21 , batch: 473 , training loss: 4.274458
[INFO] Epoch: 21 , batch: 474 , training loss: 4.071599
[INFO] Epoch: 21 , batch: 475 , training loss: 3.944508
[INFO] Epoch: 21 , batch: 476 , training loss: 4.323309
[INFO] Epoch: 21 , batch: 477 , training loss: 4.451018
[INFO] Epoch: 21 , batch: 478 , training loss: 4.487935
[INFO] Epoch: 21 , batch: 479 , training loss: 4.450729
[INFO] Epoch: 21 , batch: 480 , training loss: 4.566939
[INFO] Epoch: 21 , batch: 481 , training loss: 4.415556
[INFO] Epoch: 21 , batch: 482 , training loss: 4.553392
[INFO] Epoch: 21 , batch: 483 , training loss: 4.399238
[INFO] Epoch: 21 , batch: 484 , training loss: 4.216381
[INFO] Epoch: 21 , batch: 485 , training loss: 4.297303
[INFO] Epoch: 21 , batch: 486 , training loss: 4.175746
[INFO] Epoch: 21 , batch: 487 , training loss: 4.166611
[INFO] Epoch: 21 , batch: 488 , training loss: 4.368803
[INFO] Epoch: 21 , batch: 489 , training loss: 4.253690
[INFO] Epoch: 21 , batch: 490 , training loss: 4.305042
[INFO] Epoch: 21 , batch: 491 , training loss: 4.265046
[INFO] Epoch: 21 , batch: 492 , training loss: 4.195152
[INFO] Epoch: 21 , batch: 493 , training loss: 4.410670
[INFO] Epoch: 21 , batch: 494 , training loss: 4.315745
[INFO] Epoch: 21 , batch: 495 , training loss: 4.441544
[INFO] Epoch: 21 , batch: 496 , training loss: 4.305711
[INFO] Epoch: 21 , batch: 497 , training loss: 4.337690
[INFO] Epoch: 21 , batch: 498 , training loss: 4.359285
[INFO] Epoch: 21 , batch: 499 , training loss: 4.423803
[INFO] Epoch: 21 , batch: 500 , training loss: 4.606692
[INFO] Epoch: 21 , batch: 501 , training loss: 4.890115
[INFO] Epoch: 21 , batch: 502 , training loss: 4.973493
[INFO] Epoch: 21 , batch: 503 , training loss: 4.693977
[INFO] Epoch: 21 , batch: 504 , training loss: 4.794976
[INFO] Epoch: 21 , batch: 505 , training loss: 4.770178
[INFO] Epoch: 21 , batch: 506 , training loss: 4.728972
[INFO] Epoch: 21 , batch: 507 , training loss: 4.786963
[INFO] Epoch: 21 , batch: 508 , training loss: 4.744722
[INFO] Epoch: 21 , batch: 509 , training loss: 4.510142
[INFO] Epoch: 21 , batch: 510 , training loss: 4.579785
[INFO] Epoch: 21 , batch: 511 , training loss: 4.496682
[INFO] Epoch: 21 , batch: 512 , training loss: 4.584026
[INFO] Epoch: 21 , batch: 513 , training loss: 4.812793
[INFO] Epoch: 21 , batch: 514 , training loss: 4.479237
[INFO] Epoch: 21 , batch: 515 , training loss: 4.692903
[INFO] Epoch: 21 , batch: 516 , training loss: 4.511813
[INFO] Epoch: 21 , batch: 517 , training loss: 4.468633
[INFO] Epoch: 21 , batch: 518 , training loss: 4.441443
[INFO] Epoch: 21 , batch: 519 , training loss: 4.294902
[INFO] Epoch: 21 , batch: 520 , training loss: 4.525215
[INFO] Epoch: 21 , batch: 521 , training loss: 4.509552
[INFO] Epoch: 21 , batch: 522 , training loss: 4.551949
[INFO] Epoch: 21 , batch: 523 , training loss: 4.475194
[INFO] Epoch: 21 , batch: 524 , training loss: 4.762936
[INFO] Epoch: 21 , batch: 525 , training loss: 4.679901
[INFO] Epoch: 21 , batch: 526 , training loss: 4.453139
[INFO] Epoch: 21 , batch: 527 , training loss: 4.476323
[INFO] Epoch: 21 , batch: 528 , training loss: 4.517465
[INFO] Epoch: 21 , batch: 529 , training loss: 4.484768
[INFO] Epoch: 21 , batch: 530 , training loss: 4.324407
[INFO] Epoch: 21 , batch: 531 , training loss: 4.463418
[INFO] Epoch: 21 , batch: 532 , training loss: 4.357615
[INFO] Epoch: 21 , batch: 533 , training loss: 4.530663
[INFO] Epoch: 21 , batch: 534 , training loss: 4.507147
[INFO] Epoch: 21 , batch: 535 , training loss: 4.512737
[INFO] Epoch: 21 , batch: 536 , training loss: 4.344057
[INFO] Epoch: 21 , batch: 537 , training loss: 4.356392
[INFO] Epoch: 21 , batch: 538 , training loss: 4.427789
[INFO] Epoch: 21 , batch: 539 , training loss: 4.545143
[INFO] Epoch: 21 , batch: 540 , training loss: 5.051404
[INFO] Epoch: 21 , batch: 541 , training loss: 4.947457
[INFO] Epoch: 21 , batch: 542 , training loss: 4.840260
[INFO] Epoch: 22 , batch: 0 , training loss: 3.535732
[INFO] Epoch: 22 , batch: 1 , training loss: 3.600140
[INFO] Epoch: 22 , batch: 2 , training loss: 3.710334
[INFO] Epoch: 22 , batch: 3 , training loss: 3.531008
[INFO] Epoch: 22 , batch: 4 , training loss: 3.923650
[INFO] Epoch: 22 , batch: 5 , training loss: 3.532359
[INFO] Epoch: 22 , batch: 6 , training loss: 3.979849
[INFO] Epoch: 22 , batch: 7 , training loss: 3.834049
[INFO] Epoch: 22 , batch: 8 , training loss: 3.557210
[INFO] Epoch: 22 , batch: 9 , training loss: 3.786047
[INFO] Epoch: 22 , batch: 10 , training loss: 3.762199
[INFO] Epoch: 22 , batch: 11 , training loss: 3.708460
[INFO] Epoch: 22 , batch: 12 , training loss: 3.610972
[INFO] Epoch: 22 , batch: 13 , training loss: 3.678117
[INFO] Epoch: 22 , batch: 14 , training loss: 3.501034
[INFO] Epoch: 22 , batch: 15 , training loss: 3.689399
[INFO] Epoch: 22 , batch: 16 , training loss: 3.539367
[INFO] Epoch: 22 , batch: 17 , training loss: 3.728482
[INFO] Epoch: 22 , batch: 18 , training loss: 3.700193
[INFO] Epoch: 22 , batch: 19 , training loss: 3.401058
[INFO] Epoch: 22 , batch: 20 , training loss: 3.383205
[INFO] Epoch: 22 , batch: 21 , training loss: 3.529114
[INFO] Epoch: 22 , batch: 22 , training loss: 3.442648
[INFO] Epoch: 22 , batch: 23 , training loss: 3.629575
[INFO] Epoch: 22 , batch: 24 , training loss: 3.449159
[INFO] Epoch: 22 , batch: 25 , training loss: 3.624117
[INFO] Epoch: 22 , batch: 26 , training loss: 3.474989
[INFO] Epoch: 22 , batch: 27 , training loss: 3.442982
[INFO] Epoch: 22 , batch: 28 , training loss: 3.592732
[INFO] Epoch: 22 , batch: 29 , training loss: 3.448625
[INFO] Epoch: 22 , batch: 30 , training loss: 3.526144
[INFO] Epoch: 22 , batch: 31 , training loss: 3.553317
[INFO] Epoch: 22 , batch: 32 , training loss: 3.558340
[INFO] Epoch: 22 , batch: 33 , training loss: 3.591629
[INFO] Epoch: 22 , batch: 34 , training loss: 3.553222
[INFO] Epoch: 22 , batch: 35 , training loss: 3.574123
[INFO] Epoch: 22 , batch: 36 , training loss: 3.589702
[INFO] Epoch: 22 , batch: 37 , training loss: 3.482971
[INFO] Epoch: 22 , batch: 38 , training loss: 3.538994
[INFO] Epoch: 22 , batch: 39 , training loss: 3.385808
[INFO] Epoch: 22 , batch: 40 , training loss: 3.617836
[INFO] Epoch: 22 , batch: 41 , training loss: 3.534033
[INFO] Epoch: 22 , batch: 42 , training loss: 3.961567
[INFO] Epoch: 22 , batch: 43 , training loss: 3.678310
[INFO] Epoch: 22 , batch: 44 , training loss: 4.007816
[INFO] Epoch: 22 , batch: 45 , training loss: 3.994073
[INFO] Epoch: 22 , batch: 46 , training loss: 3.878446
[INFO] Epoch: 22 , batch: 47 , training loss: 3.673635
[INFO] Epoch: 22 , batch: 48 , training loss: 3.641337
[INFO] Epoch: 22 , batch: 49 , training loss: 3.815864
[INFO] Epoch: 22 , batch: 50 , training loss: 3.625504
[INFO] Epoch: 22 , batch: 51 , training loss: 3.824980
[INFO] Epoch: 22 , batch: 52 , training loss: 3.660762
[INFO] Epoch: 22 , batch: 53 , training loss: 3.827092
[INFO] Epoch: 22 , batch: 54 , training loss: 3.837483
[INFO] Epoch: 22 , batch: 55 , training loss: 3.889352
[INFO] Epoch: 22 , batch: 56 , training loss: 3.790634
[INFO] Epoch: 22 , batch: 57 , training loss: 3.675030
[INFO] Epoch: 22 , batch: 58 , training loss: 3.742390
[INFO] Epoch: 22 , batch: 59 , training loss: 3.806331
[INFO] Epoch: 22 , batch: 60 , training loss: 3.758780
[INFO] Epoch: 22 , batch: 61 , training loss: 3.823641
[INFO] Epoch: 22 , batch: 62 , training loss: 3.721184
[INFO] Epoch: 22 , batch: 63 , training loss: 3.930020
[INFO] Epoch: 22 , batch: 64 , training loss: 4.064172
[INFO] Epoch: 22 , batch: 65 , training loss: 3.783064
[INFO] Epoch: 22 , batch: 66 , training loss: 3.632028
[INFO] Epoch: 22 , batch: 67 , training loss: 3.743428
[INFO] Epoch: 22 , batch: 68 , training loss: 3.880526
[INFO] Epoch: 22 , batch: 69 , training loss: 3.735998
[INFO] Epoch: 22 , batch: 70 , training loss: 3.974790
[INFO] Epoch: 22 , batch: 71 , training loss: 3.857954
[INFO] Epoch: 22 , batch: 72 , training loss: 3.880054
[INFO] Epoch: 22 , batch: 73 , training loss: 3.840878
[INFO] Epoch: 22 , batch: 74 , training loss: 3.951266
[INFO] Epoch: 22 , batch: 75 , training loss: 3.842319
[INFO] Epoch: 22 , batch: 76 , training loss: 3.894178
[INFO] Epoch: 22 , batch: 77 , training loss: 3.862366
[INFO] Epoch: 22 , batch: 78 , training loss: 3.970678
[INFO] Epoch: 22 , batch: 79 , training loss: 3.797868
[INFO] Epoch: 22 , batch: 80 , training loss: 3.993914
[INFO] Epoch: 22 , batch: 81 , training loss: 3.939594
[INFO] Epoch: 22 , batch: 82 , training loss: 3.895262
[INFO] Epoch: 22 , batch: 83 , training loss: 4.045173
[INFO] Epoch: 22 , batch: 84 , training loss: 3.919914
[INFO] Epoch: 22 , batch: 85 , training loss: 4.009484
[INFO] Epoch: 22 , batch: 86 , training loss: 3.986834
[INFO] Epoch: 22 , batch: 87 , training loss: 3.956954
[INFO] Epoch: 22 , batch: 88 , training loss: 4.102227
[INFO] Epoch: 22 , batch: 89 , training loss: 3.876562
[INFO] Epoch: 22 , batch: 90 , training loss: 3.939782
[INFO] Epoch: 22 , batch: 91 , training loss: 3.889691
[INFO] Epoch: 22 , batch: 92 , training loss: 3.877462
[INFO] Epoch: 22 , batch: 93 , training loss: 4.021423
[INFO] Epoch: 22 , batch: 94 , training loss: 4.167764
[INFO] Epoch: 22 , batch: 95 , training loss: 3.916823
[INFO] Epoch: 22 , batch: 96 , training loss: 3.932213
[INFO] Epoch: 22 , batch: 97 , training loss: 3.837962
[INFO] Epoch: 22 , batch: 98 , training loss: 3.801367
[INFO] Epoch: 22 , batch: 99 , training loss: 3.849999
[INFO] Epoch: 22 , batch: 100 , training loss: 3.809560
[INFO] Epoch: 22 , batch: 101 , training loss: 3.848581
[INFO] Epoch: 22 , batch: 102 , training loss: 3.970758
[INFO] Epoch: 22 , batch: 103 , training loss: 3.767075
[INFO] Epoch: 22 , batch: 104 , training loss: 3.738731
[INFO] Epoch: 22 , batch: 105 , training loss: 4.017893
[INFO] Epoch: 22 , batch: 106 , training loss: 3.980742
[INFO] Epoch: 22 , batch: 107 , training loss: 3.860693
[INFO] Epoch: 22 , batch: 108 , training loss: 3.768820
[INFO] Epoch: 22 , batch: 109 , training loss: 3.675949
[INFO] Epoch: 22 , batch: 110 , training loss: 3.904365
[INFO] Epoch: 22 , batch: 111 , training loss: 3.959445
[INFO] Epoch: 22 , batch: 112 , training loss: 3.895341
[INFO] Epoch: 22 , batch: 113 , training loss: 3.865050
[INFO] Epoch: 22 , batch: 114 , training loss: 3.857009
[INFO] Epoch: 22 , batch: 115 , training loss: 3.866529
[INFO] Epoch: 22 , batch: 116 , training loss: 3.814882
[INFO] Epoch: 22 , batch: 117 , training loss: 4.031721
[INFO] Epoch: 22 , batch: 118 , training loss: 3.969019
[INFO] Epoch: 22 , batch: 119 , training loss: 4.104644
[INFO] Epoch: 22 , batch: 120 , training loss: 4.100604
[INFO] Epoch: 22 , batch: 121 , training loss: 3.929687
[INFO] Epoch: 22 , batch: 122 , training loss: 3.861722
[INFO] Epoch: 22 , batch: 123 , training loss: 3.887707
[INFO] Epoch: 22 , batch: 124 , training loss: 4.004436
[INFO] Epoch: 22 , batch: 125 , training loss: 3.788244
[INFO] Epoch: 22 , batch: 126 , training loss: 3.794972
[INFO] Epoch: 22 , batch: 127 , training loss: 3.800318
[INFO] Epoch: 22 , batch: 128 , training loss: 3.963127
[INFO] Epoch: 22 , batch: 129 , training loss: 3.936345
[INFO] Epoch: 22 , batch: 130 , training loss: 3.902735
[INFO] Epoch: 22 , batch: 131 , training loss: 3.916580
[INFO] Epoch: 22 , batch: 132 , training loss: 3.911674
[INFO] Epoch: 22 , batch: 133 , training loss: 3.881589
[INFO] Epoch: 22 , batch: 134 , training loss: 3.708079
[INFO] Epoch: 22 , batch: 135 , training loss: 3.731560
[INFO] Epoch: 22 , batch: 136 , training loss: 4.032222
[INFO] Epoch: 22 , batch: 137 , training loss: 3.929067
[INFO] Epoch: 22 , batch: 138 , training loss: 3.978292
[INFO] Epoch: 22 , batch: 139 , training loss: 4.585361
[INFO] Epoch: 22 , batch: 140 , training loss: 4.323711
[INFO] Epoch: 22 , batch: 141 , training loss: 4.088791
[INFO] Epoch: 22 , batch: 142 , training loss: 3.847781
[INFO] Epoch: 22 , batch: 143 , training loss: 3.986684
[INFO] Epoch: 22 , batch: 144 , training loss: 3.788888
[INFO] Epoch: 22 , batch: 145 , training loss: 3.872591
[INFO] Epoch: 22 , batch: 146 , training loss: 4.070282
[INFO] Epoch: 22 , batch: 147 , training loss: 3.754039
[INFO] Epoch: 22 , batch: 148 , training loss: 3.710750
[INFO] Epoch: 22 , batch: 149 , training loss: 3.842407
[INFO] Epoch: 22 , batch: 150 , training loss: 4.010415
[INFO] Epoch: 22 , batch: 151 , training loss: 3.902885
[INFO] Epoch: 22 , batch: 152 , training loss: 3.965855
[INFO] Epoch: 22 , batch: 153 , training loss: 3.945650
[INFO] Epoch: 22 , batch: 154 , training loss: 4.039816
[INFO] Epoch: 22 , batch: 155 , training loss: 4.272637
[INFO] Epoch: 22 , batch: 156 , training loss: 3.948688
[INFO] Epoch: 22 , batch: 157 , training loss: 3.958417
[INFO] Epoch: 22 , batch: 158 , training loss: 4.111804
[INFO] Epoch: 22 , batch: 159 , training loss: 3.969852
[INFO] Epoch: 22 , batch: 160 , training loss: 4.284193
[INFO] Epoch: 22 , batch: 161 , training loss: 4.404652
[INFO] Epoch: 22 , batch: 162 , training loss: 4.368562
[INFO] Epoch: 22 , batch: 163 , training loss: 4.472313
[INFO] Epoch: 22 , batch: 164 , training loss: 4.454547
[INFO] Epoch: 22 , batch: 165 , training loss: 4.375045
[INFO] Epoch: 22 , batch: 166 , training loss: 4.248967
[INFO] Epoch: 22 , batch: 167 , training loss: 4.339148
[INFO] Epoch: 22 , batch: 168 , training loss: 4.080171
[INFO] Epoch: 22 , batch: 169 , training loss: 4.082107
[INFO] Epoch: 22 , batch: 170 , training loss: 4.203021
[INFO] Epoch: 22 , batch: 171 , training loss: 3.608708
[INFO] Epoch: 22 , batch: 172 , training loss: 3.826175
[INFO] Epoch: 22 , batch: 173 , training loss: 4.160381
[INFO] Epoch: 22 , batch: 174 , training loss: 4.524995
[INFO] Epoch: 22 , batch: 175 , training loss: 4.935135
[INFO] Epoch: 22 , batch: 176 , training loss: 4.562914
[INFO] Epoch: 22 , batch: 177 , training loss: 4.195167
[INFO] Epoch: 22 , batch: 178 , training loss: 4.178140
[INFO] Epoch: 22 , batch: 179 , training loss: 4.249036
[INFO] Epoch: 22 , batch: 180 , training loss: 4.188581
[INFO] Epoch: 22 , batch: 181 , training loss: 4.453651
[INFO] Epoch: 22 , batch: 182 , training loss: 4.382007
[INFO] Epoch: 22 , batch: 183 , training loss: 4.379166
[INFO] Epoch: 22 , batch: 184 , training loss: 4.244601
[INFO] Epoch: 22 , batch: 185 , training loss: 4.181922
[INFO] Epoch: 22 , batch: 186 , training loss: 4.371449
[INFO] Epoch: 22 , batch: 187 , training loss: 4.445240
[INFO] Epoch: 22 , batch: 188 , training loss: 4.457078
[INFO] Epoch: 22 , batch: 189 , training loss: 4.351175
[INFO] Epoch: 22 , batch: 190 , training loss: 4.371615
[INFO] Epoch: 22 , batch: 191 , training loss: 4.483774
[INFO] Epoch: 22 , batch: 192 , training loss: 4.306287
[INFO] Epoch: 22 , batch: 193 , training loss: 4.384323
[INFO] Epoch: 22 , batch: 194 , training loss: 4.337333
[INFO] Epoch: 22 , batch: 195 , training loss: 4.243265
[INFO] Epoch: 22 , batch: 196 , training loss: 4.121975
[INFO] Epoch: 22 , batch: 197 , training loss: 4.208858
[INFO] Epoch: 22 , batch: 198 , training loss: 4.144154
[INFO] Epoch: 22 , batch: 199 , training loss: 4.268509
[INFO] Epoch: 22 , batch: 200 , training loss: 4.177817
[INFO] Epoch: 22 , batch: 201 , training loss: 4.080199
[INFO] Epoch: 22 , batch: 202 , training loss: 4.069936
[INFO] Epoch: 22 , batch: 203 , training loss: 4.173008
[INFO] Epoch: 22 , batch: 204 , training loss: 4.312619
[INFO] Epoch: 22 , batch: 205 , training loss: 3.865937
[INFO] Epoch: 22 , batch: 206 , training loss: 3.819588
[INFO] Epoch: 22 , batch: 207 , training loss: 3.816557
[INFO] Epoch: 22 , batch: 208 , training loss: 4.144049
[INFO] Epoch: 22 , batch: 209 , training loss: 4.085417
[INFO] Epoch: 22 , batch: 210 , training loss: 4.100749
[INFO] Epoch: 22 , batch: 211 , training loss: 4.104231
[INFO] Epoch: 22 , batch: 212 , training loss: 4.200074
[INFO] Epoch: 22 , batch: 213 , training loss: 4.160191
[INFO] Epoch: 22 , batch: 214 , training loss: 4.268137
[INFO] Epoch: 22 , batch: 215 , training loss: 4.468440
[INFO] Epoch: 22 , batch: 216 , training loss: 4.160697
[INFO] Epoch: 22 , batch: 217 , training loss: 4.108254
[INFO] Epoch: 22 , batch: 218 , training loss: 4.104350
[INFO] Epoch: 22 , batch: 219 , training loss: 4.215907
[INFO] Epoch: 22 , batch: 220 , training loss: 4.003137
[INFO] Epoch: 22 , batch: 221 , training loss: 4.034736
[INFO] Epoch: 22 , batch: 222 , training loss: 4.176284
[INFO] Epoch: 22 , batch: 223 , training loss: 4.253734
[INFO] Epoch: 22 , batch: 224 , training loss: 4.316754
[INFO] Epoch: 22 , batch: 225 , training loss: 4.202970
[INFO] Epoch: 22 , batch: 226 , training loss: 4.341215
[INFO] Epoch: 22 , batch: 227 , training loss: 4.290143
[INFO] Epoch: 22 , batch: 228 , training loss: 4.337286
[INFO] Epoch: 22 , batch: 229 , training loss: 4.198445
[INFO] Epoch: 22 , batch: 230 , training loss: 4.055168
[INFO] Epoch: 22 , batch: 231 , training loss: 3.910154
[INFO] Epoch: 22 , batch: 232 , training loss: 4.060166
[INFO] Epoch: 22 , batch: 233 , training loss: 4.074449
[INFO] Epoch: 22 , batch: 234 , training loss: 3.778419
[INFO] Epoch: 22 , batch: 235 , training loss: 3.862509
[INFO] Epoch: 22 , batch: 236 , training loss: 3.994226
[INFO] Epoch: 22 , batch: 237 , training loss: 4.201802
[INFO] Epoch: 22 , batch: 238 , training loss: 3.952991
[INFO] Epoch: 22 , batch: 239 , training loss: 4.015959
[INFO] Epoch: 22 , batch: 240 , training loss: 4.071332
[INFO] Epoch: 22 , batch: 241 , training loss: 3.865831
[INFO] Epoch: 22 , batch: 242 , training loss: 3.899503
[INFO] Epoch: 22 , batch: 243 , training loss: 4.210029
[INFO] Epoch: 22 , batch: 244 , training loss: 4.108380
[INFO] Epoch: 22 , batch: 245 , training loss: 4.094925
[INFO] Epoch: 22 , batch: 246 , training loss: 3.800462
[INFO] Epoch: 22 , batch: 247 , training loss: 3.975002
[INFO] Epoch: 22 , batch: 248 , training loss: 4.043177
[INFO] Epoch: 22 , batch: 249 , training loss: 4.040419
[INFO] Epoch: 22 , batch: 250 , training loss: 3.808630
[INFO] Epoch: 22 , batch: 251 , training loss: 4.271561
[INFO] Epoch: 22 , batch: 252 , training loss: 3.952508
[INFO] Epoch: 22 , batch: 253 , training loss: 3.894899
[INFO] Epoch: 22 , batch: 254 , training loss: 4.182298
[INFO] Epoch: 22 , batch: 255 , training loss: 4.133879
[INFO] Epoch: 22 , batch: 256 , training loss: 4.154315
[INFO] Epoch: 22 , batch: 257 , training loss: 4.305455
[INFO] Epoch: 22 , batch: 258 , training loss: 4.353942
[INFO] Epoch: 22 , batch: 259 , training loss: 4.396063
[INFO] Epoch: 22 , batch: 260 , training loss: 4.112015
[INFO] Epoch: 22 , batch: 261 , training loss: 4.271761
[INFO] Epoch: 22 , batch: 262 , training loss: 4.449747
[INFO] Epoch: 22 , batch: 263 , training loss: 4.625865
[INFO] Epoch: 22 , batch: 264 , training loss: 3.932840
[INFO] Epoch: 22 , batch: 265 , training loss: 4.085159
[INFO] Epoch: 22 , batch: 266 , training loss: 4.488040
[INFO] Epoch: 22 , batch: 267 , training loss: 4.233006
[INFO] Epoch: 22 , batch: 268 , training loss: 4.160921
[INFO] Epoch: 22 , batch: 269 , training loss: 4.140587
[INFO] Epoch: 22 , batch: 270 , training loss: 4.161863
[INFO] Epoch: 22 , batch: 271 , training loss: 4.205555
[INFO] Epoch: 22 , batch: 272 , training loss: 4.189443
[INFO] Epoch: 22 , batch: 273 , training loss: 4.208905
[INFO] Epoch: 22 , batch: 274 , training loss: 4.290286
[INFO] Epoch: 22 , batch: 275 , training loss: 4.174840
[INFO] Epoch: 22 , batch: 276 , training loss: 4.196074
[INFO] Epoch: 22 , batch: 277 , training loss: 4.369286
[INFO] Epoch: 22 , batch: 278 , training loss: 4.032509
[INFO] Epoch: 22 , batch: 279 , training loss: 4.044879
[INFO] Epoch: 22 , batch: 280 , training loss: 4.006287
[INFO] Epoch: 22 , batch: 281 , training loss: 4.154788
[INFO] Epoch: 22 , batch: 282 , training loss: 4.049187
[INFO] Epoch: 22 , batch: 283 , training loss: 4.078813
[INFO] Epoch: 22 , batch: 284 , training loss: 4.098253
[INFO] Epoch: 22 , batch: 285 , training loss: 4.028273
[INFO] Epoch: 22 , batch: 286 , training loss: 4.042041
[INFO] Epoch: 22 , batch: 287 , training loss: 3.977065
[INFO] Epoch: 22 , batch: 288 , training loss: 3.986012
[INFO] Epoch: 22 , batch: 289 , training loss: 4.030142
[INFO] Epoch: 22 , batch: 290 , training loss: 3.816368
[INFO] Epoch: 22 , batch: 291 , training loss: 3.774886
[INFO] Epoch: 22 , batch: 292 , training loss: 3.914210
[INFO] Epoch: 22 , batch: 293 , training loss: 3.819332
[INFO] Epoch: 22 , batch: 294 , training loss: 4.473561
[INFO] Epoch: 22 , batch: 295 , training loss: 4.259553
[INFO] Epoch: 22 , batch: 296 , training loss: 4.202535
[INFO] Epoch: 22 , batch: 297 , training loss: 4.138087
[INFO] Epoch: 22 , batch: 298 , training loss: 3.973516
[INFO] Epoch: 22 , batch: 299 , training loss: 4.014138
[INFO] Epoch: 22 , batch: 300 , training loss: 3.980928
[INFO] Epoch: 22 , batch: 301 , training loss: 3.919093
[INFO] Epoch: 22 , batch: 302 , training loss: 4.088377
[INFO] Epoch: 22 , batch: 303 , training loss: 4.118366
[INFO] Epoch: 22 , batch: 304 , training loss: 4.275190
[INFO] Epoch: 22 , batch: 305 , training loss: 4.059021
[INFO] Epoch: 22 , batch: 306 , training loss: 4.200453
[INFO] Epoch: 22 , batch: 307 , training loss: 4.197085
[INFO] Epoch: 22 , batch: 308 , training loss: 4.027752
[INFO] Epoch: 22 , batch: 309 , training loss: 4.042444
[INFO] Epoch: 22 , batch: 310 , training loss: 3.922647
[INFO] Epoch: 22 , batch: 311 , training loss: 3.935445
[INFO] Epoch: 22 , batch: 312 , training loss: 3.860765
[INFO] Epoch: 22 , batch: 313 , training loss: 3.969163
[INFO] Epoch: 22 , batch: 314 , training loss: 4.032908
[INFO] Epoch: 22 , batch: 315 , training loss: 4.080650
[INFO] Epoch: 22 , batch: 316 , training loss: 4.316736
[INFO] Epoch: 22 , batch: 317 , training loss: 4.770545
[INFO] Epoch: 22 , batch: 318 , training loss: 4.924907
[INFO] Epoch: 22 , batch: 319 , training loss: 4.500525
[INFO] Epoch: 22 , batch: 320 , training loss: 4.046311
[INFO] Epoch: 22 , batch: 321 , training loss: 3.879860
[INFO] Epoch: 22 , batch: 322 , training loss: 4.003729
[INFO] Epoch: 22 , batch: 323 , training loss: 4.020105
[INFO] Epoch: 22 , batch: 324 , training loss: 3.966155
[INFO] Epoch: 22 , batch: 325 , training loss: 4.139319
[INFO] Epoch: 22 , batch: 326 , training loss: 4.192011
[INFO] Epoch: 22 , batch: 327 , training loss: 4.096578
[INFO] Epoch: 22 , batch: 328 , training loss: 4.075414
[INFO] Epoch: 22 , batch: 329 , training loss: 4.000760
[INFO] Epoch: 22 , batch: 330 , training loss: 4.032105
[INFO] Epoch: 22 , batch: 331 , training loss: 4.156000
[INFO] Epoch: 22 , batch: 332 , training loss: 3.960567
[INFO] Epoch: 22 , batch: 333 , training loss: 3.988718
[INFO] Epoch: 22 , batch: 334 , training loss: 3.981921
[INFO] Epoch: 22 , batch: 335 , training loss: 4.113945
[INFO] Epoch: 22 , batch: 336 , training loss: 4.143285
[INFO] Epoch: 22 , batch: 337 , training loss: 4.153551
[INFO] Epoch: 22 , batch: 338 , training loss: 4.366905
[INFO] Epoch: 22 , batch: 339 , training loss: 4.217477
[INFO] Epoch: 22 , batch: 340 , training loss: 4.385867
[INFO] Epoch: 22 , batch: 341 , training loss: 4.157824
[INFO] Epoch: 22 , batch: 342 , training loss: 3.927030
[INFO] Epoch: 22 , batch: 343 , training loss: 3.999175
[INFO] Epoch: 22 , batch: 344 , training loss: 3.849710
[INFO] Epoch: 22 , batch: 345 , training loss: 3.987537
[INFO] Epoch: 22 , batch: 346 , training loss: 4.044797
[INFO] Epoch: 22 , batch: 347 , training loss: 3.951644
[INFO] Epoch: 22 , batch: 348 , training loss: 4.062132
[INFO] Epoch: 22 , batch: 349 , training loss: 4.204264
[INFO] Epoch: 22 , batch: 350 , training loss: 3.982725
[INFO] Epoch: 22 , batch: 351 , training loss: 4.046774
[INFO] Epoch: 22 , batch: 352 , training loss: 4.072354
[INFO] Epoch: 22 , batch: 353 , training loss: 4.040518
[INFO] Epoch: 22 , batch: 354 , training loss: 4.150077
[INFO] Epoch: 22 , batch: 355 , training loss: 4.209392
[INFO] Epoch: 22 , batch: 356 , training loss: 4.030221
[INFO] Epoch: 22 , batch: 357 , training loss: 4.106090
[INFO] Epoch: 22 , batch: 358 , training loss: 4.020123
[INFO] Epoch: 22 , batch: 359 , training loss: 4.003704
[INFO] Epoch: 22 , batch: 360 , training loss: 4.097908
[INFO] Epoch: 22 , batch: 361 , training loss: 4.048985
[INFO] Epoch: 22 , batch: 362 , training loss: 4.161939
[INFO] Epoch: 22 , batch: 363 , training loss: 4.032025
[INFO] Epoch: 22 , batch: 364 , training loss: 4.103019
[INFO] Epoch: 22 , batch: 365 , training loss: 4.023348
[INFO] Epoch: 22 , batch: 366 , training loss: 4.153189
[INFO] Epoch: 22 , batch: 367 , training loss: 4.222913
[INFO] Epoch: 22 , batch: 368 , training loss: 4.626835
[INFO] Epoch: 22 , batch: 369 , training loss: 4.283986
[INFO] Epoch: 22 , batch: 370 , training loss: 4.068466
[INFO] Epoch: 22 , batch: 371 , training loss: 4.458809
[INFO] Epoch: 22 , batch: 372 , training loss: 4.757122
[INFO] Epoch: 22 , batch: 373 , training loss: 4.833255
[INFO] Epoch: 22 , batch: 374 , training loss: 4.915976
[INFO] Epoch: 22 , batch: 375 , training loss: 4.897595
[INFO] Epoch: 22 , batch: 376 , training loss: 4.812383
[INFO] Epoch: 22 , batch: 377 , training loss: 4.548043
[INFO] Epoch: 22 , batch: 378 , training loss: 4.659146
[INFO] Epoch: 22 , batch: 379 , training loss: 4.624871
[INFO] Epoch: 22 , batch: 380 , training loss: 4.774127
[INFO] Epoch: 22 , batch: 381 , training loss: 4.492582
[INFO] Epoch: 22 , batch: 382 , training loss: 4.723000
[INFO] Epoch: 22 , batch: 383 , training loss: 4.736183
[INFO] Epoch: 22 , batch: 384 , training loss: 4.750616
[INFO] Epoch: 22 , batch: 385 , training loss: 4.467385
[INFO] Epoch: 22 , batch: 386 , training loss: 4.730846
[INFO] Epoch: 22 , batch: 387 , training loss: 4.681052
[INFO] Epoch: 22 , batch: 388 , training loss: 4.486769
[INFO] Epoch: 22 , batch: 389 , training loss: 4.344983
[INFO] Epoch: 22 , batch: 390 , training loss: 4.325877
[INFO] Epoch: 22 , batch: 391 , training loss: 4.373150
[INFO] Epoch: 22 , batch: 392 , training loss: 4.723576
[INFO] Epoch: 22 , batch: 393 , training loss: 4.606026
[INFO] Epoch: 22 , batch: 394 , training loss: 4.655257
[INFO] Epoch: 22 , batch: 395 , training loss: 4.502304
[INFO] Epoch: 22 , batch: 396 , training loss: 4.276341
[INFO] Epoch: 22 , batch: 397 , training loss: 4.445549
[INFO] Epoch: 22 , batch: 398 , training loss: 4.302621
[INFO] Epoch: 22 , batch: 399 , training loss: 4.381607
[INFO] Epoch: 22 , batch: 400 , training loss: 4.327811
[INFO] Epoch: 22 , batch: 401 , training loss: 4.765815
[INFO] Epoch: 22 , batch: 402 , training loss: 4.480042
[INFO] Epoch: 22 , batch: 403 , training loss: 4.268904
[INFO] Epoch: 22 , batch: 404 , training loss: 4.469366
[INFO] Epoch: 22 , batch: 405 , training loss: 4.525435
[INFO] Epoch: 22 , batch: 406 , training loss: 4.432456
[INFO] Epoch: 22 , batch: 407 , training loss: 4.485256
[INFO] Epoch: 22 , batch: 408 , training loss: 4.433733
[INFO] Epoch: 22 , batch: 409 , training loss: 4.438496
[INFO] Epoch: 22 , batch: 410 , training loss: 4.460830
[INFO] Epoch: 22 , batch: 411 , training loss: 4.655949
[INFO] Epoch: 22 , batch: 412 , training loss: 4.502064
[INFO] Epoch: 22 , batch: 413 , training loss: 4.392982
[INFO] Epoch: 22 , batch: 414 , training loss: 4.420274
[INFO] Epoch: 22 , batch: 415 , training loss: 4.431082
[INFO] Epoch: 22 , batch: 416 , training loss: 4.505835
[INFO] Epoch: 22 , batch: 417 , training loss: 4.429517
[INFO] Epoch: 22 , batch: 418 , training loss: 4.445196
[INFO] Epoch: 22 , batch: 419 , training loss: 4.413516
[INFO] Epoch: 22 , batch: 420 , training loss: 4.384641
[INFO] Epoch: 22 , batch: 421 , training loss: 4.378319
[INFO] Epoch: 22 , batch: 422 , training loss: 4.263878
[INFO] Epoch: 22 , batch: 423 , training loss: 4.466033
[INFO] Epoch: 22 , batch: 424 , training loss: 4.642418
[INFO] Epoch: 22 , batch: 425 , training loss: 4.484175
[INFO] Epoch: 22 , batch: 426 , training loss: 4.228379
[INFO] Epoch: 22 , batch: 427 , training loss: 4.457648
[INFO] Epoch: 22 , batch: 428 , training loss: 4.347596
[INFO] Epoch: 22 , batch: 429 , training loss: 4.203267
[INFO] Epoch: 22 , batch: 430 , training loss: 4.469458
[INFO] Epoch: 22 , batch: 431 , training loss: 4.061762
[INFO] Epoch: 22 , batch: 432 , training loss: 4.137925
[INFO] Epoch: 22 , batch: 433 , training loss: 4.181976
[INFO] Epoch: 22 , batch: 434 , training loss: 4.038469
[INFO] Epoch: 22 , batch: 435 , training loss: 4.402244
[INFO] Epoch: 22 , batch: 436 , training loss: 4.484204
[INFO] Epoch: 22 , batch: 437 , training loss: 4.238235
[INFO] Epoch: 22 , batch: 438 , training loss: 4.083428
[INFO] Epoch: 22 , batch: 439 , training loss: 4.314193
[INFO] Epoch: 22 , batch: 440 , training loss: 4.426641
[INFO] Epoch: 22 , batch: 441 , training loss: 4.523264
[INFO] Epoch: 22 , batch: 442 , training loss: 4.314402
[INFO] Epoch: 22 , batch: 443 , training loss: 4.498798
[INFO] Epoch: 22 , batch: 444 , training loss: 4.107156
[INFO] Epoch: 22 , batch: 445 , training loss: 4.015761
[INFO] Epoch: 22 , batch: 446 , training loss: 3.955608
[INFO] Epoch: 22 , batch: 447 , training loss: 4.135260
[INFO] Epoch: 22 , batch: 448 , training loss: 4.246992
[INFO] Epoch: 22 , batch: 449 , training loss: 4.661652
[INFO] Epoch: 22 , batch: 450 , training loss: 4.704299
[INFO] Epoch: 22 , batch: 451 , training loss: 4.610034
[INFO] Epoch: 22 , batch: 452 , training loss: 4.413200
[INFO] Epoch: 22 , batch: 453 , training loss: 4.176860
[INFO] Epoch: 22 , batch: 454 , training loss: 4.318647
[INFO] Epoch: 22 , batch: 455 , training loss: 4.359852
[INFO] Epoch: 22 , batch: 456 , training loss: 4.367646
[INFO] Epoch: 22 , batch: 457 , training loss: 4.473902
[INFO] Epoch: 22 , batch: 458 , training loss: 4.189608
[INFO] Epoch: 22 , batch: 459 , training loss: 4.171294
[INFO] Epoch: 22 , batch: 460 , training loss: 4.269712
[INFO] Epoch: 22 , batch: 461 , training loss: 4.257159
[INFO] Epoch: 22 , batch: 462 , training loss: 4.312439
[INFO] Epoch: 22 , batch: 463 , training loss: 4.225235
[INFO] Epoch: 22 , batch: 464 , training loss: 4.420543
[INFO] Epoch: 22 , batch: 465 , training loss: 4.349014
[INFO] Epoch: 22 , batch: 466 , training loss: 4.422315
[INFO] Epoch: 22 , batch: 467 , training loss: 4.405996
[INFO] Epoch: 22 , batch: 468 , training loss: 4.376809
[INFO] Epoch: 22 , batch: 469 , training loss: 4.401723
[INFO] Epoch: 22 , batch: 470 , training loss: 4.210081
[INFO] Epoch: 22 , batch: 471 , training loss: 4.288544
[INFO] Epoch: 22 , batch: 472 , training loss: 4.348813
[INFO] Epoch: 22 , batch: 473 , training loss: 4.276106
[INFO] Epoch: 22 , batch: 474 , training loss: 4.080505
[INFO] Epoch: 22 , batch: 475 , training loss: 3.957559
[INFO] Epoch: 22 , batch: 476 , training loss: 4.317081
[INFO] Epoch: 22 , batch: 477 , training loss: 4.457881
[INFO] Epoch: 22 , batch: 478 , training loss: 4.502279
[INFO] Epoch: 22 , batch: 479 , training loss: 4.467157
[INFO] Epoch: 22 , batch: 480 , training loss: 4.556788
[INFO] Epoch: 22 , batch: 481 , training loss: 4.419491
[INFO] Epoch: 22 , batch: 482 , training loss: 4.543165
[INFO] Epoch: 22 , batch: 483 , training loss: 4.398217
[INFO] Epoch: 22 , batch: 484 , training loss: 4.198388
[INFO] Epoch: 22 , batch: 485 , training loss: 4.303179
[INFO] Epoch: 22 , batch: 486 , training loss: 4.168667
[INFO] Epoch: 22 , batch: 487 , training loss: 4.158074
[INFO] Epoch: 22 , batch: 488 , training loss: 4.380368
[INFO] Epoch: 22 , batch: 489 , training loss: 4.245741
[INFO] Epoch: 22 , batch: 490 , training loss: 4.311570
[INFO] Epoch: 22 , batch: 491 , training loss: 4.249345
[INFO] Epoch: 22 , batch: 492 , training loss: 4.195712
[INFO] Epoch: 22 , batch: 493 , training loss: 4.394576
[INFO] Epoch: 22 , batch: 494 , training loss: 4.306802
[INFO] Epoch: 22 , batch: 495 , training loss: 4.428045
[INFO] Epoch: 22 , batch: 496 , training loss: 4.304755
[INFO] Epoch: 22 , batch: 497 , training loss: 4.350015
[INFO] Epoch: 22 , batch: 498 , training loss: 4.355707
[INFO] Epoch: 22 , batch: 499 , training loss: 4.419638
[INFO] Epoch: 22 , batch: 500 , training loss: 4.586757
[INFO] Epoch: 22 , batch: 501 , training loss: 4.888363
[INFO] Epoch: 22 , batch: 502 , training loss: 4.967867
[INFO] Epoch: 22 , batch: 503 , training loss: 4.684680
[INFO] Epoch: 22 , batch: 504 , training loss: 4.795930
[INFO] Epoch: 22 , batch: 505 , training loss: 4.754216
[INFO] Epoch: 22 , batch: 506 , training loss: 4.722075
[INFO] Epoch: 22 , batch: 507 , training loss: 4.776002
[INFO] Epoch: 22 , batch: 508 , training loss: 4.723602
[INFO] Epoch: 22 , batch: 509 , training loss: 4.503615
[INFO] Epoch: 22 , batch: 510 , training loss: 4.566967
[INFO] Epoch: 22 , batch: 511 , training loss: 4.475422
[INFO] Epoch: 22 , batch: 512 , training loss: 4.578145
[INFO] Epoch: 22 , batch: 513 , training loss: 4.811012
[INFO] Epoch: 22 , batch: 514 , training loss: 4.469884
[INFO] Epoch: 22 , batch: 515 , training loss: 4.687004
[INFO] Epoch: 22 , batch: 516 , training loss: 4.511474
[INFO] Epoch: 22 , batch: 517 , training loss: 4.470622
[INFO] Epoch: 22 , batch: 518 , training loss: 4.447214
[INFO] Epoch: 22 , batch: 519 , training loss: 4.294633
[INFO] Epoch: 22 , batch: 520 , training loss: 4.526982
[INFO] Epoch: 22 , batch: 521 , training loss: 4.501670
[INFO] Epoch: 22 , batch: 522 , training loss: 4.559813
[INFO] Epoch: 22 , batch: 523 , training loss: 4.484785
[INFO] Epoch: 22 , batch: 524 , training loss: 4.758256
[INFO] Epoch: 22 , batch: 525 , training loss: 4.677458
[INFO] Epoch: 22 , batch: 526 , training loss: 4.447854
[INFO] Epoch: 22 , batch: 527 , training loss: 4.479021
[INFO] Epoch: 22 , batch: 528 , training loss: 4.502516
[INFO] Epoch: 22 , batch: 529 , training loss: 4.488229
[INFO] Epoch: 22 , batch: 530 , training loss: 4.323900
[INFO] Epoch: 22 , batch: 531 , training loss: 4.466653
[INFO] Epoch: 22 , batch: 532 , training loss: 4.356201
[INFO] Epoch: 22 , batch: 533 , training loss: 4.524820
[INFO] Epoch: 22 , batch: 534 , training loss: 4.501402
[INFO] Epoch: 22 , batch: 535 , training loss: 4.519441
[INFO] Epoch: 22 , batch: 536 , training loss: 4.331077
[INFO] Epoch: 22 , batch: 537 , training loss: 4.350124
[INFO] Epoch: 22 , batch: 538 , training loss: 4.418141
[INFO] Epoch: 22 , batch: 539 , training loss: 4.525543
[INFO] Epoch: 22 , batch: 540 , training loss: 5.041564
[INFO] Epoch: 22 , batch: 541 , training loss: 4.962597
[INFO] Epoch: 22 , batch: 542 , training loss: 4.836722
[INFO] Epoch: 23 , batch: 0 , training loss: 3.493666
[INFO] Epoch: 23 , batch: 1 , training loss: 3.583946
[INFO] Epoch: 23 , batch: 2 , training loss: 3.676225
[INFO] Epoch: 23 , batch: 3 , training loss: 3.481229
[INFO] Epoch: 23 , batch: 4 , training loss: 3.909597
[INFO] Epoch: 23 , batch: 5 , training loss: 3.523798
[INFO] Epoch: 23 , batch: 6 , training loss: 3.969872
[INFO] Epoch: 23 , batch: 7 , training loss: 3.815775
[INFO] Epoch: 23 , batch: 8 , training loss: 3.552488
[INFO] Epoch: 23 , batch: 9 , training loss: 3.786598
[INFO] Epoch: 23 , batch: 10 , training loss: 3.750958
[INFO] Epoch: 23 , batch: 11 , training loss: 3.727735
[INFO] Epoch: 23 , batch: 12 , training loss: 3.635067
[INFO] Epoch: 23 , batch: 13 , training loss: 3.691478
[INFO] Epoch: 23 , batch: 14 , training loss: 3.514215
[INFO] Epoch: 23 , batch: 15 , training loss: 3.697857
[INFO] Epoch: 23 , batch: 16 , training loss: 3.517086
[INFO] Epoch: 23 , batch: 17 , training loss: 3.725130
[INFO] Epoch: 23 , batch: 18 , training loss: 3.675411
[INFO] Epoch: 23 , batch: 19 , training loss: 3.398734
[INFO] Epoch: 23 , batch: 20 , training loss: 3.379896
[INFO] Epoch: 23 , batch: 21 , training loss: 3.530985
[INFO] Epoch: 23 , batch: 22 , training loss: 3.429621
[INFO] Epoch: 23 , batch: 23 , training loss: 3.633286
[INFO] Epoch: 23 , batch: 24 , training loss: 3.460572
[INFO] Epoch: 23 , batch: 25 , training loss: 3.619395
[INFO] Epoch: 23 , batch: 26 , training loss: 3.466151
[INFO] Epoch: 23 , batch: 27 , training loss: 3.429202
[INFO] Epoch: 23 , batch: 28 , training loss: 3.601176
[INFO] Epoch: 23 , batch: 29 , training loss: 3.423062
[INFO] Epoch: 23 , batch: 30 , training loss: 3.528320
[INFO] Epoch: 23 , batch: 31 , training loss: 3.525206
[INFO] Epoch: 23 , batch: 32 , training loss: 3.559659
[INFO] Epoch: 23 , batch: 33 , training loss: 3.584468
[INFO] Epoch: 23 , batch: 34 , training loss: 3.559922
[INFO] Epoch: 23 , batch: 35 , training loss: 3.575256
[INFO] Epoch: 23 , batch: 36 , training loss: 3.585375
[INFO] Epoch: 23 , batch: 37 , training loss: 3.489948
[INFO] Epoch: 23 , batch: 38 , training loss: 3.525467
[INFO] Epoch: 23 , batch: 39 , training loss: 3.409743
[INFO] Epoch: 23 , batch: 40 , training loss: 3.637011
[INFO] Epoch: 23 , batch: 41 , training loss: 3.536236
[INFO] Epoch: 23 , batch: 42 , training loss: 3.938276
[INFO] Epoch: 23 , batch: 43 , training loss: 3.680714
[INFO] Epoch: 23 , batch: 44 , training loss: 4.021550
[INFO] Epoch: 23 , batch: 45 , training loss: 3.985773
[INFO] Epoch: 23 , batch: 46 , training loss: 3.860739
[INFO] Epoch: 23 , batch: 47 , training loss: 3.677876
[INFO] Epoch: 23 , batch: 48 , training loss: 3.666776
[INFO] Epoch: 23 , batch: 49 , training loss: 3.832799
[INFO] Epoch: 23 , batch: 50 , training loss: 3.634094
[INFO] Epoch: 23 , batch: 51 , training loss: 3.791596
[INFO] Epoch: 23 , batch: 52 , training loss: 3.692327
[INFO] Epoch: 23 , batch: 53 , training loss: 3.850133
[INFO] Epoch: 23 , batch: 54 , training loss: 3.829772
[INFO] Epoch: 23 , batch: 55 , training loss: 3.927534
[INFO] Epoch: 23 , batch: 56 , training loss: 3.799407
[INFO] Epoch: 23 , batch: 57 , training loss: 3.672200
[INFO] Epoch: 23 , batch: 58 , training loss: 3.739318
[INFO] Epoch: 23 , batch: 59 , training loss: 3.795794
[INFO] Epoch: 23 , batch: 60 , training loss: 3.763080
[INFO] Epoch: 23 , batch: 61 , training loss: 3.827706
[INFO] Epoch: 23 , batch: 62 , training loss: 3.720220
[INFO] Epoch: 23 , batch: 63 , training loss: 3.911446
[INFO] Epoch: 23 , batch: 64 , training loss: 4.055153
[INFO] Epoch: 23 , batch: 65 , training loss: 3.822817
[INFO] Epoch: 23 , batch: 66 , training loss: 3.648663
[INFO] Epoch: 23 , batch: 67 , training loss: 3.709786
[INFO] Epoch: 23 , batch: 68 , training loss: 3.873937
[INFO] Epoch: 23 , batch: 69 , training loss: 3.744910
[INFO] Epoch: 23 , batch: 70 , training loss: 3.972503
[INFO] Epoch: 23 , batch: 71 , training loss: 3.876617
[INFO] Epoch: 23 , batch: 72 , training loss: 3.875311
[INFO] Epoch: 23 , batch: 73 , training loss: 3.823500
[INFO] Epoch: 23 , batch: 74 , training loss: 3.958650
[INFO] Epoch: 23 , batch: 75 , training loss: 3.850619
[INFO] Epoch: 23 , batch: 76 , training loss: 3.896277
[INFO] Epoch: 23 , batch: 77 , training loss: 3.875462
[INFO] Epoch: 23 , batch: 78 , training loss: 3.988455
[INFO] Epoch: 23 , batch: 79 , training loss: 3.804513
[INFO] Epoch: 23 , batch: 80 , training loss: 4.015523
[INFO] Epoch: 23 , batch: 81 , training loss: 3.933706
[INFO] Epoch: 23 , batch: 82 , training loss: 3.911036
[INFO] Epoch: 23 , batch: 83 , training loss: 4.045673
[INFO] Epoch: 23 , batch: 84 , training loss: 3.951166
[INFO] Epoch: 23 , batch: 85 , training loss: 4.039539
[INFO] Epoch: 23 , batch: 86 , training loss: 3.972270
[INFO] Epoch: 23 , batch: 87 , training loss: 3.953077
[INFO] Epoch: 23 , batch: 88 , training loss: 4.074009
[INFO] Epoch: 23 , batch: 89 , training loss: 3.865777
[INFO] Epoch: 23 , batch: 90 , training loss: 3.944392
[INFO] Epoch: 23 , batch: 91 , training loss: 3.878788
[INFO] Epoch: 23 , batch: 92 , training loss: 3.885359
[INFO] Epoch: 23 , batch: 93 , training loss: 4.005301
[INFO] Epoch: 23 , batch: 94 , training loss: 4.183355
[INFO] Epoch: 23 , batch: 95 , training loss: 3.906308
[INFO] Epoch: 23 , batch: 96 , training loss: 3.912633
[INFO] Epoch: 23 , batch: 97 , training loss: 3.811794
[INFO] Epoch: 23 , batch: 98 , training loss: 3.779252
[INFO] Epoch: 23 , batch: 99 , training loss: 3.853297
[INFO] Epoch: 23 , batch: 100 , training loss: 3.796324
[INFO] Epoch: 23 , batch: 101 , training loss: 3.842403
[INFO] Epoch: 23 , batch: 102 , training loss: 3.971774
[INFO] Epoch: 23 , batch: 103 , training loss: 3.750027
[INFO] Epoch: 23 , batch: 104 , training loss: 3.721724
[INFO] Epoch: 23 , batch: 105 , training loss: 4.018226
[INFO] Epoch: 23 , batch: 106 , training loss: 3.992793
[INFO] Epoch: 23 , batch: 107 , training loss: 3.837973
[INFO] Epoch: 23 , batch: 108 , training loss: 3.757208
[INFO] Epoch: 23 , batch: 109 , training loss: 3.668018
[INFO] Epoch: 23 , batch: 110 , training loss: 3.864050
[INFO] Epoch: 23 , batch: 111 , training loss: 3.971693
[INFO] Epoch: 23 , batch: 112 , training loss: 3.887172
[INFO] Epoch: 23 , batch: 113 , training loss: 3.867168
[INFO] Epoch: 23 , batch: 114 , training loss: 3.862398
[INFO] Epoch: 23 , batch: 115 , training loss: 3.869807
[INFO] Epoch: 23 , batch: 116 , training loss: 3.796775
[INFO] Epoch: 23 , batch: 117 , training loss: 4.015312
[INFO] Epoch: 23 , batch: 118 , training loss: 3.985690
[INFO] Epoch: 23 , batch: 119 , training loss: 4.111981
[INFO] Epoch: 23 , batch: 120 , training loss: 4.106222
[INFO] Epoch: 23 , batch: 121 , training loss: 3.951306
[INFO] Epoch: 23 , batch: 122 , training loss: 3.834193
[INFO] Epoch: 23 , batch: 123 , training loss: 3.866478
[INFO] Epoch: 23 , batch: 124 , training loss: 4.013079
[INFO] Epoch: 23 , batch: 125 , training loss: 3.782738
[INFO] Epoch: 23 , batch: 126 , training loss: 3.793726
[INFO] Epoch: 23 , batch: 127 , training loss: 3.789269
[INFO] Epoch: 23 , batch: 128 , training loss: 3.943070
[INFO] Epoch: 23 , batch: 129 , training loss: 3.917972
[INFO] Epoch: 23 , batch: 130 , training loss: 3.883464
[INFO] Epoch: 23 , batch: 131 , training loss: 3.879362
[INFO] Epoch: 23 , batch: 132 , training loss: 3.897391
[INFO] Epoch: 23 , batch: 133 , training loss: 3.872887
[INFO] Epoch: 23 , batch: 134 , training loss: 3.686762
[INFO] Epoch: 23 , batch: 135 , training loss: 3.738073
[INFO] Epoch: 23 , batch: 136 , training loss: 4.023673
[INFO] Epoch: 23 , batch: 137 , training loss: 3.912784
[INFO] Epoch: 23 , batch: 138 , training loss: 3.969552
[INFO] Epoch: 23 , batch: 139 , training loss: 4.552986
[INFO] Epoch: 23 , batch: 140 , training loss: 4.304200
[INFO] Epoch: 23 , batch: 141 , training loss: 4.100886
[INFO] Epoch: 23 , batch: 142 , training loss: 3.839983
[INFO] Epoch: 23 , batch: 143 , training loss: 3.951175
[INFO] Epoch: 23 , batch: 144 , training loss: 3.778785
[INFO] Epoch: 23 , batch: 145 , training loss: 3.855029
[INFO] Epoch: 23 , batch: 146 , training loss: 4.060099
[INFO] Epoch: 23 , batch: 147 , training loss: 3.718336
[INFO] Epoch: 23 , batch: 148 , training loss: 3.718889
[INFO] Epoch: 23 , batch: 149 , training loss: 3.807004
[INFO] Epoch: 23 , batch: 150 , training loss: 3.998708
[INFO] Epoch: 23 , batch: 151 , training loss: 3.906581
[INFO] Epoch: 23 , batch: 152 , training loss: 3.971301
[INFO] Epoch: 23 , batch: 153 , training loss: 3.929482
[INFO] Epoch: 23 , batch: 154 , training loss: 4.043008
[INFO] Epoch: 23 , batch: 155 , training loss: 4.248367
[INFO] Epoch: 23 , batch: 156 , training loss: 3.924129
[INFO] Epoch: 23 , batch: 157 , training loss: 3.933303
[INFO] Epoch: 23 , batch: 158 , training loss: 4.101394
[INFO] Epoch: 23 , batch: 159 , training loss: 3.939738
[INFO] Epoch: 23 , batch: 160 , training loss: 4.274933
[INFO] Epoch: 23 , batch: 161 , training loss: 4.389673
[INFO] Epoch: 23 , batch: 162 , training loss: 4.367203
[INFO] Epoch: 23 , batch: 163 , training loss: 4.464422
[INFO] Epoch: 23 , batch: 164 , training loss: 4.466718
[INFO] Epoch: 23 , batch: 165 , training loss: 4.367868
[INFO] Epoch: 23 , batch: 166 , training loss: 4.230753
[INFO] Epoch: 23 , batch: 167 , training loss: 4.320540
[INFO] Epoch: 23 , batch: 168 , training loss: 4.043233
[INFO] Epoch: 23 , batch: 169 , training loss: 4.037365
[INFO] Epoch: 23 , batch: 170 , training loss: 4.210228
[INFO] Epoch: 23 , batch: 171 , training loss: 3.575354
[INFO] Epoch: 23 , batch: 172 , training loss: 3.815120
[INFO] Epoch: 23 , batch: 173 , training loss: 4.139720
[INFO] Epoch: 23 , batch: 174 , training loss: 4.552272
[INFO] Epoch: 23 , batch: 175 , training loss: 4.909203
[INFO] Epoch: 23 , batch: 176 , training loss: 4.526211
[INFO] Epoch: 23 , batch: 177 , training loss: 4.167609
[INFO] Epoch: 23 , batch: 178 , training loss: 4.153390
[INFO] Epoch: 23 , batch: 179 , training loss: 4.209650
[INFO] Epoch: 23 , batch: 180 , training loss: 4.161496
[INFO] Epoch: 23 , batch: 181 , training loss: 4.438024
[INFO] Epoch: 23 , batch: 182 , training loss: 4.382728
[INFO] Epoch: 23 , batch: 183 , training loss: 4.360462
[INFO] Epoch: 23 , batch: 184 , training loss: 4.248213
[INFO] Epoch: 23 , batch: 185 , training loss: 4.212942
[INFO] Epoch: 23 , batch: 186 , training loss: 4.362351
[INFO] Epoch: 23 , batch: 187 , training loss: 4.450032
[INFO] Epoch: 23 , batch: 188 , training loss: 4.437749
[INFO] Epoch: 23 , batch: 189 , training loss: 4.349156
[INFO] Epoch: 23 , batch: 190 , training loss: 4.378513
[INFO] Epoch: 23 , batch: 191 , training loss: 4.480074
[INFO] Epoch: 23 , batch: 192 , training loss: 4.296537
[INFO] Epoch: 23 , batch: 193 , training loss: 4.383162
[INFO] Epoch: 23 , batch: 194 , training loss: 4.324761
[INFO] Epoch: 23 , batch: 195 , training loss: 4.226697
[INFO] Epoch: 23 , batch: 196 , training loss: 4.126587
[INFO] Epoch: 23 , batch: 197 , training loss: 4.209363
[INFO] Epoch: 23 , batch: 198 , training loss: 4.125416
[INFO] Epoch: 23 , batch: 199 , training loss: 4.279320
[INFO] Epoch: 23 , batch: 200 , training loss: 4.178292
[INFO] Epoch: 23 , batch: 201 , training loss: 4.092681
[INFO] Epoch: 23 , batch: 202 , training loss: 4.056662
[INFO] Epoch: 23 , batch: 203 , training loss: 4.153399
[INFO] Epoch: 23 , batch: 204 , training loss: 4.310536
[INFO] Epoch: 23 , batch: 205 , training loss: 3.871866
[INFO] Epoch: 23 , batch: 206 , training loss: 3.826780
[INFO] Epoch: 23 , batch: 207 , training loss: 3.817267
[INFO] Epoch: 23 , batch: 208 , training loss: 4.132343
[INFO] Epoch: 23 , batch: 209 , training loss: 4.094376
[INFO] Epoch: 23 , batch: 210 , training loss: 4.108359
[INFO] Epoch: 23 , batch: 211 , training loss: 4.104250
[INFO] Epoch: 23 , batch: 212 , training loss: 4.189413
[INFO] Epoch: 23 , batch: 213 , training loss: 4.155859
[INFO] Epoch: 23 , batch: 214 , training loss: 4.269943
[INFO] Epoch: 23 , batch: 215 , training loss: 4.491104
[INFO] Epoch: 23 , batch: 216 , training loss: 4.156210
[INFO] Epoch: 23 , batch: 217 , training loss: 4.117660
[INFO] Epoch: 23 , batch: 218 , training loss: 4.088108
[INFO] Epoch: 23 , batch: 219 , training loss: 4.206006
[INFO] Epoch: 23 , batch: 220 , training loss: 3.997887
[INFO] Epoch: 23 , batch: 221 , training loss: 4.026700
[INFO] Epoch: 23 , batch: 222 , training loss: 4.163617
[INFO] Epoch: 23 , batch: 223 , training loss: 4.241581
[INFO] Epoch: 23 , batch: 224 , training loss: 4.311203
[INFO] Epoch: 23 , batch: 225 , training loss: 4.205980
[INFO] Epoch: 23 , batch: 226 , training loss: 4.352180
[INFO] Epoch: 23 , batch: 227 , training loss: 4.294856
[INFO] Epoch: 23 , batch: 228 , training loss: 4.347102
[INFO] Epoch: 23 , batch: 229 , training loss: 4.194450
[INFO] Epoch: 23 , batch: 230 , training loss: 4.055264
[INFO] Epoch: 23 , batch: 231 , training loss: 3.916263
[INFO] Epoch: 23 , batch: 232 , training loss: 4.063020
[INFO] Epoch: 23 , batch: 233 , training loss: 4.076523
[INFO] Epoch: 23 , batch: 234 , training loss: 3.769893
[INFO] Epoch: 23 , batch: 235 , training loss: 3.877549
[INFO] Epoch: 23 , batch: 236 , training loss: 3.979688
[INFO] Epoch: 23 , batch: 237 , training loss: 4.171873
[INFO] Epoch: 23 , batch: 238 , training loss: 3.965473
[INFO] Epoch: 23 , batch: 239 , training loss: 4.020555
[INFO] Epoch: 23 , batch: 240 , training loss: 4.060200
[INFO] Epoch: 23 , batch: 241 , training loss: 3.871630
[INFO] Epoch: 23 , batch: 242 , training loss: 3.890401
[INFO] Epoch: 23 , batch: 243 , training loss: 4.187697
[INFO] Epoch: 23 , batch: 244 , training loss: 4.118798
[INFO] Epoch: 23 , batch: 245 , training loss: 4.100168
[INFO] Epoch: 23 , batch: 246 , training loss: 3.803193
[INFO] Epoch: 23 , batch: 247 , training loss: 3.982028
[INFO] Epoch: 23 , batch: 248 , training loss: 4.041995
[INFO] Epoch: 23 , batch: 249 , training loss: 4.056173
[INFO] Epoch: 23 , batch: 250 , training loss: 3.806002
[INFO] Epoch: 23 , batch: 251 , training loss: 4.260949
[INFO] Epoch: 23 , batch: 252 , training loss: 3.940991
[INFO] Epoch: 23 , batch: 253 , training loss: 3.869487
[INFO] Epoch: 23 , batch: 254 , training loss: 4.170235
[INFO] Epoch: 23 , batch: 255 , training loss: 4.124722
[INFO] Epoch: 23 , batch: 256 , training loss: 4.146879
[INFO] Epoch: 23 , batch: 257 , training loss: 4.298442
[INFO] Epoch: 23 , batch: 258 , training loss: 4.321342
[INFO] Epoch: 23 , batch: 259 , training loss: 4.392717
[INFO] Epoch: 23 , batch: 260 , training loss: 4.096366
[INFO] Epoch: 23 , batch: 261 , training loss: 4.256610
[INFO] Epoch: 23 , batch: 262 , training loss: 4.435452
[INFO] Epoch: 23 , batch: 263 , training loss: 4.621570
[INFO] Epoch: 23 , batch: 264 , training loss: 3.918173
[INFO] Epoch: 23 , batch: 265 , training loss: 4.077922
[INFO] Epoch: 23 , batch: 266 , training loss: 4.491943
[INFO] Epoch: 23 , batch: 267 , training loss: 4.244780
[INFO] Epoch: 23 , batch: 268 , training loss: 4.161012
[INFO] Epoch: 23 , batch: 269 , training loss: 4.126377
[INFO] Epoch: 23 , batch: 270 , training loss: 4.167384
[INFO] Epoch: 23 , batch: 271 , training loss: 4.202792
[INFO] Epoch: 23 , batch: 272 , training loss: 4.182381
[INFO] Epoch: 23 , batch: 273 , training loss: 4.192591
[INFO] Epoch: 23 , batch: 274 , training loss: 4.276788
[INFO] Epoch: 23 , batch: 275 , training loss: 4.150901
[INFO] Epoch: 23 , batch: 276 , training loss: 4.194484
[INFO] Epoch: 23 , batch: 277 , training loss: 4.364057
[INFO] Epoch: 23 , batch: 278 , training loss: 4.023946
[INFO] Epoch: 23 , batch: 279 , training loss: 4.042105
[INFO] Epoch: 23 , batch: 280 , training loss: 3.994977
[INFO] Epoch: 23 , batch: 281 , training loss: 4.133940
[INFO] Epoch: 23 , batch: 282 , training loss: 4.043837
[INFO] Epoch: 23 , batch: 283 , training loss: 4.063307
[INFO] Epoch: 23 , batch: 284 , training loss: 4.099359
[INFO] Epoch: 23 , batch: 285 , training loss: 4.028897
[INFO] Epoch: 23 , batch: 286 , training loss: 4.035284
[INFO] Epoch: 23 , batch: 287 , training loss: 3.973089
[INFO] Epoch: 23 , batch: 288 , training loss: 3.983140
[INFO] Epoch: 23 , batch: 289 , training loss: 4.035087
[INFO] Epoch: 23 , batch: 290 , training loss: 3.805434
[INFO] Epoch: 23 , batch: 291 , training loss: 3.783907
[INFO] Epoch: 23 , batch: 292 , training loss: 3.914851
[INFO] Epoch: 23 , batch: 293 , training loss: 3.815582
[INFO] Epoch: 23 , batch: 294 , training loss: 4.492242
[INFO] Epoch: 23 , batch: 295 , training loss: 4.250351
[INFO] Epoch: 23 , batch: 296 , training loss: 4.192773
[INFO] Epoch: 23 , batch: 297 , training loss: 4.153734
[INFO] Epoch: 23 , batch: 298 , training loss: 3.990335
[INFO] Epoch: 23 , batch: 299 , training loss: 4.011345
[INFO] Epoch: 23 , batch: 300 , training loss: 3.973300
[INFO] Epoch: 23 , batch: 301 , training loss: 3.904603
[INFO] Epoch: 23 , batch: 302 , training loss: 4.070658
[INFO] Epoch: 23 , batch: 303 , training loss: 4.102504
[INFO] Epoch: 23 , batch: 304 , training loss: 4.268781
[INFO] Epoch: 23 , batch: 305 , training loss: 4.047790
[INFO] Epoch: 23 , batch: 306 , training loss: 4.201792
[INFO] Epoch: 23 , batch: 307 , training loss: 4.196230
[INFO] Epoch: 23 , batch: 308 , training loss: 4.024728
[INFO] Epoch: 23 , batch: 309 , training loss: 4.047344
[INFO] Epoch: 23 , batch: 310 , training loss: 3.921349
[INFO] Epoch: 23 , batch: 311 , training loss: 3.929510
[INFO] Epoch: 23 , batch: 312 , training loss: 3.861115
[INFO] Epoch: 23 , batch: 313 , training loss: 3.966624
[INFO] Epoch: 23 , batch: 314 , training loss: 4.021704
[INFO] Epoch: 23 , batch: 315 , training loss: 4.074598
[INFO] Epoch: 23 , batch: 316 , training loss: 4.303917
[INFO] Epoch: 23 , batch: 317 , training loss: 4.762693
[INFO] Epoch: 23 , batch: 318 , training loss: 4.916749
[INFO] Epoch: 23 , batch: 319 , training loss: 4.490246
[INFO] Epoch: 23 , batch: 320 , training loss: 4.054679
[INFO] Epoch: 23 , batch: 321 , training loss: 3.864523
[INFO] Epoch: 23 , batch: 322 , training loss: 3.994359
[INFO] Epoch: 23 , batch: 323 , training loss: 4.007679
[INFO] Epoch: 23 , batch: 324 , training loss: 3.961980
[INFO] Epoch: 23 , batch: 325 , training loss: 4.134642
[INFO] Epoch: 23 , batch: 326 , training loss: 4.197215
[INFO] Epoch: 23 , batch: 327 , training loss: 4.085366
[INFO] Epoch: 23 , batch: 328 , training loss: 4.079521
[INFO] Epoch: 23 , batch: 329 , training loss: 3.988341
[INFO] Epoch: 23 , batch: 330 , training loss: 4.002401
[INFO] Epoch: 23 , batch: 331 , training loss: 4.156483
[INFO] Epoch: 23 , batch: 332 , training loss: 3.946127
[INFO] Epoch: 23 , batch: 333 , training loss: 3.971792
[INFO] Epoch: 23 , batch: 334 , training loss: 3.971958
[INFO] Epoch: 23 , batch: 335 , training loss: 4.103867
[INFO] Epoch: 23 , batch: 336 , training loss: 4.120444
[INFO] Epoch: 23 , batch: 337 , training loss: 4.149682
[INFO] Epoch: 23 , batch: 338 , training loss: 4.367452
[INFO] Epoch: 23 , batch: 339 , training loss: 4.230374
[INFO] Epoch: 23 , batch: 340 , training loss: 4.373496
[INFO] Epoch: 23 , batch: 341 , training loss: 4.139194
[INFO] Epoch: 23 , batch: 342 , training loss: 3.915486
[INFO] Epoch: 23 , batch: 343 , training loss: 3.988969
[INFO] Epoch: 23 , batch: 344 , training loss: 3.852189
[INFO] Epoch: 23 , batch: 345 , training loss: 3.979378
[INFO] Epoch: 23 , batch: 346 , training loss: 4.030826
[INFO] Epoch: 23 , batch: 347 , training loss: 3.935399
[INFO] Epoch: 23 , batch: 348 , training loss: 4.060678
[INFO] Epoch: 23 , batch: 349 , training loss: 4.194071
[INFO] Epoch: 23 , batch: 350 , training loss: 3.990911
[INFO] Epoch: 23 , batch: 351 , training loss: 4.047044
[INFO] Epoch: 23 , batch: 352 , training loss: 4.071439
[INFO] Epoch: 23 , batch: 353 , training loss: 4.032172
[INFO] Epoch: 23 , batch: 354 , training loss: 4.156042
[INFO] Epoch: 23 , batch: 355 , training loss: 4.192016
[INFO] Epoch: 23 , batch: 356 , training loss: 4.017415
[INFO] Epoch: 23 , batch: 357 , training loss: 4.099667
[INFO] Epoch: 23 , batch: 358 , training loss: 3.987058
[INFO] Epoch: 23 , batch: 359 , training loss: 3.994114
[INFO] Epoch: 23 , batch: 360 , training loss: 4.091204
[INFO] Epoch: 23 , batch: 361 , training loss: 4.048471
[INFO] Epoch: 23 , batch: 362 , training loss: 4.156126
[INFO] Epoch: 23 , batch: 363 , training loss: 4.040231
[INFO] Epoch: 23 , batch: 364 , training loss: 4.078350
[INFO] Epoch: 23 , batch: 365 , training loss: 4.022113
[INFO] Epoch: 23 , batch: 366 , training loss: 4.137785
[INFO] Epoch: 23 , batch: 367 , training loss: 4.217165
[INFO] Epoch: 23 , batch: 368 , training loss: 4.643432
[INFO] Epoch: 23 , batch: 369 , training loss: 4.289191
[INFO] Epoch: 23 , batch: 370 , training loss: 4.060311
[INFO] Epoch: 23 , batch: 371 , training loss: 4.440692
[INFO] Epoch: 23 , batch: 372 , training loss: 4.756535
[INFO] Epoch: 23 , batch: 373 , training loss: 4.809468
[INFO] Epoch: 23 , batch: 374 , training loss: 4.881743
[INFO] Epoch: 23 , batch: 375 , training loss: 4.882565
[INFO] Epoch: 23 , batch: 376 , training loss: 4.812322
[INFO] Epoch: 23 , batch: 377 , training loss: 4.571872
[INFO] Epoch: 23 , batch: 378 , training loss: 4.665669
[INFO] Epoch: 23 , batch: 379 , training loss: 4.614090
[INFO] Epoch: 23 , batch: 380 , training loss: 4.758373
[INFO] Epoch: 23 , batch: 381 , training loss: 4.488732
[INFO] Epoch: 23 , batch: 382 , training loss: 4.728670
[INFO] Epoch: 23 , batch: 383 , training loss: 4.719774
[INFO] Epoch: 23 , batch: 384 , training loss: 4.728458
[INFO] Epoch: 23 , batch: 385 , training loss: 4.456353
[INFO] Epoch: 23 , batch: 386 , training loss: 4.705588
[INFO] Epoch: 23 , batch: 387 , training loss: 4.660480
[INFO] Epoch: 23 , batch: 388 , training loss: 4.476178
[INFO] Epoch: 23 , batch: 389 , training loss: 4.327312
[INFO] Epoch: 23 , batch: 390 , training loss: 4.317575
[INFO] Epoch: 23 , batch: 391 , training loss: 4.367152
[INFO] Epoch: 23 , batch: 392 , training loss: 4.716045
[INFO] Epoch: 23 , batch: 393 , training loss: 4.601213
[INFO] Epoch: 23 , batch: 394 , training loss: 4.631826
[INFO] Epoch: 23 , batch: 395 , training loss: 4.507268
[INFO] Epoch: 23 , batch: 396 , training loss: 4.269970
[INFO] Epoch: 23 , batch: 397 , training loss: 4.439937
[INFO] Epoch: 23 , batch: 398 , training loss: 4.281213
[INFO] Epoch: 23 , batch: 399 , training loss: 4.375803
[INFO] Epoch: 23 , batch: 400 , training loss: 4.324115
[INFO] Epoch: 23 , batch: 401 , training loss: 4.740107
[INFO] Epoch: 23 , batch: 402 , training loss: 4.479850
[INFO] Epoch: 23 , batch: 403 , training loss: 4.270809
[INFO] Epoch: 23 , batch: 404 , training loss: 4.469642
[INFO] Epoch: 23 , batch: 405 , training loss: 4.518469
[INFO] Epoch: 23 , batch: 406 , training loss: 4.410825
[INFO] Epoch: 23 , batch: 407 , training loss: 4.475396
[INFO] Epoch: 23 , batch: 408 , training loss: 4.432763
[INFO] Epoch: 23 , batch: 409 , training loss: 4.435832
[INFO] Epoch: 23 , batch: 410 , training loss: 4.462314
[INFO] Epoch: 23 , batch: 411 , training loss: 4.656397
[INFO] Epoch: 23 , batch: 412 , training loss: 4.504628
[INFO] Epoch: 23 , batch: 413 , training loss: 4.383681
[INFO] Epoch: 23 , batch: 414 , training loss: 4.403124
[INFO] Epoch: 23 , batch: 415 , training loss: 4.429794
[INFO] Epoch: 23 , batch: 416 , training loss: 4.507581
[INFO] Epoch: 23 , batch: 417 , training loss: 4.426786
[INFO] Epoch: 23 , batch: 418 , training loss: 4.450615
[INFO] Epoch: 23 , batch: 419 , training loss: 4.405746
[INFO] Epoch: 23 , batch: 420 , training loss: 4.382164
[INFO] Epoch: 23 , batch: 421 , training loss: 4.389847
[INFO] Epoch: 23 , batch: 422 , training loss: 4.261335
[INFO] Epoch: 23 , batch: 423 , training loss: 4.454384
[INFO] Epoch: 23 , batch: 424 , training loss: 4.630162
[INFO] Epoch: 23 , batch: 425 , training loss: 4.485233
[INFO] Epoch: 23 , batch: 426 , training loss: 4.225206
[INFO] Epoch: 23 , batch: 427 , training loss: 4.465046
[INFO] Epoch: 23 , batch: 428 , training loss: 4.348075
[INFO] Epoch: 23 , batch: 429 , training loss: 4.212611
[INFO] Epoch: 23 , batch: 430 , training loss: 4.463688
[INFO] Epoch: 23 , batch: 431 , training loss: 4.066246
[INFO] Epoch: 23 , batch: 432 , training loss: 4.134190
[INFO] Epoch: 23 , batch: 433 , training loss: 4.184408
[INFO] Epoch: 23 , batch: 434 , training loss: 4.030097
[INFO] Epoch: 23 , batch: 435 , training loss: 4.393616
[INFO] Epoch: 23 , batch: 436 , training loss: 4.481909
[INFO] Epoch: 23 , batch: 437 , training loss: 4.239171
[INFO] Epoch: 23 , batch: 438 , training loss: 4.075417
[INFO] Epoch: 23 , batch: 439 , training loss: 4.306777
[INFO] Epoch: 23 , batch: 440 , training loss: 4.426539
[INFO] Epoch: 23 , batch: 441 , training loss: 4.524722
[INFO] Epoch: 23 , batch: 442 , training loss: 4.301414
[INFO] Epoch: 23 , batch: 443 , training loss: 4.492684
[INFO] Epoch: 23 , batch: 444 , training loss: 4.103446
[INFO] Epoch: 23 , batch: 445 , training loss: 4.004677
[INFO] Epoch: 23 , batch: 446 , training loss: 3.954183
[INFO] Epoch: 23 , batch: 447 , training loss: 4.133302
[INFO] Epoch: 23 , batch: 448 , training loss: 4.229522
[INFO] Epoch: 23 , batch: 449 , training loss: 4.646932
[INFO] Epoch: 23 , batch: 450 , training loss: 4.696321
[INFO] Epoch: 23 , batch: 451 , training loss: 4.615440
[INFO] Epoch: 23 , batch: 452 , training loss: 4.410672
[INFO] Epoch: 23 , batch: 453 , training loss: 4.168265
[INFO] Epoch: 23 , batch: 454 , training loss: 4.321471
[INFO] Epoch: 23 , batch: 455 , training loss: 4.353035
[INFO] Epoch: 23 , batch: 456 , training loss: 4.366722
[INFO] Epoch: 23 , batch: 457 , training loss: 4.468934
[INFO] Epoch: 23 , batch: 458 , training loss: 4.185077
[INFO] Epoch: 23 , batch: 459 , training loss: 4.163921
[INFO] Epoch: 23 , batch: 460 , training loss: 4.262219
[INFO] Epoch: 23 , batch: 461 , training loss: 4.253217
[INFO] Epoch: 23 , batch: 462 , training loss: 4.302331
[INFO] Epoch: 23 , batch: 463 , training loss: 4.217584
[INFO] Epoch: 23 , batch: 464 , training loss: 4.414650
[INFO] Epoch: 23 , batch: 465 , training loss: 4.338367
[INFO] Epoch: 23 , batch: 466 , training loss: 4.428040
[INFO] Epoch: 23 , batch: 467 , training loss: 4.402024
[INFO] Epoch: 23 , batch: 468 , training loss: 4.365315
[INFO] Epoch: 23 , batch: 469 , training loss: 4.405039
[INFO] Epoch: 23 , batch: 470 , training loss: 4.207470
[INFO] Epoch: 23 , batch: 471 , training loss: 4.286648
[INFO] Epoch: 23 , batch: 472 , training loss: 4.340879
[INFO] Epoch: 23 , batch: 473 , training loss: 4.273331
[INFO] Epoch: 23 , batch: 474 , training loss: 4.071168
[INFO] Epoch: 23 , batch: 475 , training loss: 3.946436
[INFO] Epoch: 23 , batch: 476 , training loss: 4.323354
[INFO] Epoch: 23 , batch: 477 , training loss: 4.455449
[INFO] Epoch: 23 , batch: 478 , training loss: 4.491511
[INFO] Epoch: 23 , batch: 479 , training loss: 4.444182
[INFO] Epoch: 23 , batch: 480 , training loss: 4.560356
[INFO] Epoch: 23 , batch: 481 , training loss: 4.415823
[INFO] Epoch: 23 , batch: 482 , training loss: 4.546988
[INFO] Epoch: 23 , batch: 483 , training loss: 4.396663
[INFO] Epoch: 23 , batch: 484 , training loss: 4.199243
[INFO] Epoch: 23 , batch: 485 , training loss: 4.298032
[INFO] Epoch: 23 , batch: 486 , training loss: 4.162827
[INFO] Epoch: 23 , batch: 487 , training loss: 4.160617
[INFO] Epoch: 23 , batch: 488 , training loss: 4.359507
[INFO] Epoch: 23 , batch: 489 , training loss: 4.244349
[INFO] Epoch: 23 , batch: 490 , training loss: 4.305120
[INFO] Epoch: 23 , batch: 491 , training loss: 4.253643
[INFO] Epoch: 23 , batch: 492 , training loss: 4.190834
[INFO] Epoch: 23 , batch: 493 , training loss: 4.394920
[INFO] Epoch: 23 , batch: 494 , training loss: 4.296636
[INFO] Epoch: 23 , batch: 495 , training loss: 4.437179
[INFO] Epoch: 23 , batch: 496 , training loss: 4.307119
[INFO] Epoch: 23 , batch: 497 , training loss: 4.342523
[INFO] Epoch: 23 , batch: 498 , training loss: 4.355134
[INFO] Epoch: 23 , batch: 499 , training loss: 4.422095
[INFO] Epoch: 23 , batch: 500 , training loss: 4.574202
[INFO] Epoch: 23 , batch: 501 , training loss: 4.877492
[INFO] Epoch: 23 , batch: 502 , training loss: 4.954903
[INFO] Epoch: 23 , batch: 503 , training loss: 4.671496
[INFO] Epoch: 23 , batch: 504 , training loss: 4.788667
[INFO] Epoch: 23 , batch: 505 , training loss: 4.750838
[INFO] Epoch: 23 , batch: 506 , training loss: 4.712009
[INFO] Epoch: 23 , batch: 507 , training loss: 4.771581
[INFO] Epoch: 23 , batch: 508 , training loss: 4.711407
[INFO] Epoch: 23 , batch: 509 , training loss: 4.503074
[INFO] Epoch: 23 , batch: 510 , training loss: 4.563500
[INFO] Epoch: 23 , batch: 511 , training loss: 4.465736
[INFO] Epoch: 23 , batch: 512 , training loss: 4.555527
[INFO] Epoch: 23 , batch: 513 , training loss: 4.797691
[INFO] Epoch: 23 , batch: 514 , training loss: 4.478362
[INFO] Epoch: 23 , batch: 515 , training loss: 4.681570
[INFO] Epoch: 23 , batch: 516 , training loss: 4.503556
[INFO] Epoch: 23 , batch: 517 , training loss: 4.461744
[INFO] Epoch: 23 , batch: 518 , training loss: 4.445171
[INFO] Epoch: 23 , batch: 519 , training loss: 4.292901
[INFO] Epoch: 23 , batch: 520 , training loss: 4.519702
[INFO] Epoch: 23 , batch: 521 , training loss: 4.496584
[INFO] Epoch: 23 , batch: 522 , training loss: 4.548247
[INFO] Epoch: 23 , batch: 523 , training loss: 4.480914
[INFO] Epoch: 23 , batch: 524 , training loss: 4.749511
[INFO] Epoch: 23 , batch: 525 , training loss: 4.685335
[INFO] Epoch: 23 , batch: 526 , training loss: 4.454979
[INFO] Epoch: 23 , batch: 527 , training loss: 4.477311
[INFO] Epoch: 23 , batch: 528 , training loss: 4.501332
[INFO] Epoch: 23 , batch: 529 , training loss: 4.480345
[INFO] Epoch: 23 , batch: 530 , training loss: 4.315880
[INFO] Epoch: 23 , batch: 531 , training loss: 4.458738
[INFO] Epoch: 23 , batch: 532 , training loss: 4.351257
[INFO] Epoch: 23 , batch: 533 , training loss: 4.514596
[INFO] Epoch: 23 , batch: 534 , training loss: 4.502463
[INFO] Epoch: 23 , batch: 535 , training loss: 4.497779
[INFO] Epoch: 23 , batch: 536 , training loss: 4.335875
[INFO] Epoch: 23 , batch: 537 , training loss: 4.348833
[INFO] Epoch: 23 , batch: 538 , training loss: 4.423764
[INFO] Epoch: 23 , batch: 539 , training loss: 4.524034
[INFO] Epoch: 23 , batch: 540 , training loss: 5.033264
[INFO] Epoch: 23 , batch: 541 , training loss: 4.916147
[INFO] Epoch: 23 , batch: 542 , training loss: 4.824674
[INFO] Epoch: 24 , batch: 0 , training loss: 3.442778
[INFO] Epoch: 24 , batch: 1 , training loss: 3.538242
[INFO] Epoch: 24 , batch: 2 , training loss: 3.645435
[INFO] Epoch: 24 , batch: 3 , training loss: 3.475737
[INFO] Epoch: 24 , batch: 4 , training loss: 3.879200
[INFO] Epoch: 24 , batch: 5 , training loss: 3.508421
[INFO] Epoch: 24 , batch: 6 , training loss: 3.943447
[INFO] Epoch: 24 , batch: 7 , training loss: 3.794391
[INFO] Epoch: 24 , batch: 8 , training loss: 3.527754
[INFO] Epoch: 24 , batch: 9 , training loss: 3.767602
[INFO] Epoch: 24 , batch: 10 , training loss: 3.752607
[INFO] Epoch: 24 , batch: 11 , training loss: 3.701360
[INFO] Epoch: 24 , batch: 12 , training loss: 3.598794
[INFO] Epoch: 24 , batch: 13 , training loss: 3.655897
[INFO] Epoch: 24 , batch: 14 , training loss: 3.496965
[INFO] Epoch: 24 , batch: 15 , training loss: 3.682259
[INFO] Epoch: 24 , batch: 16 , training loss: 3.525017
[INFO] Epoch: 24 , batch: 17 , training loss: 3.702670
[INFO] Epoch: 24 , batch: 18 , training loss: 3.665368
[INFO] Epoch: 24 , batch: 19 , training loss: 3.386543
[INFO] Epoch: 24 , batch: 20 , training loss: 3.368397
[INFO] Epoch: 24 , batch: 21 , training loss: 3.497095
[INFO] Epoch: 24 , batch: 22 , training loss: 3.380507
[INFO] Epoch: 24 , batch: 23 , training loss: 3.625361
[INFO] Epoch: 24 , batch: 24 , training loss: 3.414931
[INFO] Epoch: 24 , batch: 25 , training loss: 3.616201
[INFO] Epoch: 24 , batch: 26 , training loss: 3.455911
[INFO] Epoch: 24 , batch: 27 , training loss: 3.416658
[INFO] Epoch: 24 , batch: 28 , training loss: 3.577674
[INFO] Epoch: 24 , batch: 29 , training loss: 3.425723
[INFO] Epoch: 24 , batch: 30 , training loss: 3.495092
[INFO] Epoch: 24 , batch: 31 , training loss: 3.518392
[INFO] Epoch: 24 , batch: 32 , training loss: 3.522044
[INFO] Epoch: 24 , batch: 33 , training loss: 3.556437
[INFO] Epoch: 24 , batch: 34 , training loss: 3.541111
[INFO] Epoch: 24 , batch: 35 , training loss: 3.528861
[INFO] Epoch: 24 , batch: 36 , training loss: 3.580697
[INFO] Epoch: 24 , batch: 37 , training loss: 3.488569
[INFO] Epoch: 24 , batch: 38 , training loss: 3.503534
[INFO] Epoch: 24 , batch: 39 , training loss: 3.373794
[INFO] Epoch: 24 , batch: 40 , training loss: 3.603795
[INFO] Epoch: 24 , batch: 41 , training loss: 3.495627
[INFO] Epoch: 24 , batch: 42 , training loss: 3.940184
[INFO] Epoch: 24 , batch: 43 , training loss: 3.674015
[INFO] Epoch: 24 , batch: 44 , training loss: 3.996242
[INFO] Epoch: 24 , batch: 45 , training loss: 3.987000
[INFO] Epoch: 24 , batch: 46 , training loss: 3.837120
[INFO] Epoch: 24 , batch: 47 , training loss: 3.613706
[INFO] Epoch: 24 , batch: 48 , training loss: 3.621397
[INFO] Epoch: 24 , batch: 49 , training loss: 3.818364
[INFO] Epoch: 24 , batch: 50 , training loss: 3.616994
[INFO] Epoch: 24 , batch: 51 , training loss: 3.791128
[INFO] Epoch: 24 , batch: 52 , training loss: 3.692288
[INFO] Epoch: 24 , batch: 53 , training loss: 3.822711
[INFO] Epoch: 24 , batch: 54 , training loss: 3.798854
[INFO] Epoch: 24 , batch: 55 , training loss: 3.894460
[INFO] Epoch: 24 , batch: 56 , training loss: 3.740319
[INFO] Epoch: 24 , batch: 57 , training loss: 3.642056
[INFO] Epoch: 24 , batch: 58 , training loss: 3.723175
[INFO] Epoch: 24 , batch: 59 , training loss: 3.792572
[INFO] Epoch: 24 , batch: 60 , training loss: 3.758046
[INFO] Epoch: 24 , batch: 61 , training loss: 3.795507
[INFO] Epoch: 24 , batch: 62 , training loss: 3.696872
[INFO] Epoch: 24 , batch: 63 , training loss: 3.894303
[INFO] Epoch: 24 , batch: 64 , training loss: 4.062856
[INFO] Epoch: 24 , batch: 65 , training loss: 3.777419
[INFO] Epoch: 24 , batch: 66 , training loss: 3.616863
[INFO] Epoch: 24 , batch: 67 , training loss: 3.714954
[INFO] Epoch: 24 , batch: 68 , training loss: 3.828980
[INFO] Epoch: 24 , batch: 69 , training loss: 3.745247
[INFO] Epoch: 24 , batch: 70 , training loss: 3.972064
[INFO] Epoch: 24 , batch: 71 , training loss: 3.846586
[INFO] Epoch: 24 , batch: 72 , training loss: 3.861674
[INFO] Epoch: 24 , batch: 73 , training loss: 3.797081
[INFO] Epoch: 24 , batch: 74 , training loss: 3.927533
[INFO] Epoch: 24 , batch: 75 , training loss: 3.820523
[INFO] Epoch: 24 , batch: 76 , training loss: 3.847529
[INFO] Epoch: 24 , batch: 77 , training loss: 3.862182
[INFO] Epoch: 24 , batch: 78 , training loss: 3.984353
[INFO] Epoch: 24 , batch: 79 , training loss: 3.797531
[INFO] Epoch: 24 , batch: 80 , training loss: 3.987533
[INFO] Epoch: 24 , batch: 81 , training loss: 3.938375
[INFO] Epoch: 24 , batch: 82 , training loss: 3.908467
[INFO] Epoch: 24 , batch: 83 , training loss: 4.018004
[INFO] Epoch: 24 , batch: 84 , training loss: 3.919189
[INFO] Epoch: 24 , batch: 85 , training loss: 4.016837
[INFO] Epoch: 24 , batch: 86 , training loss: 3.941864
[INFO] Epoch: 24 , batch: 87 , training loss: 3.935484
[INFO] Epoch: 24 , batch: 88 , training loss: 4.044249
[INFO] Epoch: 24 , batch: 89 , training loss: 3.872498
[INFO] Epoch: 24 , batch: 90 , training loss: 3.930272
[INFO] Epoch: 24 , batch: 91 , training loss: 3.863473
[INFO] Epoch: 24 , batch: 92 , training loss: 3.868436
[INFO] Epoch: 24 , batch: 93 , training loss: 3.967792
[INFO] Epoch: 24 , batch: 94 , training loss: 4.146649
[INFO] Epoch: 24 , batch: 95 , training loss: 3.891478
[INFO] Epoch: 24 , batch: 96 , training loss: 3.895387
[INFO] Epoch: 24 , batch: 97 , training loss: 3.819848
[INFO] Epoch: 24 , batch: 98 , training loss: 3.754210
[INFO] Epoch: 24 , batch: 99 , training loss: 3.852612
[INFO] Epoch: 24 , batch: 100 , training loss: 3.806589
[INFO] Epoch: 24 , batch: 101 , training loss: 3.862662
[INFO] Epoch: 24 , batch: 102 , training loss: 3.947953
[INFO] Epoch: 24 , batch: 103 , training loss: 3.754204
[INFO] Epoch: 24 , batch: 104 , training loss: 3.716905
[INFO] Epoch: 24 , batch: 105 , training loss: 4.010868
[INFO] Epoch: 24 , batch: 106 , training loss: 3.989556
[INFO] Epoch: 24 , batch: 107 , training loss: 3.827184
[INFO] Epoch: 24 , batch: 108 , training loss: 3.765294
[INFO] Epoch: 24 , batch: 109 , training loss: 3.682343
[INFO] Epoch: 24 , batch: 110 , training loss: 3.865675
[INFO] Epoch: 24 , batch: 111 , training loss: 3.952380
[INFO] Epoch: 24 , batch: 112 , training loss: 3.849984
[INFO] Epoch: 24 , batch: 113 , training loss: 3.859447
[INFO] Epoch: 24 , batch: 114 , training loss: 3.864833
[INFO] Epoch: 24 , batch: 115 , training loss: 3.855747
[INFO] Epoch: 24 , batch: 116 , training loss: 3.780245
[INFO] Epoch: 24 , batch: 117 , training loss: 4.012486
[INFO] Epoch: 24 , batch: 118 , training loss: 3.958439
[INFO] Epoch: 24 , batch: 119 , training loss: 4.096215
[INFO] Epoch: 24 , batch: 120 , training loss: 4.062082
[INFO] Epoch: 24 , batch: 121 , training loss: 3.943633
[INFO] Epoch: 24 , batch: 122 , training loss: 3.831401
[INFO] Epoch: 24 , batch: 123 , training loss: 3.856847
[INFO] Epoch: 24 , batch: 124 , training loss: 3.997868
[INFO] Epoch: 24 , batch: 125 , training loss: 3.791464
[INFO] Epoch: 24 , batch: 126 , training loss: 3.789821
[INFO] Epoch: 24 , batch: 127 , training loss: 3.800321
[INFO] Epoch: 24 , batch: 128 , training loss: 3.934665
[INFO] Epoch: 24 , batch: 129 , training loss: 3.921332
[INFO] Epoch: 24 , batch: 130 , training loss: 3.891566
[INFO] Epoch: 24 , batch: 131 , training loss: 3.892322
[INFO] Epoch: 24 , batch: 132 , training loss: 3.885370
[INFO] Epoch: 24 , batch: 133 , training loss: 3.878357
[INFO] Epoch: 24 , batch: 134 , training loss: 3.670657
[INFO] Epoch: 24 , batch: 135 , training loss: 3.718839
[INFO] Epoch: 24 , batch: 136 , training loss: 4.008291
[INFO] Epoch: 24 , batch: 137 , training loss: 3.905694
[INFO] Epoch: 24 , batch: 138 , training loss: 3.950620
[INFO] Epoch: 24 , batch: 139 , training loss: 4.557621
[INFO] Epoch: 24 , batch: 140 , training loss: 4.309299
[INFO] Epoch: 24 , batch: 141 , training loss: 4.042729
[INFO] Epoch: 24 , batch: 142 , training loss: 3.839909
[INFO] Epoch: 24 , batch: 143 , training loss: 3.972323
[INFO] Epoch: 24 , batch: 144 , training loss: 3.767205
[INFO] Epoch: 24 , batch: 145 , training loss: 3.838673
[INFO] Epoch: 24 , batch: 146 , training loss: 4.041045
[INFO] Epoch: 24 , batch: 147 , training loss: 3.704416
[INFO] Epoch: 24 , batch: 148 , training loss: 3.722600
[INFO] Epoch: 24 , batch: 149 , training loss: 3.811279
[INFO] Epoch: 24 , batch: 150 , training loss: 4.019756
[INFO] Epoch: 24 , batch: 151 , training loss: 3.891586
[INFO] Epoch: 24 , batch: 152 , training loss: 3.966189
[INFO] Epoch: 24 , batch: 153 , training loss: 3.925026
[INFO] Epoch: 24 , batch: 154 , training loss: 4.022560
[INFO] Epoch: 24 , batch: 155 , training loss: 4.237623
[INFO] Epoch: 24 , batch: 156 , training loss: 3.943034
[INFO] Epoch: 24 , batch: 157 , training loss: 3.939991
[INFO] Epoch: 24 , batch: 158 , training loss: 4.093128
[INFO] Epoch: 24 , batch: 159 , training loss: 3.941853
[INFO] Epoch: 24 , batch: 160 , training loss: 4.242059
[INFO] Epoch: 24 , batch: 161 , training loss: 4.386879
[INFO] Epoch: 24 , batch: 162 , training loss: 4.363455
[INFO] Epoch: 24 , batch: 163 , training loss: 4.469220
[INFO] Epoch: 24 , batch: 164 , training loss: 4.456499
[INFO] Epoch: 24 , batch: 165 , training loss: 4.356982
[INFO] Epoch: 24 , batch: 166 , training loss: 4.228593
[INFO] Epoch: 24 , batch: 167 , training loss: 4.318779
[INFO] Epoch: 24 , batch: 168 , training loss: 4.031074
[INFO] Epoch: 24 , batch: 169 , training loss: 4.027354
[INFO] Epoch: 24 , batch: 170 , training loss: 4.162961
[INFO] Epoch: 24 , batch: 171 , training loss: 3.588648
[INFO] Epoch: 24 , batch: 172 , training loss: 3.794105
[INFO] Epoch: 24 , batch: 173 , training loss: 4.141975
[INFO] Epoch: 24 , batch: 174 , training loss: 4.504948
[INFO] Epoch: 24 , batch: 175 , training loss: 4.902280
[INFO] Epoch: 24 , batch: 176 , training loss: 4.534023
[INFO] Epoch: 24 , batch: 177 , training loss: 4.182628
[INFO] Epoch: 24 , batch: 178 , training loss: 4.141974
[INFO] Epoch: 24 , batch: 179 , training loss: 4.199347
[INFO] Epoch: 24 , batch: 180 , training loss: 4.157529
[INFO] Epoch: 24 , batch: 181 , training loss: 4.391123
[INFO] Epoch: 24 , batch: 182 , training loss: 4.362465
[INFO] Epoch: 24 , batch: 183 , training loss: 4.309585
[INFO] Epoch: 24 , batch: 184 , training loss: 4.221346
[INFO] Epoch: 24 , batch: 185 , training loss: 4.175173
[INFO] Epoch: 24 , batch: 186 , training loss: 4.346342
[INFO] Epoch: 24 , batch: 187 , training loss: 4.434647
[INFO] Epoch: 24 , batch: 188 , training loss: 4.434825
[INFO] Epoch: 24 , batch: 189 , training loss: 4.314362
[INFO] Epoch: 24 , batch: 190 , training loss: 4.340017
[INFO] Epoch: 24 , batch: 191 , training loss: 4.462317
[INFO] Epoch: 24 , batch: 192 , training loss: 4.258564
[INFO] Epoch: 24 , batch: 193 , training loss: 4.377512
[INFO] Epoch: 24 , batch: 194 , training loss: 4.322965
[INFO] Epoch: 24 , batch: 195 , training loss: 4.223590
[INFO] Epoch: 24 , batch: 196 , training loss: 4.123648
[INFO] Epoch: 24 , batch: 197 , training loss: 4.190167
[INFO] Epoch: 24 , batch: 198 , training loss: 4.132319
[INFO] Epoch: 24 , batch: 199 , training loss: 4.263751
[INFO] Epoch: 24 , batch: 200 , training loss: 4.136801
[INFO] Epoch: 24 , batch: 201 , training loss: 4.047968
[INFO] Epoch: 24 , batch: 202 , training loss: 4.032929
[INFO] Epoch: 24 , batch: 203 , training loss: 4.139226
[INFO] Epoch: 24 , batch: 204 , training loss: 4.291670
[INFO] Epoch: 24 , batch: 205 , training loss: 3.872464
[INFO] Epoch: 24 , batch: 206 , training loss: 3.815105
[INFO] Epoch: 24 , batch: 207 , training loss: 3.806219
[INFO] Epoch: 24 , batch: 208 , training loss: 4.128399
[INFO] Epoch: 24 , batch: 209 , training loss: 4.072383
[INFO] Epoch: 24 , batch: 210 , training loss: 4.072690
[INFO] Epoch: 24 , batch: 211 , training loss: 4.077420
[INFO] Epoch: 24 , batch: 212 , training loss: 4.184880
[INFO] Epoch: 24 , batch: 213 , training loss: 4.149732
[INFO] Epoch: 24 , batch: 214 , training loss: 4.248783
[INFO] Epoch: 24 , batch: 215 , training loss: 4.464762
[INFO] Epoch: 24 , batch: 216 , training loss: 4.157255
[INFO] Epoch: 24 , batch: 217 , training loss: 4.097315
[INFO] Epoch: 24 , batch: 218 , training loss: 4.071822
[INFO] Epoch: 24 , batch: 219 , training loss: 4.197104
[INFO] Epoch: 24 , batch: 220 , training loss: 3.987137
[INFO] Epoch: 24 , batch: 221 , training loss: 4.044361
[INFO] Epoch: 24 , batch: 222 , training loss: 4.156706
[INFO] Epoch: 24 , batch: 223 , training loss: 4.242431
[INFO] Epoch: 24 , batch: 224 , training loss: 4.313384
[INFO] Epoch: 24 , batch: 225 , training loss: 4.185537
[INFO] Epoch: 24 , batch: 226 , training loss: 4.315492
[INFO] Epoch: 24 , batch: 227 , training loss: 4.283854
[INFO] Epoch: 24 , batch: 228 , training loss: 4.324370
[INFO] Epoch: 24 , batch: 229 , training loss: 4.182920
[INFO] Epoch: 24 , batch: 230 , training loss: 4.033014
[INFO] Epoch: 24 , batch: 231 , training loss: 3.896401
[INFO] Epoch: 24 , batch: 232 , training loss: 4.050880
[INFO] Epoch: 24 , batch: 233 , training loss: 4.049424
[INFO] Epoch: 24 , batch: 234 , training loss: 3.769990
[INFO] Epoch: 24 , batch: 235 , training loss: 3.861392
[INFO] Epoch: 24 , batch: 236 , training loss: 3.982229
[INFO] Epoch: 24 , batch: 237 , training loss: 4.195764
[INFO] Epoch: 24 , batch: 238 , training loss: 3.952904
[INFO] Epoch: 24 , batch: 239 , training loss: 4.005881
[INFO] Epoch: 24 , batch: 240 , training loss: 4.065250
[INFO] Epoch: 24 , batch: 241 , training loss: 3.856192
[INFO] Epoch: 24 , batch: 242 , training loss: 3.880504
[INFO] Epoch: 24 , batch: 243 , training loss: 4.183614
[INFO] Epoch: 24 , batch: 244 , training loss: 4.099628
[INFO] Epoch: 24 , batch: 245 , training loss: 4.091608
[INFO] Epoch: 24 , batch: 246 , training loss: 3.794240
[INFO] Epoch: 24 , batch: 247 , training loss: 3.979217
[INFO] Epoch: 24 , batch: 248 , training loss: 4.038694
[INFO] Epoch: 24 , batch: 249 , training loss: 4.041281
[INFO] Epoch: 24 , batch: 250 , training loss: 3.806232
[INFO] Epoch: 24 , batch: 251 , training loss: 4.263739
[INFO] Epoch: 24 , batch: 252 , training loss: 3.938009
[INFO] Epoch: 24 , batch: 253 , training loss: 3.897313
[INFO] Epoch: 24 , batch: 254 , training loss: 4.155668
[INFO] Epoch: 24 , batch: 255 , training loss: 4.120770
[INFO] Epoch: 24 , batch: 256 , training loss: 4.146195
[INFO] Epoch: 24 , batch: 257 , training loss: 4.275772
[INFO] Epoch: 24 , batch: 258 , training loss: 4.338262
[INFO] Epoch: 24 , batch: 259 , training loss: 4.398893
[INFO] Epoch: 24 , batch: 260 , training loss: 4.101192
[INFO] Epoch: 24 , batch: 261 , training loss: 4.253073
[INFO] Epoch: 24 , batch: 262 , training loss: 4.428555
[INFO] Epoch: 24 , batch: 263 , training loss: 4.609377
[INFO] Epoch: 24 , batch: 264 , training loss: 3.923160
[INFO] Epoch: 24 , batch: 265 , training loss: 4.064771
[INFO] Epoch: 24 , batch: 266 , training loss: 4.479083
[INFO] Epoch: 24 , batch: 267 , training loss: 4.237617
[INFO] Epoch: 24 , batch: 268 , training loss: 4.157063
[INFO] Epoch: 24 , batch: 269 , training loss: 4.120946
[INFO] Epoch: 24 , batch: 270 , training loss: 4.147941
[INFO] Epoch: 24 , batch: 271 , training loss: 4.197863
[INFO] Epoch: 24 , batch: 272 , training loss: 4.172907
[INFO] Epoch: 24 , batch: 273 , training loss: 4.190953
[INFO] Epoch: 24 , batch: 274 , training loss: 4.269358
[INFO] Epoch: 24 , batch: 275 , training loss: 4.169138
[INFO] Epoch: 24 , batch: 276 , training loss: 4.194365
[INFO] Epoch: 24 , batch: 277 , training loss: 4.357836
[INFO] Epoch: 24 , batch: 278 , training loss: 4.032838
[INFO] Epoch: 24 , batch: 279 , training loss: 4.022980
[INFO] Epoch: 24 , batch: 280 , training loss: 3.981501
[INFO] Epoch: 24 , batch: 281 , training loss: 4.127517
[INFO] Epoch: 24 , batch: 282 , training loss: 4.043410
[INFO] Epoch: 24 , batch: 283 , training loss: 4.063303
[INFO] Epoch: 24 , batch: 284 , training loss: 4.084045
[INFO] Epoch: 24 , batch: 285 , training loss: 4.030355
[INFO] Epoch: 24 , batch: 286 , training loss: 4.018175
[INFO] Epoch: 24 , batch: 287 , training loss: 3.964419
[INFO] Epoch: 24 , batch: 288 , training loss: 3.977970
[INFO] Epoch: 24 , batch: 289 , training loss: 4.025095
[INFO] Epoch: 24 , batch: 290 , training loss: 3.816438
[INFO] Epoch: 24 , batch: 291 , training loss: 3.790599
[INFO] Epoch: 24 , batch: 292 , training loss: 3.899048
[INFO] Epoch: 24 , batch: 293 , training loss: 3.806915
[INFO] Epoch: 24 , batch: 294 , training loss: 4.497783
[INFO] Epoch: 24 , batch: 295 , training loss: 4.244966
[INFO] Epoch: 24 , batch: 296 , training loss: 4.193515
[INFO] Epoch: 24 , batch: 297 , training loss: 4.143901
[INFO] Epoch: 24 , batch: 298 , training loss: 3.976518
[INFO] Epoch: 24 , batch: 299 , training loss: 3.997373
[INFO] Epoch: 24 , batch: 300 , training loss: 3.971244
[INFO] Epoch: 24 , batch: 301 , training loss: 3.908069
[INFO] Epoch: 24 , batch: 302 , training loss: 4.081398
[INFO] Epoch: 24 , batch: 303 , training loss: 4.083395
[INFO] Epoch: 24 , batch: 304 , training loss: 4.248416
[INFO] Epoch: 24 , batch: 305 , training loss: 4.056365
[INFO] Epoch: 24 , batch: 306 , training loss: 4.195563
[INFO] Epoch: 24 , batch: 307 , training loss: 4.181010
[INFO] Epoch: 24 , batch: 308 , training loss: 4.025629
[INFO] Epoch: 24 , batch: 309 , training loss: 4.028351
[INFO] Epoch: 24 , batch: 310 , training loss: 3.919271
[INFO] Epoch: 24 , batch: 311 , training loss: 3.926726
[INFO] Epoch: 24 , batch: 312 , training loss: 3.847034
[INFO] Epoch: 24 , batch: 313 , training loss: 3.964113
[INFO] Epoch: 24 , batch: 314 , training loss: 4.036077
[INFO] Epoch: 24 , batch: 315 , training loss: 4.062632
[INFO] Epoch: 24 , batch: 316 , training loss: 4.304400
[INFO] Epoch: 24 , batch: 317 , training loss: 4.755501
[INFO] Epoch: 24 , batch: 318 , training loss: 4.915826
[INFO] Epoch: 24 , batch: 319 , training loss: 4.484375
[INFO] Epoch: 24 , batch: 320 , training loss: 4.040784
[INFO] Epoch: 24 , batch: 321 , training loss: 3.859481
[INFO] Epoch: 24 , batch: 322 , training loss: 3.990548
[INFO] Epoch: 24 , batch: 323 , training loss: 4.008582
[INFO] Epoch: 24 , batch: 324 , training loss: 3.956522
[INFO] Epoch: 24 , batch: 325 , training loss: 4.131144
[INFO] Epoch: 24 , batch: 326 , training loss: 4.181933
[INFO] Epoch: 24 , batch: 327 , training loss: 4.090285
[INFO] Epoch: 24 , batch: 328 , training loss: 4.062594
[INFO] Epoch: 24 , batch: 329 , training loss: 3.989887
[INFO] Epoch: 24 , batch: 330 , training loss: 3.996104
[INFO] Epoch: 24 , batch: 331 , training loss: 4.142562
[INFO] Epoch: 24 , batch: 332 , training loss: 3.949681
[INFO] Epoch: 24 , batch: 333 , training loss: 3.967705
[INFO] Epoch: 24 , batch: 334 , training loss: 3.979962
[INFO] Epoch: 24 , batch: 335 , training loss: 4.085638
[INFO] Epoch: 24 , batch: 336 , training loss: 4.122997
[INFO] Epoch: 24 , batch: 337 , training loss: 4.140917
[INFO] Epoch: 24 , batch: 338 , training loss: 4.345330
[INFO] Epoch: 24 , batch: 339 , training loss: 4.222164
[INFO] Epoch: 24 , batch: 340 , training loss: 4.378869
[INFO] Epoch: 24 , batch: 341 , training loss: 4.131017
[INFO] Epoch: 24 , batch: 342 , training loss: 3.908540
[INFO] Epoch: 24 , batch: 343 , training loss: 3.984175
[INFO] Epoch: 24 , batch: 344 , training loss: 3.844885
[INFO] Epoch: 24 , batch: 345 , training loss: 3.968719
[INFO] Epoch: 24 , batch: 346 , training loss: 4.025293
[INFO] Epoch: 24 , batch: 347 , training loss: 3.924869
[INFO] Epoch: 24 , batch: 348 , training loss: 4.065390
[INFO] Epoch: 24 , batch: 349 , training loss: 4.187400
[INFO] Epoch: 24 , batch: 350 , training loss: 3.969649
[INFO] Epoch: 24 , batch: 351 , training loss: 4.047628
[INFO] Epoch: 24 , batch: 352 , training loss: 4.064636
[INFO] Epoch: 24 , batch: 353 , training loss: 4.020905
[INFO] Epoch: 24 , batch: 354 , training loss: 4.137997
[INFO] Epoch: 24 , batch: 355 , training loss: 4.189106
[INFO] Epoch: 24 , batch: 356 , training loss: 4.011539
[INFO] Epoch: 24 , batch: 357 , training loss: 4.101030
[INFO] Epoch: 24 , batch: 358 , training loss: 3.988396
[INFO] Epoch: 24 , batch: 359 , training loss: 3.997863
[INFO] Epoch: 24 , batch: 360 , training loss: 4.089990
[INFO] Epoch: 24 , batch: 361 , training loss: 4.048702
[INFO] Epoch: 24 , batch: 362 , training loss: 4.151988
[INFO] Epoch: 24 , batch: 363 , training loss: 4.037834
[INFO] Epoch: 24 , batch: 364 , training loss: 4.085731
[INFO] Epoch: 24 , batch: 365 , training loss: 4.021878
[INFO] Epoch: 24 , batch: 366 , training loss: 4.133003
[INFO] Epoch: 24 , batch: 367 , training loss: 4.222946
[INFO] Epoch: 24 , batch: 368 , training loss: 4.636102
[INFO] Epoch: 24 , batch: 369 , training loss: 4.276100
[INFO] Epoch: 24 , batch: 370 , training loss: 4.049761
[INFO] Epoch: 24 , batch: 371 , training loss: 4.445012
[INFO] Epoch: 24 , batch: 372 , training loss: 4.751867
[INFO] Epoch: 24 , batch: 373 , training loss: 4.818162
[INFO] Epoch: 24 , batch: 374 , training loss: 4.886547
[INFO] Epoch: 24 , batch: 375 , training loss: 4.863269
[INFO] Epoch: 24 , batch: 376 , training loss: 4.810374
[INFO] Epoch: 24 , batch: 377 , training loss: 4.560298
[INFO] Epoch: 24 , batch: 378 , training loss: 4.643251
[INFO] Epoch: 24 , batch: 379 , training loss: 4.614579
[INFO] Epoch: 24 , batch: 380 , training loss: 4.746390
[INFO] Epoch: 24 , batch: 381 , training loss: 4.488092
[INFO] Epoch: 24 , batch: 382 , training loss: 4.702884
[INFO] Epoch: 24 , batch: 383 , training loss: 4.732242
[INFO] Epoch: 24 , batch: 384 , training loss: 4.725623
[INFO] Epoch: 24 , batch: 385 , training loss: 4.472732
[INFO] Epoch: 24 , batch: 386 , training loss: 4.705852
[INFO] Epoch: 24 , batch: 387 , training loss: 4.661816
[INFO] Epoch: 24 , batch: 388 , training loss: 4.479688
[INFO] Epoch: 24 , batch: 389 , training loss: 4.331052
[INFO] Epoch: 24 , batch: 390 , training loss: 4.326661
[INFO] Epoch: 24 , batch: 391 , training loss: 4.357624
[INFO] Epoch: 24 , batch: 392 , training loss: 4.728870
[INFO] Epoch: 24 , batch: 393 , training loss: 4.600407
[INFO] Epoch: 24 , batch: 394 , training loss: 4.623730
[INFO] Epoch: 24 , batch: 395 , training loss: 4.501826
[INFO] Epoch: 24 , batch: 396 , training loss: 4.263300
[INFO] Epoch: 24 , batch: 397 , training loss: 4.444821
[INFO] Epoch: 24 , batch: 398 , training loss: 4.276429
[INFO] Epoch: 24 , batch: 399 , training loss: 4.362364
[INFO] Epoch: 24 , batch: 400 , training loss: 4.320726
[INFO] Epoch: 24 , batch: 401 , training loss: 4.729815
[INFO] Epoch: 24 , batch: 402 , training loss: 4.467418
[INFO] Epoch: 24 , batch: 403 , training loss: 4.266945
[INFO] Epoch: 24 , batch: 404 , training loss: 4.458702
[INFO] Epoch: 24 , batch: 405 , training loss: 4.504178
[INFO] Epoch: 24 , batch: 406 , training loss: 4.404862
[INFO] Epoch: 24 , batch: 407 , training loss: 4.461703
[INFO] Epoch: 24 , batch: 408 , training loss: 4.432939
[INFO] Epoch: 24 , batch: 409 , training loss: 4.431720
[INFO] Epoch: 24 , batch: 410 , training loss: 4.458957
[INFO] Epoch: 24 , batch: 411 , training loss: 4.644575
[INFO] Epoch: 24 , batch: 412 , training loss: 4.509737
[INFO] Epoch: 24 , batch: 413 , training loss: 4.388160
[INFO] Epoch: 24 , batch: 414 , training loss: 4.407190
[INFO] Epoch: 24 , batch: 415 , training loss: 4.418356
[INFO] Epoch: 24 , batch: 416 , training loss: 4.516867
[INFO] Epoch: 24 , batch: 417 , training loss: 4.425504
[INFO] Epoch: 24 , batch: 418 , training loss: 4.436000
[INFO] Epoch: 24 , batch: 419 , training loss: 4.399529
[INFO] Epoch: 24 , batch: 420 , training loss: 4.387336
[INFO] Epoch: 24 , batch: 421 , training loss: 4.386078
[INFO] Epoch: 24 , batch: 422 , training loss: 4.240509
[INFO] Epoch: 24 , batch: 423 , training loss: 4.455864
[INFO] Epoch: 24 , batch: 424 , training loss: 4.641372
[INFO] Epoch: 24 , batch: 425 , training loss: 4.489256
[INFO] Epoch: 24 , batch: 426 , training loss: 4.229851
[INFO] Epoch: 24 , batch: 427 , training loss: 4.455665
[INFO] Epoch: 24 , batch: 428 , training loss: 4.336532
[INFO] Epoch: 24 , batch: 429 , training loss: 4.201082
[INFO] Epoch: 24 , batch: 430 , training loss: 4.469234
[INFO] Epoch: 24 , batch: 431 , training loss: 4.062306
[INFO] Epoch: 24 , batch: 432 , training loss: 4.126947
[INFO] Epoch: 24 , batch: 433 , training loss: 4.185467
[INFO] Epoch: 24 , batch: 434 , training loss: 4.028512
[INFO] Epoch: 24 , batch: 435 , training loss: 4.397793
[INFO] Epoch: 24 , batch: 436 , training loss: 4.478549
[INFO] Epoch: 24 , batch: 437 , training loss: 4.229162
[INFO] Epoch: 24 , batch: 438 , training loss: 4.067728
[INFO] Epoch: 24 , batch: 439 , training loss: 4.301444
[INFO] Epoch: 24 , batch: 440 , training loss: 4.418619
[INFO] Epoch: 24 , batch: 441 , training loss: 4.539315
[INFO] Epoch: 24 , batch: 442 , training loss: 4.298057
[INFO] Epoch: 24 , batch: 443 , training loss: 4.499786
[INFO] Epoch: 24 , batch: 444 , training loss: 4.085546
[INFO] Epoch: 24 , batch: 445 , training loss: 4.009332
[INFO] Epoch: 24 , batch: 446 , training loss: 3.957508
[INFO] Epoch: 24 , batch: 447 , training loss: 4.138252
[INFO] Epoch: 24 , batch: 448 , training loss: 4.247027
[INFO] Epoch: 24 , batch: 449 , training loss: 4.654344
[INFO] Epoch: 24 , batch: 450 , training loss: 4.683489
[INFO] Epoch: 24 , batch: 451 , training loss: 4.609156
[INFO] Epoch: 24 , batch: 452 , training loss: 4.409516
[INFO] Epoch: 24 , batch: 453 , training loss: 4.171182
[INFO] Epoch: 24 , batch: 454 , training loss: 4.322014
[INFO] Epoch: 24 , batch: 455 , training loss: 4.353982
[INFO] Epoch: 24 , batch: 456 , training loss: 4.375599
[INFO] Epoch: 24 , batch: 457 , training loss: 4.462733
[INFO] Epoch: 24 , batch: 458 , training loss: 4.192235
[INFO] Epoch: 24 , batch: 459 , training loss: 4.165284
[INFO] Epoch: 24 , batch: 460 , training loss: 4.260661
[INFO] Epoch: 24 , batch: 461 , training loss: 4.242824
[INFO] Epoch: 24 , batch: 462 , training loss: 4.312932
[INFO] Epoch: 24 , batch: 463 , training loss: 4.210037
[INFO] Epoch: 24 , batch: 464 , training loss: 4.406534
[INFO] Epoch: 24 , batch: 465 , training loss: 4.341939
[INFO] Epoch: 24 , batch: 466 , training loss: 4.425065
[INFO] Epoch: 24 , batch: 467 , training loss: 4.407778
[INFO] Epoch: 24 , batch: 468 , training loss: 4.360726
[INFO] Epoch: 24 , batch: 469 , training loss: 4.387693
[INFO] Epoch: 24 , batch: 470 , training loss: 4.199793
[INFO] Epoch: 24 , batch: 471 , training loss: 4.291157
[INFO] Epoch: 24 , batch: 472 , training loss: 4.349675
[INFO] Epoch: 24 , batch: 473 , training loss: 4.264325
[INFO] Epoch: 24 , batch: 474 , training loss: 4.068288
[INFO] Epoch: 24 , batch: 475 , training loss: 3.933150
[INFO] Epoch: 24 , batch: 476 , training loss: 4.319448
[INFO] Epoch: 24 , batch: 477 , training loss: 4.451077
[INFO] Epoch: 24 , batch: 478 , training loss: 4.492027
[INFO] Epoch: 24 , batch: 479 , training loss: 4.442012
[INFO] Epoch: 24 , batch: 480 , training loss: 4.546384
[INFO] Epoch: 24 , batch: 481 , training loss: 4.409551
[INFO] Epoch: 24 , batch: 482 , training loss: 4.551346
[INFO] Epoch: 24 , batch: 483 , training loss: 4.388135
[INFO] Epoch: 24 , batch: 484 , training loss: 4.192466
[INFO] Epoch: 24 , batch: 485 , training loss: 4.292370
[INFO] Epoch: 24 , batch: 486 , training loss: 4.163130
[INFO] Epoch: 24 , batch: 487 , training loss: 4.153722
[INFO] Epoch: 24 , batch: 488 , training loss: 4.358035
[INFO] Epoch: 24 , batch: 489 , training loss: 4.244438
[INFO] Epoch: 24 , batch: 490 , training loss: 4.294605
[INFO] Epoch: 24 , batch: 491 , training loss: 4.231643
[INFO] Epoch: 24 , batch: 492 , training loss: 4.185569
[INFO] Epoch: 24 , batch: 493 , training loss: 4.396780
[INFO] Epoch: 24 , batch: 494 , training loss: 4.306425
[INFO] Epoch: 24 , batch: 495 , training loss: 4.421322
[INFO] Epoch: 24 , batch: 496 , training loss: 4.298272
[INFO] Epoch: 24 , batch: 497 , training loss: 4.343147
[INFO] Epoch: 24 , batch: 498 , training loss: 4.346078
[INFO] Epoch: 24 , batch: 499 , training loss: 4.415096
[INFO] Epoch: 24 , batch: 500 , training loss: 4.575883
[INFO] Epoch: 24 , batch: 501 , training loss: 4.877179
[INFO] Epoch: 24 , batch: 502 , training loss: 4.950748
[INFO] Epoch: 24 , batch: 503 , training loss: 4.660860
[INFO] Epoch: 24 , batch: 504 , training loss: 4.780525
[INFO] Epoch: 24 , batch: 505 , training loss: 4.757411
[INFO] Epoch: 24 , batch: 506 , training loss: 4.723289
[INFO] Epoch: 24 , batch: 507 , training loss: 4.778090
[INFO] Epoch: 24 , batch: 508 , training loss: 4.702556
[INFO] Epoch: 24 , batch: 509 , training loss: 4.491106
[INFO] Epoch: 24 , batch: 510 , training loss: 4.564786
[INFO] Epoch: 24 , batch: 511 , training loss: 4.481035
[INFO] Epoch: 24 , batch: 512 , training loss: 4.566440
[INFO] Epoch: 24 , batch: 513 , training loss: 4.809632
[INFO] Epoch: 24 , batch: 514 , training loss: 4.475487
[INFO] Epoch: 24 , batch: 515 , training loss: 4.694453
[INFO] Epoch: 24 , batch: 516 , training loss: 4.521738
[INFO] Epoch: 24 , batch: 517 , training loss: 4.469754
[INFO] Epoch: 24 , batch: 518 , training loss: 4.446593
[INFO] Epoch: 24 , batch: 519 , training loss: 4.295332
[INFO] Epoch: 24 , batch: 520 , training loss: 4.524719
[INFO] Epoch: 24 , batch: 521 , training loss: 4.506552
[INFO] Epoch: 24 , batch: 522 , training loss: 4.563619
[INFO] Epoch: 24 , batch: 523 , training loss: 4.482058
[INFO] Epoch: 24 , batch: 524 , training loss: 4.760128
[INFO] Epoch: 24 , batch: 525 , training loss: 4.675168
[INFO] Epoch: 24 , batch: 526 , training loss: 4.444285
[INFO] Epoch: 24 , batch: 527 , training loss: 4.486217
[INFO] Epoch: 24 , batch: 528 , training loss: 4.511311
[INFO] Epoch: 24 , batch: 529 , training loss: 4.475780
[INFO] Epoch: 24 , batch: 530 , training loss: 4.326696
[INFO] Epoch: 24 , batch: 531 , training loss: 4.456399
[INFO] Epoch: 24 , batch: 532 , training loss: 4.358611
[INFO] Epoch: 24 , batch: 533 , training loss: 4.512409
[INFO] Epoch: 24 , batch: 534 , training loss: 4.500571
[INFO] Epoch: 24 , batch: 535 , training loss: 4.514052
[INFO] Epoch: 24 , batch: 536 , training loss: 4.329621
[INFO] Epoch: 24 , batch: 537 , training loss: 4.344283
[INFO] Epoch: 24 , batch: 538 , training loss: 4.423541
[INFO] Epoch: 24 , batch: 539 , training loss: 4.526313
[INFO] Epoch: 24 , batch: 540 , training loss: 5.016872
[INFO] Epoch: 24 , batch: 541 , training loss: 4.940693
[INFO] Epoch: 24 , batch: 542 , training loss: 4.831867
[INFO] Epoch: 25 , batch: 0 , training loss: 3.416558
[INFO] Epoch: 25 , batch: 1 , training loss: 3.509607
[INFO] Epoch: 25 , batch: 2 , training loss: 3.611799
[INFO] Epoch: 25 , batch: 3 , training loss: 3.451436
[INFO] Epoch: 25 , batch: 4 , training loss: 3.862441
[INFO] Epoch: 25 , batch: 5 , training loss: 3.498816
[INFO] Epoch: 25 , batch: 6 , training loss: 3.935005
[INFO] Epoch: 25 , batch: 7 , training loss: 3.824951
[INFO] Epoch: 25 , batch: 8 , training loss: 3.521230
[INFO] Epoch: 25 , batch: 9 , training loss: 3.761055
[INFO] Epoch: 25 , batch: 10 , training loss: 3.766895
[INFO] Epoch: 25 , batch: 11 , training loss: 3.720410
[INFO] Epoch: 25 , batch: 12 , training loss: 3.612210
[INFO] Epoch: 25 , batch: 13 , training loss: 3.663030
[INFO] Epoch: 25 , batch: 14 , training loss: 3.517439
[INFO] Epoch: 25 , batch: 15 , training loss: 3.693327
[INFO] Epoch: 25 , batch: 16 , training loss: 3.551548
[INFO] Epoch: 25 , batch: 17 , training loss: 3.757520
[INFO] Epoch: 25 , batch: 18 , training loss: 3.687187
[INFO] Epoch: 25 , batch: 19 , training loss: 3.382062
[INFO] Epoch: 25 , batch: 20 , training loss: 3.354783
[INFO] Epoch: 25 , batch: 21 , training loss: 3.526530
[INFO] Epoch: 25 , batch: 22 , training loss: 3.406600
[INFO] Epoch: 25 , batch: 23 , training loss: 3.644736
[INFO] Epoch: 25 , batch: 24 , training loss: 3.424871
[INFO] Epoch: 25 , batch: 25 , training loss: 3.612205
[INFO] Epoch: 25 , batch: 26 , training loss: 3.477973
[INFO] Epoch: 25 , batch: 27 , training loss: 3.422548
[INFO] Epoch: 25 , batch: 28 , training loss: 3.578920
[INFO] Epoch: 25 , batch: 29 , training loss: 3.405188
[INFO] Epoch: 25 , batch: 30 , training loss: 3.489884
[INFO] Epoch: 25 , batch: 31 , training loss: 3.538402
[INFO] Epoch: 25 , batch: 32 , training loss: 3.528192
[INFO] Epoch: 25 , batch: 33 , training loss: 3.573305
[INFO] Epoch: 25 , batch: 34 , training loss: 3.540926
[INFO] Epoch: 25 , batch: 35 , training loss: 3.544930
[INFO] Epoch: 25 , batch: 36 , training loss: 3.580547
[INFO] Epoch: 25 , batch: 37 , training loss: 3.468771
[INFO] Epoch: 25 , batch: 38 , training loss: 3.529872
[INFO] Epoch: 25 , batch: 39 , training loss: 3.384695
[INFO] Epoch: 25 , batch: 40 , training loss: 3.601292
[INFO] Epoch: 25 , batch: 41 , training loss: 3.523965
[INFO] Epoch: 25 , batch: 42 , training loss: 3.914521
[INFO] Epoch: 25 , batch: 43 , training loss: 3.680643
[INFO] Epoch: 25 , batch: 44 , training loss: 4.016828
[INFO] Epoch: 25 , batch: 45 , training loss: 3.934337
[INFO] Epoch: 25 , batch: 46 , training loss: 3.842506
[INFO] Epoch: 25 , batch: 47 , training loss: 3.655483
[INFO] Epoch: 25 , batch: 48 , training loss: 3.643461
[INFO] Epoch: 25 , batch: 49 , training loss: 3.775019
[INFO] Epoch: 25 , batch: 50 , training loss: 3.616969
[INFO] Epoch: 25 , batch: 51 , training loss: 3.838332
[INFO] Epoch: 25 , batch: 52 , training loss: 3.656239
[INFO] Epoch: 25 , batch: 53 , training loss: 3.807096
[INFO] Epoch: 25 , batch: 54 , training loss: 3.780951
[INFO] Epoch: 25 , batch: 55 , training loss: 3.853872
[INFO] Epoch: 25 , batch: 56 , training loss: 3.732144
[INFO] Epoch: 25 , batch: 57 , training loss: 3.623989
[INFO] Epoch: 25 , batch: 58 , training loss: 3.709737
[INFO] Epoch: 25 , batch: 59 , training loss: 3.767968
[INFO] Epoch: 25 , batch: 60 , training loss: 3.739885
[INFO] Epoch: 25 , batch: 61 , training loss: 3.779042
[INFO] Epoch: 25 , batch: 62 , training loss: 3.701354
[INFO] Epoch: 25 , batch: 63 , training loss: 3.890467
[INFO] Epoch: 25 , batch: 64 , training loss: 4.063918
[INFO] Epoch: 25 , batch: 65 , training loss: 3.782907
[INFO] Epoch: 25 , batch: 66 , training loss: 3.644809
[INFO] Epoch: 25 , batch: 67 , training loss: 3.702044
[INFO] Epoch: 25 , batch: 68 , training loss: 3.850804
[INFO] Epoch: 25 , batch: 69 , training loss: 3.729197
[INFO] Epoch: 25 , batch: 70 , training loss: 3.929963
[INFO] Epoch: 25 , batch: 71 , training loss: 3.838694
[INFO] Epoch: 25 , batch: 72 , training loss: 3.846609
[INFO] Epoch: 25 , batch: 73 , training loss: 3.804154
[INFO] Epoch: 25 , batch: 74 , training loss: 3.937153
[INFO] Epoch: 25 , batch: 75 , training loss: 3.830525
[INFO] Epoch: 25 , batch: 76 , training loss: 3.858176
[INFO] Epoch: 25 , batch: 77 , training loss: 3.860158
[INFO] Epoch: 25 , batch: 78 , training loss: 3.978835
[INFO] Epoch: 25 , batch: 79 , training loss: 3.783416
[INFO] Epoch: 25 , batch: 80 , training loss: 3.985251
[INFO] Epoch: 25 , batch: 81 , training loss: 3.936563
[INFO] Epoch: 25 , batch: 82 , training loss: 3.890820
[INFO] Epoch: 25 , batch: 83 , training loss: 4.000388
[INFO] Epoch: 25 , batch: 84 , training loss: 3.905283
[INFO] Epoch: 25 , batch: 85 , training loss: 4.032177
[INFO] Epoch: 25 , batch: 86 , training loss: 3.954344
[INFO] Epoch: 25 , batch: 87 , training loss: 3.935825
[INFO] Epoch: 25 , batch: 88 , training loss: 4.058786
[INFO] Epoch: 25 , batch: 89 , training loss: 3.855247
[INFO] Epoch: 25 , batch: 90 , training loss: 3.911863
[INFO] Epoch: 25 , batch: 91 , training loss: 3.887104
[INFO] Epoch: 25 , batch: 92 , training loss: 3.847142
[INFO] Epoch: 25 , batch: 93 , training loss: 3.984616
[INFO] Epoch: 25 , batch: 94 , training loss: 4.160100
[INFO] Epoch: 25 , batch: 95 , training loss: 3.892115
[INFO] Epoch: 25 , batch: 96 , training loss: 3.911780
[INFO] Epoch: 25 , batch: 97 , training loss: 3.815796
[INFO] Epoch: 25 , batch: 98 , training loss: 3.772333
[INFO] Epoch: 25 , batch: 99 , training loss: 3.861579
[INFO] Epoch: 25 , batch: 100 , training loss: 3.786978
[INFO] Epoch: 25 , batch: 101 , training loss: 3.847651
[INFO] Epoch: 25 , batch: 102 , training loss: 3.942278
[INFO] Epoch: 25 , batch: 103 , training loss: 3.749055
[INFO] Epoch: 25 , batch: 104 , training loss: 3.725897
[INFO] Epoch: 25 , batch: 105 , training loss: 3.986479
[INFO] Epoch: 25 , batch: 106 , training loss: 3.972832
[INFO] Epoch: 25 , batch: 107 , training loss: 3.792760
[INFO] Epoch: 25 , batch: 108 , training loss: 3.767375
[INFO] Epoch: 25 , batch: 109 , training loss: 3.692986
[INFO] Epoch: 25 , batch: 110 , training loss: 3.863955
[INFO] Epoch: 25 , batch: 111 , training loss: 3.931999
[INFO] Epoch: 25 , batch: 112 , training loss: 3.848744
[INFO] Epoch: 25 , batch: 113 , training loss: 3.844669
[INFO] Epoch: 25 , batch: 114 , training loss: 3.857985
[INFO] Epoch: 25 , batch: 115 , training loss: 3.869964
[INFO] Epoch: 25 , batch: 116 , training loss: 3.778167
[INFO] Epoch: 25 , batch: 117 , training loss: 3.999062
[INFO] Epoch: 25 , batch: 118 , training loss: 3.965112
[INFO] Epoch: 25 , batch: 119 , training loss: 4.089905
[INFO] Epoch: 25 , batch: 120 , training loss: 4.075552
[INFO] Epoch: 25 , batch: 121 , training loss: 3.928729
[INFO] Epoch: 25 , batch: 122 , training loss: 3.808167
[INFO] Epoch: 25 , batch: 123 , training loss: 3.851743
[INFO] Epoch: 25 , batch: 124 , training loss: 3.962420
[INFO] Epoch: 25 , batch: 125 , training loss: 3.794517
[INFO] Epoch: 25 , batch: 126 , training loss: 3.773149
[INFO] Epoch: 25 , batch: 127 , training loss: 3.771389
[INFO] Epoch: 25 , batch: 128 , training loss: 3.917308
[INFO] Epoch: 25 , batch: 129 , training loss: 3.895662
[INFO] Epoch: 25 , batch: 130 , training loss: 3.871534
[INFO] Epoch: 25 , batch: 131 , training loss: 3.884169
[INFO] Epoch: 25 , batch: 132 , training loss: 3.871185
[INFO] Epoch: 25 , batch: 133 , training loss: 3.853610
[INFO] Epoch: 25 , batch: 134 , training loss: 3.658640
[INFO] Epoch: 25 , batch: 135 , training loss: 3.711029
[INFO] Epoch: 25 , batch: 136 , training loss: 4.009604
[INFO] Epoch: 25 , batch: 137 , training loss: 3.890774
[INFO] Epoch: 25 , batch: 138 , training loss: 3.950164
[INFO] Epoch: 25 , batch: 139 , training loss: 4.519316
[INFO] Epoch: 25 , batch: 140 , training loss: 4.305246
[INFO] Epoch: 25 , batch: 141 , training loss: 4.073341
[INFO] Epoch: 25 , batch: 142 , training loss: 3.821216
[INFO] Epoch: 25 , batch: 143 , training loss: 3.950836
[INFO] Epoch: 25 , batch: 144 , training loss: 3.752151
[INFO] Epoch: 25 , batch: 145 , training loss: 3.817605
[INFO] Epoch: 25 , batch: 146 , training loss: 4.020041
[INFO] Epoch: 25 , batch: 147 , training loss: 3.704753
[INFO] Epoch: 25 , batch: 148 , training loss: 3.702389
[INFO] Epoch: 25 , batch: 149 , training loss: 3.796267
[INFO] Epoch: 25 , batch: 150 , training loss: 4.027012
[INFO] Epoch: 25 , batch: 151 , training loss: 3.900700
[INFO] Epoch: 25 , batch: 152 , training loss: 3.963675
[INFO] Epoch: 25 , batch: 153 , training loss: 3.918921
[INFO] Epoch: 25 , batch: 154 , training loss: 4.009897
[INFO] Epoch: 25 , batch: 155 , training loss: 4.249650
[INFO] Epoch: 25 , batch: 156 , training loss: 3.928752
[INFO] Epoch: 25 , batch: 157 , training loss: 3.934595
[INFO] Epoch: 25 , batch: 158 , training loss: 4.092956
[INFO] Epoch: 25 , batch: 159 , training loss: 3.929708
[INFO] Epoch: 25 , batch: 160 , training loss: 4.244259
[INFO] Epoch: 25 , batch: 161 , training loss: 4.381424
[INFO] Epoch: 25 , batch: 162 , training loss: 4.342125
[INFO] Epoch: 25 , batch: 163 , training loss: 4.435009
[INFO] Epoch: 25 , batch: 164 , training loss: 4.441163
[INFO] Epoch: 25 , batch: 165 , training loss: 4.353606
[INFO] Epoch: 25 , batch: 166 , training loss: 4.203942
[INFO] Epoch: 25 , batch: 167 , training loss: 4.314036
[INFO] Epoch: 25 , batch: 168 , training loss: 4.037642
[INFO] Epoch: 25 , batch: 169 , training loss: 3.963304
[INFO] Epoch: 25 , batch: 170 , training loss: 4.147129
[INFO] Epoch: 25 , batch: 171 , training loss: 3.542069
[INFO] Epoch: 25 , batch: 172 , training loss: 3.770230
[INFO] Epoch: 25 , batch: 173 , training loss: 4.113195
[INFO] Epoch: 25 , batch: 174 , training loss: 4.508214
[INFO] Epoch: 25 , batch: 175 , training loss: 4.910775
[INFO] Epoch: 25 , batch: 176 , training loss: 4.491314
[INFO] Epoch: 25 , batch: 177 , training loss: 4.212178
[INFO] Epoch: 25 , batch: 178 , training loss: 4.153824
[INFO] Epoch: 25 , batch: 179 , training loss: 4.230575
[INFO] Epoch: 25 , batch: 180 , training loss: 4.158806
[INFO] Epoch: 25 , batch: 181 , training loss: 4.422716
[INFO] Epoch: 25 , batch: 182 , training loss: 4.354772
[INFO] Epoch: 25 , batch: 183 , training loss: 4.330587
[INFO] Epoch: 25 , batch: 184 , training loss: 4.223830
[INFO] Epoch: 25 , batch: 185 , training loss: 4.161529
[INFO] Epoch: 25 , batch: 186 , training loss: 4.331527
[INFO] Epoch: 25 , batch: 187 , training loss: 4.443465
[INFO] Epoch: 25 , batch: 188 , training loss: 4.417449
[INFO] Epoch: 25 , batch: 189 , training loss: 4.317533
[INFO] Epoch: 25 , batch: 190 , training loss: 4.364005
[INFO] Epoch: 25 , batch: 191 , training loss: 4.454556
[INFO] Epoch: 25 , batch: 192 , training loss: 4.255065
[INFO] Epoch: 25 , batch: 193 , training loss: 4.368718
[INFO] Epoch: 25 , batch: 194 , training loss: 4.318828
[INFO] Epoch: 25 , batch: 195 , training loss: 4.207379
[INFO] Epoch: 25 , batch: 196 , training loss: 4.114399
[INFO] Epoch: 25 , batch: 197 , training loss: 4.190074
[INFO] Epoch: 25 , batch: 198 , training loss: 4.115568
[INFO] Epoch: 25 , batch: 199 , training loss: 4.246304
[INFO] Epoch: 25 , batch: 200 , training loss: 4.164042
[INFO] Epoch: 25 , batch: 201 , training loss: 4.064403
[INFO] Epoch: 25 , batch: 202 , training loss: 4.039060
[INFO] Epoch: 25 , batch: 203 , training loss: 4.163739
[INFO] Epoch: 25 , batch: 204 , training loss: 4.284172
[INFO] Epoch: 25 , batch: 205 , training loss: 3.856913
[INFO] Epoch: 25 , batch: 206 , training loss: 3.809260
[INFO] Epoch: 25 , batch: 207 , training loss: 3.806496
[INFO] Epoch: 25 , batch: 208 , training loss: 4.117306
[INFO] Epoch: 25 , batch: 209 , training loss: 4.093596
[INFO] Epoch: 25 , batch: 210 , training loss: 4.105047
[INFO] Epoch: 25 , batch: 211 , training loss: 4.093573
[INFO] Epoch: 25 , batch: 212 , training loss: 4.181934
[INFO] Epoch: 25 , batch: 213 , training loss: 4.137740
[INFO] Epoch: 25 , batch: 214 , training loss: 4.257579
[INFO] Epoch: 25 , batch: 215 , training loss: 4.458563
[INFO] Epoch: 25 , batch: 216 , training loss: 4.152019
[INFO] Epoch: 25 , batch: 217 , training loss: 4.084213
[INFO] Epoch: 25 , batch: 218 , training loss: 4.081698
[INFO] Epoch: 25 , batch: 219 , training loss: 4.180942
[INFO] Epoch: 25 , batch: 220 , training loss: 3.981010
[INFO] Epoch: 25 , batch: 221 , training loss: 4.029345
[INFO] Epoch: 25 , batch: 222 , training loss: 4.170502
[INFO] Epoch: 25 , batch: 223 , training loss: 4.228782
[INFO] Epoch: 25 , batch: 224 , training loss: 4.308353
[INFO] Epoch: 25 , batch: 225 , training loss: 4.181062
[INFO] Epoch: 25 , batch: 226 , training loss: 4.318958
[INFO] Epoch: 25 , batch: 227 , training loss: 4.284492
[INFO] Epoch: 25 , batch: 228 , training loss: 4.321132
[INFO] Epoch: 25 , batch: 229 , training loss: 4.192201
[INFO] Epoch: 25 , batch: 230 , training loss: 4.040928
[INFO] Epoch: 25 , batch: 231 , training loss: 3.905645
[INFO] Epoch: 25 , batch: 232 , training loss: 4.048693
[INFO] Epoch: 25 , batch: 233 , training loss: 4.054852
[INFO] Epoch: 25 , batch: 234 , training loss: 3.766033
[INFO] Epoch: 25 , batch: 235 , training loss: 3.866768
[INFO] Epoch: 25 , batch: 236 , training loss: 3.986022
[INFO] Epoch: 25 , batch: 237 , training loss: 4.184465
[INFO] Epoch: 25 , batch: 238 , training loss: 3.950833
[INFO] Epoch: 25 , batch: 239 , training loss: 4.004611
[INFO] Epoch: 25 , batch: 240 , training loss: 4.055752
[INFO] Epoch: 25 , batch: 241 , training loss: 3.848790
[INFO] Epoch: 25 , batch: 242 , training loss: 3.885335
[INFO] Epoch: 25 , batch: 243 , training loss: 4.182096
[INFO] Epoch: 25 , batch: 244 , training loss: 4.099290
[INFO] Epoch: 25 , batch: 245 , training loss: 4.088330
[INFO] Epoch: 25 , batch: 246 , training loss: 3.792629
[INFO] Epoch: 25 , batch: 247 , training loss: 3.955794
[INFO] Epoch: 25 , batch: 248 , training loss: 4.030107
[INFO] Epoch: 25 , batch: 249 , training loss: 4.048144
[INFO] Epoch: 25 , batch: 250 , training loss: 3.804253
[INFO] Epoch: 25 , batch: 251 , training loss: 4.254292
[INFO] Epoch: 25 , batch: 252 , training loss: 3.937339
[INFO] Epoch: 25 , batch: 253 , training loss: 3.891360
[INFO] Epoch: 25 , batch: 254 , training loss: 4.155566
[INFO] Epoch: 25 , batch: 255 , training loss: 4.126067
[INFO] Epoch: 25 , batch: 256 , training loss: 4.139182
[INFO] Epoch: 25 , batch: 257 , training loss: 4.274908
[INFO] Epoch: 25 , batch: 258 , training loss: 4.328508
[INFO] Epoch: 25 , batch: 259 , training loss: 4.403284
[INFO] Epoch: 25 , batch: 260 , training loss: 4.089803
[INFO] Epoch: 25 , batch: 261 , training loss: 4.243506
[INFO] Epoch: 25 , batch: 262 , training loss: 4.431859
[INFO] Epoch: 25 , batch: 263 , training loss: 4.598379
[INFO] Epoch: 25 , batch: 264 , training loss: 3.920560
[INFO] Epoch: 25 , batch: 265 , training loss: 4.052459
[INFO] Epoch: 25 , batch: 266 , training loss: 4.473635
[INFO] Epoch: 25 , batch: 267 , training loss: 4.238991
[INFO] Epoch: 25 , batch: 268 , training loss: 4.144754
[INFO] Epoch: 25 , batch: 269 , training loss: 4.126007
[INFO] Epoch: 25 , batch: 270 , training loss: 4.145087
[INFO] Epoch: 25 , batch: 271 , training loss: 4.192232
[INFO] Epoch: 25 , batch: 272 , training loss: 4.162575
[INFO] Epoch: 25 , batch: 273 , training loss: 4.208732
[INFO] Epoch: 25 , batch: 274 , training loss: 4.269270
[INFO] Epoch: 25 , batch: 275 , training loss: 4.142354
[INFO] Epoch: 25 , batch: 276 , training loss: 4.200578
[INFO] Epoch: 25 , batch: 277 , training loss: 4.352101
[INFO] Epoch: 25 , batch: 278 , training loss: 4.033854
[INFO] Epoch: 25 , batch: 279 , training loss: 4.035887
[INFO] Epoch: 25 , batch: 280 , training loss: 3.995677
[INFO] Epoch: 25 , batch: 281 , training loss: 4.117385
[INFO] Epoch: 25 , batch: 282 , training loss: 4.040195
[INFO] Epoch: 25 , batch: 283 , training loss: 4.060623
[INFO] Epoch: 25 , batch: 284 , training loss: 4.091467
[INFO] Epoch: 25 , batch: 285 , training loss: 4.029840
[INFO] Epoch: 25 , batch: 286 , training loss: 4.023544
[INFO] Epoch: 25 , batch: 287 , training loss: 3.971965
[INFO] Epoch: 25 , batch: 288 , training loss: 3.974639
[INFO] Epoch: 25 , batch: 289 , training loss: 4.019839
[INFO] Epoch: 25 , batch: 290 , training loss: 3.793032
[INFO] Epoch: 25 , batch: 291 , training loss: 3.781032
[INFO] Epoch: 25 , batch: 292 , training loss: 3.904037
[INFO] Epoch: 25 , batch: 293 , training loss: 3.802743
[INFO] Epoch: 25 , batch: 294 , training loss: 4.491381
[INFO] Epoch: 25 , batch: 295 , training loss: 4.244940
[INFO] Epoch: 25 , batch: 296 , training loss: 4.194924
[INFO] Epoch: 25 , batch: 297 , training loss: 4.135729
[INFO] Epoch: 25 , batch: 298 , training loss: 3.959748
[INFO] Epoch: 25 , batch: 299 , training loss: 4.001553
[INFO] Epoch: 25 , batch: 300 , training loss: 3.959970
[INFO] Epoch: 25 , batch: 301 , training loss: 3.905745
[INFO] Epoch: 25 , batch: 302 , training loss: 4.065706
[INFO] Epoch: 25 , batch: 303 , training loss: 4.083936
[INFO] Epoch: 25 , batch: 304 , training loss: 4.258574
[INFO] Epoch: 25 , batch: 305 , training loss: 4.054033
[INFO] Epoch: 25 , batch: 306 , training loss: 4.191342
[INFO] Epoch: 25 , batch: 307 , training loss: 4.187022
[INFO] Epoch: 25 , batch: 308 , training loss: 4.034461
[INFO] Epoch: 25 , batch: 309 , training loss: 4.023559
[INFO] Epoch: 25 , batch: 310 , training loss: 3.911141
[INFO] Epoch: 25 , batch: 311 , training loss: 3.914794
[INFO] Epoch: 25 , batch: 312 , training loss: 3.852500
[INFO] Epoch: 25 , batch: 313 , training loss: 3.958103
[INFO] Epoch: 25 , batch: 314 , training loss: 4.015583
[INFO] Epoch: 25 , batch: 315 , training loss: 4.055896
[INFO] Epoch: 25 , batch: 316 , training loss: 4.294199
[INFO] Epoch: 25 , batch: 317 , training loss: 4.758629
[INFO] Epoch: 25 , batch: 318 , training loss: 4.907373
[INFO] Epoch: 25 , batch: 319 , training loss: 4.488211
[INFO] Epoch: 25 , batch: 320 , training loss: 4.039871
[INFO] Epoch: 25 , batch: 321 , training loss: 3.863622
[INFO] Epoch: 25 , batch: 322 , training loss: 3.973411
[INFO] Epoch: 25 , batch: 323 , training loss: 4.010785
[INFO] Epoch: 25 , batch: 324 , training loss: 3.957513
[INFO] Epoch: 25 , batch: 325 , training loss: 4.138565
[INFO] Epoch: 25 , batch: 326 , training loss: 4.177728
[INFO] Epoch: 25 , batch: 327 , training loss: 4.094099
[INFO] Epoch: 25 , batch: 328 , training loss: 4.073348
[INFO] Epoch: 25 , batch: 329 , training loss: 3.990953
[INFO] Epoch: 25 , batch: 330 , training loss: 4.011618
[INFO] Epoch: 25 , batch: 331 , training loss: 4.138396
[INFO] Epoch: 25 , batch: 332 , training loss: 3.939777
[INFO] Epoch: 25 , batch: 333 , training loss: 3.967474
[INFO] Epoch: 25 , batch: 334 , training loss: 3.969811
[INFO] Epoch: 25 , batch: 335 , training loss: 4.082199
[INFO] Epoch: 25 , batch: 336 , training loss: 4.118881
[INFO] Epoch: 25 , batch: 337 , training loss: 4.152658
[INFO] Epoch: 25 , batch: 338 , training loss: 4.365376
[INFO] Epoch: 25 , batch: 339 , training loss: 4.230955
[INFO] Epoch: 25 , batch: 340 , training loss: 4.360138
[INFO] Epoch: 25 , batch: 341 , training loss: 4.140224
[INFO] Epoch: 25 , batch: 342 , training loss: 3.912739
[INFO] Epoch: 25 , batch: 343 , training loss: 3.975547
[INFO] Epoch: 25 , batch: 344 , training loss: 3.836051
[INFO] Epoch: 25 , batch: 345 , training loss: 3.970891
[INFO] Epoch: 25 , batch: 346 , training loss: 4.037284
[INFO] Epoch: 25 , batch: 347 , training loss: 3.919479
[INFO] Epoch: 25 , batch: 348 , training loss: 4.041471
[INFO] Epoch: 25 , batch: 349 , training loss: 4.199106
[INFO] Epoch: 25 , batch: 350 , training loss: 3.971488
[INFO] Epoch: 25 , batch: 351 , training loss: 4.041483
[INFO] Epoch: 25 , batch: 352 , training loss: 4.053897
[INFO] Epoch: 25 , batch: 353 , training loss: 4.012135
[INFO] Epoch: 25 , batch: 354 , training loss: 4.137962
[INFO] Epoch: 25 , batch: 355 , training loss: 4.183583
[INFO] Epoch: 25 , batch: 356 , training loss: 4.011341
[INFO] Epoch: 25 , batch: 357 , training loss: 4.097463
[INFO] Epoch: 25 , batch: 358 , training loss: 3.978956
[INFO] Epoch: 25 , batch: 359 , training loss: 3.986722
[INFO] Epoch: 25 , batch: 360 , training loss: 4.077614
[INFO] Epoch: 25 , batch: 361 , training loss: 4.037882
[INFO] Epoch: 25 , batch: 362 , training loss: 4.137547
[INFO] Epoch: 25 , batch: 363 , training loss: 4.032185
[INFO] Epoch: 25 , batch: 364 , training loss: 4.077918
[INFO] Epoch: 25 , batch: 365 , training loss: 4.021038
[INFO] Epoch: 25 , batch: 366 , training loss: 4.136202
[INFO] Epoch: 25 , batch: 367 , training loss: 4.199367
[INFO] Epoch: 25 , batch: 368 , training loss: 4.611519
[INFO] Epoch: 25 , batch: 369 , training loss: 4.296673
[INFO] Epoch: 25 , batch: 370 , training loss: 4.048784
[INFO] Epoch: 25 , batch: 371 , training loss: 4.423965
[INFO] Epoch: 25 , batch: 372 , training loss: 4.740328
[INFO] Epoch: 25 , batch: 373 , training loss: 4.794584
[INFO] Epoch: 25 , batch: 374 , training loss: 4.855459
[INFO] Epoch: 25 , batch: 375 , training loss: 4.858240
[INFO] Epoch: 25 , batch: 376 , training loss: 4.809923
[INFO] Epoch: 25 , batch: 377 , training loss: 4.565572
[INFO] Epoch: 25 , batch: 378 , training loss: 4.646380
[INFO] Epoch: 25 , batch: 379 , training loss: 4.613163
[INFO] Epoch: 25 , batch: 380 , training loss: 4.720204
[INFO] Epoch: 25 , batch: 381 , training loss: 4.466590
[INFO] Epoch: 25 , batch: 382 , training loss: 4.674809
[INFO] Epoch: 25 , batch: 383 , training loss: 4.711802
[INFO] Epoch: 25 , batch: 384 , training loss: 4.742166
[INFO] Epoch: 25 , batch: 385 , training loss: 4.477077
[INFO] Epoch: 25 , batch: 386 , training loss: 4.715925
[INFO] Epoch: 25 , batch: 387 , training loss: 4.656392
[INFO] Epoch: 25 , batch: 388 , training loss: 4.465376
[INFO] Epoch: 25 , batch: 389 , training loss: 4.309186
[INFO] Epoch: 25 , batch: 390 , training loss: 4.294167
[INFO] Epoch: 25 , batch: 391 , training loss: 4.349165
[INFO] Epoch: 25 , batch: 392 , training loss: 4.711997
[INFO] Epoch: 25 , batch: 393 , training loss: 4.601093
[INFO] Epoch: 25 , batch: 394 , training loss: 4.613737
[INFO] Epoch: 25 , batch: 395 , training loss: 4.511994
[INFO] Epoch: 25 , batch: 396 , training loss: 4.244054
[INFO] Epoch: 25 , batch: 397 , training loss: 4.431280
[INFO] Epoch: 25 , batch: 398 , training loss: 4.286703
[INFO] Epoch: 25 , batch: 399 , training loss: 4.352919
[INFO] Epoch: 25 , batch: 400 , training loss: 4.328029
[INFO] Epoch: 25 , batch: 401 , training loss: 4.726716
[INFO] Epoch: 25 , batch: 402 , training loss: 4.455235
[INFO] Epoch: 25 , batch: 403 , training loss: 4.266808
[INFO] Epoch: 25 , batch: 404 , training loss: 4.469315
[INFO] Epoch: 25 , batch: 405 , training loss: 4.508047
[INFO] Epoch: 25 , batch: 406 , training loss: 4.404239
[INFO] Epoch: 25 , batch: 407 , training loss: 4.462448
[INFO] Epoch: 25 , batch: 408 , training loss: 4.432148
[INFO] Epoch: 25 , batch: 409 , training loss: 4.428841
[INFO] Epoch: 25 , batch: 410 , training loss: 4.458909
[INFO] Epoch: 25 , batch: 411 , training loss: 4.651493
[INFO] Epoch: 25 , batch: 412 , training loss: 4.499750
[INFO] Epoch: 25 , batch: 413 , training loss: 4.379069
[INFO] Epoch: 25 , batch: 414 , training loss: 4.398714
[INFO] Epoch: 25 , batch: 415 , training loss: 4.425824
[INFO] Epoch: 25 , batch: 416 , training loss: 4.506193
[INFO] Epoch: 25 , batch: 417 , training loss: 4.428888
[INFO] Epoch: 25 , batch: 418 , training loss: 4.442020
[INFO] Epoch: 25 , batch: 419 , training loss: 4.404092
[INFO] Epoch: 25 , batch: 420 , training loss: 4.379090
[INFO] Epoch: 25 , batch: 421 , training loss: 4.386597
[INFO] Epoch: 25 , batch: 422 , training loss: 4.232241
[INFO] Epoch: 25 , batch: 423 , training loss: 4.440827
[INFO] Epoch: 25 , batch: 424 , training loss: 4.619991
[INFO] Epoch: 25 , batch: 425 , training loss: 4.507276
[INFO] Epoch: 25 , batch: 426 , training loss: 4.233644
[INFO] Epoch: 25 , batch: 427 , training loss: 4.450181
[INFO] Epoch: 25 , batch: 428 , training loss: 4.344429
[INFO] Epoch: 25 , batch: 429 , training loss: 4.212005
[INFO] Epoch: 25 , batch: 430 , training loss: 4.465150
[INFO] Epoch: 25 , batch: 431 , training loss: 4.068851
[INFO] Epoch: 25 , batch: 432 , training loss: 4.122627
[INFO] Epoch: 25 , batch: 433 , training loss: 4.166985
[INFO] Epoch: 25 , batch: 434 , training loss: 4.030869
[INFO] Epoch: 25 , batch: 435 , training loss: 4.398872
[INFO] Epoch: 25 , batch: 436 , training loss: 4.480893
[INFO] Epoch: 25 , batch: 437 , training loss: 4.231346
[INFO] Epoch: 25 , batch: 438 , training loss: 4.065554
[INFO] Epoch: 25 , batch: 439 , training loss: 4.288258
[INFO] Epoch: 25 , batch: 440 , training loss: 4.421456
[INFO] Epoch: 25 , batch: 441 , training loss: 4.520288
[INFO] Epoch: 25 , batch: 442 , training loss: 4.300009
[INFO] Epoch: 25 , batch: 443 , training loss: 4.491006
[INFO] Epoch: 25 , batch: 444 , training loss: 4.096418
[INFO] Epoch: 25 , batch: 445 , training loss: 4.013212
[INFO] Epoch: 25 , batch: 446 , training loss: 3.952721
[INFO] Epoch: 25 , batch: 447 , training loss: 4.124364
[INFO] Epoch: 25 , batch: 448 , training loss: 4.240140
[INFO] Epoch: 25 , batch: 449 , training loss: 4.648641
[INFO] Epoch: 25 , batch: 450 , training loss: 4.687637
[INFO] Epoch: 25 , batch: 451 , training loss: 4.611529
[INFO] Epoch: 25 , batch: 452 , training loss: 4.406103
[INFO] Epoch: 25 , batch: 453 , training loss: 4.166615
[INFO] Epoch: 25 , batch: 454 , training loss: 4.311553
[INFO] Epoch: 25 , batch: 455 , training loss: 4.355656
[INFO] Epoch: 25 , batch: 456 , training loss: 4.369893
[INFO] Epoch: 25 , batch: 457 , training loss: 4.462658
[INFO] Epoch: 25 , batch: 458 , training loss: 4.186255
[INFO] Epoch: 25 , batch: 459 , training loss: 4.155343
[INFO] Epoch: 25 , batch: 460 , training loss: 4.259318
[INFO] Epoch: 25 , batch: 461 , training loss: 4.239209
[INFO] Epoch: 25 , batch: 462 , training loss: 4.308615
[INFO] Epoch: 25 , batch: 463 , training loss: 4.219513
[INFO] Epoch: 25 , batch: 464 , training loss: 4.413731
[INFO] Epoch: 25 , batch: 465 , training loss: 4.333587
[INFO] Epoch: 25 , batch: 466 , training loss: 4.420908
[INFO] Epoch: 25 , batch: 467 , training loss: 4.390608
[INFO] Epoch: 25 , batch: 468 , training loss: 4.370219
[INFO] Epoch: 25 , batch: 469 , training loss: 4.390684
[INFO] Epoch: 25 , batch: 470 , training loss: 4.195234
[INFO] Epoch: 25 , batch: 471 , training loss: 4.281517
[INFO] Epoch: 25 , batch: 472 , training loss: 4.339797
[INFO] Epoch: 25 , batch: 473 , training loss: 4.276010
[INFO] Epoch: 25 , batch: 474 , training loss: 4.074103
[INFO] Epoch: 25 , batch: 475 , training loss: 3.940822
[INFO] Epoch: 25 , batch: 476 , training loss: 4.308854
[INFO] Epoch: 25 , batch: 477 , training loss: 4.434536
[INFO] Epoch: 25 , batch: 478 , training loss: 4.486443
[INFO] Epoch: 25 , batch: 479 , training loss: 4.442837
[INFO] Epoch: 25 , batch: 480 , training loss: 4.553679
[INFO] Epoch: 25 , batch: 481 , training loss: 4.407602
[INFO] Epoch: 25 , batch: 482 , training loss: 4.555138
[INFO] Epoch: 25 , batch: 483 , training loss: 4.378521
[INFO] Epoch: 25 , batch: 484 , training loss: 4.194275
[INFO] Epoch: 25 , batch: 485 , training loss: 4.281402
[INFO] Epoch: 25 , batch: 486 , training loss: 4.169297
[INFO] Epoch: 25 , batch: 487 , training loss: 4.148319
[INFO] Epoch: 25 , batch: 488 , training loss: 4.357521
[INFO] Epoch: 25 , batch: 489 , training loss: 4.237053
[INFO] Epoch: 25 , batch: 490 , training loss: 4.300635
[INFO] Epoch: 25 , batch: 491 , training loss: 4.232051
[INFO] Epoch: 25 , batch: 492 , training loss: 4.184888
[INFO] Epoch: 25 , batch: 493 , training loss: 4.382380
[INFO] Epoch: 25 , batch: 494 , training loss: 4.304252
[INFO] Epoch: 25 , batch: 495 , training loss: 4.431504
[INFO] Epoch: 25 , batch: 496 , training loss: 4.300902
[INFO] Epoch: 25 , batch: 497 , training loss: 4.344223
[INFO] Epoch: 25 , batch: 498 , training loss: 4.342124
[INFO] Epoch: 25 , batch: 499 , training loss: 4.421135
[INFO] Epoch: 25 , batch: 500 , training loss: 4.563114
[INFO] Epoch: 25 , batch: 501 , training loss: 4.874953
[INFO] Epoch: 25 , batch: 502 , training loss: 4.936601
[INFO] Epoch: 25 , batch: 503 , training loss: 4.639326
[INFO] Epoch: 25 , batch: 504 , training loss: 4.766673
[INFO] Epoch: 25 , batch: 505 , training loss: 4.722536
[INFO] Epoch: 25 , batch: 506 , training loss: 4.696264
[INFO] Epoch: 25 , batch: 507 , training loss: 4.753356
[INFO] Epoch: 25 , batch: 508 , training loss: 4.695627
[INFO] Epoch: 25 , batch: 509 , training loss: 4.463727
[INFO] Epoch: 25 , batch: 510 , training loss: 4.540559
[INFO] Epoch: 25 , batch: 511 , training loss: 4.454646
[INFO] Epoch: 25 , batch: 512 , training loss: 4.543538
[INFO] Epoch: 25 , batch: 513 , training loss: 4.788887
[INFO] Epoch: 25 , batch: 514 , training loss: 4.459840
[INFO] Epoch: 25 , batch: 515 , training loss: 4.676601
[INFO] Epoch: 25 , batch: 516 , training loss: 4.498931
[INFO] Epoch: 25 , batch: 517 , training loss: 4.459868
[INFO] Epoch: 25 , batch: 518 , training loss: 4.424607
[INFO] Epoch: 25 , batch: 519 , training loss: 4.269792
[INFO] Epoch: 25 , batch: 520 , training loss: 4.511934
[INFO] Epoch: 25 , batch: 521 , training loss: 4.482802
[INFO] Epoch: 25 , batch: 522 , training loss: 4.545615
[INFO] Epoch: 25 , batch: 523 , training loss: 4.467346
[INFO] Epoch: 25 , batch: 524 , training loss: 4.753573
[INFO] Epoch: 25 , batch: 525 , training loss: 4.670964
[INFO] Epoch: 25 , batch: 526 , training loss: 4.436815
[INFO] Epoch: 25 , batch: 527 , training loss: 4.467363
[INFO] Epoch: 25 , batch: 528 , training loss: 4.489671
[INFO] Epoch: 25 , batch: 529 , training loss: 4.478685
[INFO] Epoch: 25 , batch: 530 , training loss: 4.312632
[INFO] Epoch: 25 , batch: 531 , training loss: 4.452332
[INFO] Epoch: 25 , batch: 532 , training loss: 4.350206
[INFO] Epoch: 25 , batch: 533 , training loss: 4.507098
[INFO] Epoch: 25 , batch: 534 , training loss: 4.491722
[INFO] Epoch: 25 , batch: 535 , training loss: 4.504512
[INFO] Epoch: 25 , batch: 536 , training loss: 4.323790
[INFO] Epoch: 25 , batch: 537 , training loss: 4.335202
[INFO] Epoch: 25 , batch: 538 , training loss: 4.417718
[INFO] Epoch: 25 , batch: 539 , training loss: 4.509513
[INFO] Epoch: 25 , batch: 540 , training loss: 5.024322
[INFO] Epoch: 25 , batch: 541 , training loss: 4.915364
[INFO] Epoch: 25 , batch: 542 , training loss: 4.809739
[INFO] Epoch: 26 , batch: 0 , training loss: 3.440657
[INFO] Epoch: 26 , batch: 1 , training loss: 3.492995
[INFO] Epoch: 26 , batch: 2 , training loss: 3.628366
[INFO] Epoch: 26 , batch: 3 , training loss: 3.446495
[INFO] Epoch: 26 , batch: 4 , training loss: 3.864983
[INFO] Epoch: 26 , batch: 5 , training loss: 3.501254
[INFO] Epoch: 26 , batch: 6 , training loss: 3.917457
[INFO] Epoch: 26 , batch: 7 , training loss: 3.790826
[INFO] Epoch: 26 , batch: 8 , training loss: 3.500021
[INFO] Epoch: 26 , batch: 9 , training loss: 3.753566
[INFO] Epoch: 26 , batch: 10 , training loss: 3.744934
[INFO] Epoch: 26 , batch: 11 , training loss: 3.687579
[INFO] Epoch: 26 , batch: 12 , training loss: 3.596699
[INFO] Epoch: 26 , batch: 13 , training loss: 3.650691
[INFO] Epoch: 26 , batch: 14 , training loss: 3.485591
[INFO] Epoch: 26 , batch: 15 , training loss: 3.687907
[INFO] Epoch: 26 , batch: 16 , training loss: 3.534054
[INFO] Epoch: 26 , batch: 17 , training loss: 3.722404
[INFO] Epoch: 26 , batch: 18 , training loss: 3.664670
[INFO] Epoch: 26 , batch: 19 , training loss: 3.379203
[INFO] Epoch: 26 , batch: 20 , training loss: 3.364367
[INFO] Epoch: 26 , batch: 21 , training loss: 3.509759
[INFO] Epoch: 26 , batch: 22 , training loss: 3.381578
[INFO] Epoch: 26 , batch: 23 , training loss: 3.611459
[INFO] Epoch: 26 , batch: 24 , training loss: 3.385146
[INFO] Epoch: 26 , batch: 25 , training loss: 3.615835
[INFO] Epoch: 26 , batch: 26 , training loss: 3.486052
[INFO] Epoch: 26 , batch: 27 , training loss: 3.417610
[INFO] Epoch: 26 , batch: 28 , training loss: 3.560451
[INFO] Epoch: 26 , batch: 29 , training loss: 3.410403
[INFO] Epoch: 26 , batch: 30 , training loss: 3.485613
[INFO] Epoch: 26 , batch: 31 , training loss: 3.509752
[INFO] Epoch: 26 , batch: 32 , training loss: 3.521020
[INFO] Epoch: 26 , batch: 33 , training loss: 3.599835
[INFO] Epoch: 26 , batch: 34 , training loss: 3.518143
[INFO] Epoch: 26 , batch: 35 , training loss: 3.527597
[INFO] Epoch: 26 , batch: 36 , training loss: 3.551524
[INFO] Epoch: 26 , batch: 37 , training loss: 3.452501
[INFO] Epoch: 26 , batch: 38 , training loss: 3.516788
[INFO] Epoch: 26 , batch: 39 , training loss: 3.378913
[INFO] Epoch: 26 , batch: 40 , training loss: 3.579661
[INFO] Epoch: 26 , batch: 41 , training loss: 3.475152
[INFO] Epoch: 26 , batch: 42 , training loss: 3.888819
[INFO] Epoch: 26 , batch: 43 , training loss: 3.679676
[INFO] Epoch: 26 , batch: 44 , training loss: 4.014812
[INFO] Epoch: 26 , batch: 45 , training loss: 3.903246
[INFO] Epoch: 26 , batch: 46 , training loss: 3.807333
[INFO] Epoch: 26 , batch: 47 , training loss: 3.607447
[INFO] Epoch: 26 , batch: 48 , training loss: 3.598361
[INFO] Epoch: 26 , batch: 49 , training loss: 3.783588
[INFO] Epoch: 26 , batch: 50 , training loss: 3.597199
[INFO] Epoch: 26 , batch: 51 , training loss: 3.803788
[INFO] Epoch: 26 , batch: 52 , training loss: 3.664665
[INFO] Epoch: 26 , batch: 53 , training loss: 3.788780
[INFO] Epoch: 26 , batch: 54 , training loss: 3.784293
[INFO] Epoch: 26 , batch: 55 , training loss: 3.864806
[INFO] Epoch: 26 , batch: 56 , training loss: 3.743558
[INFO] Epoch: 26 , batch: 57 , training loss: 3.641309
[INFO] Epoch: 26 , batch: 58 , training loss: 3.705662
[INFO] Epoch: 26 , batch: 59 , training loss: 3.767830
[INFO] Epoch: 26 , batch: 60 , training loss: 3.707736
[INFO] Epoch: 26 , batch: 61 , training loss: 3.794641
[INFO] Epoch: 26 , batch: 62 , training loss: 3.710414
[INFO] Epoch: 26 , batch: 63 , training loss: 3.902652
[INFO] Epoch: 26 , batch: 64 , training loss: 4.048859
[INFO] Epoch: 26 , batch: 65 , training loss: 3.763644
[INFO] Epoch: 26 , batch: 66 , training loss: 3.613150
[INFO] Epoch: 26 , batch: 67 , training loss: 3.695553
[INFO] Epoch: 26 , batch: 68 , training loss: 3.829386
[INFO] Epoch: 26 , batch: 69 , training loss: 3.709494
[INFO] Epoch: 26 , batch: 70 , training loss: 3.939795
[INFO] Epoch: 26 , batch: 71 , training loss: 3.847243
[INFO] Epoch: 26 , batch: 72 , training loss: 3.873536
[INFO] Epoch: 26 , batch: 73 , training loss: 3.789244
[INFO] Epoch: 26 , batch: 74 , training loss: 3.896139
[INFO] Epoch: 26 , batch: 75 , training loss: 3.815506
[INFO] Epoch: 26 , batch: 76 , training loss: 3.853880
[INFO] Epoch: 26 , batch: 77 , training loss: 3.847839
[INFO] Epoch: 26 , batch: 78 , training loss: 3.966612
[INFO] Epoch: 26 , batch: 79 , training loss: 3.753551
[INFO] Epoch: 26 , batch: 80 , training loss: 3.980011
[INFO] Epoch: 26 , batch: 81 , training loss: 3.932285
[INFO] Epoch: 26 , batch: 82 , training loss: 3.855842
[INFO] Epoch: 26 , batch: 83 , training loss: 4.019629
[INFO] Epoch: 26 , batch: 84 , training loss: 3.909848
[INFO] Epoch: 26 , batch: 85 , training loss: 4.006160
[INFO] Epoch: 26 , batch: 86 , training loss: 3.937532
[INFO] Epoch: 26 , batch: 87 , training loss: 3.921842
[INFO] Epoch: 26 , batch: 88 , training loss: 4.034037
[INFO] Epoch: 26 , batch: 89 , training loss: 3.844336
[INFO] Epoch: 26 , batch: 90 , training loss: 3.912674
[INFO] Epoch: 26 , batch: 91 , training loss: 3.886356
[INFO] Epoch: 26 , batch: 92 , training loss: 3.856903
[INFO] Epoch: 26 , batch: 93 , training loss: 3.976340
[INFO] Epoch: 26 , batch: 94 , training loss: 4.143619
[INFO] Epoch: 26 , batch: 95 , training loss: 3.893850
[INFO] Epoch: 26 , batch: 96 , training loss: 3.894761
[INFO] Epoch: 26 , batch: 97 , training loss: 3.805753
[INFO] Epoch: 26 , batch: 98 , training loss: 3.759569
[INFO] Epoch: 26 , batch: 99 , training loss: 3.858529
[INFO] Epoch: 26 , batch: 100 , training loss: 3.796871
[INFO] Epoch: 26 , batch: 101 , training loss: 3.839801
[INFO] Epoch: 26 , batch: 102 , training loss: 3.933973
[INFO] Epoch: 26 , batch: 103 , training loss: 3.740403
[INFO] Epoch: 26 , batch: 104 , training loss: 3.704933
[INFO] Epoch: 26 , batch: 105 , training loss: 3.979746
[INFO] Epoch: 26 , batch: 106 , training loss: 3.986110
[INFO] Epoch: 26 , batch: 107 , training loss: 3.807564
[INFO] Epoch: 26 , batch: 108 , training loss: 3.734065
[INFO] Epoch: 26 , batch: 109 , training loss: 3.688884
[INFO] Epoch: 26 , batch: 110 , training loss: 3.838018
[INFO] Epoch: 26 , batch: 111 , training loss: 3.947027
[INFO] Epoch: 26 , batch: 112 , training loss: 3.872213
[INFO] Epoch: 26 , batch: 113 , training loss: 3.837289
[INFO] Epoch: 26 , batch: 114 , training loss: 3.827134
[INFO] Epoch: 26 , batch: 115 , training loss: 3.851422
[INFO] Epoch: 26 , batch: 116 , training loss: 3.800049
[INFO] Epoch: 26 , batch: 117 , training loss: 3.994179
[INFO] Epoch: 26 , batch: 118 , training loss: 3.947232
[INFO] Epoch: 26 , batch: 119 , training loss: 4.088133
[INFO] Epoch: 26 , batch: 120 , training loss: 4.082237
[INFO] Epoch: 26 , batch: 121 , training loss: 3.934709
[INFO] Epoch: 26 , batch: 122 , training loss: 3.808599
[INFO] Epoch: 26 , batch: 123 , training loss: 3.853109
[INFO] Epoch: 26 , batch: 124 , training loss: 3.948042
[INFO] Epoch: 26 , batch: 125 , training loss: 3.770927
[INFO] Epoch: 26 , batch: 126 , training loss: 3.780243
[INFO] Epoch: 26 , batch: 127 , training loss: 3.786469
[INFO] Epoch: 26 , batch: 128 , training loss: 3.924562
[INFO] Epoch: 26 , batch: 129 , training loss: 3.909584
[INFO] Epoch: 26 , batch: 130 , training loss: 3.867352
[INFO] Epoch: 26 , batch: 131 , training loss: 3.867317
[INFO] Epoch: 26 , batch: 132 , training loss: 3.879468
[INFO] Epoch: 26 , batch: 133 , training loss: 3.860017
[INFO] Epoch: 26 , batch: 134 , training loss: 3.652817
[INFO] Epoch: 26 , batch: 135 , training loss: 3.696188
[INFO] Epoch: 26 , batch: 136 , training loss: 3.973691
[INFO] Epoch: 26 , batch: 137 , training loss: 3.876083
[INFO] Epoch: 26 , batch: 138 , training loss: 3.937947
[INFO] Epoch: 26 , batch: 139 , training loss: 4.508833
[INFO] Epoch: 26 , batch: 140 , training loss: 4.280875
[INFO] Epoch: 26 , batch: 141 , training loss: 4.018640
[INFO] Epoch: 26 , batch: 142 , training loss: 3.773895
[INFO] Epoch: 26 , batch: 143 , training loss: 3.941526
[INFO] Epoch: 26 , batch: 144 , training loss: 3.731021
[INFO] Epoch: 26 , batch: 145 , training loss: 3.820739
[INFO] Epoch: 26 , batch: 146 , training loss: 4.049176
[INFO] Epoch: 26 , batch: 147 , training loss: 3.696050
[INFO] Epoch: 26 , batch: 148 , training loss: 3.680204
[INFO] Epoch: 26 , batch: 149 , training loss: 3.787529
[INFO] Epoch: 26 , batch: 150 , training loss: 3.997046
[INFO] Epoch: 26 , batch: 151 , training loss: 3.881352
[INFO] Epoch: 26 , batch: 152 , training loss: 3.914351
[INFO] Epoch: 26 , batch: 153 , training loss: 3.937241
[INFO] Epoch: 26 , batch: 154 , training loss: 4.028485
[INFO] Epoch: 26 , batch: 155 , training loss: 4.212581
[INFO] Epoch: 26 , batch: 156 , training loss: 3.924682
[INFO] Epoch: 26 , batch: 157 , training loss: 3.917944
[INFO] Epoch: 26 , batch: 158 , training loss: 4.048583
[INFO] Epoch: 26 , batch: 159 , training loss: 3.916033
[INFO] Epoch: 26 , batch: 160 , training loss: 4.234103
[INFO] Epoch: 26 , batch: 161 , training loss: 4.363131
[INFO] Epoch: 26 , batch: 162 , training loss: 4.333297
[INFO] Epoch: 26 , batch: 163 , training loss: 4.421157
[INFO] Epoch: 26 , batch: 164 , training loss: 4.417642
[INFO] Epoch: 26 , batch: 165 , training loss: 4.333484
[INFO] Epoch: 26 , batch: 166 , training loss: 4.206159
[INFO] Epoch: 26 , batch: 167 , training loss: 4.287906
[INFO] Epoch: 26 , batch: 168 , training loss: 4.014666
[INFO] Epoch: 26 , batch: 169 , training loss: 3.945690
[INFO] Epoch: 26 , batch: 170 , training loss: 4.152219
[INFO] Epoch: 26 , batch: 171 , training loss: 3.529311
[INFO] Epoch: 26 , batch: 172 , training loss: 3.720925
[INFO] Epoch: 26 , batch: 173 , training loss: 4.102052
[INFO] Epoch: 26 , batch: 174 , training loss: 4.496240
[INFO] Epoch: 26 , batch: 175 , training loss: 4.858374
[INFO] Epoch: 26 , batch: 176 , training loss: 4.489109
[INFO] Epoch: 26 , batch: 177 , training loss: 4.182276
[INFO] Epoch: 26 , batch: 178 , training loss: 4.131262
[INFO] Epoch: 26 , batch: 179 , training loss: 4.203670
[INFO] Epoch: 26 , batch: 180 , training loss: 4.121238
[INFO] Epoch: 26 , batch: 181 , training loss: 4.418862
[INFO] Epoch: 26 , batch: 182 , training loss: 4.353765
[INFO] Epoch: 26 , batch: 183 , training loss: 4.349135
[INFO] Epoch: 26 , batch: 184 , training loss: 4.231384
[INFO] Epoch: 26 , batch: 185 , training loss: 4.157250
[INFO] Epoch: 26 , batch: 186 , training loss: 4.352125
[INFO] Epoch: 26 , batch: 187 , training loss: 4.471541
[INFO] Epoch: 26 , batch: 188 , training loss: 4.404077
[INFO] Epoch: 26 , batch: 189 , training loss: 4.308880
[INFO] Epoch: 26 , batch: 190 , training loss: 4.336034
[INFO] Epoch: 26 , batch: 191 , training loss: 4.448721
[INFO] Epoch: 26 , batch: 192 , training loss: 4.273555
[INFO] Epoch: 26 , batch: 193 , training loss: 4.342669
[INFO] Epoch: 26 , batch: 194 , training loss: 4.321523
[INFO] Epoch: 26 , batch: 195 , training loss: 4.218701
[INFO] Epoch: 26 , batch: 196 , training loss: 4.110507
[INFO] Epoch: 26 , batch: 197 , training loss: 4.182555
[INFO] Epoch: 26 , batch: 198 , training loss: 4.129856
[INFO] Epoch: 26 , batch: 199 , training loss: 4.248806
[INFO] Epoch: 26 , batch: 200 , training loss: 4.145316
[INFO] Epoch: 26 , batch: 201 , training loss: 4.062768
[INFO] Epoch: 26 , batch: 202 , training loss: 4.041630
[INFO] Epoch: 26 , batch: 203 , training loss: 4.137079
[INFO] Epoch: 26 , batch: 204 , training loss: 4.308095
[INFO] Epoch: 26 , batch: 205 , training loss: 3.862509
[INFO] Epoch: 26 , batch: 206 , training loss: 3.794338
[INFO] Epoch: 26 , batch: 207 , training loss: 3.797857
[INFO] Epoch: 26 , batch: 208 , training loss: 4.106752
[INFO] Epoch: 26 , batch: 209 , training loss: 4.076772
[INFO] Epoch: 26 , batch: 210 , training loss: 4.091568
[INFO] Epoch: 26 , batch: 211 , training loss: 4.081044
[INFO] Epoch: 26 , batch: 212 , training loss: 4.191037
[INFO] Epoch: 26 , batch: 213 , training loss: 4.137213
[INFO] Epoch: 26 , batch: 214 , training loss: 4.233525
[INFO] Epoch: 26 , batch: 215 , training loss: 4.447163
[INFO] Epoch: 26 , batch: 216 , training loss: 4.145871
[INFO] Epoch: 26 , batch: 217 , training loss: 4.095477
[INFO] Epoch: 26 , batch: 218 , training loss: 4.088553
[INFO] Epoch: 26 , batch: 219 , training loss: 4.194957
[INFO] Epoch: 26 , batch: 220 , training loss: 3.992445
[INFO] Epoch: 26 , batch: 221 , training loss: 4.018080
[INFO] Epoch: 26 , batch: 222 , training loss: 4.165966
[INFO] Epoch: 26 , batch: 223 , training loss: 4.240885
[INFO] Epoch: 26 , batch: 224 , training loss: 4.299268
[INFO] Epoch: 26 , batch: 225 , training loss: 4.185242
[INFO] Epoch: 26 , batch: 226 , training loss: 4.312483
[INFO] Epoch: 26 , batch: 227 , training loss: 4.285481
[INFO] Epoch: 26 , batch: 228 , training loss: 4.318361
[INFO] Epoch: 26 , batch: 229 , training loss: 4.195541
[INFO] Epoch: 26 , batch: 230 , training loss: 4.021036
[INFO] Epoch: 26 , batch: 231 , training loss: 3.884687
[INFO] Epoch: 26 , batch: 232 , training loss: 4.038614
[INFO] Epoch: 26 , batch: 233 , training loss: 4.065055
[INFO] Epoch: 26 , batch: 234 , training loss: 3.760023
[INFO] Epoch: 26 , batch: 235 , training loss: 3.850585
[INFO] Epoch: 26 , batch: 236 , training loss: 3.992690
[INFO] Epoch: 26 , batch: 237 , training loss: 4.182781
[INFO] Epoch: 26 , batch: 238 , training loss: 3.927735
[INFO] Epoch: 26 , batch: 239 , training loss: 3.993146
[INFO] Epoch: 26 , batch: 240 , training loss: 4.060882
[INFO] Epoch: 26 , batch: 241 , training loss: 3.860768
[INFO] Epoch: 26 , batch: 242 , training loss: 3.888384
[INFO] Epoch: 26 , batch: 243 , training loss: 4.179324
[INFO] Epoch: 26 , batch: 244 , training loss: 4.096136
[INFO] Epoch: 26 , batch: 245 , training loss: 4.092896
[INFO] Epoch: 26 , batch: 246 , training loss: 3.795844
[INFO] Epoch: 26 , batch: 247 , training loss: 3.959759
[INFO] Epoch: 26 , batch: 248 , training loss: 4.016879
[INFO] Epoch: 26 , batch: 249 , training loss: 4.049411
[INFO] Epoch: 26 , batch: 250 , training loss: 3.796565
[INFO] Epoch: 26 , batch: 251 , training loss: 4.240450
[INFO] Epoch: 26 , batch: 252 , training loss: 3.952222
[INFO] Epoch: 26 , batch: 253 , training loss: 3.892300
[INFO] Epoch: 26 , batch: 254 , training loss: 4.144032
[INFO] Epoch: 26 , batch: 255 , training loss: 4.123129
[INFO] Epoch: 26 , batch: 256 , training loss: 4.150671
[INFO] Epoch: 26 , batch: 257 , training loss: 4.273831
[INFO] Epoch: 26 , batch: 258 , training loss: 4.328373
[INFO] Epoch: 26 , batch: 259 , training loss: 4.363683
[INFO] Epoch: 26 , batch: 260 , training loss: 4.092999
[INFO] Epoch: 26 , batch: 261 , training loss: 4.249918
[INFO] Epoch: 26 , batch: 262 , training loss: 4.416177
[INFO] Epoch: 26 , batch: 263 , training loss: 4.592641
[INFO] Epoch: 26 , batch: 264 , training loss: 3.920128
[INFO] Epoch: 26 , batch: 265 , training loss: 4.051675
[INFO] Epoch: 26 , batch: 266 , training loss: 4.458125
[INFO] Epoch: 26 , batch: 267 , training loss: 4.248527
[INFO] Epoch: 26 , batch: 268 , training loss: 4.144508
[INFO] Epoch: 26 , batch: 269 , training loss: 4.127883
[INFO] Epoch: 26 , batch: 270 , training loss: 4.157252
[INFO] Epoch: 26 , batch: 271 , training loss: 4.198376
[INFO] Epoch: 26 , batch: 272 , training loss: 4.171885
[INFO] Epoch: 26 , batch: 273 , training loss: 4.191898
[INFO] Epoch: 26 , batch: 274 , training loss: 4.266509
[INFO] Epoch: 26 , batch: 275 , training loss: 4.154861
[INFO] Epoch: 26 , batch: 276 , training loss: 4.184951
[INFO] Epoch: 26 , batch: 277 , training loss: 4.344106
[INFO] Epoch: 26 , batch: 278 , training loss: 4.031124
[INFO] Epoch: 26 , batch: 279 , training loss: 4.030597
[INFO] Epoch: 26 , batch: 280 , training loss: 3.971683
[INFO] Epoch: 26 , batch: 281 , training loss: 4.123832
[INFO] Epoch: 26 , batch: 282 , training loss: 4.035511
[INFO] Epoch: 26 , batch: 283 , training loss: 4.060421
[INFO] Epoch: 26 , batch: 284 , training loss: 4.086906
[INFO] Epoch: 26 , batch: 285 , training loss: 4.013843
[INFO] Epoch: 26 , batch: 286 , training loss: 4.004196
[INFO] Epoch: 26 , batch: 287 , training loss: 3.958365
[INFO] Epoch: 26 , batch: 288 , training loss: 3.978282
[INFO] Epoch: 26 , batch: 289 , training loss: 4.013439
[INFO] Epoch: 26 , batch: 290 , training loss: 3.792166
[INFO] Epoch: 26 , batch: 291 , training loss: 3.761416
[INFO] Epoch: 26 , batch: 292 , training loss: 3.900585
[INFO] Epoch: 26 , batch: 293 , training loss: 3.793533
[INFO] Epoch: 26 , batch: 294 , training loss: 4.474367
[INFO] Epoch: 26 , batch: 295 , training loss: 4.242073
[INFO] Epoch: 26 , batch: 296 , training loss: 4.194514
[INFO] Epoch: 26 , batch: 297 , training loss: 4.142248
[INFO] Epoch: 26 , batch: 298 , training loss: 3.976090
[INFO] Epoch: 26 , batch: 299 , training loss: 3.992358
[INFO] Epoch: 26 , batch: 300 , training loss: 3.956961
[INFO] Epoch: 26 , batch: 301 , training loss: 3.902622
[INFO] Epoch: 26 , batch: 302 , training loss: 4.074963
[INFO] Epoch: 26 , batch: 303 , training loss: 4.090626
[INFO] Epoch: 26 , batch: 304 , training loss: 4.252494
[INFO] Epoch: 26 , batch: 305 , training loss: 4.047174
[INFO] Epoch: 26 , batch: 306 , training loss: 4.195624
[INFO] Epoch: 26 , batch: 307 , training loss: 4.185215
[INFO] Epoch: 26 , batch: 308 , training loss: 4.013606
[INFO] Epoch: 26 , batch: 309 , training loss: 4.033629
[INFO] Epoch: 26 , batch: 310 , training loss: 3.913062
[INFO] Epoch: 26 , batch: 311 , training loss: 3.925328
[INFO] Epoch: 26 , batch: 312 , training loss: 3.849156
[INFO] Epoch: 26 , batch: 313 , training loss: 3.960222
[INFO] Epoch: 26 , batch: 314 , training loss: 4.018248
[INFO] Epoch: 26 , batch: 315 , training loss: 4.061533
[INFO] Epoch: 26 , batch: 316 , training loss: 4.302079
[INFO] Epoch: 26 , batch: 317 , training loss: 4.738543
[INFO] Epoch: 26 , batch: 318 , training loss: 4.908622
[INFO] Epoch: 26 , batch: 319 , training loss: 4.488317
[INFO] Epoch: 26 , batch: 320 , training loss: 4.036956
[INFO] Epoch: 26 , batch: 321 , training loss: 3.862464
[INFO] Epoch: 26 , batch: 322 , training loss: 3.985454
[INFO] Epoch: 26 , batch: 323 , training loss: 4.003847
[INFO] Epoch: 26 , batch: 324 , training loss: 3.961965
[INFO] Epoch: 26 , batch: 325 , training loss: 4.121612
[INFO] Epoch: 26 , batch: 326 , training loss: 4.169719
[INFO] Epoch: 26 , batch: 327 , training loss: 4.078180
[INFO] Epoch: 26 , batch: 328 , training loss: 4.083308
[INFO] Epoch: 26 , batch: 329 , training loss: 3.986761
[INFO] Epoch: 26 , batch: 330 , training loss: 4.006773
[INFO] Epoch: 26 , batch: 331 , training loss: 4.142443
[INFO] Epoch: 26 , batch: 332 , training loss: 3.939089
[INFO] Epoch: 26 , batch: 333 , training loss: 3.962215
[INFO] Epoch: 26 , batch: 334 , training loss: 3.978219
[INFO] Epoch: 26 , batch: 335 , training loss: 4.093894
[INFO] Epoch: 26 , batch: 336 , training loss: 4.118147
[INFO] Epoch: 26 , batch: 337 , training loss: 4.141816
[INFO] Epoch: 26 , batch: 338 , training loss: 4.364497
[INFO] Epoch: 26 , batch: 339 , training loss: 4.221380
[INFO] Epoch: 26 , batch: 340 , training loss: 4.367766
[INFO] Epoch: 26 , batch: 341 , training loss: 4.131370
[INFO] Epoch: 26 , batch: 342 , training loss: 3.917536
[INFO] Epoch: 26 , batch: 343 , training loss: 3.974949
[INFO] Epoch: 26 , batch: 344 , training loss: 3.835811
[INFO] Epoch: 26 , batch: 345 , training loss: 3.980047
[INFO] Epoch: 26 , batch: 346 , training loss: 4.042087
[INFO] Epoch: 26 , batch: 347 , training loss: 3.908897
[INFO] Epoch: 26 , batch: 348 , training loss: 4.041300
[INFO] Epoch: 26 , batch: 349 , training loss: 4.184340
[INFO] Epoch: 26 , batch: 350 , training loss: 3.961051
[INFO] Epoch: 26 , batch: 351 , training loss: 4.033332
[INFO] Epoch: 26 , batch: 352 , training loss: 4.061834
[INFO] Epoch: 26 , batch: 353 , training loss: 4.011464
[INFO] Epoch: 26 , batch: 354 , training loss: 4.130480
[INFO] Epoch: 26 , batch: 355 , training loss: 4.171100
[INFO] Epoch: 26 , batch: 356 , training loss: 3.995872
[INFO] Epoch: 26 , batch: 357 , training loss: 4.091591
[INFO] Epoch: 26 , batch: 358 , training loss: 3.969153
[INFO] Epoch: 26 , batch: 359 , training loss: 4.001817
[INFO] Epoch: 26 , batch: 360 , training loss: 4.075279
[INFO] Epoch: 26 , batch: 361 , training loss: 4.048451
[INFO] Epoch: 26 , batch: 362 , training loss: 4.147748
[INFO] Epoch: 26 , batch: 363 , training loss: 4.026581
[INFO] Epoch: 26 , batch: 364 , training loss: 4.064818
[INFO] Epoch: 26 , batch: 365 , training loss: 4.017913
[INFO] Epoch: 26 , batch: 366 , training loss: 4.131279
[INFO] Epoch: 26 , batch: 367 , training loss: 4.209216
[INFO] Epoch: 26 , batch: 368 , training loss: 4.620734
[INFO] Epoch: 26 , batch: 369 , training loss: 4.282927
[INFO] Epoch: 26 , batch: 370 , training loss: 4.046932
[INFO] Epoch: 26 , batch: 371 , training loss: 4.408804
[INFO] Epoch: 26 , batch: 372 , training loss: 4.732580
[INFO] Epoch: 26 , batch: 373 , training loss: 4.796660
[INFO] Epoch: 26 , batch: 374 , training loss: 4.875736
[INFO] Epoch: 26 , batch: 375 , training loss: 4.835108
[INFO] Epoch: 26 , batch: 376 , training loss: 4.793557
[INFO] Epoch: 26 , batch: 377 , training loss: 4.534417
[INFO] Epoch: 26 , batch: 378 , training loss: 4.623355
[INFO] Epoch: 26 , batch: 379 , training loss: 4.600658
[INFO] Epoch: 26 , batch: 380 , training loss: 4.725268
[INFO] Epoch: 26 , batch: 381 , training loss: 4.472338
[INFO] Epoch: 26 , batch: 382 , training loss: 4.678787
[INFO] Epoch: 26 , batch: 383 , training loss: 4.696884
[INFO] Epoch: 26 , batch: 384 , training loss: 4.712769
[INFO] Epoch: 26 , batch: 385 , training loss: 4.449146
[INFO] Epoch: 26 , batch: 386 , training loss: 4.699198
[INFO] Epoch: 26 , batch: 387 , training loss: 4.672474
[INFO] Epoch: 26 , batch: 388 , training loss: 4.469822
[INFO] Epoch: 26 , batch: 389 , training loss: 4.311949
[INFO] Epoch: 26 , batch: 390 , training loss: 4.296525
[INFO] Epoch: 26 , batch: 391 , training loss: 4.341445
[INFO] Epoch: 26 , batch: 392 , training loss: 4.696771
[INFO] Epoch: 26 , batch: 393 , training loss: 4.581576
[INFO] Epoch: 26 , batch: 394 , training loss: 4.622933
[INFO] Epoch: 26 , batch: 395 , training loss: 4.488956
[INFO] Epoch: 26 , batch: 396 , training loss: 4.253553
[INFO] Epoch: 26 , batch: 397 , training loss: 4.435206
[INFO] Epoch: 26 , batch: 398 , training loss: 4.281866
[INFO] Epoch: 26 , batch: 399 , training loss: 4.361992
[INFO] Epoch: 26 , batch: 400 , training loss: 4.313599
[INFO] Epoch: 26 , batch: 401 , training loss: 4.729684
[INFO] Epoch: 26 , batch: 402 , training loss: 4.455291
[INFO] Epoch: 26 , batch: 403 , training loss: 4.256081
[INFO] Epoch: 26 , batch: 404 , training loss: 4.470627
[INFO] Epoch: 26 , batch: 405 , training loss: 4.499578
[INFO] Epoch: 26 , batch: 406 , training loss: 4.398774
[INFO] Epoch: 26 , batch: 407 , training loss: 4.469847
[INFO] Epoch: 26 , batch: 408 , training loss: 4.426639
[INFO] Epoch: 26 , batch: 409 , training loss: 4.410736
[INFO] Epoch: 26 , batch: 410 , training loss: 4.451373
[INFO] Epoch: 26 , batch: 411 , training loss: 4.630459
[INFO] Epoch: 26 , batch: 412 , training loss: 4.492487
[INFO] Epoch: 26 , batch: 413 , training loss: 4.373674
[INFO] Epoch: 26 , batch: 414 , training loss: 4.402977
[INFO] Epoch: 26 , batch: 415 , training loss: 4.423257
[INFO] Epoch: 26 , batch: 416 , training loss: 4.507332
[INFO] Epoch: 26 , batch: 417 , training loss: 4.414200
[INFO] Epoch: 26 , batch: 418 , training loss: 4.432202
[INFO] Epoch: 26 , batch: 419 , training loss: 4.389984
[INFO] Epoch: 26 , batch: 420 , training loss: 4.372658
[INFO] Epoch: 26 , batch: 421 , training loss: 4.368897
[INFO] Epoch: 26 , batch: 422 , training loss: 4.238345
[INFO] Epoch: 26 , batch: 423 , training loss: 4.449812
[INFO] Epoch: 26 , batch: 424 , training loss: 4.616421
[INFO] Epoch: 26 , batch: 425 , training loss: 4.474421
[INFO] Epoch: 26 , batch: 426 , training loss: 4.235191
[INFO] Epoch: 26 , batch: 427 , training loss: 4.437825
[INFO] Epoch: 26 , batch: 428 , training loss: 4.336949
[INFO] Epoch: 26 , batch: 429 , training loss: 4.208964
[INFO] Epoch: 26 , batch: 430 , training loss: 4.466377
[INFO] Epoch: 26 , batch: 431 , training loss: 4.056984
[INFO] Epoch: 26 , batch: 432 , training loss: 4.118238
[INFO] Epoch: 26 , batch: 433 , training loss: 4.170217
[INFO] Epoch: 26 , batch: 434 , training loss: 4.024630
[INFO] Epoch: 26 , batch: 435 , training loss: 4.394460
[INFO] Epoch: 26 , batch: 436 , training loss: 4.482093
[INFO] Epoch: 26 , batch: 437 , training loss: 4.223057
[INFO] Epoch: 26 , batch: 438 , training loss: 4.073946
[INFO] Epoch: 26 , batch: 439 , training loss: 4.291148
[INFO] Epoch: 26 , batch: 440 , training loss: 4.406054
[INFO] Epoch: 26 , batch: 441 , training loss: 4.516232
[INFO] Epoch: 26 , batch: 442 , training loss: 4.290245
[INFO] Epoch: 26 , batch: 443 , training loss: 4.493895
[INFO] Epoch: 26 , batch: 444 , training loss: 4.086451
[INFO] Epoch: 26 , batch: 445 , training loss: 4.002518
[INFO] Epoch: 26 , batch: 446 , training loss: 3.943746
[INFO] Epoch: 26 , batch: 447 , training loss: 4.125771
[INFO] Epoch: 26 , batch: 448 , training loss: 4.238546
[INFO] Epoch: 26 , batch: 449 , training loss: 4.625015
[INFO] Epoch: 26 , batch: 450 , training loss: 4.679728
[INFO] Epoch: 26 , batch: 451 , training loss: 4.603237
[INFO] Epoch: 26 , batch: 452 , training loss: 4.406329
[INFO] Epoch: 26 , batch: 453 , training loss: 4.160533
[INFO] Epoch: 26 , batch: 454 , training loss: 4.303722
[INFO] Epoch: 26 , batch: 455 , training loss: 4.357103
[INFO] Epoch: 26 , batch: 456 , training loss: 4.359417
[INFO] Epoch: 26 , batch: 457 , training loss: 4.452769
[INFO] Epoch: 26 , batch: 458 , training loss: 4.183629
[INFO] Epoch: 26 , batch: 459 , training loss: 4.155827
[INFO] Epoch: 26 , batch: 460 , training loss: 4.259119
[INFO] Epoch: 26 , batch: 461 , training loss: 4.239185
[INFO] Epoch: 26 , batch: 462 , training loss: 4.293337
[INFO] Epoch: 26 , batch: 463 , training loss: 4.217203
[INFO] Epoch: 26 , batch: 464 , training loss: 4.394885
[INFO] Epoch: 26 , batch: 465 , training loss: 4.333492
[INFO] Epoch: 26 , batch: 466 , training loss: 4.420270
[INFO] Epoch: 26 , batch: 467 , training loss: 4.393797
[INFO] Epoch: 26 , batch: 468 , training loss: 4.354092
[INFO] Epoch: 26 , batch: 469 , training loss: 4.383574
[INFO] Epoch: 26 , batch: 470 , training loss: 4.195175
[INFO] Epoch: 26 , batch: 471 , training loss: 4.278642
[INFO] Epoch: 26 , batch: 472 , training loss: 4.340018
[INFO] Epoch: 26 , batch: 473 , training loss: 4.255919
[INFO] Epoch: 26 , batch: 474 , training loss: 4.063363
[INFO] Epoch: 26 , batch: 475 , training loss: 3.939676
[INFO] Epoch: 26 , batch: 476 , training loss: 4.314166
[INFO] Epoch: 26 , batch: 477 , training loss: 4.457202
[INFO] Epoch: 26 , batch: 478 , training loss: 4.490473
[INFO] Epoch: 26 , batch: 479 , training loss: 4.437102
[INFO] Epoch: 26 , batch: 480 , training loss: 4.533945
[INFO] Epoch: 26 , batch: 481 , training loss: 4.409707
[INFO] Epoch: 26 , batch: 482 , training loss: 4.549994
[INFO] Epoch: 26 , batch: 483 , training loss: 4.388764
[INFO] Epoch: 26 , batch: 484 , training loss: 4.181224
[INFO] Epoch: 26 , batch: 485 , training loss: 4.280051
[INFO] Epoch: 26 , batch: 486 , training loss: 4.168382
[INFO] Epoch: 26 , batch: 487 , training loss: 4.138984
[INFO] Epoch: 26 , batch: 488 , training loss: 4.352014
[INFO] Epoch: 26 , batch: 489 , training loss: 4.231234
[INFO] Epoch: 26 , batch: 490 , training loss: 4.281603
[INFO] Epoch: 26 , batch: 491 , training loss: 4.228370
[INFO] Epoch: 26 , batch: 492 , training loss: 4.173704
[INFO] Epoch: 26 , batch: 493 , training loss: 4.380549
[INFO] Epoch: 26 , batch: 494 , training loss: 4.302267
[INFO] Epoch: 26 , batch: 495 , training loss: 4.419936
[INFO] Epoch: 26 , batch: 496 , training loss: 4.292652
[INFO] Epoch: 26 , batch: 497 , training loss: 4.345546
[INFO] Epoch: 26 , batch: 498 , training loss: 4.344999
[INFO] Epoch: 26 , batch: 499 , training loss: 4.407589
[INFO] Epoch: 26 , batch: 500 , training loss: 4.567018
[INFO] Epoch: 26 , batch: 501 , training loss: 4.869273
[INFO] Epoch: 26 , batch: 502 , training loss: 4.946193
[INFO] Epoch: 26 , batch: 503 , training loss: 4.635029
[INFO] Epoch: 26 , batch: 504 , training loss: 4.744800
[INFO] Epoch: 26 , batch: 505 , training loss: 4.716850
[INFO] Epoch: 26 , batch: 506 , training loss: 4.681826
[INFO] Epoch: 26 , batch: 507 , training loss: 4.743435
[INFO] Epoch: 26 , batch: 508 , training loss: 4.709306
[INFO] Epoch: 26 , batch: 509 , training loss: 4.475586
[INFO] Epoch: 26 , batch: 510 , training loss: 4.556375
[INFO] Epoch: 26 , batch: 511 , training loss: 4.470252
[INFO] Epoch: 26 , batch: 512 , training loss: 4.552941
[INFO] Epoch: 26 , batch: 513 , training loss: 4.800471
[INFO] Epoch: 26 , batch: 514 , training loss: 4.472095
[INFO] Epoch: 26 , batch: 515 , training loss: 4.693434
[INFO] Epoch: 26 , batch: 516 , training loss: 4.504559
[INFO] Epoch: 26 , batch: 517 , training loss: 4.459027
[INFO] Epoch: 26 , batch: 518 , training loss: 4.439665
[INFO] Epoch: 26 , batch: 519 , training loss: 4.275644
[INFO] Epoch: 26 , batch: 520 , training loss: 4.515398
[INFO] Epoch: 26 , batch: 521 , training loss: 4.493712
[INFO] Epoch: 26 , batch: 522 , training loss: 4.546211
[INFO] Epoch: 26 , batch: 523 , training loss: 4.472661
[INFO] Epoch: 26 , batch: 524 , training loss: 4.748775
[INFO] Epoch: 26 , batch: 525 , training loss: 4.678319
[INFO] Epoch: 26 , batch: 526 , training loss: 4.441760
[INFO] Epoch: 26 , batch: 527 , training loss: 4.483388
[INFO] Epoch: 26 , batch: 528 , training loss: 4.486526
[INFO] Epoch: 26 , batch: 529 , training loss: 4.480661
[INFO] Epoch: 26 , batch: 530 , training loss: 4.317507
[INFO] Epoch: 26 , batch: 531 , training loss: 4.457223
[INFO] Epoch: 26 , batch: 532 , training loss: 4.350462
[INFO] Epoch: 26 , batch: 533 , training loss: 4.515864
[INFO] Epoch: 26 , batch: 534 , training loss: 4.503072
[INFO] Epoch: 26 , batch: 535 , training loss: 4.503703
[INFO] Epoch: 26 , batch: 536 , training loss: 4.323640
[INFO] Epoch: 26 , batch: 537 , training loss: 4.335798
[INFO] Epoch: 26 , batch: 538 , training loss: 4.421181
[INFO] Epoch: 26 , batch: 539 , training loss: 4.517289
[INFO] Epoch: 26 , batch: 540 , training loss: 5.015928
[INFO] Epoch: 26 , batch: 541 , training loss: 4.918510
[INFO] Epoch: 26 , batch: 542 , training loss: 4.799430
[INFO] Epoch: 27 , batch: 0 , training loss: 3.451696
[INFO] Epoch: 27 , batch: 1 , training loss: 3.502427
[INFO] Epoch: 27 , batch: 2 , training loss: 3.621176
[INFO] Epoch: 27 , batch: 3 , training loss: 3.468929
[INFO] Epoch: 27 , batch: 4 , training loss: 3.857183
[INFO] Epoch: 27 , batch: 5 , training loss: 3.488655
[INFO] Epoch: 27 , batch: 6 , training loss: 3.934236
[INFO] Epoch: 27 , batch: 7 , training loss: 3.789948
[INFO] Epoch: 27 , batch: 8 , training loss: 3.493972
[INFO] Epoch: 27 , batch: 9 , training loss: 3.757997
[INFO] Epoch: 27 , batch: 10 , training loss: 3.744238
[INFO] Epoch: 27 , batch: 11 , training loss: 3.693318
[INFO] Epoch: 27 , batch: 12 , training loss: 3.609311
[INFO] Epoch: 27 , batch: 13 , training loss: 3.663058
[INFO] Epoch: 27 , batch: 14 , training loss: 3.488016
[INFO] Epoch: 27 , batch: 15 , training loss: 3.698607
[INFO] Epoch: 27 , batch: 16 , training loss: 3.562949
[INFO] Epoch: 27 , batch: 17 , training loss: 3.751231
[INFO] Epoch: 27 , batch: 18 , training loss: 3.713424
[INFO] Epoch: 27 , batch: 19 , training loss: 3.412028
[INFO] Epoch: 27 , batch: 20 , training loss: 3.407086
[INFO] Epoch: 27 , batch: 21 , training loss: 3.539203
[INFO] Epoch: 27 , batch: 22 , training loss: 3.420346
[INFO] Epoch: 27 , batch: 23 , training loss: 3.647465
[INFO] Epoch: 27 , batch: 24 , training loss: 3.429683
[INFO] Epoch: 27 , batch: 25 , training loss: 3.658537
[INFO] Epoch: 27 , batch: 26 , training loss: 3.496904
[INFO] Epoch: 27 , batch: 27 , training loss: 3.462496
[INFO] Epoch: 27 , batch: 28 , training loss: 3.613311
[INFO] Epoch: 27 , batch: 29 , training loss: 3.437092
[INFO] Epoch: 27 , batch: 30 , training loss: 3.525360
[INFO] Epoch: 27 , batch: 31 , training loss: 3.542467
[INFO] Epoch: 27 , batch: 32 , training loss: 3.535111
[INFO] Epoch: 27 , batch: 33 , training loss: 3.584727
[INFO] Epoch: 27 , batch: 34 , training loss: 3.546141
[INFO] Epoch: 27 , batch: 35 , training loss: 3.542468
[INFO] Epoch: 27 , batch: 36 , training loss: 3.577006
[INFO] Epoch: 27 , batch: 37 , training loss: 3.512044
[INFO] Epoch: 27 , batch: 38 , training loss: 3.540028
[INFO] Epoch: 27 , batch: 39 , training loss: 3.390610
[INFO] Epoch: 27 , batch: 40 , training loss: 3.611304
[INFO] Epoch: 27 , batch: 41 , training loss: 3.527051
[INFO] Epoch: 27 , batch: 42 , training loss: 3.952848
[INFO] Epoch: 27 , batch: 43 , training loss: 3.675387
[INFO] Epoch: 27 , batch: 44 , training loss: 4.045036
[INFO] Epoch: 27 , batch: 45 , training loss: 3.932755
[INFO] Epoch: 27 , batch: 46 , training loss: 3.907016
[INFO] Epoch: 27 , batch: 47 , training loss: 3.643051
[INFO] Epoch: 27 , batch: 48 , training loss: 3.597810
[INFO] Epoch: 27 , batch: 49 , training loss: 3.801026
[INFO] Epoch: 27 , batch: 50 , training loss: 3.621992
[INFO] Epoch: 27 , batch: 51 , training loss: 3.824416
[INFO] Epoch: 27 , batch: 52 , training loss: 3.700284
[INFO] Epoch: 27 , batch: 53 , training loss: 3.771791
[INFO] Epoch: 27 , batch: 54 , training loss: 3.813770
[INFO] Epoch: 27 , batch: 55 , training loss: 3.872065
[INFO] Epoch: 27 , batch: 56 , training loss: 3.751964
[INFO] Epoch: 27 , batch: 57 , training loss: 3.654116
[INFO] Epoch: 27 , batch: 58 , training loss: 3.703362
[INFO] Epoch: 27 , batch: 59 , training loss: 3.776262
[INFO] Epoch: 27 , batch: 60 , training loss: 3.747711
[INFO] Epoch: 27 , batch: 61 , training loss: 3.816947
[INFO] Epoch: 27 , batch: 62 , training loss: 3.709545
[INFO] Epoch: 27 , batch: 63 , training loss: 3.902083
[INFO] Epoch: 27 , batch: 64 , training loss: 4.067969
[INFO] Epoch: 27 , batch: 65 , training loss: 3.741581
[INFO] Epoch: 27 , batch: 66 , training loss: 3.626048
[INFO] Epoch: 27 , batch: 67 , training loss: 3.723210
[INFO] Epoch: 27 , batch: 68 , training loss: 3.837376
[INFO] Epoch: 27 , batch: 69 , training loss: 3.734993
[INFO] Epoch: 27 , batch: 70 , training loss: 3.951850
[INFO] Epoch: 27 , batch: 71 , training loss: 3.850848
[INFO] Epoch: 27 , batch: 72 , training loss: 3.869702
[INFO] Epoch: 27 , batch: 73 , training loss: 3.792181
[INFO] Epoch: 27 , batch: 74 , training loss: 3.919197
[INFO] Epoch: 27 , batch: 75 , training loss: 3.823628
[INFO] Epoch: 27 , batch: 76 , training loss: 3.870801
[INFO] Epoch: 27 , batch: 77 , training loss: 3.875597
[INFO] Epoch: 27 , batch: 78 , training loss: 3.988696
[INFO] Epoch: 27 , batch: 79 , training loss: 3.793365
[INFO] Epoch: 27 , batch: 80 , training loss: 4.018944
[INFO] Epoch: 27 , batch: 81 , training loss: 3.934396
[INFO] Epoch: 27 , batch: 82 , training loss: 3.871186
[INFO] Epoch: 27 , batch: 83 , training loss: 4.018871
[INFO] Epoch: 27 , batch: 84 , training loss: 3.930662
[INFO] Epoch: 27 , batch: 85 , training loss: 4.012022
[INFO] Epoch: 27 , batch: 86 , training loss: 3.939840
[INFO] Epoch: 27 , batch: 87 , training loss: 3.922926
[INFO] Epoch: 27 , batch: 88 , training loss: 4.027218
[INFO] Epoch: 27 , batch: 89 , training loss: 3.850876
[INFO] Epoch: 27 , batch: 90 , training loss: 3.944735
[INFO] Epoch: 27 , batch: 91 , training loss: 3.878165
[INFO] Epoch: 27 , batch: 92 , training loss: 3.861080
[INFO] Epoch: 27 , batch: 93 , training loss: 3.964517
[INFO] Epoch: 27 , batch: 94 , training loss: 4.159732
[INFO] Epoch: 27 , batch: 95 , training loss: 3.886449
[INFO] Epoch: 27 , batch: 96 , training loss: 3.914205
[INFO] Epoch: 27 , batch: 97 , training loss: 3.821266
[INFO] Epoch: 27 , batch: 98 , training loss: 3.761486
[INFO] Epoch: 27 , batch: 99 , training loss: 3.864441
[INFO] Epoch: 27 , batch: 100 , training loss: 3.784699
[INFO] Epoch: 27 , batch: 101 , training loss: 3.839883
[INFO] Epoch: 27 , batch: 102 , training loss: 3.958871
[INFO] Epoch: 27 , batch: 103 , training loss: 3.741493
[INFO] Epoch: 27 , batch: 104 , training loss: 3.701699
[INFO] Epoch: 27 , batch: 105 , training loss: 3.986183
[INFO] Epoch: 27 , batch: 106 , training loss: 3.958606
[INFO] Epoch: 27 , batch: 107 , training loss: 3.800161
[INFO] Epoch: 27 , batch: 108 , training loss: 3.753848
[INFO] Epoch: 27 , batch: 109 , training loss: 3.690329
[INFO] Epoch: 27 , batch: 110 , training loss: 3.833213
[INFO] Epoch: 27 , batch: 111 , training loss: 3.967120
[INFO] Epoch: 27 , batch: 112 , training loss: 3.852121
[INFO] Epoch: 27 , batch: 113 , training loss: 3.865917
[INFO] Epoch: 27 , batch: 114 , training loss: 3.835957
[INFO] Epoch: 27 , batch: 115 , training loss: 3.851763
[INFO] Epoch: 27 , batch: 116 , training loss: 3.782470
[INFO] Epoch: 27 , batch: 117 , training loss: 3.987385
[INFO] Epoch: 27 , batch: 118 , training loss: 3.964875
[INFO] Epoch: 27 , batch: 119 , training loss: 4.075300
[INFO] Epoch: 27 , batch: 120 , training loss: 4.095612
[INFO] Epoch: 27 , batch: 121 , training loss: 3.944052
[INFO] Epoch: 27 , batch: 122 , training loss: 3.820119
[INFO] Epoch: 27 , batch: 123 , training loss: 3.869395
[INFO] Epoch: 27 , batch: 124 , training loss: 3.969588
[INFO] Epoch: 27 , batch: 125 , training loss: 3.770597
[INFO] Epoch: 27 , batch: 126 , training loss: 3.780089
[INFO] Epoch: 27 , batch: 127 , training loss: 3.775726
[INFO] Epoch: 27 , batch: 128 , training loss: 3.912897
[INFO] Epoch: 27 , batch: 129 , training loss: 3.921796
[INFO] Epoch: 27 , batch: 130 , training loss: 3.889097
[INFO] Epoch: 27 , batch: 131 , training loss: 3.868214
[INFO] Epoch: 27 , batch: 132 , training loss: 3.883513
[INFO] Epoch: 27 , batch: 133 , training loss: 3.852894
[INFO] Epoch: 27 , batch: 134 , training loss: 3.648546
[INFO] Epoch: 27 , batch: 135 , training loss: 3.716643
[INFO] Epoch: 27 , batch: 136 , training loss: 3.975223
[INFO] Epoch: 27 , batch: 137 , training loss: 3.909082
[INFO] Epoch: 27 , batch: 138 , training loss: 3.961198
[INFO] Epoch: 27 , batch: 139 , training loss: 4.559356
[INFO] Epoch: 27 , batch: 140 , training loss: 4.249409
[INFO] Epoch: 27 , batch: 141 , training loss: 4.048356
[INFO] Epoch: 27 , batch: 142 , training loss: 3.820033
[INFO] Epoch: 27 , batch: 143 , training loss: 3.945364
[INFO] Epoch: 27 , batch: 144 , training loss: 3.741160
[INFO] Epoch: 27 , batch: 145 , training loss: 3.809209
[INFO] Epoch: 27 , batch: 146 , training loss: 4.051733
[INFO] Epoch: 27 , batch: 147 , training loss: 3.713650
[INFO] Epoch: 27 , batch: 148 , training loss: 3.710053
[INFO] Epoch: 27 , batch: 149 , training loss: 3.775712
[INFO] Epoch: 27 , batch: 150 , training loss: 4.005299
[INFO] Epoch: 27 , batch: 151 , training loss: 3.880191
[INFO] Epoch: 27 , batch: 152 , training loss: 3.942820
[INFO] Epoch: 27 , batch: 153 , training loss: 3.923437
[INFO] Epoch: 27 , batch: 154 , training loss: 4.024473
[INFO] Epoch: 27 , batch: 155 , training loss: 4.242305
[INFO] Epoch: 27 , batch: 156 , training loss: 3.923303
[INFO] Epoch: 27 , batch: 157 , training loss: 3.939843
[INFO] Epoch: 27 , batch: 158 , training loss: 4.051622
[INFO] Epoch: 27 , batch: 159 , training loss: 3.952979
[INFO] Epoch: 27 , batch: 160 , training loss: 4.222368
[INFO] Epoch: 27 , batch: 161 , training loss: 4.356940
[INFO] Epoch: 27 , batch: 162 , training loss: 4.297342
[INFO] Epoch: 27 , batch: 163 , training loss: 4.431112
[INFO] Epoch: 27 , batch: 164 , training loss: 4.414320
[INFO] Epoch: 27 , batch: 165 , training loss: 4.324548
[INFO] Epoch: 27 , batch: 166 , training loss: 4.219471
[INFO] Epoch: 27 , batch: 167 , training loss: 4.306170
[INFO] Epoch: 27 , batch: 168 , training loss: 3.988210
[INFO] Epoch: 27 , batch: 169 , training loss: 3.934720
[INFO] Epoch: 27 , batch: 170 , training loss: 4.178885
[INFO] Epoch: 27 , batch: 171 , training loss: 3.521582
[INFO] Epoch: 27 , batch: 172 , training loss: 3.735343
[INFO] Epoch: 27 , batch: 173 , training loss: 4.087858
[INFO] Epoch: 27 , batch: 174 , training loss: 4.479843
[INFO] Epoch: 27 , batch: 175 , training loss: 4.828465
[INFO] Epoch: 27 , batch: 176 , training loss: 4.478939
[INFO] Epoch: 27 , batch: 177 , training loss: 4.169241
[INFO] Epoch: 27 , batch: 178 , training loss: 4.122117
[INFO] Epoch: 27 , batch: 179 , training loss: 4.177784
[INFO] Epoch: 27 , batch: 180 , training loss: 4.119854
[INFO] Epoch: 27 , batch: 181 , training loss: 4.405737
[INFO] Epoch: 27 , batch: 182 , training loss: 4.346023
[INFO] Epoch: 27 , batch: 183 , training loss: 4.312277
[INFO] Epoch: 27 , batch: 184 , training loss: 4.205675
[INFO] Epoch: 27 , batch: 185 , training loss: 4.146773
[INFO] Epoch: 27 , batch: 186 , training loss: 4.321186
[INFO] Epoch: 27 , batch: 187 , training loss: 4.452135
[INFO] Epoch: 27 , batch: 188 , training loss: 4.409042
[INFO] Epoch: 27 , batch: 189 , training loss: 4.310483
[INFO] Epoch: 27 , batch: 190 , training loss: 4.325299
[INFO] Epoch: 27 , batch: 191 , training loss: 4.464282
[INFO] Epoch: 27 , batch: 192 , training loss: 4.264072
[INFO] Epoch: 27 , batch: 193 , training loss: 4.357877
[INFO] Epoch: 27 , batch: 194 , training loss: 4.304977
[INFO] Epoch: 27 , batch: 195 , training loss: 4.224295
[INFO] Epoch: 27 , batch: 196 , training loss: 4.117285
[INFO] Epoch: 27 , batch: 197 , training loss: 4.201732
[INFO] Epoch: 27 , batch: 198 , training loss: 4.122922
[INFO] Epoch: 27 , batch: 199 , training loss: 4.240798
[INFO] Epoch: 27 , batch: 200 , training loss: 4.152307
[INFO] Epoch: 27 , batch: 201 , training loss: 4.070550
[INFO] Epoch: 27 , batch: 202 , training loss: 4.057111
[INFO] Epoch: 27 , batch: 203 , training loss: 4.169464
[INFO] Epoch: 27 , batch: 204 , training loss: 4.295744
[INFO] Epoch: 27 , batch: 205 , training loss: 3.858723
[INFO] Epoch: 27 , batch: 206 , training loss: 3.804811
[INFO] Epoch: 27 , batch: 207 , training loss: 3.796805
[INFO] Epoch: 27 , batch: 208 , training loss: 4.117816
[INFO] Epoch: 27 , batch: 209 , training loss: 4.075942
[INFO] Epoch: 27 , batch: 210 , training loss: 4.082896
[INFO] Epoch: 27 , batch: 211 , training loss: 4.090269
[INFO] Epoch: 27 , batch: 212 , training loss: 4.189641
[INFO] Epoch: 27 , batch: 213 , training loss: 4.136913
[INFO] Epoch: 27 , batch: 214 , training loss: 4.227343
[INFO] Epoch: 27 , batch: 215 , training loss: 4.451578
[INFO] Epoch: 27 , batch: 216 , training loss: 4.146254
[INFO] Epoch: 27 , batch: 217 , training loss: 4.089913
[INFO] Epoch: 27 , batch: 218 , training loss: 4.068668
[INFO] Epoch: 27 , batch: 219 , training loss: 4.190138
[INFO] Epoch: 27 , batch: 220 , training loss: 3.968691
[INFO] Epoch: 27 , batch: 221 , training loss: 4.017498
[INFO] Epoch: 27 , batch: 222 , training loss: 4.153181
[INFO] Epoch: 27 , batch: 223 , training loss: 4.245170
[INFO] Epoch: 27 , batch: 224 , training loss: 4.291099
[INFO] Epoch: 27 , batch: 225 , training loss: 4.191233
[INFO] Epoch: 27 , batch: 226 , training loss: 4.311859
[INFO] Epoch: 27 , batch: 227 , training loss: 4.261411
[INFO] Epoch: 27 , batch: 228 , training loss: 4.333784
[INFO] Epoch: 27 , batch: 229 , training loss: 4.180604
[INFO] Epoch: 27 , batch: 230 , training loss: 4.037726
[INFO] Epoch: 27 , batch: 231 , training loss: 3.887949
[INFO] Epoch: 27 , batch: 232 , training loss: 4.056879
[INFO] Epoch: 27 , batch: 233 , training loss: 4.050210
[INFO] Epoch: 27 , batch: 234 , training loss: 3.755685
[INFO] Epoch: 27 , batch: 235 , training loss: 3.857636
[INFO] Epoch: 27 , batch: 236 , training loss: 3.979079
[INFO] Epoch: 27 , batch: 237 , training loss: 4.187273
[INFO] Epoch: 27 , batch: 238 , training loss: 3.953165
[INFO] Epoch: 27 , batch: 239 , training loss: 3.988603
[INFO] Epoch: 27 , batch: 240 , training loss: 4.071038
[INFO] Epoch: 27 , batch: 241 , training loss: 3.854568
[INFO] Epoch: 27 , batch: 242 , training loss: 3.879814
[INFO] Epoch: 27 , batch: 243 , training loss: 4.182476
[INFO] Epoch: 27 , batch: 244 , training loss: 4.107296
[INFO] Epoch: 27 , batch: 245 , training loss: 4.065735
[INFO] Epoch: 27 , batch: 246 , training loss: 3.787961
[INFO] Epoch: 27 , batch: 247 , training loss: 3.966476
[INFO] Epoch: 27 , batch: 248 , training loss: 4.038260
[INFO] Epoch: 27 , batch: 249 , training loss: 4.068640
[INFO] Epoch: 27 , batch: 250 , training loss: 3.798683
[INFO] Epoch: 27 , batch: 251 , training loss: 4.237332
[INFO] Epoch: 27 , batch: 252 , training loss: 3.934050
[INFO] Epoch: 27 , batch: 253 , training loss: 3.894216
[INFO] Epoch: 27 , batch: 254 , training loss: 4.147367
[INFO] Epoch: 27 , batch: 255 , training loss: 4.125618
[INFO] Epoch: 27 , batch: 256 , training loss: 4.152725
[INFO] Epoch: 27 , batch: 257 , training loss: 4.274831
[INFO] Epoch: 27 , batch: 258 , training loss: 4.336597
[INFO] Epoch: 27 , batch: 259 , training loss: 4.366506
[INFO] Epoch: 27 , batch: 260 , training loss: 4.118238
[INFO] Epoch: 27 , batch: 261 , training loss: 4.236808
[INFO] Epoch: 27 , batch: 262 , training loss: 4.413600
[INFO] Epoch: 27 , batch: 263 , training loss: 4.598183
[INFO] Epoch: 27 , batch: 264 , training loss: 3.921251
[INFO] Epoch: 27 , batch: 265 , training loss: 4.060467
[INFO] Epoch: 27 , batch: 266 , training loss: 4.493027
[INFO] Epoch: 27 , batch: 267 , training loss: 4.225084
[INFO] Epoch: 27 , batch: 268 , training loss: 4.157287
[INFO] Epoch: 27 , batch: 269 , training loss: 4.129467
[INFO] Epoch: 27 , batch: 270 , training loss: 4.148661
[INFO] Epoch: 27 , batch: 271 , training loss: 4.210896
[INFO] Epoch: 27 , batch: 272 , training loss: 4.160919
[INFO] Epoch: 27 , batch: 273 , training loss: 4.187307
[INFO] Epoch: 27 , batch: 274 , training loss: 4.257694
[INFO] Epoch: 27 , batch: 275 , training loss: 4.145610
[INFO] Epoch: 27 , batch: 276 , training loss: 4.181803
[INFO] Epoch: 27 , batch: 277 , training loss: 4.337979
[INFO] Epoch: 27 , batch: 278 , training loss: 4.021273
[INFO] Epoch: 27 , batch: 279 , training loss: 4.021656
[INFO] Epoch: 27 , batch: 280 , training loss: 3.968404
[INFO] Epoch: 27 , batch: 281 , training loss: 4.123433
[INFO] Epoch: 27 , batch: 282 , training loss: 4.034339
[INFO] Epoch: 27 , batch: 283 , training loss: 4.048928
[INFO] Epoch: 27 , batch: 284 , training loss: 4.087702
[INFO] Epoch: 27 , batch: 285 , training loss: 4.013683
[INFO] Epoch: 27 , batch: 286 , training loss: 4.022764
[INFO] Epoch: 27 , batch: 287 , training loss: 3.964788
[INFO] Epoch: 27 , batch: 288 , training loss: 3.973265
[INFO] Epoch: 27 , batch: 289 , training loss: 4.010212
[INFO] Epoch: 27 , batch: 290 , training loss: 3.796660
[INFO] Epoch: 27 , batch: 291 , training loss: 3.772285
[INFO] Epoch: 27 , batch: 292 , training loss: 3.896697
[INFO] Epoch: 27 , batch: 293 , training loss: 3.802222
[INFO] Epoch: 27 , batch: 294 , training loss: 4.469618
[INFO] Epoch: 27 , batch: 295 , training loss: 4.239637
[INFO] Epoch: 27 , batch: 296 , training loss: 4.190006
[INFO] Epoch: 27 , batch: 297 , training loss: 4.130101
[INFO] Epoch: 27 , batch: 298 , training loss: 3.975955
[INFO] Epoch: 27 , batch: 299 , training loss: 4.003025
[INFO] Epoch: 27 , batch: 300 , training loss: 3.961003
[INFO] Epoch: 27 , batch: 301 , training loss: 3.898642
[INFO] Epoch: 27 , batch: 302 , training loss: 4.086223
[INFO] Epoch: 27 , batch: 303 , training loss: 4.090849
[INFO] Epoch: 27 , batch: 304 , training loss: 4.260067
[INFO] Epoch: 27 , batch: 305 , training loss: 4.059558
[INFO] Epoch: 27 , batch: 306 , training loss: 4.191282
[INFO] Epoch: 27 , batch: 307 , training loss: 4.176379
[INFO] Epoch: 27 , batch: 308 , training loss: 4.017374
[INFO] Epoch: 27 , batch: 309 , training loss: 4.014927
[INFO] Epoch: 27 , batch: 310 , training loss: 3.912306
[INFO] Epoch: 27 , batch: 311 , training loss: 3.924600
[INFO] Epoch: 27 , batch: 312 , training loss: 3.832130
[INFO] Epoch: 27 , batch: 313 , training loss: 3.960980
[INFO] Epoch: 27 , batch: 314 , training loss: 4.017947
[INFO] Epoch: 27 , batch: 315 , training loss: 4.058860
[INFO] Epoch: 27 , batch: 316 , training loss: 4.295130
[INFO] Epoch: 27 , batch: 317 , training loss: 4.751404
[INFO] Epoch: 27 , batch: 318 , training loss: 4.900534
[INFO] Epoch: 27 , batch: 319 , training loss: 4.492521
[INFO] Epoch: 27 , batch: 320 , training loss: 4.031793
[INFO] Epoch: 27 , batch: 321 , training loss: 3.856719
[INFO] Epoch: 27 , batch: 322 , training loss: 3.982903
[INFO] Epoch: 27 , batch: 323 , training loss: 3.992671
[INFO] Epoch: 27 , batch: 324 , training loss: 3.954446
[INFO] Epoch: 27 , batch: 325 , training loss: 4.129335
[INFO] Epoch: 27 , batch: 326 , training loss: 4.166157
[INFO] Epoch: 27 , batch: 327 , training loss: 4.082276
[INFO] Epoch: 27 , batch: 328 , training loss: 4.077407
[INFO] Epoch: 27 , batch: 329 , training loss: 3.975129
[INFO] Epoch: 27 , batch: 330 , training loss: 3.995633
[INFO] Epoch: 27 , batch: 331 , training loss: 4.139604
[INFO] Epoch: 27 , batch: 332 , training loss: 3.939951
[INFO] Epoch: 27 , batch: 333 , training loss: 3.965746
[INFO] Epoch: 27 , batch: 334 , training loss: 3.973357
[INFO] Epoch: 27 , batch: 335 , training loss: 4.094950
[INFO] Epoch: 27 , batch: 336 , training loss: 4.126659
[INFO] Epoch: 27 , batch: 337 , training loss: 4.140671
[INFO] Epoch: 27 , batch: 338 , training loss: 4.355421
[INFO] Epoch: 27 , batch: 339 , training loss: 4.225440
[INFO] Epoch: 27 , batch: 340 , training loss: 4.362352
[INFO] Epoch: 27 , batch: 341 , training loss: 4.112783
[INFO] Epoch: 27 , batch: 342 , training loss: 3.905879
[INFO] Epoch: 27 , batch: 343 , training loss: 3.988426
[INFO] Epoch: 27 , batch: 344 , training loss: 3.842176
[INFO] Epoch: 27 , batch: 345 , training loss: 3.962092
[INFO] Epoch: 27 , batch: 346 , training loss: 4.034122
[INFO] Epoch: 27 , batch: 347 , training loss: 3.922028
[INFO] Epoch: 27 , batch: 348 , training loss: 4.038270
[INFO] Epoch: 27 , batch: 349 , training loss: 4.191504
[INFO] Epoch: 27 , batch: 350 , training loss: 3.978095
[INFO] Epoch: 27 , batch: 351 , training loss: 4.036965
[INFO] Epoch: 27 , batch: 352 , training loss: 4.051227
[INFO] Epoch: 27 , batch: 353 , training loss: 4.024426
[INFO] Epoch: 27 , batch: 354 , training loss: 4.140671
[INFO] Epoch: 27 , batch: 355 , training loss: 4.182563
[INFO] Epoch: 27 , batch: 356 , training loss: 4.007898
[INFO] Epoch: 27 , batch: 357 , training loss: 4.084361
[INFO] Epoch: 27 , batch: 358 , training loss: 3.981675
[INFO] Epoch: 27 , batch: 359 , training loss: 3.983211
[INFO] Epoch: 27 , batch: 360 , training loss: 4.092914
[INFO] Epoch: 27 , batch: 361 , training loss: 4.039781
[INFO] Epoch: 27 , batch: 362 , training loss: 4.145450
[INFO] Epoch: 27 , batch: 363 , training loss: 4.021604
[INFO] Epoch: 27 , batch: 364 , training loss: 4.080026
[INFO] Epoch: 27 , batch: 365 , training loss: 4.008881
[INFO] Epoch: 27 , batch: 366 , training loss: 4.134218
[INFO] Epoch: 27 , batch: 367 , training loss: 4.209881
[INFO] Epoch: 27 , batch: 368 , training loss: 4.612159
[INFO] Epoch: 27 , batch: 369 , training loss: 4.266073
[INFO] Epoch: 27 , batch: 370 , training loss: 4.038502
[INFO] Epoch: 27 , batch: 371 , training loss: 4.425709
[INFO] Epoch: 27 , batch: 372 , training loss: 4.708275
[INFO] Epoch: 27 , batch: 373 , training loss: 4.800369
[INFO] Epoch: 27 , batch: 374 , training loss: 4.863418
[INFO] Epoch: 27 , batch: 375 , training loss: 4.859029
[INFO] Epoch: 27 , batch: 376 , training loss: 4.787897
[INFO] Epoch: 27 , batch: 377 , training loss: 4.553514
[INFO] Epoch: 27 , batch: 378 , training loss: 4.631427
[INFO] Epoch: 27 , batch: 379 , training loss: 4.601541
[INFO] Epoch: 27 , batch: 380 , training loss: 4.727079
[INFO] Epoch: 27 , batch: 381 , training loss: 4.457714
[INFO] Epoch: 27 , batch: 382 , training loss: 4.681319
[INFO] Epoch: 27 , batch: 383 , training loss: 4.695844
[INFO] Epoch: 27 , batch: 384 , training loss: 4.722276
[INFO] Epoch: 27 , batch: 385 , training loss: 4.444624
[INFO] Epoch: 27 , batch: 386 , training loss: 4.713146
[INFO] Epoch: 27 , batch: 387 , training loss: 4.680047
[INFO] Epoch: 27 , batch: 388 , training loss: 4.463681
[INFO] Epoch: 27 , batch: 389 , training loss: 4.313227
[INFO] Epoch: 27 , batch: 390 , training loss: 4.289320
[INFO] Epoch: 27 , batch: 391 , training loss: 4.358420
[INFO] Epoch: 27 , batch: 392 , training loss: 4.696968
[INFO] Epoch: 27 , batch: 393 , training loss: 4.565037
[INFO] Epoch: 27 , batch: 394 , training loss: 4.622518
[INFO] Epoch: 27 , batch: 395 , training loss: 4.483638
[INFO] Epoch: 27 , batch: 396 , training loss: 4.242853
[INFO] Epoch: 27 , batch: 397 , training loss: 4.421132
[INFO] Epoch: 27 , batch: 398 , training loss: 4.269464
[INFO] Epoch: 27 , batch: 399 , training loss: 4.362125
[INFO] Epoch: 27 , batch: 400 , training loss: 4.314254
[INFO] Epoch: 27 , batch: 401 , training loss: 4.735690
[INFO] Epoch: 27 , batch: 402 , training loss: 4.465676
[INFO] Epoch: 27 , batch: 403 , training loss: 4.244952
[INFO] Epoch: 27 , batch: 404 , training loss: 4.447228
[INFO] Epoch: 27 , batch: 405 , training loss: 4.482921
[INFO] Epoch: 27 , batch: 406 , training loss: 4.398343
[INFO] Epoch: 27 , batch: 407 , training loss: 4.463541
[INFO] Epoch: 27 , batch: 408 , training loss: 4.407623
[INFO] Epoch: 27 , batch: 409 , training loss: 4.409210
[INFO] Epoch: 27 , batch: 410 , training loss: 4.451382
[INFO] Epoch: 27 , batch: 411 , training loss: 4.635731
[INFO] Epoch: 27 , batch: 412 , training loss: 4.492610
[INFO] Epoch: 27 , batch: 413 , training loss: 4.373788
[INFO] Epoch: 27 , batch: 414 , training loss: 4.394992
[INFO] Epoch: 27 , batch: 415 , training loss: 4.413409
[INFO] Epoch: 27 , batch: 416 , training loss: 4.502365
[INFO] Epoch: 27 , batch: 417 , training loss: 4.405644
[INFO] Epoch: 27 , batch: 418 , training loss: 4.435339
[INFO] Epoch: 27 , batch: 419 , training loss: 4.401268
[INFO] Epoch: 27 , batch: 420 , training loss: 4.376006
[INFO] Epoch: 27 , batch: 421 , training loss: 4.367841
[INFO] Epoch: 27 , batch: 422 , training loss: 4.243467
[INFO] Epoch: 27 , batch: 423 , training loss: 4.446556
[INFO] Epoch: 27 , batch: 424 , training loss: 4.613311
[INFO] Epoch: 27 , batch: 425 , training loss: 4.482559
[INFO] Epoch: 27 , batch: 426 , training loss: 4.229428
[INFO] Epoch: 27 , batch: 427 , training loss: 4.444444
[INFO] Epoch: 27 , batch: 428 , training loss: 4.328633
[INFO] Epoch: 27 , batch: 429 , training loss: 4.192085
[INFO] Epoch: 27 , batch: 430 , training loss: 4.466788
[INFO] Epoch: 27 , batch: 431 , training loss: 4.062726
[INFO] Epoch: 27 , batch: 432 , training loss: 4.123181
[INFO] Epoch: 27 , batch: 433 , training loss: 4.155300
[INFO] Epoch: 27 , batch: 434 , training loss: 4.029251
[INFO] Epoch: 27 , batch: 435 , training loss: 4.392120
[INFO] Epoch: 27 , batch: 436 , training loss: 4.468263
[INFO] Epoch: 27 , batch: 437 , training loss: 4.234021
[INFO] Epoch: 27 , batch: 438 , training loss: 4.074498
[INFO] Epoch: 27 , batch: 439 , training loss: 4.303785
[INFO] Epoch: 27 , batch: 440 , training loss: 4.419727
[INFO] Epoch: 27 , batch: 441 , training loss: 4.514805
[INFO] Epoch: 27 , batch: 442 , training loss: 4.292098
[INFO] Epoch: 27 , batch: 443 , training loss: 4.503326
[INFO] Epoch: 27 , batch: 444 , training loss: 4.088178
[INFO] Epoch: 27 , batch: 445 , training loss: 3.996630
[INFO] Epoch: 27 , batch: 446 , training loss: 3.945982
[INFO] Epoch: 27 , batch: 447 , training loss: 4.124734
[INFO] Epoch: 27 , batch: 448 , training loss: 4.224205
[INFO] Epoch: 27 , batch: 449 , training loss: 4.636402
[INFO] Epoch: 27 , batch: 450 , training loss: 4.677916
[INFO] Epoch: 27 , batch: 451 , training loss: 4.602821
[INFO] Epoch: 27 , batch: 452 , training loss: 4.393618
[INFO] Epoch: 27 , batch: 453 , training loss: 4.163610
[INFO] Epoch: 27 , batch: 454 , training loss: 4.317065
[INFO] Epoch: 27 , batch: 455 , training loss: 4.350585
[INFO] Epoch: 27 , batch: 456 , training loss: 4.356205
[INFO] Epoch: 27 , batch: 457 , training loss: 4.449037
[INFO] Epoch: 27 , batch: 458 , training loss: 4.183836
[INFO] Epoch: 27 , batch: 459 , training loss: 4.165347
[INFO] Epoch: 27 , batch: 460 , training loss: 4.251011
[INFO] Epoch: 27 , batch: 461 , training loss: 4.246621
[INFO] Epoch: 27 , batch: 462 , training loss: 4.300502
[INFO] Epoch: 27 , batch: 463 , training loss: 4.216137
[INFO] Epoch: 27 , batch: 464 , training loss: 4.400607
[INFO] Epoch: 27 , batch: 465 , training loss: 4.326506
[INFO] Epoch: 27 , batch: 466 , training loss: 4.419045
[INFO] Epoch: 27 , batch: 467 , training loss: 4.395226
[INFO] Epoch: 27 , batch: 468 , training loss: 4.376112
[INFO] Epoch: 27 , batch: 469 , training loss: 4.383002
[INFO] Epoch: 27 , batch: 470 , training loss: 4.206326
[INFO] Epoch: 27 , batch: 471 , training loss: 4.289999
[INFO] Epoch: 27 , batch: 472 , training loss: 4.337890
[INFO] Epoch: 27 , batch: 473 , training loss: 4.261372
[INFO] Epoch: 27 , batch: 474 , training loss: 4.071848
[INFO] Epoch: 27 , batch: 475 , training loss: 3.947767
[INFO] Epoch: 27 , batch: 476 , training loss: 4.304054
[INFO] Epoch: 27 , batch: 477 , training loss: 4.450282
[INFO] Epoch: 27 , batch: 478 , training loss: 4.488062
[INFO] Epoch: 27 , batch: 479 , training loss: 4.438447
[INFO] Epoch: 27 , batch: 480 , training loss: 4.546402
[INFO] Epoch: 27 , batch: 481 , training loss: 4.402221
[INFO] Epoch: 27 , batch: 482 , training loss: 4.542381
[INFO] Epoch: 27 , batch: 483 , training loss: 4.384419
[INFO] Epoch: 27 , batch: 484 , training loss: 4.184617
[INFO] Epoch: 27 , batch: 485 , training loss: 4.270292
[INFO] Epoch: 27 , batch: 486 , training loss: 4.156402
[INFO] Epoch: 27 , batch: 487 , training loss: 4.149637
[INFO] Epoch: 27 , batch: 488 , training loss: 4.349715
[INFO] Epoch: 27 , batch: 489 , training loss: 4.234087
[INFO] Epoch: 27 , batch: 490 , training loss: 4.281555
[INFO] Epoch: 27 , batch: 491 , training loss: 4.231757
[INFO] Epoch: 27 , batch: 492 , training loss: 4.179675
[INFO] Epoch: 27 , batch: 493 , training loss: 4.381051
[INFO] Epoch: 27 , batch: 494 , training loss: 4.309352
[INFO] Epoch: 27 , batch: 495 , training loss: 4.436921
[INFO] Epoch: 27 , batch: 496 , training loss: 4.294169
[INFO] Epoch: 27 , batch: 497 , training loss: 4.345660
[INFO] Epoch: 27 , batch: 498 , training loss: 4.342630
[INFO] Epoch: 27 , batch: 499 , training loss: 4.412208
[INFO] Epoch: 27 , batch: 500 , training loss: 4.565000
[INFO] Epoch: 27 , batch: 501 , training loss: 4.869153
[INFO] Epoch: 27 , batch: 502 , training loss: 4.947656
[INFO] Epoch: 27 , batch: 503 , training loss: 4.631608
[INFO] Epoch: 27 , batch: 504 , training loss: 4.759306
[INFO] Epoch: 27 , batch: 505 , training loss: 4.717161
[INFO] Epoch: 27 , batch: 506 , training loss: 4.686043
[INFO] Epoch: 27 , batch: 507 , training loss: 4.750580
[INFO] Epoch: 27 , batch: 508 , training loss: 4.699227
[INFO] Epoch: 27 , batch: 509 , training loss: 4.479150
[INFO] Epoch: 27 , batch: 510 , training loss: 4.554902
[INFO] Epoch: 27 , batch: 511 , training loss: 4.463421
[INFO] Epoch: 27 , batch: 512 , training loss: 4.553061
[INFO] Epoch: 27 , batch: 513 , training loss: 4.794993
[INFO] Epoch: 27 , batch: 514 , training loss: 4.454137
[INFO] Epoch: 27 , batch: 515 , training loss: 4.684126
[INFO] Epoch: 27 , batch: 516 , training loss: 4.515023
[INFO] Epoch: 27 , batch: 517 , training loss: 4.454835
[INFO] Epoch: 27 , batch: 518 , training loss: 4.424308
[INFO] Epoch: 27 , batch: 519 , training loss: 4.276465
[INFO] Epoch: 27 , batch: 520 , training loss: 4.507351
[INFO] Epoch: 27 , batch: 521 , training loss: 4.482439
[INFO] Epoch: 27 , batch: 522 , training loss: 4.537808
[INFO] Epoch: 27 , batch: 523 , training loss: 4.452198
[INFO] Epoch: 27 , batch: 524 , training loss: 4.740744
[INFO] Epoch: 27 , batch: 525 , training loss: 4.687170
[INFO] Epoch: 27 , batch: 526 , training loss: 4.441179
[INFO] Epoch: 27 , batch: 527 , training loss: 4.482961
[INFO] Epoch: 27 , batch: 528 , training loss: 4.483321
[INFO] Epoch: 27 , batch: 529 , training loss: 4.479165
[INFO] Epoch: 27 , batch: 530 , training loss: 4.304698
[INFO] Epoch: 27 , batch: 531 , training loss: 4.461957
[INFO] Epoch: 27 , batch: 532 , training loss: 4.354210
[INFO] Epoch: 27 , batch: 533 , training loss: 4.502678
[INFO] Epoch: 27 , batch: 534 , training loss: 4.490312
[INFO] Epoch: 27 , batch: 535 , training loss: 4.503215
[INFO] Epoch: 27 , batch: 536 , training loss: 4.331960
[INFO] Epoch: 27 , batch: 537 , training loss: 4.329336
[INFO] Epoch: 27 , batch: 538 , training loss: 4.413561
[INFO] Epoch: 27 , batch: 539 , training loss: 4.528327
[INFO] Epoch: 27 , batch: 540 , training loss: 5.010351
[INFO] Epoch: 27 , batch: 541 , training loss: 4.910140
[INFO] Epoch: 27 , batch: 542 , training loss: 4.802558
[INFO] Epoch: 28 , batch: 0 , training loss: 3.444866
[INFO] Epoch: 28 , batch: 1 , training loss: 3.533592
[INFO] Epoch: 28 , batch: 2 , training loss: 3.636822
[INFO] Epoch: 28 , batch: 3 , training loss: 3.451903
[INFO] Epoch: 28 , batch: 4 , training loss: 3.836146
[INFO] Epoch: 28 , batch: 5 , training loss: 3.509428
[INFO] Epoch: 28 , batch: 6 , training loss: 3.893773
[INFO] Epoch: 28 , batch: 7 , training loss: 3.809108
[INFO] Epoch: 28 , batch: 8 , training loss: 3.490863
[INFO] Epoch: 28 , batch: 9 , training loss: 3.768178
[INFO] Epoch: 28 , batch: 10 , training loss: 3.740209
[INFO] Epoch: 28 , batch: 11 , training loss: 3.686396
[INFO] Epoch: 28 , batch: 12 , training loss: 3.580002
[INFO] Epoch: 28 , batch: 13 , training loss: 3.642688
[INFO] Epoch: 28 , batch: 14 , training loss: 3.494656
[INFO] Epoch: 28 , batch: 15 , training loss: 3.678121
[INFO] Epoch: 28 , batch: 16 , training loss: 3.573603
[INFO] Epoch: 28 , batch: 17 , training loss: 3.746726
[INFO] Epoch: 28 , batch: 18 , training loss: 3.692083
[INFO] Epoch: 28 , batch: 19 , training loss: 3.389185
[INFO] Epoch: 28 , batch: 20 , training loss: 3.415228
[INFO] Epoch: 28 , batch: 21 , training loss: 3.503991
[INFO] Epoch: 28 , batch: 22 , training loss: 3.396725
[INFO] Epoch: 28 , batch: 23 , training loss: 3.628905
[INFO] Epoch: 28 , batch: 24 , training loss: 3.418952
[INFO] Epoch: 28 , batch: 25 , training loss: 3.592603
[INFO] Epoch: 28 , batch: 26 , training loss: 3.454845
[INFO] Epoch: 28 , batch: 27 , training loss: 3.441054
[INFO] Epoch: 28 , batch: 28 , training loss: 3.577617
[INFO] Epoch: 28 , batch: 29 , training loss: 3.420756
[INFO] Epoch: 28 , batch: 30 , training loss: 3.487341
[INFO] Epoch: 28 , batch: 31 , training loss: 3.522007
[INFO] Epoch: 28 , batch: 32 , training loss: 3.521311
[INFO] Epoch: 28 , batch: 33 , training loss: 3.559732
[INFO] Epoch: 28 , batch: 34 , training loss: 3.534060
[INFO] Epoch: 28 , batch: 35 , training loss: 3.522924
[INFO] Epoch: 28 , batch: 36 , training loss: 3.552005
[INFO] Epoch: 28 , batch: 37 , training loss: 3.470107
[INFO] Epoch: 28 , batch: 38 , training loss: 3.516595
[INFO] Epoch: 28 , batch: 39 , training loss: 3.390605
[INFO] Epoch: 28 , batch: 40 , training loss: 3.594691
[INFO] Epoch: 28 , batch: 41 , training loss: 3.482946
[INFO] Epoch: 28 , batch: 42 , training loss: 3.927410
[INFO] Epoch: 28 , batch: 43 , training loss: 3.648885
[INFO] Epoch: 28 , batch: 44 , training loss: 4.012269
[INFO] Epoch: 28 , batch: 45 , training loss: 3.957893
[INFO] Epoch: 28 , batch: 46 , training loss: 3.891238
[INFO] Epoch: 28 , batch: 47 , training loss: 3.576811
[INFO] Epoch: 28 , batch: 48 , training loss: 3.633524
[INFO] Epoch: 28 , batch: 49 , training loss: 3.782282
[INFO] Epoch: 28 , batch: 50 , training loss: 3.600480
[INFO] Epoch: 28 , batch: 51 , training loss: 3.802199
[INFO] Epoch: 28 , batch: 52 , training loss: 3.684324
[INFO] Epoch: 28 , batch: 53 , training loss: 3.788584
[INFO] Epoch: 28 , batch: 54 , training loss: 3.749143
[INFO] Epoch: 28 , batch: 55 , training loss: 3.851713
[INFO] Epoch: 28 , batch: 56 , training loss: 3.740274
[INFO] Epoch: 28 , batch: 57 , training loss: 3.638266
[INFO] Epoch: 28 , batch: 58 , training loss: 3.708144
[INFO] Epoch: 28 , batch: 59 , training loss: 3.747894
[INFO] Epoch: 28 , batch: 60 , training loss: 3.721190
[INFO] Epoch: 28 , batch: 61 , training loss: 3.788067
[INFO] Epoch: 28 , batch: 62 , training loss: 3.688046
[INFO] Epoch: 28 , batch: 63 , training loss: 3.881391
[INFO] Epoch: 28 , batch: 64 , training loss: 4.058503
[INFO] Epoch: 28 , batch: 65 , training loss: 3.757443
[INFO] Epoch: 28 , batch: 66 , training loss: 3.634044
[INFO] Epoch: 28 , batch: 67 , training loss: 3.704085
[INFO] Epoch: 28 , batch: 68 , training loss: 3.828377
[INFO] Epoch: 28 , batch: 69 , training loss: 3.724726
[INFO] Epoch: 28 , batch: 70 , training loss: 3.955343
[INFO] Epoch: 28 , batch: 71 , training loss: 3.856257
[INFO] Epoch: 28 , batch: 72 , training loss: 3.860216
[INFO] Epoch: 28 , batch: 73 , training loss: 3.784318
[INFO] Epoch: 28 , batch: 74 , training loss: 3.936257
[INFO] Epoch: 28 , batch: 75 , training loss: 3.818424
[INFO] Epoch: 28 , batch: 76 , training loss: 3.861807
[INFO] Epoch: 28 , batch: 77 , training loss: 3.834268
[INFO] Epoch: 28 , batch: 78 , training loss: 3.983052
[INFO] Epoch: 28 , batch: 79 , training loss: 3.785319
[INFO] Epoch: 28 , batch: 80 , training loss: 3.975067
[INFO] Epoch: 28 , batch: 81 , training loss: 3.920714
[INFO] Epoch: 28 , batch: 82 , training loss: 3.888266
[INFO] Epoch: 28 , batch: 83 , training loss: 4.019877
[INFO] Epoch: 28 , batch: 84 , training loss: 3.899558
[INFO] Epoch: 28 , batch: 85 , training loss: 3.973510
[INFO] Epoch: 28 , batch: 86 , training loss: 3.938268
[INFO] Epoch: 28 , batch: 87 , training loss: 3.925423
[INFO] Epoch: 28 , batch: 88 , training loss: 4.029782
[INFO] Epoch: 28 , batch: 89 , training loss: 3.861127
[INFO] Epoch: 28 , batch: 90 , training loss: 3.915907
[INFO] Epoch: 28 , batch: 91 , training loss: 3.890888
[INFO] Epoch: 28 , batch: 92 , training loss: 3.851937
[INFO] Epoch: 28 , batch: 93 , training loss: 3.953799
[INFO] Epoch: 28 , batch: 94 , training loss: 4.137129
[INFO] Epoch: 28 , batch: 95 , training loss: 3.903552
[INFO] Epoch: 28 , batch: 96 , training loss: 3.897827
[INFO] Epoch: 28 , batch: 97 , training loss: 3.813473
[INFO] Epoch: 28 , batch: 98 , training loss: 3.751468
[INFO] Epoch: 28 , batch: 99 , training loss: 3.838330
[INFO] Epoch: 28 , batch: 100 , training loss: 3.771652
[INFO] Epoch: 28 , batch: 101 , training loss: 3.833026
[INFO] Epoch: 28 , batch: 102 , training loss: 3.939628
[INFO] Epoch: 28 , batch: 103 , training loss: 3.734574
[INFO] Epoch: 28 , batch: 104 , training loss: 3.688170
[INFO] Epoch: 28 , batch: 105 , training loss: 3.971038
[INFO] Epoch: 28 , batch: 106 , training loss: 3.950447
[INFO] Epoch: 28 , batch: 107 , training loss: 3.791796
[INFO] Epoch: 28 , batch: 108 , training loss: 3.728431
[INFO] Epoch: 28 , batch: 109 , training loss: 3.674466
[INFO] Epoch: 28 , batch: 110 , training loss: 3.846703
[INFO] Epoch: 28 , batch: 111 , training loss: 3.917555
[INFO] Epoch: 28 , batch: 112 , training loss: 3.833105
[INFO] Epoch: 28 , batch: 113 , training loss: 3.863424
[INFO] Epoch: 28 , batch: 114 , training loss: 3.824182
[INFO] Epoch: 28 , batch: 115 , training loss: 3.869116
[INFO] Epoch: 28 , batch: 116 , training loss: 3.751819
[INFO] Epoch: 28 , batch: 117 , training loss: 4.000577
[INFO] Epoch: 28 , batch: 118 , training loss: 3.960417
[INFO] Epoch: 28 , batch: 119 , training loss: 4.059906
[INFO] Epoch: 28 , batch: 120 , training loss: 4.067576
[INFO] Epoch: 28 , batch: 121 , training loss: 3.903751
[INFO] Epoch: 28 , batch: 122 , training loss: 3.807699
[INFO] Epoch: 28 , batch: 123 , training loss: 3.851513
[INFO] Epoch: 28 , batch: 124 , training loss: 3.965393
[INFO] Epoch: 28 , batch: 125 , training loss: 3.757807
[INFO] Epoch: 28 , batch: 126 , training loss: 3.772719
[INFO] Epoch: 28 , batch: 127 , training loss: 3.737134
[INFO] Epoch: 28 , batch: 128 , training loss: 3.908524
[INFO] Epoch: 28 , batch: 129 , training loss: 3.932384
[INFO] Epoch: 28 , batch: 130 , training loss: 3.886470
[INFO] Epoch: 28 , batch: 131 , training loss: 3.873960
[INFO] Epoch: 28 , batch: 132 , training loss: 3.863906
[INFO] Epoch: 28 , batch: 133 , training loss: 3.836223
[INFO] Epoch: 28 , batch: 134 , training loss: 3.656585
[INFO] Epoch: 28 , batch: 135 , training loss: 3.697567
[INFO] Epoch: 28 , batch: 136 , training loss: 3.981066
[INFO] Epoch: 28 , batch: 137 , training loss: 3.870335
[INFO] Epoch: 28 , batch: 138 , training loss: 3.944337
[INFO] Epoch: 28 , batch: 139 , training loss: 4.527487
[INFO] Epoch: 28 , batch: 140 , training loss: 4.267715
[INFO] Epoch: 28 , batch: 141 , training loss: 4.063170
[INFO] Epoch: 28 , batch: 142 , training loss: 3.832256
[INFO] Epoch: 28 , batch: 143 , training loss: 3.950640
[INFO] Epoch: 28 , batch: 144 , training loss: 3.762164
[INFO] Epoch: 28 , batch: 145 , training loss: 3.819252
[INFO] Epoch: 28 , batch: 146 , training loss: 4.035727
[INFO] Epoch: 28 , batch: 147 , training loss: 3.701643
[INFO] Epoch: 28 , batch: 148 , training loss: 3.703572
[INFO] Epoch: 28 , batch: 149 , training loss: 3.785190
[INFO] Epoch: 28 , batch: 150 , training loss: 3.989225
[INFO] Epoch: 28 , batch: 151 , training loss: 3.880289
[INFO] Epoch: 28 , batch: 152 , training loss: 3.949690
[INFO] Epoch: 28 , batch: 153 , training loss: 3.890211
[INFO] Epoch: 28 , batch: 154 , training loss: 4.006356
[INFO] Epoch: 28 , batch: 155 , training loss: 4.231184
[INFO] Epoch: 28 , batch: 156 , training loss: 3.939540
[INFO] Epoch: 28 , batch: 157 , training loss: 3.906048
[INFO] Epoch: 28 , batch: 158 , training loss: 4.067684
[INFO] Epoch: 28 , batch: 159 , training loss: 3.927633
[INFO] Epoch: 28 , batch: 160 , training loss: 4.232694
[INFO] Epoch: 28 , batch: 161 , training loss: 4.349198
[INFO] Epoch: 28 , batch: 162 , training loss: 4.345533
[INFO] Epoch: 28 , batch: 163 , training loss: 4.428839
[INFO] Epoch: 28 , batch: 164 , training loss: 4.414767
[INFO] Epoch: 28 , batch: 165 , training loss: 4.338937
[INFO] Epoch: 28 , batch: 166 , training loss: 4.176022
[INFO] Epoch: 28 , batch: 167 , training loss: 4.271346
[INFO] Epoch: 28 , batch: 168 , training loss: 3.926880
[INFO] Epoch: 28 , batch: 169 , training loss: 3.921708
[INFO] Epoch: 28 , batch: 170 , training loss: 4.118697
[INFO] Epoch: 28 , batch: 171 , training loss: 3.516316
[INFO] Epoch: 28 , batch: 172 , training loss: 3.711259
[INFO] Epoch: 28 , batch: 173 , training loss: 4.066997
[INFO] Epoch: 28 , batch: 174 , training loss: 4.485818
[INFO] Epoch: 28 , batch: 175 , training loss: 4.839577
[INFO] Epoch: 28 , batch: 176 , training loss: 4.451766
[INFO] Epoch: 28 , batch: 177 , training loss: 4.135906
[INFO] Epoch: 28 , batch: 178 , training loss: 4.115190
[INFO] Epoch: 28 , batch: 179 , training loss: 4.179710
[INFO] Epoch: 28 , batch: 180 , training loss: 4.117599
[INFO] Epoch: 28 , batch: 181 , training loss: 4.379057
[INFO] Epoch: 28 , batch: 182 , training loss: 4.343482
[INFO] Epoch: 28 , batch: 183 , training loss: 4.324645
[INFO] Epoch: 28 , batch: 184 , training loss: 4.204625
[INFO] Epoch: 28 , batch: 185 , training loss: 4.141201
[INFO] Epoch: 28 , batch: 186 , training loss: 4.336583
[INFO] Epoch: 28 , batch: 187 , training loss: 4.439200
[INFO] Epoch: 28 , batch: 188 , training loss: 4.388790
[INFO] Epoch: 28 , batch: 189 , training loss: 4.302820
[INFO] Epoch: 28 , batch: 190 , training loss: 4.335313
[INFO] Epoch: 28 , batch: 191 , training loss: 4.430976
[INFO] Epoch: 28 , batch: 192 , training loss: 4.257716
[INFO] Epoch: 28 , batch: 193 , training loss: 4.344766
[INFO] Epoch: 28 , batch: 194 , training loss: 4.307786
[INFO] Epoch: 28 , batch: 195 , training loss: 4.204240
[INFO] Epoch: 28 , batch: 196 , training loss: 4.106681
[INFO] Epoch: 28 , batch: 197 , training loss: 4.198083
[INFO] Epoch: 28 , batch: 198 , training loss: 4.108598
[INFO] Epoch: 28 , batch: 199 , training loss: 4.255317
[INFO] Epoch: 28 , batch: 200 , training loss: 4.154877
[INFO] Epoch: 28 , batch: 201 , training loss: 4.066544
[INFO] Epoch: 28 , batch: 202 , training loss: 4.054049
[INFO] Epoch: 28 , batch: 203 , training loss: 4.150037
[INFO] Epoch: 28 , batch: 204 , training loss: 4.280864
[INFO] Epoch: 28 , batch: 205 , training loss: 3.864309
[INFO] Epoch: 28 , batch: 206 , training loss: 3.801160
[INFO] Epoch: 28 , batch: 207 , training loss: 3.793042
[INFO] Epoch: 28 , batch: 208 , training loss: 4.103795
[INFO] Epoch: 28 , batch: 209 , training loss: 4.070376
[INFO] Epoch: 28 , batch: 210 , training loss: 4.072897
[INFO] Epoch: 28 , batch: 211 , training loss: 4.075189
[INFO] Epoch: 28 , batch: 212 , training loss: 4.179198
[INFO] Epoch: 28 , batch: 213 , training loss: 4.136633
[INFO] Epoch: 28 , batch: 214 , training loss: 4.207785
[INFO] Epoch: 28 , batch: 215 , training loss: 4.440870
[INFO] Epoch: 28 , batch: 216 , training loss: 4.132982
[INFO] Epoch: 28 , batch: 217 , training loss: 4.072561
[INFO] Epoch: 28 , batch: 218 , training loss: 4.084123
[INFO] Epoch: 28 , batch: 219 , training loss: 4.197377
[INFO] Epoch: 28 , batch: 220 , training loss: 3.982697
[INFO] Epoch: 28 , batch: 221 , training loss: 4.008171
[INFO] Epoch: 28 , batch: 222 , training loss: 4.170563
[INFO] Epoch: 28 , batch: 223 , training loss: 4.244666
[INFO] Epoch: 28 , batch: 224 , training loss: 4.311713
[INFO] Epoch: 28 , batch: 225 , training loss: 4.194905
[INFO] Epoch: 28 , batch: 226 , training loss: 4.301584
[INFO] Epoch: 28 , batch: 227 , training loss: 4.258687
[INFO] Epoch: 28 , batch: 228 , training loss: 4.320065
[INFO] Epoch: 28 , batch: 229 , training loss: 4.179090
[INFO] Epoch: 28 , batch: 230 , training loss: 4.029874
[INFO] Epoch: 28 , batch: 231 , training loss: 3.901306
[INFO] Epoch: 28 , batch: 232 , training loss: 4.040927
[INFO] Epoch: 28 , batch: 233 , training loss: 4.046019
[INFO] Epoch: 28 , batch: 234 , training loss: 3.759734
[INFO] Epoch: 28 , batch: 235 , training loss: 3.848788
[INFO] Epoch: 28 , batch: 236 , training loss: 3.956427
[INFO] Epoch: 28 , batch: 237 , training loss: 4.176497
[INFO] Epoch: 28 , batch: 238 , training loss: 3.941466
[INFO] Epoch: 28 , batch: 239 , training loss: 3.995915
[INFO] Epoch: 28 , batch: 240 , training loss: 4.052513
[INFO] Epoch: 28 , batch: 241 , training loss: 3.850525
[INFO] Epoch: 28 , batch: 242 , training loss: 3.875442
[INFO] Epoch: 28 , batch: 243 , training loss: 4.174005
[INFO] Epoch: 28 , batch: 244 , training loss: 4.094372
[INFO] Epoch: 28 , batch: 245 , training loss: 4.090877
[INFO] Epoch: 28 , batch: 246 , training loss: 3.785237
[INFO] Epoch: 28 , batch: 247 , training loss: 3.938556
[INFO] Epoch: 28 , batch: 248 , training loss: 4.019049
[INFO] Epoch: 28 , batch: 249 , training loss: 4.041077
[INFO] Epoch: 28 , batch: 250 , training loss: 3.785665
[INFO] Epoch: 28 , batch: 251 , training loss: 4.246645
[INFO] Epoch: 28 , batch: 252 , training loss: 3.927529
[INFO] Epoch: 28 , batch: 253 , training loss: 3.878198
[INFO] Epoch: 28 , batch: 254 , training loss: 4.134672
[INFO] Epoch: 28 , batch: 255 , training loss: 4.107643
[INFO] Epoch: 28 , batch: 256 , training loss: 4.118462
[INFO] Epoch: 28 , batch: 257 , training loss: 4.265381
[INFO] Epoch: 28 , batch: 258 , training loss: 4.326707
[INFO] Epoch: 28 , batch: 259 , training loss: 4.365023
[INFO] Epoch: 28 , batch: 260 , training loss: 4.094018
[INFO] Epoch: 28 , batch: 261 , training loss: 4.244626
[INFO] Epoch: 28 , batch: 262 , training loss: 4.418670
[INFO] Epoch: 28 , batch: 263 , training loss: 4.595548
[INFO] Epoch: 28 , batch: 264 , training loss: 3.898854
[INFO] Epoch: 28 , batch: 265 , training loss: 4.039895
[INFO] Epoch: 28 , batch: 266 , training loss: 4.468166
[INFO] Epoch: 28 , batch: 267 , training loss: 4.225869
[INFO] Epoch: 28 , batch: 268 , training loss: 4.152826
[INFO] Epoch: 28 , batch: 269 , training loss: 4.120973
[INFO] Epoch: 28 , batch: 270 , training loss: 4.134768
[INFO] Epoch: 28 , batch: 271 , training loss: 4.198053
[INFO] Epoch: 28 , batch: 272 , training loss: 4.156471
[INFO] Epoch: 28 , batch: 273 , training loss: 4.189643
[INFO] Epoch: 28 , batch: 274 , training loss: 4.258386
[INFO] Epoch: 28 , batch: 275 , training loss: 4.149665
[INFO] Epoch: 28 , batch: 276 , training loss: 4.187842
[INFO] Epoch: 28 , batch: 277 , training loss: 4.339937
[INFO] Epoch: 28 , batch: 278 , training loss: 4.028089
[INFO] Epoch: 28 , batch: 279 , training loss: 4.029702
[INFO] Epoch: 28 , batch: 280 , training loss: 3.982634
[INFO] Epoch: 28 , batch: 281 , training loss: 4.123231
[INFO] Epoch: 28 , batch: 282 , training loss: 4.033655
[INFO] Epoch: 28 , batch: 283 , training loss: 4.051469
[INFO] Epoch: 28 , batch: 284 , training loss: 4.082840
[INFO] Epoch: 28 , batch: 285 , training loss: 4.015509
[INFO] Epoch: 28 , batch: 286 , training loss: 4.012783
[INFO] Epoch: 28 , batch: 287 , training loss: 3.971920
[INFO] Epoch: 28 , batch: 288 , training loss: 3.961865
[INFO] Epoch: 28 , batch: 289 , training loss: 4.014165
[INFO] Epoch: 28 , batch: 290 , training loss: 3.792888
[INFO] Epoch: 28 , batch: 291 , training loss: 3.765081
[INFO] Epoch: 28 , batch: 292 , training loss: 3.886738
[INFO] Epoch: 28 , batch: 293 , training loss: 3.807568
[INFO] Epoch: 28 , batch: 294 , training loss: 4.472419
[INFO] Epoch: 28 , batch: 295 , training loss: 4.248285
[INFO] Epoch: 28 , batch: 296 , training loss: 4.181558
[INFO] Epoch: 28 , batch: 297 , training loss: 4.123464
[INFO] Epoch: 28 , batch: 298 , training loss: 3.959478
[INFO] Epoch: 28 , batch: 299 , training loss: 4.001054
[INFO] Epoch: 28 , batch: 300 , training loss: 3.964420
[INFO] Epoch: 28 , batch: 301 , training loss: 3.905358
[INFO] Epoch: 28 , batch: 302 , training loss: 4.057871
[INFO] Epoch: 28 , batch: 303 , training loss: 4.086337
[INFO] Epoch: 28 , batch: 304 , training loss: 4.239853
[INFO] Epoch: 28 , batch: 305 , training loss: 4.039708
[INFO] Epoch: 28 , batch: 306 , training loss: 4.190133
[INFO] Epoch: 28 , batch: 307 , training loss: 4.157473
[INFO] Epoch: 28 , batch: 308 , training loss: 4.007015
[INFO] Epoch: 28 , batch: 309 , training loss: 4.011406
[INFO] Epoch: 28 , batch: 310 , training loss: 3.905828
[INFO] Epoch: 28 , batch: 311 , training loss: 3.903760
[INFO] Epoch: 28 , batch: 312 , training loss: 3.838007
[INFO] Epoch: 28 , batch: 313 , training loss: 3.962729
[INFO] Epoch: 28 , batch: 314 , training loss: 4.010002
[INFO] Epoch: 28 , batch: 315 , training loss: 4.053236
[INFO] Epoch: 28 , batch: 316 , training loss: 4.286990
[INFO] Epoch: 28 , batch: 317 , training loss: 4.743004
[INFO] Epoch: 28 , batch: 318 , training loss: 4.894460
[INFO] Epoch: 28 , batch: 319 , training loss: 4.475456
[INFO] Epoch: 28 , batch: 320 , training loss: 4.028839
[INFO] Epoch: 28 , batch: 321 , training loss: 3.852798
[INFO] Epoch: 28 , batch: 322 , training loss: 3.985745
[INFO] Epoch: 28 , batch: 323 , training loss: 3.997092
[INFO] Epoch: 28 , batch: 324 , training loss: 3.945850
[INFO] Epoch: 28 , batch: 325 , training loss: 4.108104
[INFO] Epoch: 28 , batch: 326 , training loss: 4.158897
[INFO] Epoch: 28 , batch: 327 , training loss: 4.075431
[INFO] Epoch: 28 , batch: 328 , training loss: 4.066656
[INFO] Epoch: 28 , batch: 329 , training loss: 3.978192
[INFO] Epoch: 28 , batch: 330 , training loss: 3.997819
[INFO] Epoch: 28 , batch: 331 , training loss: 4.133098
[INFO] Epoch: 28 , batch: 332 , training loss: 3.934911
[INFO] Epoch: 28 , batch: 333 , training loss: 3.969534
[INFO] Epoch: 28 , batch: 334 , training loss: 3.975576
[INFO] Epoch: 28 , batch: 335 , training loss: 4.071424
[INFO] Epoch: 28 , batch: 336 , training loss: 4.125371
[INFO] Epoch: 28 , batch: 337 , training loss: 4.140953
[INFO] Epoch: 28 , batch: 338 , training loss: 4.355531
[INFO] Epoch: 28 , batch: 339 , training loss: 4.219446
[INFO] Epoch: 28 , batch: 340 , training loss: 4.349159
[INFO] Epoch: 28 , batch: 341 , training loss: 4.109550
[INFO] Epoch: 28 , batch: 342 , training loss: 3.910322
[INFO] Epoch: 28 , batch: 343 , training loss: 3.980208
[INFO] Epoch: 28 , batch: 344 , training loss: 3.842948
[INFO] Epoch: 28 , batch: 345 , training loss: 3.967857
[INFO] Epoch: 28 , batch: 346 , training loss: 4.041434
[INFO] Epoch: 28 , batch: 347 , training loss: 3.906412
[INFO] Epoch: 28 , batch: 348 , training loss: 4.038291
[INFO] Epoch: 28 , batch: 349 , training loss: 4.182367
[INFO] Epoch: 28 , batch: 350 , training loss: 3.959002
[INFO] Epoch: 28 , batch: 351 , training loss: 4.030107
[INFO] Epoch: 28 , batch: 352 , training loss: 4.045922
[INFO] Epoch: 28 , batch: 353 , training loss: 4.021278
[INFO] Epoch: 28 , batch: 354 , training loss: 4.139619
[INFO] Epoch: 28 , batch: 355 , training loss: 4.188673
[INFO] Epoch: 28 , batch: 356 , training loss: 4.009168
[INFO] Epoch: 28 , batch: 357 , training loss: 4.092291
[INFO] Epoch: 28 , batch: 358 , training loss: 3.974981
[INFO] Epoch: 28 , batch: 359 , training loss: 3.986786
[INFO] Epoch: 28 , batch: 360 , training loss: 4.087577
[INFO] Epoch: 28 , batch: 361 , training loss: 4.034844
[INFO] Epoch: 28 , batch: 362 , training loss: 4.144446
[INFO] Epoch: 28 , batch: 363 , training loss: 4.031394
[INFO] Epoch: 28 , batch: 364 , training loss: 4.060367
[INFO] Epoch: 28 , batch: 365 , training loss: 4.016593
[INFO] Epoch: 28 , batch: 366 , training loss: 4.130289
[INFO] Epoch: 28 , batch: 367 , training loss: 4.206152
[INFO] Epoch: 28 , batch: 368 , training loss: 4.613274
[INFO] Epoch: 28 , batch: 369 , training loss: 4.264289
[INFO] Epoch: 28 , batch: 370 , training loss: 4.033293
[INFO] Epoch: 28 , batch: 371 , training loss: 4.391437
[INFO] Epoch: 28 , batch: 372 , training loss: 4.710516
[INFO] Epoch: 28 , batch: 373 , training loss: 4.812785
[INFO] Epoch: 28 , batch: 374 , training loss: 4.878618
[INFO] Epoch: 28 , batch: 375 , training loss: 4.864779
[INFO] Epoch: 28 , batch: 376 , training loss: 4.779728
[INFO] Epoch: 28 , batch: 377 , training loss: 4.548334
[INFO] Epoch: 28 , batch: 378 , training loss: 4.620091
[INFO] Epoch: 28 , batch: 379 , training loss: 4.598153
[INFO] Epoch: 28 , batch: 380 , training loss: 4.712662
[INFO] Epoch: 28 , batch: 381 , training loss: 4.451712
[INFO] Epoch: 28 , batch: 382 , training loss: 4.684360
[INFO] Epoch: 28 , batch: 383 , training loss: 4.683962
[INFO] Epoch: 28 , batch: 384 , training loss: 4.703112
[INFO] Epoch: 28 , batch: 385 , training loss: 4.418665
[INFO] Epoch: 28 , batch: 386 , training loss: 4.693871
[INFO] Epoch: 28 , batch: 387 , training loss: 4.663429
[INFO] Epoch: 28 , batch: 388 , training loss: 4.460428
[INFO] Epoch: 28 , batch: 389 , training loss: 4.316635
[INFO] Epoch: 28 , batch: 390 , training loss: 4.287569
[INFO] Epoch: 28 , batch: 391 , training loss: 4.339319
[INFO] Epoch: 28 , batch: 392 , training loss: 4.694115
[INFO] Epoch: 28 , batch: 393 , training loss: 4.570000
[INFO] Epoch: 28 , batch: 394 , training loss: 4.618125
[INFO] Epoch: 28 , batch: 395 , training loss: 4.487077
[INFO] Epoch: 28 , batch: 396 , training loss: 4.251206
[INFO] Epoch: 28 , batch: 397 , training loss: 4.419139
[INFO] Epoch: 28 , batch: 398 , training loss: 4.274901
[INFO] Epoch: 28 , batch: 399 , training loss: 4.351549
[INFO] Epoch: 28 , batch: 400 , training loss: 4.318789
[INFO] Epoch: 28 , batch: 401 , training loss: 4.730034
[INFO] Epoch: 28 , batch: 402 , training loss: 4.456411
[INFO] Epoch: 28 , batch: 403 , training loss: 4.254053
[INFO] Epoch: 28 , batch: 404 , training loss: 4.441740
[INFO] Epoch: 28 , batch: 405 , training loss: 4.486851
[INFO] Epoch: 28 , batch: 406 , training loss: 4.410018
[INFO] Epoch: 28 , batch: 407 , training loss: 4.461326
[INFO] Epoch: 28 , batch: 408 , training loss: 4.418515
[INFO] Epoch: 28 , batch: 409 , training loss: 4.411584
[INFO] Epoch: 28 , batch: 410 , training loss: 4.431154
[INFO] Epoch: 28 , batch: 411 , training loss: 4.636755
[INFO] Epoch: 28 , batch: 412 , training loss: 4.487083
[INFO] Epoch: 28 , batch: 413 , training loss: 4.379498
[INFO] Epoch: 28 , batch: 414 , training loss: 4.388168
[INFO] Epoch: 28 , batch: 415 , training loss: 4.415497
[INFO] Epoch: 28 , batch: 416 , training loss: 4.501091
[INFO] Epoch: 28 , batch: 417 , training loss: 4.406854
[INFO] Epoch: 28 , batch: 418 , training loss: 4.434397
[INFO] Epoch: 28 , batch: 419 , training loss: 4.395314
[INFO] Epoch: 28 , batch: 420 , training loss: 4.364282
[INFO] Epoch: 28 , batch: 421 , training loss: 4.377005
[INFO] Epoch: 28 , batch: 422 , training loss: 4.233783
[INFO] Epoch: 28 , batch: 423 , training loss: 4.428110
[INFO] Epoch: 28 , batch: 424 , training loss: 4.605690
[INFO] Epoch: 28 , batch: 425 , training loss: 4.468824
[INFO] Epoch: 28 , batch: 426 , training loss: 4.225497
[INFO] Epoch: 28 , batch: 427 , training loss: 4.446411
[INFO] Epoch: 28 , batch: 428 , training loss: 4.330540
[INFO] Epoch: 28 , batch: 429 , training loss: 4.194366
[INFO] Epoch: 28 , batch: 430 , training loss: 4.459184
[INFO] Epoch: 28 , batch: 431 , training loss: 4.058590
[INFO] Epoch: 28 , batch: 432 , training loss: 4.118321
[INFO] Epoch: 28 , batch: 433 , training loss: 4.164511
[INFO] Epoch: 28 , batch: 434 , training loss: 4.022216
[INFO] Epoch: 28 , batch: 435 , training loss: 4.386689
[INFO] Epoch: 28 , batch: 436 , training loss: 4.464220
[INFO] Epoch: 28 , batch: 437 , training loss: 4.224142
[INFO] Epoch: 28 , batch: 438 , training loss: 4.061399
[INFO] Epoch: 28 , batch: 439 , training loss: 4.292663
[INFO] Epoch: 28 , batch: 440 , training loss: 4.409121
[INFO] Epoch: 28 , batch: 441 , training loss: 4.521734
[INFO] Epoch: 28 , batch: 442 , training loss: 4.292892
[INFO] Epoch: 28 , batch: 443 , training loss: 4.480155
[INFO] Epoch: 28 , batch: 444 , training loss: 4.092703
[INFO] Epoch: 28 , batch: 445 , training loss: 3.992276
[INFO] Epoch: 28 , batch: 446 , training loss: 3.938279
[INFO] Epoch: 28 , batch: 447 , training loss: 4.128405
[INFO] Epoch: 28 , batch: 448 , training loss: 4.234560
[INFO] Epoch: 28 , batch: 449 , training loss: 4.629679
[INFO] Epoch: 28 , batch: 450 , training loss: 4.668657
[INFO] Epoch: 28 , batch: 451 , training loss: 4.590400
[INFO] Epoch: 28 , batch: 452 , training loss: 4.395420
[INFO] Epoch: 28 , batch: 453 , training loss: 4.159369
[INFO] Epoch: 28 , batch: 454 , training loss: 4.301704
[INFO] Epoch: 28 , batch: 455 , training loss: 4.350094
[INFO] Epoch: 28 , batch: 456 , training loss: 4.355821
[INFO] Epoch: 28 , batch: 457 , training loss: 4.445278
[INFO] Epoch: 28 , batch: 458 , training loss: 4.176891
[INFO] Epoch: 28 , batch: 459 , training loss: 4.152937
[INFO] Epoch: 28 , batch: 460 , training loss: 4.245792
[INFO] Epoch: 28 , batch: 461 , training loss: 4.227525
[INFO] Epoch: 28 , batch: 462 , training loss: 4.293802
[INFO] Epoch: 28 , batch: 463 , training loss: 4.200973
[INFO] Epoch: 28 , batch: 464 , training loss: 4.395206
[INFO] Epoch: 28 , batch: 465 , training loss: 4.321919
[INFO] Epoch: 28 , batch: 466 , training loss: 4.421550
[INFO] Epoch: 28 , batch: 467 , training loss: 4.389501
[INFO] Epoch: 28 , batch: 468 , training loss: 4.356543
[INFO] Epoch: 28 , batch: 469 , training loss: 4.380594
[INFO] Epoch: 28 , batch: 470 , training loss: 4.198503
[INFO] Epoch: 28 , batch: 471 , training loss: 4.283644
[INFO] Epoch: 28 , batch: 472 , training loss: 4.330035
[INFO] Epoch: 28 , batch: 473 , training loss: 4.260186
[INFO] Epoch: 28 , batch: 474 , training loss: 4.059906
[INFO] Epoch: 28 , batch: 475 , training loss: 3.944440
[INFO] Epoch: 28 , batch: 476 , training loss: 4.312725
[INFO] Epoch: 28 , batch: 477 , training loss: 4.438375
[INFO] Epoch: 28 , batch: 478 , training loss: 4.471551
[INFO] Epoch: 28 , batch: 479 , training loss: 4.437591
[INFO] Epoch: 28 , batch: 480 , training loss: 4.527384
[INFO] Epoch: 28 , batch: 481 , training loss: 4.394398
[INFO] Epoch: 28 , batch: 482 , training loss: 4.531253
[INFO] Epoch: 28 , batch: 483 , training loss: 4.376482
[INFO] Epoch: 28 , batch: 484 , training loss: 4.195490
[INFO] Epoch: 28 , batch: 485 , training loss: 4.274770
[INFO] Epoch: 28 , batch: 486 , training loss: 4.160801
[INFO] Epoch: 28 , batch: 487 , training loss: 4.144096
[INFO] Epoch: 28 , batch: 488 , training loss: 4.351357
[INFO] Epoch: 28 , batch: 489 , training loss: 4.237316
[INFO] Epoch: 28 , batch: 490 , training loss: 4.280758
[INFO] Epoch: 28 , batch: 491 , training loss: 4.233381
[INFO] Epoch: 28 , batch: 492 , training loss: 4.187585
[INFO] Epoch: 28 , batch: 493 , training loss: 4.376097
[INFO] Epoch: 28 , batch: 494 , training loss: 4.300067
[INFO] Epoch: 28 , batch: 495 , training loss: 4.421243
[INFO] Epoch: 28 , batch: 496 , training loss: 4.289496
[INFO] Epoch: 28 , batch: 497 , training loss: 4.336001
[INFO] Epoch: 28 , batch: 498 , training loss: 4.332448
[INFO] Epoch: 28 , batch: 499 , training loss: 4.395142
[INFO] Epoch: 28 , batch: 500 , training loss: 4.574998
[INFO] Epoch: 28 , batch: 501 , training loss: 4.853470
[INFO] Epoch: 28 , batch: 502 , training loss: 4.958674
[INFO] Epoch: 28 , batch: 503 , training loss: 4.621540
[INFO] Epoch: 28 , batch: 504 , training loss: 4.754751
[INFO] Epoch: 28 , batch: 505 , training loss: 4.729418
[INFO] Epoch: 28 , batch: 506 , training loss: 4.676009
[INFO] Epoch: 28 , batch: 507 , training loss: 4.737453
[INFO] Epoch: 28 , batch: 508 , training loss: 4.684157
[INFO] Epoch: 28 , batch: 509 , training loss: 4.474226
[INFO] Epoch: 28 , batch: 510 , training loss: 4.543627
[INFO] Epoch: 28 , batch: 511 , training loss: 4.459155
[INFO] Epoch: 28 , batch: 512 , training loss: 4.553687
[INFO] Epoch: 28 , batch: 513 , training loss: 4.779125
[INFO] Epoch: 28 , batch: 514 , training loss: 4.455364
[INFO] Epoch: 28 , batch: 515 , training loss: 4.679802
[INFO] Epoch: 28 , batch: 516 , training loss: 4.505417
[INFO] Epoch: 28 , batch: 517 , training loss: 4.454031
[INFO] Epoch: 28 , batch: 518 , training loss: 4.437279
[INFO] Epoch: 28 , batch: 519 , training loss: 4.275205
[INFO] Epoch: 28 , batch: 520 , training loss: 4.503972
[INFO] Epoch: 28 , batch: 521 , training loss: 4.478168
[INFO] Epoch: 28 , batch: 522 , training loss: 4.527626
[INFO] Epoch: 28 , batch: 523 , training loss: 4.455277
[INFO] Epoch: 28 , batch: 524 , training loss: 4.739892
[INFO] Epoch: 28 , batch: 525 , training loss: 4.673759
[INFO] Epoch: 28 , batch: 526 , training loss: 4.436548
[INFO] Epoch: 28 , batch: 527 , training loss: 4.471461
[INFO] Epoch: 28 , batch: 528 , training loss: 4.485686
[INFO] Epoch: 28 , batch: 529 , training loss: 4.471972
[INFO] Epoch: 28 , batch: 530 , training loss: 4.301136
[INFO] Epoch: 28 , batch: 531 , training loss: 4.447358
[INFO] Epoch: 28 , batch: 532 , training loss: 4.346837
[INFO] Epoch: 28 , batch: 533 , training loss: 4.505355
[INFO] Epoch: 28 , batch: 534 , training loss: 4.481361
[INFO] Epoch: 28 , batch: 535 , training loss: 4.490926
[INFO] Epoch: 28 , batch: 536 , training loss: 4.318632
[INFO] Epoch: 28 , batch: 537 , training loss: 4.326573
[INFO] Epoch: 28 , batch: 538 , training loss: 4.407193
[INFO] Epoch: 28 , batch: 539 , training loss: 4.505018
[INFO] Epoch: 28 , batch: 540 , training loss: 5.002406
[INFO] Epoch: 28 , batch: 541 , training loss: 4.884267
[INFO] Epoch: 28 , batch: 542 , training loss: 4.787741
[INFO] Epoch: 29 , batch: 0 , training loss: 3.466456
[INFO] Epoch: 29 , batch: 1 , training loss: 3.474121
[INFO] Epoch: 29 , batch: 2 , training loss: 3.617921
[INFO] Epoch: 29 , batch: 3 , training loss: 3.430556
[INFO] Epoch: 29 , batch: 4 , training loss: 3.829509
[INFO] Epoch: 29 , batch: 5 , training loss: 3.477757
[INFO] Epoch: 29 , batch: 6 , training loss: 3.881408
[INFO] Epoch: 29 , batch: 7 , training loss: 3.777920
[INFO] Epoch: 29 , batch: 8 , training loss: 3.491685
[INFO] Epoch: 29 , batch: 9 , training loss: 3.761796
[INFO] Epoch: 29 , batch: 10 , training loss: 3.718475
[INFO] Epoch: 29 , batch: 11 , training loss: 3.693305
[INFO] Epoch: 29 , batch: 12 , training loss: 3.586301
[INFO] Epoch: 29 , batch: 13 , training loss: 3.650352
[INFO] Epoch: 29 , batch: 14 , training loss: 3.479212
[INFO] Epoch: 29 , batch: 15 , training loss: 3.678818
[INFO] Epoch: 29 , batch: 16 , training loss: 3.531374
[INFO] Epoch: 29 , batch: 17 , training loss: 3.724738
[INFO] Epoch: 29 , batch: 18 , training loss: 3.665122
[INFO] Epoch: 29 , batch: 19 , training loss: 3.394440
[INFO] Epoch: 29 , batch: 20 , training loss: 3.377602
[INFO] Epoch: 29 , batch: 21 , training loss: 3.521134
[INFO] Epoch: 29 , batch: 22 , training loss: 3.403036
[INFO] Epoch: 29 , batch: 23 , training loss: 3.631635
[INFO] Epoch: 29 , batch: 24 , training loss: 3.420141
[INFO] Epoch: 29 , batch: 25 , training loss: 3.611912
[INFO] Epoch: 29 , batch: 26 , training loss: 3.461146
[INFO] Epoch: 29 , batch: 27 , training loss: 3.433811
[INFO] Epoch: 29 , batch: 28 , training loss: 3.593722
[INFO] Epoch: 29 , batch: 29 , training loss: 3.421154
[INFO] Epoch: 29 , batch: 30 , training loss: 3.505581
[INFO] Epoch: 29 , batch: 31 , training loss: 3.531111
[INFO] Epoch: 29 , batch: 32 , training loss: 3.519284
[INFO] Epoch: 29 , batch: 33 , training loss: 3.577469
[INFO] Epoch: 29 , batch: 34 , training loss: 3.537958
[INFO] Epoch: 29 , batch: 35 , training loss: 3.520417
[INFO] Epoch: 29 , batch: 36 , training loss: 3.564768
[INFO] Epoch: 29 , batch: 37 , training loss: 3.471617
[INFO] Epoch: 29 , batch: 38 , training loss: 3.505979
[INFO] Epoch: 29 , batch: 39 , training loss: 3.377781
[INFO] Epoch: 29 , batch: 40 , training loss: 3.612019
[INFO] Epoch: 29 , batch: 41 , training loss: 3.481309
[INFO] Epoch: 29 , batch: 42 , training loss: 3.947167
[INFO] Epoch: 29 , batch: 43 , training loss: 3.669187
[INFO] Epoch: 29 , batch: 44 , training loss: 4.020103
[INFO] Epoch: 29 , batch: 45 , training loss: 3.900146
[INFO] Epoch: 29 , batch: 46 , training loss: 3.832696
[INFO] Epoch: 29 , batch: 47 , training loss: 3.582004
[INFO] Epoch: 29 , batch: 48 , training loss: 3.600773
[INFO] Epoch: 29 , batch: 49 , training loss: 3.807556
[INFO] Epoch: 29 , batch: 50 , training loss: 3.622627
[INFO] Epoch: 29 , batch: 51 , training loss: 3.806555
[INFO] Epoch: 29 , batch: 52 , training loss: 3.709786
[INFO] Epoch: 29 , batch: 53 , training loss: 3.807979
[INFO] Epoch: 29 , batch: 54 , training loss: 3.780185
[INFO] Epoch: 29 , batch: 55 , training loss: 3.891547
[INFO] Epoch: 29 , batch: 56 , training loss: 3.743534
[INFO] Epoch: 29 , batch: 57 , training loss: 3.664430
[INFO] Epoch: 29 , batch: 58 , training loss: 3.721065
[INFO] Epoch: 29 , batch: 59 , training loss: 3.796407
[INFO] Epoch: 29 , batch: 60 , training loss: 3.709133
[INFO] Epoch: 29 , batch: 61 , training loss: 3.828140
[INFO] Epoch: 29 , batch: 62 , training loss: 3.703637
[INFO] Epoch: 29 , batch: 63 , training loss: 3.903989
[INFO] Epoch: 29 , batch: 64 , training loss: 4.080074
[INFO] Epoch: 29 , batch: 65 , training loss: 3.773184
[INFO] Epoch: 29 , batch: 66 , training loss: 3.621562
[INFO] Epoch: 29 , batch: 67 , training loss: 3.694457
[INFO] Epoch: 29 , batch: 68 , training loss: 3.838072
[INFO] Epoch: 29 , batch: 69 , training loss: 3.748598
[INFO] Epoch: 29 , batch: 70 , training loss: 3.963457
[INFO] Epoch: 29 , batch: 71 , training loss: 3.839071
[INFO] Epoch: 29 , batch: 72 , training loss: 3.869567
[INFO] Epoch: 29 , batch: 73 , training loss: 3.825867
[INFO] Epoch: 29 , batch: 74 , training loss: 3.915833
[INFO] Epoch: 29 , batch: 75 , training loss: 3.826746
[INFO] Epoch: 29 , batch: 76 , training loss: 3.863606
[INFO] Epoch: 29 , batch: 77 , training loss: 3.851821
[INFO] Epoch: 29 , batch: 78 , training loss: 3.966318
[INFO] Epoch: 29 , batch: 79 , training loss: 3.783789
[INFO] Epoch: 29 , batch: 80 , training loss: 3.995553
[INFO] Epoch: 29 , batch: 81 , training loss: 3.924372
[INFO] Epoch: 29 , batch: 82 , training loss: 3.875954
[INFO] Epoch: 29 , batch: 83 , training loss: 4.018281
[INFO] Epoch: 29 , batch: 84 , training loss: 3.932915
[INFO] Epoch: 29 , batch: 85 , training loss: 4.006324
[INFO] Epoch: 29 , batch: 86 , training loss: 3.943975
[INFO] Epoch: 29 , batch: 87 , training loss: 3.932543
[INFO] Epoch: 29 , batch: 88 , training loss: 4.049393
[INFO] Epoch: 29 , batch: 89 , training loss: 3.852750
[INFO] Epoch: 29 , batch: 90 , training loss: 3.944372
[INFO] Epoch: 29 , batch: 91 , training loss: 3.869921
[INFO] Epoch: 29 , batch: 92 , training loss: 3.842722
[INFO] Epoch: 29 , batch: 93 , training loss: 3.976917
[INFO] Epoch: 29 , batch: 94 , training loss: 4.144831
[INFO] Epoch: 29 , batch: 95 , training loss: 3.897526
[INFO] Epoch: 29 , batch: 96 , training loss: 3.899900
[INFO] Epoch: 29 , batch: 97 , training loss: 3.812230
[INFO] Epoch: 29 , batch: 98 , training loss: 3.777219
[INFO] Epoch: 29 , batch: 99 , training loss: 3.859603
[INFO] Epoch: 29 , batch: 100 , training loss: 3.787194
[INFO] Epoch: 29 , batch: 101 , training loss: 3.848935
[INFO] Epoch: 29 , batch: 102 , training loss: 3.919585
[INFO] Epoch: 29 , batch: 103 , training loss: 3.741558
[INFO] Epoch: 29 , batch: 104 , training loss: 3.707958
[INFO] Epoch: 29 , batch: 105 , training loss: 3.966090
[INFO] Epoch: 29 , batch: 106 , training loss: 3.940017
[INFO] Epoch: 29 , batch: 107 , training loss: 3.812486
[INFO] Epoch: 29 , batch: 108 , training loss: 3.758187
[INFO] Epoch: 29 , batch: 109 , training loss: 3.690279
[INFO] Epoch: 29 , batch: 110 , training loss: 3.848762
[INFO] Epoch: 29 , batch: 111 , training loss: 3.926234
[INFO] Epoch: 29 , batch: 112 , training loss: 3.836316
[INFO] Epoch: 29 , batch: 113 , training loss: 3.858187
[INFO] Epoch: 29 , batch: 114 , training loss: 3.837519
[INFO] Epoch: 29 , batch: 115 , training loss: 3.832246
[INFO] Epoch: 29 , batch: 116 , training loss: 3.756687
[INFO] Epoch: 29 , batch: 117 , training loss: 3.990547
[INFO] Epoch: 29 , batch: 118 , training loss: 3.944479
[INFO] Epoch: 29 , batch: 119 , training loss: 4.078324
[INFO] Epoch: 29 , batch: 120 , training loss: 4.069090
[INFO] Epoch: 29 , batch: 121 , training loss: 3.926036
[INFO] Epoch: 29 , batch: 122 , training loss: 3.847567
[INFO] Epoch: 29 , batch: 123 , training loss: 3.869159
[INFO] Epoch: 29 , batch: 124 , training loss: 3.962319
[INFO] Epoch: 29 , batch: 125 , training loss: 3.768520
[INFO] Epoch: 29 , batch: 126 , training loss: 3.793517
[INFO] Epoch: 29 , batch: 127 , training loss: 3.759960
[INFO] Epoch: 29 , batch: 128 , training loss: 3.900732
[INFO] Epoch: 29 , batch: 129 , training loss: 3.896259
[INFO] Epoch: 29 , batch: 130 , training loss: 3.892584
[INFO] Epoch: 29 , batch: 131 , training loss: 3.868011
[INFO] Epoch: 29 , batch: 132 , training loss: 3.878707
[INFO] Epoch: 29 , batch: 133 , training loss: 3.832322
[INFO] Epoch: 29 , batch: 134 , training loss: 3.660885
[INFO] Epoch: 29 , batch: 135 , training loss: 3.693678
[INFO] Epoch: 29 , batch: 136 , training loss: 3.988071
[INFO] Epoch: 29 , batch: 137 , training loss: 3.856409
[INFO] Epoch: 29 , batch: 138 , training loss: 3.954062
[INFO] Epoch: 29 , batch: 139 , training loss: 4.526516
[INFO] Epoch: 29 , batch: 140 , training loss: 4.267565
[INFO] Epoch: 29 , batch: 141 , training loss: 4.067882
[INFO] Epoch: 29 , batch: 142 , training loss: 3.804046
[INFO] Epoch: 29 , batch: 143 , training loss: 3.971187
[INFO] Epoch: 29 , batch: 144 , training loss: 3.720532
[INFO] Epoch: 29 , batch: 145 , training loss: 3.810504
[INFO] Epoch: 29 , batch: 146 , training loss: 4.038425
[INFO] Epoch: 29 , batch: 147 , training loss: 3.726890
[INFO] Epoch: 29 , batch: 148 , training loss: 3.682016
[INFO] Epoch: 29 , batch: 149 , training loss: 3.772803
[INFO] Epoch: 29 , batch: 150 , training loss: 3.965666
[INFO] Epoch: 29 , batch: 151 , training loss: 3.859331
[INFO] Epoch: 29 , batch: 152 , training loss: 3.927664
[INFO] Epoch: 29 , batch: 153 , training loss: 3.914783
[INFO] Epoch: 29 , batch: 154 , training loss: 4.006483
[INFO] Epoch: 29 , batch: 155 , training loss: 4.214760
[INFO] Epoch: 29 , batch: 156 , training loss: 3.916329
[INFO] Epoch: 29 , batch: 157 , training loss: 3.913204
[INFO] Epoch: 29 , batch: 158 , training loss: 4.041306
[INFO] Epoch: 29 , batch: 159 , training loss: 3.920664
[INFO] Epoch: 29 , batch: 160 , training loss: 4.198460
[INFO] Epoch: 29 , batch: 161 , training loss: 4.330881
[INFO] Epoch: 29 , batch: 162 , training loss: 4.294934
[INFO] Epoch: 29 , batch: 163 , training loss: 4.427391
[INFO] Epoch: 29 , batch: 164 , training loss: 4.415952
[INFO] Epoch: 29 , batch: 165 , training loss: 4.346388
[INFO] Epoch: 29 , batch: 166 , training loss: 4.182350
[INFO] Epoch: 29 , batch: 167 , training loss: 4.262110
[INFO] Epoch: 29 , batch: 168 , training loss: 3.956205
[INFO] Epoch: 29 , batch: 169 , training loss: 3.921870
[INFO] Epoch: 29 , batch: 170 , training loss: 4.139611
[INFO] Epoch: 29 , batch: 171 , training loss: 3.505650
[INFO] Epoch: 29 , batch: 172 , training loss: 3.741970
[INFO] Epoch: 29 , batch: 173 , training loss: 4.089174
[INFO] Epoch: 29 , batch: 174 , training loss: 4.473758
[INFO] Epoch: 29 , batch: 175 , training loss: 4.840441
[INFO] Epoch: 29 , batch: 176 , training loss: 4.438291
[INFO] Epoch: 29 , batch: 177 , training loss: 4.140775
[INFO] Epoch: 29 , batch: 178 , training loss: 4.123844
[INFO] Epoch: 29 , batch: 179 , training loss: 4.177924
[INFO] Epoch: 29 , batch: 180 , training loss: 4.123610
[INFO] Epoch: 29 , batch: 181 , training loss: 4.367788
[INFO] Epoch: 29 , batch: 182 , training loss: 4.358871
[INFO] Epoch: 29 , batch: 183 , training loss: 4.309985
[INFO] Epoch: 29 , batch: 184 , training loss: 4.175995
[INFO] Epoch: 29 , batch: 185 , training loss: 4.126166
[INFO] Epoch: 29 , batch: 186 , training loss: 4.325388
[INFO] Epoch: 29 , batch: 187 , training loss: 4.440248
[INFO] Epoch: 29 , batch: 188 , training loss: 4.412222
[INFO] Epoch: 29 , batch: 189 , training loss: 4.311187
[INFO] Epoch: 29 , batch: 190 , training loss: 4.313129
[INFO] Epoch: 29 , batch: 191 , training loss: 4.442297
[INFO] Epoch: 29 , batch: 192 , training loss: 4.250385
[INFO] Epoch: 29 , batch: 193 , training loss: 4.344503
[INFO] Epoch: 29 , batch: 194 , training loss: 4.293022
[INFO] Epoch: 29 , batch: 195 , training loss: 4.211145
[INFO] Epoch: 29 , batch: 196 , training loss: 4.096835
[INFO] Epoch: 29 , batch: 197 , training loss: 4.180636
[INFO] Epoch: 29 , batch: 198 , training loss: 4.112506
[INFO] Epoch: 29 , batch: 199 , training loss: 4.265993
[INFO] Epoch: 29 , batch: 200 , training loss: 4.151038
[INFO] Epoch: 29 , batch: 201 , training loss: 4.063261
[INFO] Epoch: 29 , batch: 202 , training loss: 4.069540
[INFO] Epoch: 29 , batch: 203 , training loss: 4.162795
[INFO] Epoch: 29 , batch: 204 , training loss: 4.299768
[INFO] Epoch: 29 , batch: 205 , training loss: 3.851560
[INFO] Epoch: 29 , batch: 206 , training loss: 3.816710
[INFO] Epoch: 29 , batch: 207 , training loss: 3.800192
[INFO] Epoch: 29 , batch: 208 , training loss: 4.126364
[INFO] Epoch: 29 , batch: 209 , training loss: 4.069971
[INFO] Epoch: 29 , batch: 210 , training loss: 4.083055
[INFO] Epoch: 29 , batch: 211 , training loss: 4.085710
[INFO] Epoch: 29 , batch: 212 , training loss: 4.176504
[INFO] Epoch: 29 , batch: 213 , training loss: 4.150410
[INFO] Epoch: 29 , batch: 214 , training loss: 4.216884
[INFO] Epoch: 29 , batch: 215 , training loss: 4.419947
[INFO] Epoch: 29 , batch: 216 , training loss: 4.113228
[INFO] Epoch: 29 , batch: 217 , training loss: 4.097047
[INFO] Epoch: 29 , batch: 218 , training loss: 4.082290
[INFO] Epoch: 29 , batch: 219 , training loss: 4.188545
[INFO] Epoch: 29 , batch: 220 , training loss: 3.987619
[INFO] Epoch: 29 , batch: 221 , training loss: 4.004220
[INFO] Epoch: 29 , batch: 222 , training loss: 4.178091
[INFO] Epoch: 29 , batch: 223 , training loss: 4.251774
[INFO] Epoch: 29 , batch: 224 , training loss: 4.286422
[INFO] Epoch: 29 , batch: 225 , training loss: 4.193595
[INFO] Epoch: 29 , batch: 226 , training loss: 4.324057
[INFO] Epoch: 29 , batch: 227 , training loss: 4.257511
[INFO] Epoch: 29 , batch: 228 , training loss: 4.310971
[INFO] Epoch: 29 , batch: 229 , training loss: 4.171314
[INFO] Epoch: 29 , batch: 230 , training loss: 4.043789
[INFO] Epoch: 29 , batch: 231 , training loss: 3.889739
[INFO] Epoch: 29 , batch: 232 , training loss: 4.056470
[INFO] Epoch: 29 , batch: 233 , training loss: 4.064636
[INFO] Epoch: 29 , batch: 234 , training loss: 3.771378
[INFO] Epoch: 29 , batch: 235 , training loss: 3.854185
[INFO] Epoch: 29 , batch: 236 , training loss: 3.959099
[INFO] Epoch: 29 , batch: 237 , training loss: 4.185318
[INFO] Epoch: 29 , batch: 238 , training loss: 3.951921
[INFO] Epoch: 29 , batch: 239 , training loss: 3.993764
[INFO] Epoch: 29 , batch: 240 , training loss: 4.043985
[INFO] Epoch: 29 , batch: 241 , training loss: 3.851399
[INFO] Epoch: 29 , batch: 242 , training loss: 3.877200
[INFO] Epoch: 29 , batch: 243 , training loss: 4.170786
[INFO] Epoch: 29 , batch: 244 , training loss: 4.094886
[INFO] Epoch: 29 , batch: 245 , training loss: 4.080737
[INFO] Epoch: 29 , batch: 246 , training loss: 3.788902
[INFO] Epoch: 29 , batch: 247 , training loss: 3.944036
[INFO] Epoch: 29 , batch: 248 , training loss: 4.012857
[INFO] Epoch: 29 , batch: 249 , training loss: 4.046214
[INFO] Epoch: 29 , batch: 250 , training loss: 3.786801
[INFO] Epoch: 29 , batch: 251 , training loss: 4.249701
[INFO] Epoch: 29 , batch: 252 , training loss: 3.921389
[INFO] Epoch: 29 , batch: 253 , training loss: 3.871955
[INFO] Epoch: 29 , batch: 254 , training loss: 4.150641
[INFO] Epoch: 29 , batch: 255 , training loss: 4.120451
[INFO] Epoch: 29 , batch: 256 , training loss: 4.142938
[INFO] Epoch: 29 , batch: 257 , training loss: 4.255370
[INFO] Epoch: 29 , batch: 258 , training loss: 4.315669
[INFO] Epoch: 29 , batch: 259 , training loss: 4.381701
[INFO] Epoch: 29 , batch: 260 , training loss: 4.085906
[INFO] Epoch: 29 , batch: 261 , training loss: 4.234346
[INFO] Epoch: 29 , batch: 262 , training loss: 4.424886
[INFO] Epoch: 29 , batch: 263 , training loss: 4.588107
[INFO] Epoch: 29 , batch: 264 , training loss: 3.906089
[INFO] Epoch: 29 , batch: 265 , training loss: 4.057195
[INFO] Epoch: 29 , batch: 266 , training loss: 4.470611
[INFO] Epoch: 29 , batch: 267 , training loss: 4.235456
[INFO] Epoch: 29 , batch: 268 , training loss: 4.137709
[INFO] Epoch: 29 , batch: 269 , training loss: 4.132727
[INFO] Epoch: 29 , batch: 270 , training loss: 4.140772
[INFO] Epoch: 29 , batch: 271 , training loss: 4.189218
[INFO] Epoch: 29 , batch: 272 , training loss: 4.152231
[INFO] Epoch: 29 , batch: 273 , training loss: 4.189426
[INFO] Epoch: 29 , batch: 274 , training loss: 4.234144
[INFO] Epoch: 29 , batch: 275 , training loss: 4.135736
[INFO] Epoch: 29 , batch: 276 , training loss: 4.188471
[INFO] Epoch: 29 , batch: 277 , training loss: 4.321792
[INFO] Epoch: 29 , batch: 278 , training loss: 4.016679
[INFO] Epoch: 29 , batch: 279 , training loss: 4.025144
[INFO] Epoch: 29 , batch: 280 , training loss: 3.971922
[INFO] Epoch: 29 , batch: 281 , training loss: 4.116309
[INFO] Epoch: 29 , batch: 282 , training loss: 4.016069
[INFO] Epoch: 29 , batch: 283 , training loss: 4.058479
[INFO] Epoch: 29 , batch: 284 , training loss: 4.081935
[INFO] Epoch: 29 , batch: 285 , training loss: 4.011172
[INFO] Epoch: 29 , batch: 286 , training loss: 4.001385
[INFO] Epoch: 29 , batch: 287 , training loss: 3.971933
[INFO] Epoch: 29 , batch: 288 , training loss: 3.960395
[INFO] Epoch: 29 , batch: 289 , training loss: 4.007455
[INFO] Epoch: 29 , batch: 290 , training loss: 3.797297
[INFO] Epoch: 29 , batch: 291 , training loss: 3.760194
[INFO] Epoch: 29 , batch: 292 , training loss: 3.886262
[INFO] Epoch: 29 , batch: 293 , training loss: 3.807821
[INFO] Epoch: 29 , batch: 294 , training loss: 4.484947
[INFO] Epoch: 29 , batch: 295 , training loss: 4.233147
[INFO] Epoch: 29 , batch: 296 , training loss: 4.172487
[INFO] Epoch: 29 , batch: 297 , training loss: 4.133070
[INFO] Epoch: 29 , batch: 298 , training loss: 3.971436
[INFO] Epoch: 29 , batch: 299 , training loss: 3.992489
[INFO] Epoch: 29 , batch: 300 , training loss: 3.953040
[INFO] Epoch: 29 , batch: 301 , training loss: 3.903006
[INFO] Epoch: 29 , batch: 302 , training loss: 4.071870
[INFO] Epoch: 29 , batch: 303 , training loss: 4.075867
[INFO] Epoch: 29 , batch: 304 , training loss: 4.244768
[INFO] Epoch: 29 , batch: 305 , training loss: 4.044497
[INFO] Epoch: 29 , batch: 306 , training loss: 4.174998
[INFO] Epoch: 29 , batch: 307 , training loss: 4.165463
[INFO] Epoch: 29 , batch: 308 , training loss: 4.008060
[INFO] Epoch: 29 , batch: 309 , training loss: 4.017115
[INFO] Epoch: 29 , batch: 310 , training loss: 3.902413
[INFO] Epoch: 29 , batch: 311 , training loss: 3.907505
[INFO] Epoch: 29 , batch: 312 , training loss: 3.839536
[INFO] Epoch: 29 , batch: 313 , training loss: 3.956186
[INFO] Epoch: 29 , batch: 314 , training loss: 4.001106
[INFO] Epoch: 29 , batch: 315 , training loss: 4.059600
[INFO] Epoch: 29 , batch: 316 , training loss: 4.289968
[INFO] Epoch: 29 , batch: 317 , training loss: 4.731688
[INFO] Epoch: 29 , batch: 318 , training loss: 4.883799
[INFO] Epoch: 29 , batch: 319 , training loss: 4.489538
[INFO] Epoch: 29 , batch: 320 , training loss: 4.023583
[INFO] Epoch: 29 , batch: 321 , training loss: 3.854245
[INFO] Epoch: 29 , batch: 322 , training loss: 3.974643
[INFO] Epoch: 29 , batch: 323 , training loss: 3.989976
[INFO] Epoch: 29 , batch: 324 , training loss: 3.939953
[INFO] Epoch: 29 , batch: 325 , training loss: 4.120005
[INFO] Epoch: 29 , batch: 326 , training loss: 4.169123
[INFO] Epoch: 29 , batch: 327 , training loss: 4.076411
[INFO] Epoch: 29 , batch: 328 , training loss: 4.064631
[INFO] Epoch: 29 , batch: 329 , training loss: 3.981138
[INFO] Epoch: 29 , batch: 330 , training loss: 3.991783
[INFO] Epoch: 29 , batch: 331 , training loss: 4.142583
[INFO] Epoch: 29 , batch: 332 , training loss: 3.941055
[INFO] Epoch: 29 , batch: 333 , training loss: 3.959742
[INFO] Epoch: 29 , batch: 334 , training loss: 3.975062
[INFO] Epoch: 29 , batch: 335 , training loss: 4.075826
[INFO] Epoch: 29 , batch: 336 , training loss: 4.111161
[INFO] Epoch: 29 , batch: 337 , training loss: 4.131544
[INFO] Epoch: 29 , batch: 338 , training loss: 4.353619
[INFO] Epoch: 29 , batch: 339 , training loss: 4.220750
[INFO] Epoch: 29 , batch: 340 , training loss: 4.352341
[INFO] Epoch: 29 , batch: 341 , training loss: 4.105444
[INFO] Epoch: 29 , batch: 342 , training loss: 3.898965
[INFO] Epoch: 29 , batch: 343 , training loss: 3.964471
[INFO] Epoch: 29 , batch: 344 , training loss: 3.831518
[INFO] Epoch: 29 , batch: 345 , training loss: 3.966489
[INFO] Epoch: 29 , batch: 346 , training loss: 4.005778
[INFO] Epoch: 29 , batch: 347 , training loss: 3.914416
[INFO] Epoch: 29 , batch: 348 , training loss: 4.028217
[INFO] Epoch: 29 , batch: 349 , training loss: 4.199459
[INFO] Epoch: 29 , batch: 350 , training loss: 3.954938
[INFO] Epoch: 29 , batch: 351 , training loss: 4.021774
[INFO] Epoch: 29 , batch: 352 , training loss: 4.042565
[INFO] Epoch: 29 , batch: 353 , training loss: 4.018798
[INFO] Epoch: 29 , batch: 354 , training loss: 4.125913
[INFO] Epoch: 29 , batch: 355 , training loss: 4.171340
[INFO] Epoch: 29 , batch: 356 , training loss: 4.009067
[INFO] Epoch: 29 , batch: 357 , training loss: 4.105312
[INFO] Epoch: 29 , batch: 358 , training loss: 3.965953
[INFO] Epoch: 29 , batch: 359 , training loss: 3.989806
[INFO] Epoch: 29 , batch: 360 , training loss: 4.065557
[INFO] Epoch: 29 , batch: 361 , training loss: 4.033433
[INFO] Epoch: 29 , batch: 362 , training loss: 4.138832
[INFO] Epoch: 29 , batch: 363 , training loss: 4.020034
[INFO] Epoch: 29 , batch: 364 , training loss: 4.077178
[INFO] Epoch: 29 , batch: 365 , training loss: 4.007302
[INFO] Epoch: 29 , batch: 366 , training loss: 4.118443
[INFO] Epoch: 29 , batch: 367 , training loss: 4.189244
[INFO] Epoch: 29 , batch: 368 , training loss: 4.602366
[INFO] Epoch: 29 , batch: 369 , training loss: 4.252139
[INFO] Epoch: 29 , batch: 370 , training loss: 4.035094
[INFO] Epoch: 29 , batch: 371 , training loss: 4.404035
[INFO] Epoch: 29 , batch: 372 , training loss: 4.714411
[INFO] Epoch: 29 , batch: 373 , training loss: 4.802745
[INFO] Epoch: 29 , batch: 374 , training loss: 4.850937
[INFO] Epoch: 29 , batch: 375 , training loss: 4.851557
[INFO] Epoch: 29 , batch: 376 , training loss: 4.793653
[INFO] Epoch: 29 , batch: 377 , training loss: 4.550604
[INFO] Epoch: 29 , batch: 378 , training loss: 4.629012
[INFO] Epoch: 29 , batch: 379 , training loss: 4.612844
[INFO] Epoch: 29 , batch: 380 , training loss: 4.709353
[INFO] Epoch: 29 , batch: 381 , training loss: 4.447474
[INFO] Epoch: 29 , batch: 382 , training loss: 4.668631
[INFO] Epoch: 29 , batch: 383 , training loss: 4.686090
[INFO] Epoch: 29 , batch: 384 , training loss: 4.705625
[INFO] Epoch: 29 , batch: 385 , training loss: 4.430741
[INFO] Epoch: 29 , batch: 386 , training loss: 4.715077
[INFO] Epoch: 29 , batch: 387 , training loss: 4.658928
[INFO] Epoch: 29 , batch: 388 , training loss: 4.453530
[INFO] Epoch: 29 , batch: 389 , training loss: 4.310539
[INFO] Epoch: 29 , batch: 390 , training loss: 4.289075
[INFO] Epoch: 29 , batch: 391 , training loss: 4.353276
[INFO] Epoch: 29 , batch: 392 , training loss: 4.684665
[INFO] Epoch: 29 , batch: 393 , training loss: 4.562865
[INFO] Epoch: 29 , batch: 394 , training loss: 4.605034
[INFO] Epoch: 29 , batch: 395 , training loss: 4.485929
[INFO] Epoch: 29 , batch: 396 , training loss: 4.240542
[INFO] Epoch: 29 , batch: 397 , training loss: 4.419948
[INFO] Epoch: 29 , batch: 398 , training loss: 4.278537
[INFO] Epoch: 29 , batch: 399 , training loss: 4.345805
[INFO] Epoch: 29 , batch: 400 , training loss: 4.311694
[INFO] Epoch: 29 , batch: 401 , training loss: 4.714466
[INFO] Epoch: 29 , batch: 402 , training loss: 4.439661
[INFO] Epoch: 29 , batch: 403 , training loss: 4.239698
[INFO] Epoch: 29 , batch: 404 , training loss: 4.437980
[INFO] Epoch: 29 , batch: 405 , training loss: 4.488470
[INFO] Epoch: 29 , batch: 406 , training loss: 4.409516
[INFO] Epoch: 29 , batch: 407 , training loss: 4.449913
[INFO] Epoch: 29 , batch: 408 , training loss: 4.426150
[INFO] Epoch: 29 , batch: 409 , training loss: 4.408658
[INFO] Epoch: 29 , batch: 410 , training loss: 4.441232
[INFO] Epoch: 29 , batch: 411 , training loss: 4.623487
[INFO] Epoch: 29 , batch: 412 , training loss: 4.489554
[INFO] Epoch: 29 , batch: 413 , training loss: 4.379904
[INFO] Epoch: 29 , batch: 414 , training loss: 4.377357
[INFO] Epoch: 29 , batch: 415 , training loss: 4.402968
[INFO] Epoch: 29 , batch: 416 , training loss: 4.493449
[INFO] Epoch: 29 , batch: 417 , training loss: 4.388943
[INFO] Epoch: 29 , batch: 418 , training loss: 4.429241
[INFO] Epoch: 29 , batch: 419 , training loss: 4.397572
[INFO] Epoch: 29 , batch: 420 , training loss: 4.359112
[INFO] Epoch: 29 , batch: 421 , training loss: 4.358779
[INFO] Epoch: 29 , batch: 422 , training loss: 4.237297
[INFO] Epoch: 29 , batch: 423 , training loss: 4.430287
[INFO] Epoch: 29 , batch: 424 , training loss: 4.609240
[INFO] Epoch: 29 , batch: 425 , training loss: 4.452306
[INFO] Epoch: 29 , batch: 426 , training loss: 4.228594
[INFO] Epoch: 29 , batch: 427 , training loss: 4.448600
[INFO] Epoch: 29 , batch: 428 , training loss: 4.324708
[INFO] Epoch: 29 , batch: 429 , training loss: 4.181465
[INFO] Epoch: 29 , batch: 430 , training loss: 4.456378
[INFO] Epoch: 29 , batch: 431 , training loss: 4.051638
[INFO] Epoch: 29 , batch: 432 , training loss: 4.106312
[INFO] Epoch: 29 , batch: 433 , training loss: 4.151453
[INFO] Epoch: 29 , batch: 434 , training loss: 4.020079
[INFO] Epoch: 29 , batch: 435 , training loss: 4.382424
[INFO] Epoch: 29 , batch: 436 , training loss: 4.461382
[INFO] Epoch: 29 , batch: 437 , training loss: 4.221449
[INFO] Epoch: 29 , batch: 438 , training loss: 4.066573
[INFO] Epoch: 29 , batch: 439 , training loss: 4.281501
[INFO] Epoch: 29 , batch: 440 , training loss: 4.417690
[INFO] Epoch: 29 , batch: 441 , training loss: 4.524320
[INFO] Epoch: 29 , batch: 442 , training loss: 4.277797
[INFO] Epoch: 29 , batch: 443 , training loss: 4.469579
[INFO] Epoch: 29 , batch: 444 , training loss: 4.097154
[INFO] Epoch: 29 , batch: 445 , training loss: 3.989790
[INFO] Epoch: 29 , batch: 446 , training loss: 3.938573
[INFO] Epoch: 29 , batch: 447 , training loss: 4.121870
[INFO] Epoch: 29 , batch: 448 , training loss: 4.237482
[INFO] Epoch: 29 , batch: 449 , training loss: 4.629518
[INFO] Epoch: 29 , batch: 450 , training loss: 4.671311
[INFO] Epoch: 29 , batch: 451 , training loss: 4.593420
[INFO] Epoch: 29 , batch: 452 , training loss: 4.385559
[INFO] Epoch: 29 , batch: 453 , training loss: 4.148232
[INFO] Epoch: 29 , batch: 454 , training loss: 4.305537
[INFO] Epoch: 29 , batch: 455 , training loss: 4.360629
[INFO] Epoch: 29 , batch: 456 , training loss: 4.348279
[INFO] Epoch: 29 , batch: 457 , training loss: 4.437699
[INFO] Epoch: 29 , batch: 458 , training loss: 4.164005
[INFO] Epoch: 29 , batch: 459 , training loss: 4.157064
[INFO] Epoch: 29 , batch: 460 , training loss: 4.246787
[INFO] Epoch: 29 , batch: 461 , training loss: 4.232127
[INFO] Epoch: 29 , batch: 462 , training loss: 4.296139
[INFO] Epoch: 29 , batch: 463 , training loss: 4.205188
[INFO] Epoch: 29 , batch: 464 , training loss: 4.386312
[INFO] Epoch: 29 , batch: 465 , training loss: 4.323776
[INFO] Epoch: 29 , batch: 466 , training loss: 4.404941
[INFO] Epoch: 29 , batch: 467 , training loss: 4.399209
[INFO] Epoch: 29 , batch: 468 , training loss: 4.357162
[INFO] Epoch: 29 , batch: 469 , training loss: 4.377049
[INFO] Epoch: 29 , batch: 470 , training loss: 4.199048
[INFO] Epoch: 29 , batch: 471 , training loss: 4.277928
[INFO] Epoch: 29 , batch: 472 , training loss: 4.330667
[INFO] Epoch: 29 , batch: 473 , training loss: 4.261586
[INFO] Epoch: 29 , batch: 474 , training loss: 4.054977
[INFO] Epoch: 29 , batch: 475 , training loss: 3.944858
[INFO] Epoch: 29 , batch: 476 , training loss: 4.310035
[INFO] Epoch: 29 , batch: 477 , training loss: 4.428009
[INFO] Epoch: 29 , batch: 478 , training loss: 4.484725
[INFO] Epoch: 29 , batch: 479 , training loss: 4.432838
[INFO] Epoch: 29 , batch: 480 , training loss: 4.539079
[INFO] Epoch: 29 , batch: 481 , training loss: 4.404900
[INFO] Epoch: 29 , batch: 482 , training loss: 4.526487
[INFO] Epoch: 29 , batch: 483 , training loss: 4.389946
[INFO] Epoch: 29 , batch: 484 , training loss: 4.183145
[INFO] Epoch: 29 , batch: 485 , training loss: 4.283078
[INFO] Epoch: 29 , batch: 486 , training loss: 4.155602
[INFO] Epoch: 29 , batch: 487 , training loss: 4.142878
[INFO] Epoch: 29 , batch: 488 , training loss: 4.351208
[INFO] Epoch: 29 , batch: 489 , training loss: 4.233959
[INFO] Epoch: 29 , batch: 490 , training loss: 4.274277
[INFO] Epoch: 29 , batch: 491 , training loss: 4.217134
[INFO] Epoch: 29 , batch: 492 , training loss: 4.185245
[INFO] Epoch: 29 , batch: 493 , training loss: 4.378529
[INFO] Epoch: 29 , batch: 494 , training loss: 4.292958
[INFO] Epoch: 29 , batch: 495 , training loss: 4.414941
[INFO] Epoch: 29 , batch: 496 , training loss: 4.293699
[INFO] Epoch: 29 , batch: 497 , training loss: 4.332685
[INFO] Epoch: 29 , batch: 498 , training loss: 4.328243
[INFO] Epoch: 29 , batch: 499 , training loss: 4.392149
[INFO] Epoch: 29 , batch: 500 , training loss: 4.572586
[INFO] Epoch: 29 , batch: 501 , training loss: 4.874216
[INFO] Epoch: 29 , batch: 502 , training loss: 4.931268
[INFO] Epoch: 29 , batch: 503 , training loss: 4.606159
[INFO] Epoch: 29 , batch: 504 , training loss: 4.732462
[INFO] Epoch: 29 , batch: 505 , training loss: 4.712843
[INFO] Epoch: 29 , batch: 506 , training loss: 4.661664
[INFO] Epoch: 29 , batch: 507 , training loss: 4.731905
[INFO] Epoch: 29 , batch: 508 , training loss: 4.677897
[INFO] Epoch: 29 , batch: 509 , training loss: 4.472060
[INFO] Epoch: 29 , batch: 510 , training loss: 4.534222
[INFO] Epoch: 29 , batch: 511 , training loss: 4.448604
[INFO] Epoch: 29 , batch: 512 , training loss: 4.546458
[INFO] Epoch: 29 , batch: 513 , training loss: 4.781199
[INFO] Epoch: 29 , batch: 514 , training loss: 4.433415
[INFO] Epoch: 29 , batch: 515 , training loss: 4.676667
[INFO] Epoch: 29 , batch: 516 , training loss: 4.504673
[INFO] Epoch: 29 , batch: 517 , training loss: 4.449198
[INFO] Epoch: 29 , batch: 518 , training loss: 4.423872
[INFO] Epoch: 29 , batch: 519 , training loss: 4.272805
[INFO] Epoch: 29 , batch: 520 , training loss: 4.498884
[INFO] Epoch: 29 , batch: 521 , training loss: 4.472191
[INFO] Epoch: 29 , batch: 522 , training loss: 4.529152
[INFO] Epoch: 29 , batch: 523 , training loss: 4.434724
[INFO] Epoch: 29 , batch: 524 , training loss: 4.728923
[INFO] Epoch: 29 , batch: 525 , training loss: 4.662930
[INFO] Epoch: 29 , batch: 526 , training loss: 4.424679
[INFO] Epoch: 29 , batch: 527 , training loss: 4.467442
[INFO] Epoch: 29 , batch: 528 , training loss: 4.487639
[INFO] Epoch: 29 , batch: 529 , training loss: 4.466920
[INFO] Epoch: 29 , batch: 530 , training loss: 4.304345
[INFO] Epoch: 29 , batch: 531 , training loss: 4.457941
[INFO] Epoch: 29 , batch: 532 , training loss: 4.334333
[INFO] Epoch: 29 , batch: 533 , training loss: 4.507024
[INFO] Epoch: 29 , batch: 534 , training loss: 4.478985
[INFO] Epoch: 29 , batch: 535 , training loss: 4.489795
[INFO] Epoch: 29 , batch: 536 , training loss: 4.309596
[INFO] Epoch: 29 , batch: 537 , training loss: 4.323554
[INFO] Epoch: 29 , batch: 538 , training loss: 4.407470
[INFO] Epoch: 29 , batch: 539 , training loss: 4.498620
[INFO] Epoch: 29 , batch: 540 , training loss: 4.984113
[INFO] Epoch: 29 , batch: 541 , training loss: 4.888569
[INFO] Epoch: 29 , batch: 542 , training loss: 4.787845
[INFO] Epoch: 30 , batch: 0 , training loss: 3.497849
[INFO] Epoch: 30 , batch: 1 , training loss: 3.476054
[INFO] Epoch: 30 , batch: 2 , training loss: 3.609108
[INFO] Epoch: 30 , batch: 3 , training loss: 3.459047
[INFO] Epoch: 30 , batch: 4 , training loss: 3.821100
[INFO] Epoch: 30 , batch: 5 , training loss: 3.475893
[INFO] Epoch: 30 , batch: 6 , training loss: 3.857255
[INFO] Epoch: 30 , batch: 7 , training loss: 3.773277
[INFO] Epoch: 30 , batch: 8 , training loss: 3.504608
[INFO] Epoch: 30 , batch: 9 , training loss: 3.778530
[INFO] Epoch: 30 , batch: 10 , training loss: 3.730917
[INFO] Epoch: 30 , batch: 11 , training loss: 3.673449
[INFO] Epoch: 30 , batch: 12 , training loss: 3.568165
[INFO] Epoch: 30 , batch: 13 , training loss: 3.649535
[INFO] Epoch: 30 , batch: 14 , training loss: 3.484738
[INFO] Epoch: 30 , batch: 15 , training loss: 3.693915
[INFO] Epoch: 30 , batch: 16 , training loss: 3.539377
[INFO] Epoch: 30 , batch: 17 , training loss: 3.737866
[INFO] Epoch: 30 , batch: 18 , training loss: 3.679854
[INFO] Epoch: 30 , batch: 19 , training loss: 3.395240
[INFO] Epoch: 30 , batch: 20 , training loss: 3.388462
[INFO] Epoch: 30 , batch: 21 , training loss: 3.516974
[INFO] Epoch: 30 , batch: 22 , training loss: 3.406834
[INFO] Epoch: 30 , batch: 23 , training loss: 3.592921
[INFO] Epoch: 30 , batch: 24 , training loss: 3.420390
[INFO] Epoch: 30 , batch: 25 , training loss: 3.608186
[INFO] Epoch: 30 , batch: 26 , training loss: 3.467601
[INFO] Epoch: 30 , batch: 27 , training loss: 3.422900
[INFO] Epoch: 30 , batch: 28 , training loss: 3.559092
[INFO] Epoch: 30 , batch: 29 , training loss: 3.405630
[INFO] Epoch: 30 , batch: 30 , training loss: 3.479357
[INFO] Epoch: 30 , batch: 31 , training loss: 3.520759
[INFO] Epoch: 30 , batch: 32 , training loss: 3.509787
[INFO] Epoch: 30 , batch: 33 , training loss: 3.535752
[INFO] Epoch: 30 , batch: 34 , training loss: 3.499608
[INFO] Epoch: 30 , batch: 35 , training loss: 3.535938
[INFO] Epoch: 30 , batch: 36 , training loss: 3.550793
[INFO] Epoch: 30 , batch: 37 , training loss: 3.470926
[INFO] Epoch: 30 , batch: 38 , training loss: 3.502307
[INFO] Epoch: 30 , batch: 39 , training loss: 3.345170
[INFO] Epoch: 30 , batch: 40 , training loss: 3.595012
[INFO] Epoch: 30 , batch: 41 , training loss: 3.461956
[INFO] Epoch: 30 , batch: 42 , training loss: 3.891568
[INFO] Epoch: 30 , batch: 43 , training loss: 3.649668
[INFO] Epoch: 30 , batch: 44 , training loss: 4.010300
[INFO] Epoch: 30 , batch: 45 , training loss: 3.887607
[INFO] Epoch: 30 , batch: 46 , training loss: 3.774868
[INFO] Epoch: 30 , batch: 47 , training loss: 3.598044
[INFO] Epoch: 30 , batch: 48 , training loss: 3.579467
[INFO] Epoch: 30 , batch: 49 , training loss: 3.764987
[INFO] Epoch: 30 , batch: 50 , training loss: 3.612375
[INFO] Epoch: 30 , batch: 51 , training loss: 3.786332
[INFO] Epoch: 30 , batch: 52 , training loss: 3.655195
[INFO] Epoch: 30 , batch: 53 , training loss: 3.783507
[INFO] Epoch: 30 , batch: 54 , training loss: 3.776508
[INFO] Epoch: 30 , batch: 55 , training loss: 3.856870
[INFO] Epoch: 30 , batch: 56 , training loss: 3.731568
[INFO] Epoch: 30 , batch: 57 , training loss: 3.624754
[INFO] Epoch: 30 , batch: 58 , training loss: 3.700766
[INFO] Epoch: 30 , batch: 59 , training loss: 3.762835
[INFO] Epoch: 30 , batch: 60 , training loss: 3.719819
[INFO] Epoch: 30 , batch: 61 , training loss: 3.786683
[INFO] Epoch: 30 , batch: 62 , training loss: 3.683962
[INFO] Epoch: 30 , batch: 63 , training loss: 3.877161
[INFO] Epoch: 30 , batch: 64 , training loss: 4.053602
[INFO] Epoch: 30 , batch: 65 , training loss: 3.727616
[INFO] Epoch: 30 , batch: 66 , training loss: 3.603681
[INFO] Epoch: 30 , batch: 67 , training loss: 3.686138
[INFO] Epoch: 30 , batch: 68 , training loss: 3.839491
[INFO] Epoch: 30 , batch: 69 , training loss: 3.699445
[INFO] Epoch: 30 , batch: 70 , training loss: 3.931627
[INFO] Epoch: 30 , batch: 71 , training loss: 3.809798
[INFO] Epoch: 30 , batch: 72 , training loss: 3.869091
[INFO] Epoch: 30 , batch: 73 , training loss: 3.794285
[INFO] Epoch: 30 , batch: 74 , training loss: 3.897324
[INFO] Epoch: 30 , batch: 75 , training loss: 3.807053
[INFO] Epoch: 30 , batch: 76 , training loss: 3.852769
[INFO] Epoch: 30 , batch: 77 , training loss: 3.810099
[INFO] Epoch: 30 , batch: 78 , training loss: 3.957856
[INFO] Epoch: 30 , batch: 79 , training loss: 3.761226
[INFO] Epoch: 30 , batch: 80 , training loss: 3.988309
[INFO] Epoch: 30 , batch: 81 , training loss: 3.905463
[INFO] Epoch: 30 , batch: 82 , training loss: 3.864331
[INFO] Epoch: 30 , batch: 83 , training loss: 3.999910
[INFO] Epoch: 30 , batch: 84 , training loss: 3.930002
[INFO] Epoch: 30 , batch: 85 , training loss: 3.986516
[INFO] Epoch: 30 , batch: 86 , training loss: 3.928396
[INFO] Epoch: 30 , batch: 87 , training loss: 3.925850
[INFO] Epoch: 30 , batch: 88 , training loss: 4.050184
[INFO] Epoch: 30 , batch: 89 , training loss: 3.848131
[INFO] Epoch: 30 , batch: 90 , training loss: 3.916724
[INFO] Epoch: 30 , batch: 91 , training loss: 3.868432
[INFO] Epoch: 30 , batch: 92 , training loss: 3.851956
[INFO] Epoch: 30 , batch: 93 , training loss: 3.943287
[INFO] Epoch: 30 , batch: 94 , training loss: 4.122288
[INFO] Epoch: 30 , batch: 95 , training loss: 3.878865
[INFO] Epoch: 30 , batch: 96 , training loss: 3.899702
[INFO] Epoch: 30 , batch: 97 , training loss: 3.784470
[INFO] Epoch: 30 , batch: 98 , training loss: 3.766783
[INFO] Epoch: 30 , batch: 99 , training loss: 3.834233
[INFO] Epoch: 30 , batch: 100 , training loss: 3.787467
[INFO] Epoch: 30 , batch: 101 , training loss: 3.819452
[INFO] Epoch: 30 , batch: 102 , training loss: 3.929075
[INFO] Epoch: 30 , batch: 103 , training loss: 3.734523
[INFO] Epoch: 30 , batch: 104 , training loss: 3.678916
[INFO] Epoch: 30 , batch: 105 , training loss: 3.960329
[INFO] Epoch: 30 , batch: 106 , training loss: 3.927126
[INFO] Epoch: 30 , batch: 107 , training loss: 3.834437
[INFO] Epoch: 30 , batch: 108 , training loss: 3.714509
[INFO] Epoch: 30 , batch: 109 , training loss: 3.657863
[INFO] Epoch: 30 , batch: 110 , training loss: 3.830683
[INFO] Epoch: 30 , batch: 111 , training loss: 3.916102
[INFO] Epoch: 30 , batch: 112 , training loss: 3.839928
[INFO] Epoch: 30 , batch: 113 , training loss: 3.840510
[INFO] Epoch: 30 , batch: 114 , training loss: 3.818931
[INFO] Epoch: 30 , batch: 115 , training loss: 3.842662
[INFO] Epoch: 30 , batch: 116 , training loss: 3.762882
[INFO] Epoch: 30 , batch: 117 , training loss: 3.974357
[INFO] Epoch: 30 , batch: 118 , training loss: 3.919946
[INFO] Epoch: 30 , batch: 119 , training loss: 4.057162
[INFO] Epoch: 30 , batch: 120 , training loss: 4.040986
[INFO] Epoch: 30 , batch: 121 , training loss: 3.897621
[INFO] Epoch: 30 , batch: 122 , training loss: 3.811487
[INFO] Epoch: 30 , batch: 123 , training loss: 3.852988
[INFO] Epoch: 30 , batch: 124 , training loss: 3.936222
[INFO] Epoch: 30 , batch: 125 , training loss: 3.750980
[INFO] Epoch: 30 , batch: 126 , training loss: 3.745561
[INFO] Epoch: 30 , batch: 127 , training loss: 3.746163
[INFO] Epoch: 30 , batch: 128 , training loss: 3.881850
[INFO] Epoch: 30 , batch: 129 , training loss: 3.885917
[INFO] Epoch: 30 , batch: 130 , training loss: 3.888746
[INFO] Epoch: 30 , batch: 131 , training loss: 3.864520
[INFO] Epoch: 30 , batch: 132 , training loss: 3.840459
[INFO] Epoch: 30 , batch: 133 , training loss: 3.826322
[INFO] Epoch: 30 , batch: 134 , training loss: 3.653499
[INFO] Epoch: 30 , batch: 135 , training loss: 3.705086
[INFO] Epoch: 30 , batch: 136 , training loss: 3.969504
[INFO] Epoch: 30 , batch: 137 , training loss: 3.867896
[INFO] Epoch: 30 , batch: 138 , training loss: 3.932025
[INFO] Epoch: 30 , batch: 139 , training loss: 4.488827
[INFO] Epoch: 30 , batch: 140 , training loss: 4.279239
[INFO] Epoch: 30 , batch: 141 , training loss: 4.049168
[INFO] Epoch: 30 , batch: 142 , training loss: 3.798573
[INFO] Epoch: 30 , batch: 143 , training loss: 3.951085
[INFO] Epoch: 30 , batch: 144 , training loss: 3.745976
[INFO] Epoch: 30 , batch: 145 , training loss: 3.811807
[INFO] Epoch: 30 , batch: 146 , training loss: 4.017204
[INFO] Epoch: 30 , batch: 147 , training loss: 3.720129
[INFO] Epoch: 30 , batch: 148 , training loss: 3.699927
[INFO] Epoch: 30 , batch: 149 , training loss: 3.780618
[INFO] Epoch: 30 , batch: 150 , training loss: 4.001690
[INFO] Epoch: 30 , batch: 151 , training loss: 3.860458
[INFO] Epoch: 30 , batch: 152 , training loss: 3.920650
[INFO] Epoch: 30 , batch: 153 , training loss: 3.886452
[INFO] Epoch: 30 , batch: 154 , training loss: 4.000293
[INFO] Epoch: 30 , batch: 155 , training loss: 4.194804
[INFO] Epoch: 30 , batch: 156 , training loss: 3.922394
[INFO] Epoch: 30 , batch: 157 , training loss: 3.912766
[INFO] Epoch: 30 , batch: 158 , training loss: 4.047222
[INFO] Epoch: 30 , batch: 159 , training loss: 3.953774
[INFO] Epoch: 30 , batch: 160 , training loss: 4.203882
[INFO] Epoch: 30 , batch: 161 , training loss: 4.327670
[INFO] Epoch: 30 , batch: 162 , training loss: 4.309073
[INFO] Epoch: 30 , batch: 163 , training loss: 4.436524
[INFO] Epoch: 30 , batch: 164 , training loss: 4.419474
[INFO] Epoch: 30 , batch: 165 , training loss: 4.335934
[INFO] Epoch: 30 , batch: 166 , training loss: 4.163224
[INFO] Epoch: 30 , batch: 167 , training loss: 4.259225
[INFO] Epoch: 30 , batch: 168 , training loss: 3.958500
[INFO] Epoch: 30 , batch: 169 , training loss: 3.909834
[INFO] Epoch: 30 , batch: 170 , training loss: 4.161281
[INFO] Epoch: 30 , batch: 171 , training loss: 3.528275
[INFO] Epoch: 30 , batch: 172 , training loss: 3.735267
[INFO] Epoch: 30 , batch: 173 , training loss: 4.086017
[INFO] Epoch: 30 , batch: 174 , training loss: 4.484707
[INFO] Epoch: 30 , batch: 175 , training loss: 4.836543
[INFO] Epoch: 30 , batch: 176 , training loss: 4.468784
[INFO] Epoch: 30 , batch: 177 , training loss: 4.141888
[INFO] Epoch: 30 , batch: 178 , training loss: 4.095826
[INFO] Epoch: 30 , batch: 179 , training loss: 4.157189
[INFO] Epoch: 30 , batch: 180 , training loss: 4.126630
[INFO] Epoch: 30 , batch: 181 , training loss: 4.391108
[INFO] Epoch: 30 , batch: 182 , training loss: 4.338942
[INFO] Epoch: 30 , batch: 183 , training loss: 4.313470
[INFO] Epoch: 30 , batch: 184 , training loss: 4.220944
[INFO] Epoch: 30 , batch: 185 , training loss: 4.154056
[INFO] Epoch: 30 , batch: 186 , training loss: 4.321281
[INFO] Epoch: 30 , batch: 187 , training loss: 4.450346
[INFO] Epoch: 30 , batch: 188 , training loss: 4.397287
[INFO] Epoch: 30 , batch: 189 , training loss: 4.297472
[INFO] Epoch: 30 , batch: 190 , training loss: 4.328681
[INFO] Epoch: 30 , batch: 191 , training loss: 4.420570
[INFO] Epoch: 30 , batch: 192 , training loss: 4.248061
[INFO] Epoch: 30 , batch: 193 , training loss: 4.350572
[INFO] Epoch: 30 , batch: 194 , training loss: 4.319663
[INFO] Epoch: 30 , batch: 195 , training loss: 4.232423
[INFO] Epoch: 30 , batch: 196 , training loss: 4.109850
[INFO] Epoch: 30 , batch: 197 , training loss: 4.174169
[INFO] Epoch: 30 , batch: 198 , training loss: 4.124040
[INFO] Epoch: 30 , batch: 199 , training loss: 4.257170
[INFO] Epoch: 30 , batch: 200 , training loss: 4.151453
[INFO] Epoch: 30 , batch: 201 , training loss: 4.052730
[INFO] Epoch: 30 , batch: 202 , training loss: 4.032529
[INFO] Epoch: 30 , batch: 203 , training loss: 4.164305
[INFO] Epoch: 30 , batch: 204 , training loss: 4.296545
[INFO] Epoch: 30 , batch: 205 , training loss: 3.859168
[INFO] Epoch: 30 , batch: 206 , training loss: 3.804071
[INFO] Epoch: 30 , batch: 207 , training loss: 3.788886
[INFO] Epoch: 30 , batch: 208 , training loss: 4.127502
[INFO] Epoch: 30 , batch: 209 , training loss: 4.082748
[INFO] Epoch: 30 , batch: 210 , training loss: 4.083956
[INFO] Epoch: 30 , batch: 211 , training loss: 4.076026
[INFO] Epoch: 30 , batch: 212 , training loss: 4.185637
[INFO] Epoch: 30 , batch: 213 , training loss: 4.145418
[INFO] Epoch: 30 , batch: 214 , training loss: 4.230805
[INFO] Epoch: 30 , batch: 215 , training loss: 4.429820
[INFO] Epoch: 30 , batch: 216 , training loss: 4.130323
[INFO] Epoch: 30 , batch: 217 , training loss: 4.089792
[INFO] Epoch: 30 , batch: 218 , training loss: 4.075658
[INFO] Epoch: 30 , batch: 219 , training loss: 4.211993
[INFO] Epoch: 30 , batch: 220 , training loss: 3.986363
[INFO] Epoch: 30 , batch: 221 , training loss: 4.017284
[INFO] Epoch: 30 , batch: 222 , training loss: 4.171287
[INFO] Epoch: 30 , batch: 223 , training loss: 4.247244
[INFO] Epoch: 30 , batch: 224 , training loss: 4.281986
[INFO] Epoch: 30 , batch: 225 , training loss: 4.197801
[INFO] Epoch: 30 , batch: 226 , training loss: 4.294511
[INFO] Epoch: 30 , batch: 227 , training loss: 4.249123
[INFO] Epoch: 30 , batch: 228 , training loss: 4.313085
[INFO] Epoch: 30 , batch: 229 , training loss: 4.168042
[INFO] Epoch: 30 , batch: 230 , training loss: 4.027028
[INFO] Epoch: 30 , batch: 231 , training loss: 3.898910
[INFO] Epoch: 30 , batch: 232 , training loss: 4.058305
[INFO] Epoch: 30 , batch: 233 , training loss: 4.052422
[INFO] Epoch: 30 , batch: 234 , training loss: 3.760791
[INFO] Epoch: 30 , batch: 235 , training loss: 3.838008
[INFO] Epoch: 30 , batch: 236 , training loss: 3.969867
[INFO] Epoch: 30 , batch: 237 , training loss: 4.180137
[INFO] Epoch: 30 , batch: 238 , training loss: 3.937971
[INFO] Epoch: 30 , batch: 239 , training loss: 3.999747
[INFO] Epoch: 30 , batch: 240 , training loss: 4.052539
[INFO] Epoch: 30 , batch: 241 , training loss: 3.856140
[INFO] Epoch: 30 , batch: 242 , training loss: 3.878922
[INFO] Epoch: 30 , batch: 243 , training loss: 4.179188
[INFO] Epoch: 30 , batch: 244 , training loss: 4.100419
[INFO] Epoch: 30 , batch: 245 , training loss: 4.079906
[INFO] Epoch: 30 , batch: 246 , training loss: 3.773692
[INFO] Epoch: 30 , batch: 247 , training loss: 3.947669
[INFO] Epoch: 30 , batch: 248 , training loss: 4.014730
[INFO] Epoch: 30 , batch: 249 , training loss: 4.044420
[INFO] Epoch: 30 , batch: 250 , training loss: 3.800373
[INFO] Epoch: 30 , batch: 251 , training loss: 4.239244
[INFO] Epoch: 30 , batch: 252 , training loss: 3.927317
[INFO] Epoch: 30 , batch: 253 , training loss: 3.868181
[INFO] Epoch: 30 , batch: 254 , training loss: 4.152911
[INFO] Epoch: 30 , batch: 255 , training loss: 4.109083
[INFO] Epoch: 30 , batch: 256 , training loss: 4.134770
[INFO] Epoch: 30 , batch: 257 , training loss: 4.238494
[INFO] Epoch: 30 , batch: 258 , training loss: 4.318788
[INFO] Epoch: 30 , batch: 259 , training loss: 4.371112
[INFO] Epoch: 30 , batch: 260 , training loss: 4.083664
[INFO] Epoch: 30 , batch: 261 , training loss: 4.241888
[INFO] Epoch: 30 , batch: 262 , training loss: 4.408599
[INFO] Epoch: 30 , batch: 263 , training loss: 4.592096
[INFO] Epoch: 30 , batch: 264 , training loss: 3.903403
[INFO] Epoch: 30 , batch: 265 , training loss: 4.036810
[INFO] Epoch: 30 , batch: 266 , training loss: 4.473347
[INFO] Epoch: 30 , batch: 267 , training loss: 4.233097
[INFO] Epoch: 30 , batch: 268 , training loss: 4.140178
[INFO] Epoch: 30 , batch: 269 , training loss: 4.114770
[INFO] Epoch: 30 , batch: 270 , training loss: 4.153994
[INFO] Epoch: 30 , batch: 271 , training loss: 4.188547
[INFO] Epoch: 30 , batch: 272 , training loss: 4.152956
[INFO] Epoch: 30 , batch: 273 , training loss: 4.187169
[INFO] Epoch: 30 , batch: 274 , training loss: 4.251317
[INFO] Epoch: 30 , batch: 275 , training loss: 4.132901
[INFO] Epoch: 30 , batch: 276 , training loss: 4.192574
[INFO] Epoch: 30 , batch: 277 , training loss: 4.338089
[INFO] Epoch: 30 , batch: 278 , training loss: 4.024746
[INFO] Epoch: 30 , batch: 279 , training loss: 4.017783
[INFO] Epoch: 30 , batch: 280 , training loss: 3.968505
[INFO] Epoch: 30 , batch: 281 , training loss: 4.117059
[INFO] Epoch: 30 , batch: 282 , training loss: 4.033186
[INFO] Epoch: 30 , batch: 283 , training loss: 4.039083
[INFO] Epoch: 30 , batch: 284 , training loss: 4.076853
[INFO] Epoch: 30 , batch: 285 , training loss: 4.013334
[INFO] Epoch: 30 , batch: 286 , training loss: 4.005858
[INFO] Epoch: 30 , batch: 287 , training loss: 3.970555
[INFO] Epoch: 30 , batch: 288 , training loss: 3.977772
[INFO] Epoch: 30 , batch: 289 , training loss: 4.011031
[INFO] Epoch: 30 , batch: 290 , training loss: 3.800623
[INFO] Epoch: 30 , batch: 291 , training loss: 3.758186
[INFO] Epoch: 30 , batch: 292 , training loss: 3.891629
[INFO] Epoch: 30 , batch: 293 , training loss: 3.808102
[INFO] Epoch: 30 , batch: 294 , training loss: 4.466591
[INFO] Epoch: 30 , batch: 295 , training loss: 4.244961
[INFO] Epoch: 30 , batch: 296 , training loss: 4.169003
[INFO] Epoch: 30 , batch: 297 , training loss: 4.134739
[INFO] Epoch: 30 , batch: 298 , training loss: 3.972039
[INFO] Epoch: 30 , batch: 299 , training loss: 3.985810
[INFO] Epoch: 30 , batch: 300 , training loss: 3.957419
[INFO] Epoch: 30 , batch: 301 , training loss: 3.900557
[INFO] Epoch: 30 , batch: 302 , training loss: 4.073118
[INFO] Epoch: 30 , batch: 303 , training loss: 4.081755
[INFO] Epoch: 30 , batch: 304 , training loss: 4.225404
[INFO] Epoch: 30 , batch: 305 , training loss: 4.034520
[INFO] Epoch: 30 , batch: 306 , training loss: 4.173444
[INFO] Epoch: 30 , batch: 307 , training loss: 4.157482
[INFO] Epoch: 30 , batch: 308 , training loss: 4.011975
[INFO] Epoch: 30 , batch: 309 , training loss: 4.012965
[INFO] Epoch: 30 , batch: 310 , training loss: 3.908726
[INFO] Epoch: 30 , batch: 311 , training loss: 3.908575
[INFO] Epoch: 30 , batch: 312 , training loss: 3.841016
[INFO] Epoch: 30 , batch: 313 , training loss: 3.968089
[INFO] Epoch: 30 , batch: 314 , training loss: 4.012925
[INFO] Epoch: 30 , batch: 315 , training loss: 4.075356
[INFO] Epoch: 30 , batch: 316 , training loss: 4.294506
[INFO] Epoch: 30 , batch: 317 , training loss: 4.750353
[INFO] Epoch: 30 , batch: 318 , training loss: 4.875778
[INFO] Epoch: 30 , batch: 319 , training loss: 4.485462
[INFO] Epoch: 30 , batch: 320 , training loss: 4.024755
[INFO] Epoch: 30 , batch: 321 , training loss: 3.846945
[INFO] Epoch: 30 , batch: 322 , training loss: 3.980002
[INFO] Epoch: 30 , batch: 323 , training loss: 3.984473
[INFO] Epoch: 30 , batch: 324 , training loss: 3.945593
[INFO] Epoch: 30 , batch: 325 , training loss: 4.110397
[INFO] Epoch: 30 , batch: 326 , training loss: 4.172503
[INFO] Epoch: 30 , batch: 327 , training loss: 4.064380
[INFO] Epoch: 30 , batch: 328 , training loss: 4.061231
[INFO] Epoch: 30 , batch: 329 , training loss: 3.967992
[INFO] Epoch: 30 , batch: 330 , training loss: 3.988554
[INFO] Epoch: 30 , batch: 331 , training loss: 4.139022
[INFO] Epoch: 30 , batch: 332 , training loss: 3.927513
[INFO] Epoch: 30 , batch: 333 , training loss: 3.955073
[INFO] Epoch: 30 , batch: 334 , training loss: 3.961703
[INFO] Epoch: 30 , batch: 335 , training loss: 4.082763
[INFO] Epoch: 30 , batch: 336 , training loss: 4.121615
[INFO] Epoch: 30 , batch: 337 , training loss: 4.133505
[INFO] Epoch: 30 , batch: 338 , training loss: 4.343191
[INFO] Epoch: 30 , batch: 339 , training loss: 4.230080
[INFO] Epoch: 30 , batch: 340 , training loss: 4.339065
[INFO] Epoch: 30 , batch: 341 , training loss: 4.113415
[INFO] Epoch: 30 , batch: 342 , training loss: 3.902761
[INFO] Epoch: 30 , batch: 343 , training loss: 3.967264
[INFO] Epoch: 30 , batch: 344 , training loss: 3.827782
[INFO] Epoch: 30 , batch: 345 , training loss: 3.959042
[INFO] Epoch: 30 , batch: 346 , training loss: 4.006603
[INFO] Epoch: 30 , batch: 347 , training loss: 3.913802
[INFO] Epoch: 30 , batch: 348 , training loss: 4.022504
[INFO] Epoch: 30 , batch: 349 , training loss: 4.203995
[INFO] Epoch: 30 , batch: 350 , training loss: 3.954708
[INFO] Epoch: 30 , batch: 351 , training loss: 4.020188
[INFO] Epoch: 30 , batch: 352 , training loss: 4.056029
[INFO] Epoch: 30 , batch: 353 , training loss: 4.013560
[INFO] Epoch: 30 , batch: 354 , training loss: 4.131057
[INFO] Epoch: 30 , batch: 355 , training loss: 4.172922
[INFO] Epoch: 30 , batch: 356 , training loss: 4.003778
[INFO] Epoch: 30 , batch: 357 , training loss: 4.091139
[INFO] Epoch: 30 , batch: 358 , training loss: 3.976356
[INFO] Epoch: 30 , batch: 359 , training loss: 4.017684
[INFO] Epoch: 30 , batch: 360 , training loss: 4.058861
[INFO] Epoch: 30 , batch: 361 , training loss: 4.044708
[INFO] Epoch: 30 , batch: 362 , training loss: 4.137592
[INFO] Epoch: 30 , batch: 363 , training loss: 4.036547
[INFO] Epoch: 30 , batch: 364 , training loss: 4.059174
[INFO] Epoch: 30 , batch: 365 , training loss: 4.007099
[INFO] Epoch: 30 , batch: 366 , training loss: 4.112756
[INFO] Epoch: 30 , batch: 367 , training loss: 4.203161
[INFO] Epoch: 30 , batch: 368 , training loss: 4.602510
[INFO] Epoch: 30 , batch: 369 , training loss: 4.272447
[INFO] Epoch: 30 , batch: 370 , training loss: 4.035641
[INFO] Epoch: 30 , batch: 371 , training loss: 4.405181
[INFO] Epoch: 30 , batch: 372 , training loss: 4.697744
[INFO] Epoch: 30 , batch: 373 , training loss: 4.800159
[INFO] Epoch: 30 , batch: 374 , training loss: 4.862163
[INFO] Epoch: 30 , batch: 375 , training loss: 4.854647
[INFO] Epoch: 30 , batch: 376 , training loss: 4.771058
[INFO] Epoch: 30 , batch: 377 , training loss: 4.541887
[INFO] Epoch: 30 , batch: 378 , training loss: 4.624506
[INFO] Epoch: 30 , batch: 379 , training loss: 4.601971
[INFO] Epoch: 30 , batch: 380 , training loss: 4.706177
[INFO] Epoch: 30 , batch: 381 , training loss: 4.446621
[INFO] Epoch: 30 , batch: 382 , training loss: 4.669867
[INFO] Epoch: 30 , batch: 383 , training loss: 4.690215
[INFO] Epoch: 30 , batch: 384 , training loss: 4.703263
[INFO] Epoch: 30 , batch: 385 , training loss: 4.402507
[INFO] Epoch: 30 , batch: 386 , training loss: 4.698261
[INFO] Epoch: 30 , batch: 387 , training loss: 4.645996
[INFO] Epoch: 30 , batch: 388 , training loss: 4.450012
[INFO] Epoch: 30 , batch: 389 , training loss: 4.296625
[INFO] Epoch: 30 , batch: 390 , training loss: 4.276291
[INFO] Epoch: 30 , batch: 391 , training loss: 4.346346
[INFO] Epoch: 30 , batch: 392 , training loss: 4.701930
[INFO] Epoch: 30 , batch: 393 , training loss: 4.589298
[INFO] Epoch: 30 , batch: 394 , training loss: 4.615217
[INFO] Epoch: 30 , batch: 395 , training loss: 4.487204
[INFO] Epoch: 30 , batch: 396 , training loss: 4.258152
[INFO] Epoch: 30 , batch: 397 , training loss: 4.419003
[INFO] Epoch: 30 , batch: 398 , training loss: 4.272945
[INFO] Epoch: 30 , batch: 399 , training loss: 4.355290
[INFO] Epoch: 30 , batch: 400 , training loss: 4.319513
[INFO] Epoch: 30 , batch: 401 , training loss: 4.712195
[INFO] Epoch: 30 , batch: 402 , training loss: 4.458195
[INFO] Epoch: 30 , batch: 403 , training loss: 4.234753
[INFO] Epoch: 30 , batch: 404 , training loss: 4.444191
[INFO] Epoch: 30 , batch: 405 , training loss: 4.505773
[INFO] Epoch: 30 , batch: 406 , training loss: 4.407897
[INFO] Epoch: 30 , batch: 407 , training loss: 4.463412
[INFO] Epoch: 30 , batch: 408 , training loss: 4.420869
[INFO] Epoch: 30 , batch: 409 , training loss: 4.393322
[INFO] Epoch: 30 , batch: 410 , training loss: 4.429545
[INFO] Epoch: 30 , batch: 411 , training loss: 4.642255
[INFO] Epoch: 30 , batch: 412 , training loss: 4.481915
[INFO] Epoch: 30 , batch: 413 , training loss: 4.366529
[INFO] Epoch: 30 , batch: 414 , training loss: 4.395612
[INFO] Epoch: 30 , batch: 415 , training loss: 4.408405
[INFO] Epoch: 30 , batch: 416 , training loss: 4.491871
[INFO] Epoch: 30 , batch: 417 , training loss: 4.415546
[INFO] Epoch: 30 , batch: 418 , training loss: 4.434031
[INFO] Epoch: 30 , batch: 419 , training loss: 4.413493
[INFO] Epoch: 30 , batch: 420 , training loss: 4.366831
[INFO] Epoch: 30 , batch: 421 , training loss: 4.359765
[INFO] Epoch: 30 , batch: 422 , training loss: 4.219430
[INFO] Epoch: 30 , batch: 423 , training loss: 4.432107
[INFO] Epoch: 30 , batch: 424 , training loss: 4.598634
[INFO] Epoch: 30 , batch: 425 , training loss: 4.484703
[INFO] Epoch: 30 , batch: 426 , training loss: 4.219970
[INFO] Epoch: 30 , batch: 427 , training loss: 4.444133
[INFO] Epoch: 30 , batch: 428 , training loss: 4.331522
[INFO] Epoch: 30 , batch: 429 , training loss: 4.194685
[INFO] Epoch: 30 , batch: 430 , training loss: 4.463861
[INFO] Epoch: 30 , batch: 431 , training loss: 4.054281
[INFO] Epoch: 30 , batch: 432 , training loss: 4.125563
[INFO] Epoch: 30 , batch: 433 , training loss: 4.151874
[INFO] Epoch: 30 , batch: 434 , training loss: 4.031004
[INFO] Epoch: 30 , batch: 435 , training loss: 4.385948
[INFO] Epoch: 30 , batch: 436 , training loss: 4.463932
[INFO] Epoch: 30 , batch: 437 , training loss: 4.222861
[INFO] Epoch: 30 , batch: 438 , training loss: 4.076599
[INFO] Epoch: 30 , batch: 439 , training loss: 4.288190
[INFO] Epoch: 30 , batch: 440 , training loss: 4.413049
[INFO] Epoch: 30 , batch: 441 , training loss: 4.522920
[INFO] Epoch: 30 , batch: 442 , training loss: 4.288856
[INFO] Epoch: 30 , batch: 443 , training loss: 4.475740
[INFO] Epoch: 30 , batch: 444 , training loss: 4.095394
[INFO] Epoch: 30 , batch: 445 , training loss: 3.997664
[INFO] Epoch: 30 , batch: 446 , training loss: 3.943568
[INFO] Epoch: 30 , batch: 447 , training loss: 4.125134
[INFO] Epoch: 30 , batch: 448 , training loss: 4.229343
[INFO] Epoch: 30 , batch: 449 , training loss: 4.628497
[INFO] Epoch: 30 , batch: 450 , training loss: 4.677433
[INFO] Epoch: 30 , batch: 451 , training loss: 4.597786
[INFO] Epoch: 30 , batch: 452 , training loss: 4.391963
[INFO] Epoch: 30 , batch: 453 , training loss: 4.156668
[INFO] Epoch: 30 , batch: 454 , training loss: 4.299716
[INFO] Epoch: 30 , batch: 455 , training loss: 4.356205
[INFO] Epoch: 30 , batch: 456 , training loss: 4.354156
[INFO] Epoch: 30 , batch: 457 , training loss: 4.453626
[INFO] Epoch: 30 , batch: 458 , training loss: 4.166264
[INFO] Epoch: 30 , batch: 459 , training loss: 4.160080
[INFO] Epoch: 30 , batch: 460 , training loss: 4.254231
[INFO] Epoch: 30 , batch: 461 , training loss: 4.228927
[INFO] Epoch: 30 , batch: 462 , training loss: 4.299782
[INFO] Epoch: 30 , batch: 463 , training loss: 4.206926
[INFO] Epoch: 30 , batch: 464 , training loss: 4.374520
[INFO] Epoch: 30 , batch: 465 , training loss: 4.314630
[INFO] Epoch: 30 , batch: 466 , training loss: 4.413612
[INFO] Epoch: 30 , batch: 467 , training loss: 4.388812
[INFO] Epoch: 30 , batch: 468 , training loss: 4.357563
[INFO] Epoch: 30 , batch: 469 , training loss: 4.375258
[INFO] Epoch: 30 , batch: 470 , training loss: 4.200588
[INFO] Epoch: 30 , batch: 471 , training loss: 4.282010
[INFO] Epoch: 30 , batch: 472 , training loss: 4.332325
[INFO] Epoch: 30 , batch: 473 , training loss: 4.278943
[INFO] Epoch: 30 , batch: 474 , training loss: 4.048851
[INFO] Epoch: 30 , batch: 475 , training loss: 3.938731
[INFO] Epoch: 30 , batch: 476 , training loss: 4.301747
[INFO] Epoch: 30 , batch: 477 , training loss: 4.437088
[INFO] Epoch: 30 , batch: 478 , training loss: 4.482457
[INFO] Epoch: 30 , batch: 479 , training loss: 4.434726
[INFO] Epoch: 30 , batch: 480 , training loss: 4.518260
[INFO] Epoch: 30 , batch: 481 , training loss: 4.402479
[INFO] Epoch: 30 , batch: 482 , training loss: 4.535573
[INFO] Epoch: 30 , batch: 483 , training loss: 4.393038
[INFO] Epoch: 30 , batch: 484 , training loss: 4.184116
[INFO] Epoch: 30 , batch: 485 , training loss: 4.265045
[INFO] Epoch: 30 , batch: 486 , training loss: 4.151978
[INFO] Epoch: 30 , batch: 487 , training loss: 4.141864
[INFO] Epoch: 30 , batch: 488 , training loss: 4.356053
[INFO] Epoch: 30 , batch: 489 , training loss: 4.248478
[INFO] Epoch: 30 , batch: 490 , training loss: 4.275428
[INFO] Epoch: 30 , batch: 491 , training loss: 4.228431
[INFO] Epoch: 30 , batch: 492 , training loss: 4.185769
[INFO] Epoch: 30 , batch: 493 , training loss: 4.372152
[INFO] Epoch: 30 , batch: 494 , training loss: 4.307948
[INFO] Epoch: 30 , batch: 495 , training loss: 4.408144
[INFO] Epoch: 30 , batch: 496 , training loss: 4.291176
[INFO] Epoch: 30 , batch: 497 , training loss: 4.340476
[INFO] Epoch: 30 , batch: 498 , training loss: 4.341342
[INFO] Epoch: 30 , batch: 499 , training loss: 4.400064
[INFO] Epoch: 30 , batch: 500 , training loss: 4.566034
[INFO] Epoch: 30 , batch: 501 , training loss: 4.872123
[INFO] Epoch: 30 , batch: 502 , training loss: 4.926494
[INFO] Epoch: 30 , batch: 503 , training loss: 4.624836
[INFO] Epoch: 30 , batch: 504 , training loss: 4.743031
[INFO] Epoch: 30 , batch: 505 , training loss: 4.724679
[INFO] Epoch: 30 , batch: 506 , training loss: 4.665862
[INFO] Epoch: 30 , batch: 507 , training loss: 4.738718
[INFO] Epoch: 30 , batch: 508 , training loss: 4.675277
[INFO] Epoch: 30 , batch: 509 , training loss: 4.457933
[INFO] Epoch: 30 , batch: 510 , training loss: 4.557796
[INFO] Epoch: 30 , batch: 511 , training loss: 4.455191
[INFO] Epoch: 30 , batch: 512 , training loss: 4.562096
[INFO] Epoch: 30 , batch: 513 , training loss: 4.781840
[INFO] Epoch: 30 , batch: 514 , training loss: 4.445841
[INFO] Epoch: 30 , batch: 515 , training loss: 4.681080
[INFO] Epoch: 30 , batch: 516 , training loss: 4.493295
[INFO] Epoch: 30 , batch: 517 , training loss: 4.452330
[INFO] Epoch: 30 , batch: 518 , training loss: 4.417687
[INFO] Epoch: 30 , batch: 519 , training loss: 4.270505
[INFO] Epoch: 30 , batch: 520 , training loss: 4.507660
[INFO] Epoch: 30 , batch: 521 , training loss: 4.482118
[INFO] Epoch: 30 , batch: 522 , training loss: 4.537097
[INFO] Epoch: 30 , batch: 523 , training loss: 4.432198
[INFO] Epoch: 30 , batch: 524 , training loss: 4.731092
[INFO] Epoch: 30 , batch: 525 , training loss: 4.671314
[INFO] Epoch: 30 , batch: 526 , training loss: 4.436521
[INFO] Epoch: 30 , batch: 527 , training loss: 4.472281
[INFO] Epoch: 30 , batch: 528 , training loss: 4.483728
[INFO] Epoch: 30 , batch: 529 , training loss: 4.462131
[INFO] Epoch: 30 , batch: 530 , training loss: 4.307642
[INFO] Epoch: 30 , batch: 531 , training loss: 4.449184
[INFO] Epoch: 30 , batch: 532 , training loss: 4.346369
[INFO] Epoch: 30 , batch: 533 , training loss: 4.503152
[INFO] Epoch: 30 , batch: 534 , training loss: 4.473449
[INFO] Epoch: 30 , batch: 535 , training loss: 4.488865
[INFO] Epoch: 30 , batch: 536 , training loss: 4.310813
[INFO] Epoch: 30 , batch: 537 , training loss: 4.319891
[INFO] Epoch: 30 , batch: 538 , training loss: 4.409590
[INFO] Epoch: 30 , batch: 539 , training loss: 4.503062
[INFO] Epoch: 30 , batch: 540 , training loss: 5.003854
[INFO] Epoch: 30 , batch: 541 , training loss: 4.891434
[INFO] Epoch: 30 , batch: 542 , training loss: 4.797685
[INFO] Epoch: 31 , batch: 0 , training loss: 3.439329
[INFO] Epoch: 31 , batch: 1 , training loss: 3.509872
[INFO] Epoch: 31 , batch: 2 , training loss: 3.638178
[INFO] Epoch: 31 , batch: 3 , training loss: 3.435228
[INFO] Epoch: 31 , batch: 4 , training loss: 3.857067
[INFO] Epoch: 31 , batch: 5 , training loss: 3.502662
[INFO] Epoch: 31 , batch: 6 , training loss: 3.868301
[INFO] Epoch: 31 , batch: 7 , training loss: 3.813380
[INFO] Epoch: 31 , batch: 8 , training loss: 3.496804
[INFO] Epoch: 31 , batch: 9 , training loss: 3.788056
[INFO] Epoch: 31 , batch: 10 , training loss: 3.726301
[INFO] Epoch: 31 , batch: 11 , training loss: 3.694201
[INFO] Epoch: 31 , batch: 12 , training loss: 3.578923
[INFO] Epoch: 31 , batch: 13 , training loss: 3.637639
[INFO] Epoch: 31 , batch: 14 , training loss: 3.492787
[INFO] Epoch: 31 , batch: 15 , training loss: 3.694308
[INFO] Epoch: 31 , batch: 16 , training loss: 3.556828
[INFO] Epoch: 31 , batch: 17 , training loss: 3.739945
[INFO] Epoch: 31 , batch: 18 , training loss: 3.680885
[INFO] Epoch: 31 , batch: 19 , training loss: 3.403329
[INFO] Epoch: 31 , batch: 20 , training loss: 3.410572
[INFO] Epoch: 31 , batch: 21 , training loss: 3.520678
[INFO] Epoch: 31 , batch: 22 , training loss: 3.393255
[INFO] Epoch: 31 , batch: 23 , training loss: 3.627077
[INFO] Epoch: 31 , batch: 24 , training loss: 3.428023
[INFO] Epoch: 31 , batch: 25 , training loss: 3.621655
[INFO] Epoch: 31 , batch: 26 , training loss: 3.486001
[INFO] Epoch: 31 , batch: 27 , training loss: 3.431009
[INFO] Epoch: 31 , batch: 28 , training loss: 3.587033
[INFO] Epoch: 31 , batch: 29 , training loss: 3.403460
[INFO] Epoch: 31 , batch: 30 , training loss: 3.480051
[INFO] Epoch: 31 , batch: 31 , training loss: 3.517164
[INFO] Epoch: 31 , batch: 32 , training loss: 3.510227
[INFO] Epoch: 31 , batch: 33 , training loss: 3.539112
[INFO] Epoch: 31 , batch: 34 , training loss: 3.498938
[INFO] Epoch: 31 , batch: 35 , training loss: 3.521951
[INFO] Epoch: 31 , batch: 36 , training loss: 3.531714
[INFO] Epoch: 31 , batch: 37 , training loss: 3.482788
[INFO] Epoch: 31 , batch: 38 , training loss: 3.489673
[INFO] Epoch: 31 , batch: 39 , training loss: 3.364741
[INFO] Epoch: 31 , batch: 40 , training loss: 3.599073
[INFO] Epoch: 31 , batch: 41 , training loss: 3.476037
[INFO] Epoch: 31 , batch: 42 , training loss: 3.899541
[INFO] Epoch: 31 , batch: 43 , training loss: 3.647063
[INFO] Epoch: 31 , batch: 44 , training loss: 3.946510
[INFO] Epoch: 31 , batch: 45 , training loss: 3.862220
[INFO] Epoch: 31 , batch: 46 , training loss: 3.777455
[INFO] Epoch: 31 , batch: 47 , training loss: 3.621994
[INFO] Epoch: 31 , batch: 48 , training loss: 3.592080
[INFO] Epoch: 31 , batch: 49 , training loss: 3.803568
[INFO] Epoch: 31 , batch: 50 , training loss: 3.602132
[INFO] Epoch: 31 , batch: 51 , training loss: 3.792609
[INFO] Epoch: 31 , batch: 52 , training loss: 3.670077
[INFO] Epoch: 31 , batch: 53 , training loss: 3.801173
[INFO] Epoch: 31 , batch: 54 , training loss: 3.786947
[INFO] Epoch: 31 , batch: 55 , training loss: 3.880553
[INFO] Epoch: 31 , batch: 56 , training loss: 3.727732
[INFO] Epoch: 31 , batch: 57 , training loss: 3.629651
[INFO] Epoch: 31 , batch: 58 , training loss: 3.664589
[INFO] Epoch: 31 , batch: 59 , training loss: 3.775428
[INFO] Epoch: 31 , batch: 60 , training loss: 3.739897
[INFO] Epoch: 31 , batch: 61 , training loss: 3.787159
[INFO] Epoch: 31 , batch: 62 , training loss: 3.696488
[INFO] Epoch: 31 , batch: 63 , training loss: 3.870026
[INFO] Epoch: 31 , batch: 64 , training loss: 4.064228
[INFO] Epoch: 31 , batch: 65 , training loss: 3.775954
[INFO] Epoch: 31 , batch: 66 , training loss: 3.609665
[INFO] Epoch: 31 , batch: 67 , training loss: 3.687189
[INFO] Epoch: 31 , batch: 68 , training loss: 3.859824
[INFO] Epoch: 31 , batch: 69 , training loss: 3.717025
[INFO] Epoch: 31 , batch: 70 , training loss: 3.921646
[INFO] Epoch: 31 , batch: 71 , training loss: 3.831801
[INFO] Epoch: 31 , batch: 72 , training loss: 3.868740
[INFO] Epoch: 31 , batch: 73 , training loss: 3.796972
[INFO] Epoch: 31 , batch: 74 , training loss: 3.910465
[INFO] Epoch: 31 , batch: 75 , training loss: 3.815392
[INFO] Epoch: 31 , batch: 76 , training loss: 3.859059
[INFO] Epoch: 31 , batch: 77 , training loss: 3.856905
[INFO] Epoch: 31 , batch: 78 , training loss: 3.968250
[INFO] Epoch: 31 , batch: 79 , training loss: 3.768489
[INFO] Epoch: 31 , batch: 80 , training loss: 3.972022
[INFO] Epoch: 31 , batch: 81 , training loss: 3.914720
[INFO] Epoch: 31 , batch: 82 , training loss: 3.867270
[INFO] Epoch: 31 , batch: 83 , training loss: 4.029929
[INFO] Epoch: 31 , batch: 84 , training loss: 3.909017
[INFO] Epoch: 31 , batch: 85 , training loss: 3.995907
[INFO] Epoch: 31 , batch: 86 , training loss: 3.930227
[INFO] Epoch: 31 , batch: 87 , training loss: 3.931821
[INFO] Epoch: 31 , batch: 88 , training loss: 4.044516
[INFO] Epoch: 31 , batch: 89 , training loss: 3.857125
[INFO] Epoch: 31 , batch: 90 , training loss: 3.918536
[INFO] Epoch: 31 , batch: 91 , training loss: 3.866998
[INFO] Epoch: 31 , batch: 92 , training loss: 3.846708
[INFO] Epoch: 31 , batch: 93 , training loss: 3.964098
[INFO] Epoch: 31 , batch: 94 , training loss: 4.136389
[INFO] Epoch: 31 , batch: 95 , training loss: 3.897878
[INFO] Epoch: 31 , batch: 96 , training loss: 3.892875
[INFO] Epoch: 31 , batch: 97 , training loss: 3.808561
[INFO] Epoch: 31 , batch: 98 , training loss: 3.781473
[INFO] Epoch: 31 , batch: 99 , training loss: 3.844347
[INFO] Epoch: 31 , batch: 100 , training loss: 3.797552
[INFO] Epoch: 31 , batch: 101 , training loss: 3.838133
[INFO] Epoch: 31 , batch: 102 , training loss: 3.939880
[INFO] Epoch: 31 , batch: 103 , training loss: 3.732158
[INFO] Epoch: 31 , batch: 104 , training loss: 3.685018
[INFO] Epoch: 31 , batch: 105 , training loss: 3.992616
[INFO] Epoch: 31 , batch: 106 , training loss: 3.922744
[INFO] Epoch: 31 , batch: 107 , training loss: 3.824537
[INFO] Epoch: 31 , batch: 108 , training loss: 3.755501
[INFO] Epoch: 31 , batch: 109 , training loss: 3.679664
[INFO] Epoch: 31 , batch: 110 , training loss: 3.831145
[INFO] Epoch: 31 , batch: 111 , training loss: 3.956016
[INFO] Epoch: 31 , batch: 112 , training loss: 3.840187
[INFO] Epoch: 31 , batch: 113 , training loss: 3.825010
[INFO] Epoch: 31 , batch: 114 , training loss: 3.839653
[INFO] Epoch: 31 , batch: 115 , training loss: 3.850215
[INFO] Epoch: 31 , batch: 116 , training loss: 3.763921
[INFO] Epoch: 31 , batch: 117 , training loss: 3.989840
[INFO] Epoch: 31 , batch: 118 , training loss: 3.926285
[INFO] Epoch: 31 , batch: 119 , training loss: 4.042540
[INFO] Epoch: 31 , batch: 120 , training loss: 4.066174
[INFO] Epoch: 31 , batch: 121 , training loss: 3.917860
[INFO] Epoch: 31 , batch: 122 , training loss: 3.827778
[INFO] Epoch: 31 , batch: 123 , training loss: 3.849711
[INFO] Epoch: 31 , batch: 124 , training loss: 3.955431
[INFO] Epoch: 31 , batch: 125 , training loss: 3.760432
[INFO] Epoch: 31 , batch: 126 , training loss: 3.762170
[INFO] Epoch: 31 , batch: 127 , training loss: 3.769094
[INFO] Epoch: 31 , batch: 128 , training loss: 3.879438
[INFO] Epoch: 31 , batch: 129 , training loss: 3.889146
[INFO] Epoch: 31 , batch: 130 , training loss: 3.870842
[INFO] Epoch: 31 , batch: 131 , training loss: 3.880592
[INFO] Epoch: 31 , batch: 132 , training loss: 3.842322
[INFO] Epoch: 31 , batch: 133 , training loss: 3.839687
[INFO] Epoch: 31 , batch: 134 , training loss: 3.643567
[INFO] Epoch: 31 , batch: 135 , training loss: 3.680144
[INFO] Epoch: 31 , batch: 136 , training loss: 3.957332
[INFO] Epoch: 31 , batch: 137 , training loss: 3.871960
[INFO] Epoch: 31 , batch: 138 , training loss: 3.948043
[INFO] Epoch: 31 , batch: 139 , training loss: 4.516893
[INFO] Epoch: 31 , batch: 140 , training loss: 4.268270
[INFO] Epoch: 31 , batch: 141 , training loss: 4.004954
[INFO] Epoch: 31 , batch: 142 , training loss: 3.794993
[INFO] Epoch: 31 , batch: 143 , training loss: 3.937017
[INFO] Epoch: 31 , batch: 144 , training loss: 3.744049
[INFO] Epoch: 31 , batch: 145 , training loss: 3.823285
[INFO] Epoch: 31 , batch: 146 , training loss: 4.029833
[INFO] Epoch: 31 , batch: 147 , training loss: 3.681419
[INFO] Epoch: 31 , batch: 148 , training loss: 3.691612
[INFO] Epoch: 31 , batch: 149 , training loss: 3.766212
[INFO] Epoch: 31 , batch: 150 , training loss: 3.986542
[INFO] Epoch: 31 , batch: 151 , training loss: 3.867281
[INFO] Epoch: 31 , batch: 152 , training loss: 3.923849
[INFO] Epoch: 31 , batch: 153 , training loss: 3.893305
[INFO] Epoch: 31 , batch: 154 , training loss: 4.007319
[INFO] Epoch: 31 , batch: 155 , training loss: 4.208933
[INFO] Epoch: 31 , batch: 156 , training loss: 3.922397
[INFO] Epoch: 31 , batch: 157 , training loss: 3.912324
[INFO] Epoch: 31 , batch: 158 , training loss: 4.027855
[INFO] Epoch: 31 , batch: 159 , training loss: 3.932562
[INFO] Epoch: 31 , batch: 160 , training loss: 4.225262
[INFO] Epoch: 31 , batch: 161 , training loss: 4.318202
[INFO] Epoch: 31 , batch: 162 , training loss: 4.295205
[INFO] Epoch: 31 , batch: 163 , training loss: 4.413493
[INFO] Epoch: 31 , batch: 164 , training loss: 4.410931
[INFO] Epoch: 31 , batch: 165 , training loss: 4.341697
[INFO] Epoch: 31 , batch: 166 , training loss: 4.173459
[INFO] Epoch: 31 , batch: 167 , training loss: 4.256952
[INFO] Epoch: 31 , batch: 168 , training loss: 3.959587
[INFO] Epoch: 31 , batch: 169 , training loss: 3.901176
[INFO] Epoch: 31 , batch: 170 , training loss: 4.128373
[INFO] Epoch: 31 , batch: 171 , training loss: 3.528424
[INFO] Epoch: 31 , batch: 172 , training loss: 3.720403
[INFO] Epoch: 31 , batch: 173 , training loss: 4.099134
[INFO] Epoch: 31 , batch: 174 , training loss: 4.483310
[INFO] Epoch: 31 , batch: 175 , training loss: 4.820299
[INFO] Epoch: 31 , batch: 176 , training loss: 4.442861
[INFO] Epoch: 31 , batch: 177 , training loss: 4.133468
[INFO] Epoch: 31 , batch: 178 , training loss: 4.106658
[INFO] Epoch: 31 , batch: 179 , training loss: 4.161127
[INFO] Epoch: 31 , batch: 180 , training loss: 4.092608
[INFO] Epoch: 31 , batch: 181 , training loss: 4.395048
[INFO] Epoch: 31 , batch: 182 , training loss: 4.342223
[INFO] Epoch: 31 , batch: 183 , training loss: 4.300709
[INFO] Epoch: 31 , batch: 184 , training loss: 4.210307
[INFO] Epoch: 31 , batch: 185 , training loss: 4.151858
[INFO] Epoch: 31 , batch: 186 , training loss: 4.311322
[INFO] Epoch: 31 , batch: 187 , training loss: 4.431235
[INFO] Epoch: 31 , batch: 188 , training loss: 4.418375
[INFO] Epoch: 31 , batch: 189 , training loss: 4.292660
[INFO] Epoch: 31 , batch: 190 , training loss: 4.298533
[INFO] Epoch: 31 , batch: 191 , training loss: 4.428373
[INFO] Epoch: 31 , batch: 192 , training loss: 4.242453
[INFO] Epoch: 31 , batch: 193 , training loss: 4.357138
[INFO] Epoch: 31 , batch: 194 , training loss: 4.314411
[INFO] Epoch: 31 , batch: 195 , training loss: 4.210847
[INFO] Epoch: 31 , batch: 196 , training loss: 4.111875
[INFO] Epoch: 31 , batch: 197 , training loss: 4.169211
[INFO] Epoch: 31 , batch: 198 , training loss: 4.109615
[INFO] Epoch: 31 , batch: 199 , training loss: 4.255269
[INFO] Epoch: 31 , batch: 200 , training loss: 4.133987
[INFO] Epoch: 31 , batch: 201 , training loss: 4.070572
[INFO] Epoch: 31 , batch: 202 , training loss: 4.049018
[INFO] Epoch: 31 , batch: 203 , training loss: 4.151361
[INFO] Epoch: 31 , batch: 204 , training loss: 4.306142
[INFO] Epoch: 31 , batch: 205 , training loss: 3.867072
[INFO] Epoch: 31 , batch: 206 , training loss: 3.814507
[INFO] Epoch: 31 , batch: 207 , training loss: 3.795082
[INFO] Epoch: 31 , batch: 208 , training loss: 4.125620
[INFO] Epoch: 31 , batch: 209 , training loss: 4.059510
[INFO] Epoch: 31 , batch: 210 , training loss: 4.067029
[INFO] Epoch: 31 , batch: 211 , training loss: 4.078425
[INFO] Epoch: 31 , batch: 212 , training loss: 4.186955
[INFO] Epoch: 31 , batch: 213 , training loss: 4.122302
[INFO] Epoch: 31 , batch: 214 , training loss: 4.242366
[INFO] Epoch: 31 , batch: 215 , training loss: 4.437693
[INFO] Epoch: 31 , batch: 216 , training loss: 4.125955
[INFO] Epoch: 31 , batch: 217 , training loss: 4.088099
[INFO] Epoch: 31 , batch: 218 , training loss: 4.073333
[INFO] Epoch: 31 , batch: 219 , training loss: 4.193414
[INFO] Epoch: 31 , batch: 220 , training loss: 3.977598
[INFO] Epoch: 31 , batch: 221 , training loss: 4.010374
[INFO] Epoch: 31 , batch: 222 , training loss: 4.173488
[INFO] Epoch: 31 , batch: 223 , training loss: 4.250100
[INFO] Epoch: 31 , batch: 224 , training loss: 4.292322
[INFO] Epoch: 31 , batch: 225 , training loss: 4.185462
[INFO] Epoch: 31 , batch: 226 , training loss: 4.302047
[INFO] Epoch: 31 , batch: 227 , training loss: 4.250080
[INFO] Epoch: 31 , batch: 228 , training loss: 4.302789
[INFO] Epoch: 31 , batch: 229 , training loss: 4.169006
[INFO] Epoch: 31 , batch: 230 , training loss: 4.021461
[INFO] Epoch: 31 , batch: 231 , training loss: 3.909995
[INFO] Epoch: 31 , batch: 232 , training loss: 4.042108
[INFO] Epoch: 31 , batch: 233 , training loss: 4.063009
[INFO] Epoch: 31 , batch: 234 , training loss: 3.752596
[INFO] Epoch: 31 , batch: 235 , training loss: 3.855317
[INFO] Epoch: 31 , batch: 236 , training loss: 3.961542
[INFO] Epoch: 31 , batch: 237 , training loss: 4.180961
[INFO] Epoch: 31 , batch: 238 , training loss: 3.925836
[INFO] Epoch: 31 , batch: 239 , training loss: 3.983234
[INFO] Epoch: 31 , batch: 240 , training loss: 4.047294
[INFO] Epoch: 31 , batch: 241 , training loss: 3.849423
[INFO] Epoch: 31 , batch: 242 , training loss: 3.874319
[INFO] Epoch: 31 , batch: 243 , training loss: 4.161342
[INFO] Epoch: 31 , batch: 244 , training loss: 4.087515
[INFO] Epoch: 31 , batch: 245 , training loss: 4.083878
[INFO] Epoch: 31 , batch: 246 , training loss: 3.786125
[INFO] Epoch: 31 , batch: 247 , training loss: 3.940570
[INFO] Epoch: 31 , batch: 248 , training loss: 4.004916
[INFO] Epoch: 31 , batch: 249 , training loss: 4.036658
[INFO] Epoch: 31 , batch: 250 , training loss: 3.796484
[INFO] Epoch: 31 , batch: 251 , training loss: 4.227957
[INFO] Epoch: 31 , batch: 252 , training loss: 3.939750
[INFO] Epoch: 31 , batch: 253 , training loss: 3.902307
[INFO] Epoch: 31 , batch: 254 , training loss: 4.136807
[INFO] Epoch: 31 , batch: 255 , training loss: 4.105764
[INFO] Epoch: 31 , batch: 256 , training loss: 4.142634
[INFO] Epoch: 31 , batch: 257 , training loss: 4.269409
[INFO] Epoch: 31 , batch: 258 , training loss: 4.313911
[INFO] Epoch: 31 , batch: 259 , training loss: 4.360574
[INFO] Epoch: 31 , batch: 260 , training loss: 4.101611
[INFO] Epoch: 31 , batch: 261 , training loss: 4.225333
[INFO] Epoch: 31 , batch: 262 , training loss: 4.405295
[INFO] Epoch: 31 , batch: 263 , training loss: 4.582843
[INFO] Epoch: 31 , batch: 264 , training loss: 3.895084
[INFO] Epoch: 31 , batch: 265 , training loss: 4.029799
[INFO] Epoch: 31 , batch: 266 , training loss: 4.471405
[INFO] Epoch: 31 , batch: 267 , training loss: 4.219085
[INFO] Epoch: 31 , batch: 268 , training loss: 4.145514
[INFO] Epoch: 31 , batch: 269 , training loss: 4.111264
[INFO] Epoch: 31 , batch: 270 , training loss: 4.139040
[INFO] Epoch: 31 , batch: 271 , training loss: 4.176955
[INFO] Epoch: 31 , batch: 272 , training loss: 4.163149
[INFO] Epoch: 31 , batch: 273 , training loss: 4.170452
[INFO] Epoch: 31 , batch: 274 , training loss: 4.239334
[INFO] Epoch: 31 , batch: 275 , training loss: 4.120026
[INFO] Epoch: 31 , batch: 276 , training loss: 4.176977
[INFO] Epoch: 31 , batch: 277 , training loss: 4.340510
[INFO] Epoch: 31 , batch: 278 , training loss: 4.022000
[INFO] Epoch: 31 , batch: 279 , training loss: 4.020457
[INFO] Epoch: 31 , batch: 280 , training loss: 3.977253
[INFO] Epoch: 31 , batch: 281 , training loss: 4.119306
[INFO] Epoch: 31 , batch: 282 , training loss: 4.024217
[INFO] Epoch: 31 , batch: 283 , training loss: 4.049460
[INFO] Epoch: 31 , batch: 284 , training loss: 4.075461
[INFO] Epoch: 31 , batch: 285 , training loss: 4.023939
[INFO] Epoch: 31 , batch: 286 , training loss: 4.001468
[INFO] Epoch: 31 , batch: 287 , training loss: 3.955011
[INFO] Epoch: 31 , batch: 288 , training loss: 3.971889
[INFO] Epoch: 31 , batch: 289 , training loss: 4.016378
[INFO] Epoch: 31 , batch: 290 , training loss: 3.802624
[INFO] Epoch: 31 , batch: 291 , training loss: 3.755386
[INFO] Epoch: 31 , batch: 292 , training loss: 3.891625
[INFO] Epoch: 31 , batch: 293 , training loss: 3.803304
[INFO] Epoch: 31 , batch: 294 , training loss: 4.472015
[INFO] Epoch: 31 , batch: 295 , training loss: 4.233472
[INFO] Epoch: 31 , batch: 296 , training loss: 4.169188
[INFO] Epoch: 31 , batch: 297 , training loss: 4.132438
[INFO] Epoch: 31 , batch: 298 , training loss: 3.966772
[INFO] Epoch: 31 , batch: 299 , training loss: 3.987926
[INFO] Epoch: 31 , batch: 300 , training loss: 3.951645
[INFO] Epoch: 31 , batch: 301 , training loss: 3.909275
[INFO] Epoch: 31 , batch: 302 , training loss: 4.060532
[INFO] Epoch: 31 , batch: 303 , training loss: 4.077509
[INFO] Epoch: 31 , batch: 304 , training loss: 4.241754
[INFO] Epoch: 31 , batch: 305 , training loss: 4.036422
[INFO] Epoch: 31 , batch: 306 , training loss: 4.175984
[INFO] Epoch: 31 , batch: 307 , training loss: 4.164825
[INFO] Epoch: 31 , batch: 308 , training loss: 4.010141
[INFO] Epoch: 31 , batch: 309 , training loss: 4.003190
[INFO] Epoch: 31 , batch: 310 , training loss: 3.905020
[INFO] Epoch: 31 , batch: 311 , training loss: 3.907624
[INFO] Epoch: 31 , batch: 312 , training loss: 3.840566
[INFO] Epoch: 31 , batch: 313 , training loss: 3.943585
[INFO] Epoch: 31 , batch: 314 , training loss: 4.019223
[INFO] Epoch: 31 , batch: 315 , training loss: 4.061868
[INFO] Epoch: 31 , batch: 316 , training loss: 4.297271
[INFO] Epoch: 31 , batch: 317 , training loss: 4.713210
[INFO] Epoch: 31 , batch: 318 , training loss: 4.865756
[INFO] Epoch: 31 , batch: 319 , training loss: 4.482814
[INFO] Epoch: 31 , batch: 320 , training loss: 4.018989
[INFO] Epoch: 31 , batch: 321 , training loss: 3.851911
[INFO] Epoch: 31 , batch: 322 , training loss: 3.972990
[INFO] Epoch: 31 , batch: 323 , training loss: 3.991966
[INFO] Epoch: 31 , batch: 324 , training loss: 3.950720
[INFO] Epoch: 31 , batch: 325 , training loss: 4.106798
[INFO] Epoch: 31 , batch: 326 , training loss: 4.164518
[INFO] Epoch: 31 , batch: 327 , training loss: 4.061443
[INFO] Epoch: 31 , batch: 328 , training loss: 4.058655
[INFO] Epoch: 31 , batch: 329 , training loss: 3.979730
[INFO] Epoch: 31 , batch: 330 , training loss: 3.990659
[INFO] Epoch: 31 , batch: 331 , training loss: 4.127418
[INFO] Epoch: 31 , batch: 332 , training loss: 3.935114
[INFO] Epoch: 31 , batch: 333 , training loss: 3.965046
[INFO] Epoch: 31 , batch: 334 , training loss: 3.953194
[INFO] Epoch: 31 , batch: 335 , training loss: 4.079471
[INFO] Epoch: 31 , batch: 336 , training loss: 4.111824
[INFO] Epoch: 31 , batch: 337 , training loss: 4.126071
[INFO] Epoch: 31 , batch: 338 , training loss: 4.337575
[INFO] Epoch: 31 , batch: 339 , training loss: 4.211453
[INFO] Epoch: 31 , batch: 340 , training loss: 4.355148
[INFO] Epoch: 31 , batch: 341 , training loss: 4.106306
[INFO] Epoch: 31 , batch: 342 , training loss: 3.916543
[INFO] Epoch: 31 , batch: 343 , training loss: 3.969486
[INFO] Epoch: 31 , batch: 344 , training loss: 3.823057
[INFO] Epoch: 31 , batch: 345 , training loss: 3.963353
[INFO] Epoch: 31 , batch: 346 , training loss: 4.008415
[INFO] Epoch: 31 , batch: 347 , training loss: 3.923664
[INFO] Epoch: 31 , batch: 348 , training loss: 4.029425
[INFO] Epoch: 31 , batch: 349 , training loss: 4.173121
[INFO] Epoch: 31 , batch: 350 , training loss: 3.954924
[INFO] Epoch: 31 , batch: 351 , training loss: 4.019674
[INFO] Epoch: 31 , batch: 352 , training loss: 4.038956
[INFO] Epoch: 31 , batch: 353 , training loss: 4.001965
[INFO] Epoch: 31 , batch: 354 , training loss: 4.122385
[INFO] Epoch: 31 , batch: 355 , training loss: 4.161735
[INFO] Epoch: 31 , batch: 356 , training loss: 4.004396
[INFO] Epoch: 31 , batch: 357 , training loss: 4.093859
[INFO] Epoch: 31 , batch: 358 , training loss: 3.972743
[INFO] Epoch: 31 , batch: 359 , training loss: 4.001626
[INFO] Epoch: 31 , batch: 360 , training loss: 4.075549
[INFO] Epoch: 31 , batch: 361 , training loss: 4.032529
[INFO] Epoch: 31 , batch: 362 , training loss: 4.135277
[INFO] Epoch: 31 , batch: 363 , training loss: 4.018833
[INFO] Epoch: 31 , batch: 364 , training loss: 4.065386
[INFO] Epoch: 31 , batch: 365 , training loss: 4.010431
[INFO] Epoch: 31 , batch: 366 , training loss: 4.124207
[INFO] Epoch: 31 , batch: 367 , training loss: 4.205831
[INFO] Epoch: 31 , batch: 368 , training loss: 4.597466
[INFO] Epoch: 31 , batch: 369 , training loss: 4.248845
[INFO] Epoch: 31 , batch: 370 , training loss: 4.034673
[INFO] Epoch: 31 , batch: 371 , training loss: 4.388506
[INFO] Epoch: 31 , batch: 372 , training loss: 4.698581
[INFO] Epoch: 31 , batch: 373 , training loss: 4.786936
[INFO] Epoch: 31 , batch: 374 , training loss: 4.870215
[INFO] Epoch: 31 , batch: 375 , training loss: 4.849916
[INFO] Epoch: 31 , batch: 376 , training loss: 4.777598
[INFO] Epoch: 31 , batch: 377 , training loss: 4.536828
[INFO] Epoch: 31 , batch: 378 , training loss: 4.633690
[INFO] Epoch: 31 , batch: 379 , training loss: 4.595798
[INFO] Epoch: 31 , batch: 380 , training loss: 4.702430
[INFO] Epoch: 31 , batch: 381 , training loss: 4.439616
[INFO] Epoch: 31 , batch: 382 , training loss: 4.658923
[INFO] Epoch: 31 , batch: 383 , training loss: 4.682344
[INFO] Epoch: 31 , batch: 384 , training loss: 4.689384
[INFO] Epoch: 31 , batch: 385 , training loss: 4.402963
[INFO] Epoch: 31 , batch: 386 , training loss: 4.698235
[INFO] Epoch: 31 , batch: 387 , training loss: 4.646628
[INFO] Epoch: 31 , batch: 388 , training loss: 4.444311
[INFO] Epoch: 31 , batch: 389 , training loss: 4.285382
[INFO] Epoch: 31 , batch: 390 , training loss: 4.279198
[INFO] Epoch: 31 , batch: 391 , training loss: 4.337142
[INFO] Epoch: 31 , batch: 392 , training loss: 4.676795
[INFO] Epoch: 31 , batch: 393 , training loss: 4.556190
[INFO] Epoch: 31 , batch: 394 , training loss: 4.623708
[INFO] Epoch: 31 , batch: 395 , training loss: 4.474083
[INFO] Epoch: 31 , batch: 396 , training loss: 4.242834
[INFO] Epoch: 31 , batch: 397 , training loss: 4.419482
[INFO] Epoch: 31 , batch: 398 , training loss: 4.267465
[INFO] Epoch: 31 , batch: 399 , training loss: 4.343933
[INFO] Epoch: 31 , batch: 400 , training loss: 4.316956
[INFO] Epoch: 31 , batch: 401 , training loss: 4.722883
[INFO] Epoch: 31 , batch: 402 , training loss: 4.454281
[INFO] Epoch: 31 , batch: 403 , training loss: 4.233187
[INFO] Epoch: 31 , batch: 404 , training loss: 4.451577
[INFO] Epoch: 31 , batch: 405 , training loss: 4.501886
[INFO] Epoch: 31 , batch: 406 , training loss: 4.405061
[INFO] Epoch: 31 , batch: 407 , training loss: 4.455420
[INFO] Epoch: 31 , batch: 408 , training loss: 4.426211
[INFO] Epoch: 31 , batch: 409 , training loss: 4.387759
[INFO] Epoch: 31 , batch: 410 , training loss: 4.446282
[INFO] Epoch: 31 , batch: 411 , training loss: 4.635852
[INFO] Epoch: 31 , batch: 412 , training loss: 4.478940
[INFO] Epoch: 31 , batch: 413 , training loss: 4.345071
[INFO] Epoch: 31 , batch: 414 , training loss: 4.388001
[INFO] Epoch: 31 , batch: 415 , training loss: 4.413071
[INFO] Epoch: 31 , batch: 416 , training loss: 4.509774
[INFO] Epoch: 31 , batch: 417 , training loss: 4.402715
[INFO] Epoch: 31 , batch: 418 , training loss: 4.426621
[INFO] Epoch: 31 , batch: 419 , training loss: 4.391036
[INFO] Epoch: 31 , batch: 420 , training loss: 4.361168
[INFO] Epoch: 31 , batch: 421 , training loss: 4.366965
[INFO] Epoch: 31 , batch: 422 , training loss: 4.228342
[INFO] Epoch: 31 , batch: 423 , training loss: 4.424539
[INFO] Epoch: 31 , batch: 424 , training loss: 4.609660
[INFO] Epoch: 31 , batch: 425 , training loss: 4.466956
[INFO] Epoch: 31 , batch: 426 , training loss: 4.204469
[INFO] Epoch: 31 , batch: 427 , training loss: 4.448658
[INFO] Epoch: 31 , batch: 428 , training loss: 4.325103
[INFO] Epoch: 31 , batch: 429 , training loss: 4.182825
[INFO] Epoch: 31 , batch: 430 , training loss: 4.444211
[INFO] Epoch: 31 , batch: 431 , training loss: 4.055050
[INFO] Epoch: 31 , batch: 432 , training loss: 4.100899
[INFO] Epoch: 31 , batch: 433 , training loss: 4.150132
[INFO] Epoch: 31 , batch: 434 , training loss: 4.015907
[INFO] Epoch: 31 , batch: 435 , training loss: 4.382561
[INFO] Epoch: 31 , batch: 436 , training loss: 4.460142
[INFO] Epoch: 31 , batch: 437 , training loss: 4.209556
[INFO] Epoch: 31 , batch: 438 , training loss: 4.059745
[INFO] Epoch: 31 , batch: 439 , training loss: 4.280701
[INFO] Epoch: 31 , batch: 440 , training loss: 4.408838
[INFO] Epoch: 31 , batch: 441 , training loss: 4.520191
[INFO] Epoch: 31 , batch: 442 , training loss: 4.279351
[INFO] Epoch: 31 , batch: 443 , training loss: 4.465413
[INFO] Epoch: 31 , batch: 444 , training loss: 4.099858
[INFO] Epoch: 31 , batch: 445 , training loss: 3.994041
[INFO] Epoch: 31 , batch: 446 , training loss: 3.928791
[INFO] Epoch: 31 , batch: 447 , training loss: 4.110722
[INFO] Epoch: 31 , batch: 448 , training loss: 4.215868
[INFO] Epoch: 31 , batch: 449 , training loss: 4.614089
[INFO] Epoch: 31 , batch: 450 , training loss: 4.662323
[INFO] Epoch: 31 , batch: 451 , training loss: 4.596629
[INFO] Epoch: 31 , batch: 452 , training loss: 4.385080
[INFO] Epoch: 31 , batch: 453 , training loss: 4.152638
[INFO] Epoch: 31 , batch: 454 , training loss: 4.298561
[INFO] Epoch: 31 , batch: 455 , training loss: 4.359117
[INFO] Epoch: 31 , batch: 456 , training loss: 4.355596
[INFO] Epoch: 31 , batch: 457 , training loss: 4.439548
[INFO] Epoch: 31 , batch: 458 , training loss: 4.163177
[INFO] Epoch: 31 , batch: 459 , training loss: 4.158579
[INFO] Epoch: 31 , batch: 460 , training loss: 4.237953
[INFO] Epoch: 31 , batch: 461 , training loss: 4.214118
[INFO] Epoch: 31 , batch: 462 , training loss: 4.306893
[INFO] Epoch: 31 , batch: 463 , training loss: 4.205280
[INFO] Epoch: 31 , batch: 464 , training loss: 4.384878
[INFO] Epoch: 31 , batch: 465 , training loss: 4.310884
[INFO] Epoch: 31 , batch: 466 , training loss: 4.404248
[INFO] Epoch: 31 , batch: 467 , training loss: 4.378173
[INFO] Epoch: 31 , batch: 468 , training loss: 4.369140
[INFO] Epoch: 31 , batch: 469 , training loss: 4.377940
[INFO] Epoch: 31 , batch: 470 , training loss: 4.200459
[INFO] Epoch: 31 , batch: 471 , training loss: 4.282496
[INFO] Epoch: 31 , batch: 472 , training loss: 4.326581
[INFO] Epoch: 31 , batch: 473 , training loss: 4.257361
[INFO] Epoch: 31 , batch: 474 , training loss: 4.051730
[INFO] Epoch: 31 , batch: 475 , training loss: 3.948345
[INFO] Epoch: 31 , batch: 476 , training loss: 4.309249
[INFO] Epoch: 31 , batch: 477 , training loss: 4.440182
[INFO] Epoch: 31 , batch: 478 , training loss: 4.489681
[INFO] Epoch: 31 , batch: 479 , training loss: 4.427635
[INFO] Epoch: 31 , batch: 480 , training loss: 4.522795
[INFO] Epoch: 31 , batch: 481 , training loss: 4.398314
[INFO] Epoch: 31 , batch: 482 , training loss: 4.528106
[INFO] Epoch: 31 , batch: 483 , training loss: 4.377065
[INFO] Epoch: 31 , batch: 484 , training loss: 4.183615
[INFO] Epoch: 31 , batch: 485 , training loss: 4.262965
[INFO] Epoch: 31 , batch: 486 , training loss: 4.167443
[INFO] Epoch: 31 , batch: 487 , training loss: 4.144200
[INFO] Epoch: 31 , batch: 488 , training loss: 4.349745
[INFO] Epoch: 31 , batch: 489 , training loss: 4.229835
[INFO] Epoch: 31 , batch: 490 , training loss: 4.275411
[INFO] Epoch: 31 , batch: 491 , training loss: 4.229147
[INFO] Epoch: 31 , batch: 492 , training loss: 4.189068
[INFO] Epoch: 31 , batch: 493 , training loss: 4.369299
[INFO] Epoch: 31 , batch: 494 , training loss: 4.294154
[INFO] Epoch: 31 , batch: 495 , training loss: 4.404045
[INFO] Epoch: 31 , batch: 496 , training loss: 4.289168
[INFO] Epoch: 31 , batch: 497 , training loss: 4.319612
[INFO] Epoch: 31 , batch: 498 , training loss: 4.326581
[INFO] Epoch: 31 , batch: 499 , training loss: 4.384009
[INFO] Epoch: 31 , batch: 500 , training loss: 4.550455
[INFO] Epoch: 31 , batch: 501 , training loss: 4.847777
[INFO] Epoch: 31 , batch: 502 , training loss: 4.944515
[INFO] Epoch: 31 , batch: 503 , training loss: 4.609279
[INFO] Epoch: 31 , batch: 504 , training loss: 4.737441
[INFO] Epoch: 31 , batch: 505 , training loss: 4.706540
[INFO] Epoch: 31 , batch: 506 , training loss: 4.659866
[INFO] Epoch: 31 , batch: 507 , training loss: 4.730592
[INFO] Epoch: 31 , batch: 508 , training loss: 4.674541
[INFO] Epoch: 31 , batch: 509 , training loss: 4.456388
[INFO] Epoch: 31 , batch: 510 , training loss: 4.542814
[INFO] Epoch: 31 , batch: 511 , training loss: 4.445266
[INFO] Epoch: 31 , batch: 512 , training loss: 4.539536
[INFO] Epoch: 31 , batch: 513 , training loss: 4.780515
[INFO] Epoch: 31 , batch: 514 , training loss: 4.437050
[INFO] Epoch: 31 , batch: 515 , training loss: 4.667806
[INFO] Epoch: 31 , batch: 516 , training loss: 4.493062
[INFO] Epoch: 31 , batch: 517 , training loss: 4.441772
[INFO] Epoch: 31 , batch: 518 , training loss: 4.406398
[INFO] Epoch: 31 , batch: 519 , training loss: 4.270903
[INFO] Epoch: 31 , batch: 520 , training loss: 4.494762
[INFO] Epoch: 31 , batch: 521 , training loss: 4.453998
[INFO] Epoch: 31 , batch: 522 , training loss: 4.516282
[INFO] Epoch: 31 , batch: 523 , training loss: 4.433503
[INFO] Epoch: 31 , batch: 524 , training loss: 4.729079
[INFO] Epoch: 31 , batch: 525 , training loss: 4.665685
[INFO] Epoch: 31 , batch: 526 , training loss: 4.418682
[INFO] Epoch: 31 , batch: 527 , training loss: 4.470989
[INFO] Epoch: 31 , batch: 528 , training loss: 4.473429
[INFO] Epoch: 31 , batch: 529 , training loss: 4.446704
[INFO] Epoch: 31 , batch: 530 , training loss: 4.299715
[INFO] Epoch: 31 , batch: 531 , training loss: 4.433044
[INFO] Epoch: 31 , batch: 532 , training loss: 4.336783
[INFO] Epoch: 31 , batch: 533 , training loss: 4.497947
[INFO] Epoch: 31 , batch: 534 , training loss: 4.460766
[INFO] Epoch: 31 , batch: 535 , training loss: 4.477258
[INFO] Epoch: 31 , batch: 536 , training loss: 4.314695
[INFO] Epoch: 31 , batch: 537 , training loss: 4.330916
[INFO] Epoch: 31 , batch: 538 , training loss: 4.404785
[INFO] Epoch: 31 , batch: 539 , training loss: 4.494699
[INFO] Epoch: 31 , batch: 540 , training loss: 4.993454
[INFO] Epoch: 31 , batch: 541 , training loss: 4.865489
[INFO] Epoch: 31 , batch: 542 , training loss: 4.787632
[INFO] Epoch: 32 , batch: 0 , training loss: 3.446098
[INFO] Epoch: 32 , batch: 1 , training loss: 3.450423
[INFO] Epoch: 32 , batch: 2 , training loss: 3.636770
[INFO] Epoch: 32 , batch: 3 , training loss: 3.461913
[INFO] Epoch: 32 , batch: 4 , training loss: 3.829872
[INFO] Epoch: 32 , batch: 5 , training loss: 3.457291
[INFO] Epoch: 32 , batch: 6 , training loss: 3.811360
[INFO] Epoch: 32 , batch: 7 , training loss: 3.788784
[INFO] Epoch: 32 , batch: 8 , training loss: 3.483991
[INFO] Epoch: 32 , batch: 9 , training loss: 3.760585
[INFO] Epoch: 32 , batch: 10 , training loss: 3.722362
[INFO] Epoch: 32 , batch: 11 , training loss: 3.656205
[INFO] Epoch: 32 , batch: 12 , training loss: 3.580532
[INFO] Epoch: 32 , batch: 13 , training loss: 3.635979
[INFO] Epoch: 32 , batch: 14 , training loss: 3.476665
[INFO] Epoch: 32 , batch: 15 , training loss: 3.673391
[INFO] Epoch: 32 , batch: 16 , training loss: 3.540093
[INFO] Epoch: 32 , batch: 17 , training loss: 3.730699
[INFO] Epoch: 32 , batch: 18 , training loss: 3.699834
[INFO] Epoch: 32 , batch: 19 , training loss: 3.373827
[INFO] Epoch: 32 , batch: 20 , training loss: 3.382874
[INFO] Epoch: 32 , batch: 21 , training loss: 3.523908
[INFO] Epoch: 32 , batch: 22 , training loss: 3.380940
[INFO] Epoch: 32 , batch: 23 , training loss: 3.629167
[INFO] Epoch: 32 , batch: 24 , training loss: 3.406816
[INFO] Epoch: 32 , batch: 25 , training loss: 3.597775
[INFO] Epoch: 32 , batch: 26 , training loss: 3.469733
[INFO] Epoch: 32 , batch: 27 , training loss: 3.414615
[INFO] Epoch: 32 , batch: 28 , training loss: 3.589528
[INFO] Epoch: 32 , batch: 29 , training loss: 3.405532
[INFO] Epoch: 32 , batch: 30 , training loss: 3.487023
[INFO] Epoch: 32 , batch: 31 , training loss: 3.523380
[INFO] Epoch: 32 , batch: 32 , training loss: 3.527195
[INFO] Epoch: 32 , batch: 33 , training loss: 3.546218
[INFO] Epoch: 32 , batch: 34 , training loss: 3.497326
[INFO] Epoch: 32 , batch: 35 , training loss: 3.528619
[INFO] Epoch: 32 , batch: 36 , training loss: 3.551009
[INFO] Epoch: 32 , batch: 37 , training loss: 3.464372
[INFO] Epoch: 32 , batch: 38 , training loss: 3.450941
[INFO] Epoch: 32 , batch: 39 , training loss: 3.334960
[INFO] Epoch: 32 , batch: 40 , training loss: 3.603827
[INFO] Epoch: 32 , batch: 41 , training loss: 3.501840
[INFO] Epoch: 32 , batch: 42 , training loss: 3.914659
[INFO] Epoch: 32 , batch: 43 , training loss: 3.634931
[INFO] Epoch: 32 , batch: 44 , training loss: 4.006691
[INFO] Epoch: 32 , batch: 45 , training loss: 3.867586
[INFO] Epoch: 32 , batch: 46 , training loss: 3.776662
[INFO] Epoch: 32 , batch: 47 , training loss: 3.568219
[INFO] Epoch: 32 , batch: 48 , training loss: 3.574005
[INFO] Epoch: 32 , batch: 49 , training loss: 3.760711
[INFO] Epoch: 32 , batch: 50 , training loss: 3.587662
[INFO] Epoch: 32 , batch: 51 , training loss: 3.778870
[INFO] Epoch: 32 , batch: 52 , training loss: 3.671301
[INFO] Epoch: 32 , batch: 53 , training loss: 3.761624
[INFO] Epoch: 32 , batch: 54 , training loss: 3.746540
[INFO] Epoch: 32 , batch: 55 , training loss: 3.869561
[INFO] Epoch: 32 , batch: 56 , training loss: 3.714490
[INFO] Epoch: 32 , batch: 57 , training loss: 3.618840
[INFO] Epoch: 32 , batch: 58 , training loss: 3.678004
[INFO] Epoch: 32 , batch: 59 , training loss: 3.755766
[INFO] Epoch: 32 , batch: 60 , training loss: 3.715469
[INFO] Epoch: 32 , batch: 61 , training loss: 3.779564
[INFO] Epoch: 32 , batch: 62 , training loss: 3.675735
[INFO] Epoch: 32 , batch: 63 , training loss: 3.878691
[INFO] Epoch: 32 , batch: 64 , training loss: 4.068102
[INFO] Epoch: 32 , batch: 65 , training loss: 3.738925
[INFO] Epoch: 32 , batch: 66 , training loss: 3.591778
[INFO] Epoch: 32 , batch: 67 , training loss: 3.674794
[INFO] Epoch: 32 , batch: 68 , training loss: 3.809756
[INFO] Epoch: 32 , batch: 69 , training loss: 3.716534
[INFO] Epoch: 32 , batch: 70 , training loss: 3.925512
[INFO] Epoch: 32 , batch: 71 , training loss: 3.814045
[INFO] Epoch: 32 , batch: 72 , training loss: 3.873946
[INFO] Epoch: 32 , batch: 73 , training loss: 3.806408
[INFO] Epoch: 32 , batch: 74 , training loss: 3.886204
[INFO] Epoch: 32 , batch: 75 , training loss: 3.815627
[INFO] Epoch: 32 , batch: 76 , training loss: 3.871935
[INFO] Epoch: 32 , batch: 77 , training loss: 3.837328
[INFO] Epoch: 32 , batch: 78 , training loss: 3.978065
[INFO] Epoch: 32 , batch: 79 , training loss: 3.794934
[INFO] Epoch: 32 , batch: 80 , training loss: 3.983956
[INFO] Epoch: 32 , batch: 81 , training loss: 3.909627
[INFO] Epoch: 32 , batch: 82 , training loss: 3.857981
[INFO] Epoch: 32 , batch: 83 , training loss: 3.968462
[INFO] Epoch: 32 , batch: 84 , training loss: 3.898735
[INFO] Epoch: 32 , batch: 85 , training loss: 3.999299
[INFO] Epoch: 32 , batch: 86 , training loss: 3.910846
[INFO] Epoch: 32 , batch: 87 , training loss: 3.916266
[INFO] Epoch: 32 , batch: 88 , training loss: 4.032526
[INFO] Epoch: 32 , batch: 89 , training loss: 3.834467
[INFO] Epoch: 32 , batch: 90 , training loss: 3.915808
[INFO] Epoch: 32 , batch: 91 , training loss: 3.857909
[INFO] Epoch: 32 , batch: 92 , training loss: 3.858686
[INFO] Epoch: 32 , batch: 93 , training loss: 3.960463
[INFO] Epoch: 32 , batch: 94 , training loss: 4.142089
[INFO] Epoch: 32 , batch: 95 , training loss: 3.868244
[INFO] Epoch: 32 , batch: 96 , training loss: 3.904063
[INFO] Epoch: 32 , batch: 97 , training loss: 3.803171
[INFO] Epoch: 32 , batch: 98 , training loss: 3.780024
[INFO] Epoch: 32 , batch: 99 , training loss: 3.856162
[INFO] Epoch: 32 , batch: 100 , training loss: 3.772224
[INFO] Epoch: 32 , batch: 101 , training loss: 3.849427
[INFO] Epoch: 32 , batch: 102 , training loss: 3.934347
[INFO] Epoch: 32 , batch: 103 , training loss: 3.727922
[INFO] Epoch: 32 , batch: 104 , training loss: 3.708195
[INFO] Epoch: 32 , batch: 105 , training loss: 3.957026
[INFO] Epoch: 32 , batch: 106 , training loss: 3.923738
[INFO] Epoch: 32 , batch: 107 , training loss: 3.824325
[INFO] Epoch: 32 , batch: 108 , training loss: 3.712126
[INFO] Epoch: 32 , batch: 109 , training loss: 3.673464
[INFO] Epoch: 32 , batch: 110 , training loss: 3.851325
[INFO] Epoch: 32 , batch: 111 , training loss: 3.927350
[INFO] Epoch: 32 , batch: 112 , training loss: 3.853655
[INFO] Epoch: 32 , batch: 113 , training loss: 3.859948
[INFO] Epoch: 32 , batch: 114 , training loss: 3.829257
[INFO] Epoch: 32 , batch: 115 , training loss: 3.849092
[INFO] Epoch: 32 , batch: 116 , training loss: 3.763849
[INFO] Epoch: 32 , batch: 117 , training loss: 3.995292
[INFO] Epoch: 32 , batch: 118 , training loss: 3.950835
[INFO] Epoch: 32 , batch: 119 , training loss: 4.067119
[INFO] Epoch: 32 , batch: 120 , training loss: 4.046502
[INFO] Epoch: 32 , batch: 121 , training loss: 3.910789
[INFO] Epoch: 32 , batch: 122 , training loss: 3.812874
[INFO] Epoch: 32 , batch: 123 , training loss: 3.866526
[INFO] Epoch: 32 , batch: 124 , training loss: 3.942314
[INFO] Epoch: 32 , batch: 125 , training loss: 3.766634
[INFO] Epoch: 32 , batch: 126 , training loss: 3.769479
[INFO] Epoch: 32 , batch: 127 , training loss: 3.754521
[INFO] Epoch: 32 , batch: 128 , training loss: 3.896291
[INFO] Epoch: 32 , batch: 129 , training loss: 3.898830
[INFO] Epoch: 32 , batch: 130 , training loss: 3.848221
[INFO] Epoch: 32 , batch: 131 , training loss: 3.869803
[INFO] Epoch: 32 , batch: 132 , training loss: 3.849882
[INFO] Epoch: 32 , batch: 133 , training loss: 3.835554
[INFO] Epoch: 32 , batch: 134 , training loss: 3.663915
[INFO] Epoch: 32 , batch: 135 , training loss: 3.704182
[INFO] Epoch: 32 , batch: 136 , training loss: 3.970865
[INFO] Epoch: 32 , batch: 137 , training loss: 3.872139
[INFO] Epoch: 32 , batch: 138 , training loss: 3.945400
[INFO] Epoch: 32 , batch: 139 , training loss: 4.539841
[INFO] Epoch: 32 , batch: 140 , training loss: 4.254407
[INFO] Epoch: 32 , batch: 141 , training loss: 4.021350
[INFO] Epoch: 32 , batch: 142 , training loss: 3.812930
[INFO] Epoch: 32 , batch: 143 , training loss: 3.930682
[INFO] Epoch: 32 , batch: 144 , training loss: 3.735476
[INFO] Epoch: 32 , batch: 145 , training loss: 3.830010
[INFO] Epoch: 32 , batch: 146 , training loss: 4.002322
[INFO] Epoch: 32 , batch: 147 , training loss: 3.705169
[INFO] Epoch: 32 , batch: 148 , training loss: 3.679346
[INFO] Epoch: 32 , batch: 149 , training loss: 3.746970
[INFO] Epoch: 32 , batch: 150 , training loss: 3.977873
[INFO] Epoch: 32 , batch: 151 , training loss: 3.870308
[INFO] Epoch: 32 , batch: 152 , training loss: 3.913093
[INFO] Epoch: 32 , batch: 153 , training loss: 3.896497
[INFO] Epoch: 32 , batch: 154 , training loss: 3.995698
[INFO] Epoch: 32 , batch: 155 , training loss: 4.195435
[INFO] Epoch: 32 , batch: 156 , training loss: 3.926141
[INFO] Epoch: 32 , batch: 157 , training loss: 3.893711
[INFO] Epoch: 32 , batch: 158 , training loss: 4.051844
[INFO] Epoch: 32 , batch: 159 , training loss: 3.903687
[INFO] Epoch: 32 , batch: 160 , training loss: 4.193755
[INFO] Epoch: 32 , batch: 161 , training loss: 4.307246
[INFO] Epoch: 32 , batch: 162 , training loss: 4.286651
[INFO] Epoch: 32 , batch: 163 , training loss: 4.400972
[INFO] Epoch: 32 , batch: 164 , training loss: 4.376385
[INFO] Epoch: 32 , batch: 165 , training loss: 4.328299
[INFO] Epoch: 32 , batch: 166 , training loss: 4.148138
[INFO] Epoch: 32 , batch: 167 , training loss: 4.191732
[INFO] Epoch: 32 , batch: 168 , training loss: 3.877663
[INFO] Epoch: 32 , batch: 169 , training loss: 3.863229
[INFO] Epoch: 32 , batch: 170 , training loss: 4.080021
[INFO] Epoch: 32 , batch: 171 , training loss: 3.458874
[INFO] Epoch: 32 , batch: 172 , training loss: 3.699601
[INFO] Epoch: 32 , batch: 173 , training loss: 4.078299
[INFO] Epoch: 32 , batch: 174 , training loss: 4.487806
[INFO] Epoch: 32 , batch: 175 , training loss: 4.822941
[INFO] Epoch: 32 , batch: 176 , training loss: 4.449084
[INFO] Epoch: 32 , batch: 177 , training loss: 4.114842
[INFO] Epoch: 32 , batch: 178 , training loss: 4.115361
[INFO] Epoch: 32 , batch: 179 , training loss: 4.138825
[INFO] Epoch: 32 , batch: 180 , training loss: 4.101257
[INFO] Epoch: 32 , batch: 181 , training loss: 4.378230
[INFO] Epoch: 32 , batch: 182 , training loss: 4.341756
[INFO] Epoch: 32 , batch: 183 , training loss: 4.310340
[INFO] Epoch: 32 , batch: 184 , training loss: 4.207240
[INFO] Epoch: 32 , batch: 185 , training loss: 4.141570
[INFO] Epoch: 32 , batch: 186 , training loss: 4.314011
[INFO] Epoch: 32 , batch: 187 , training loss: 4.407503
[INFO] Epoch: 32 , batch: 188 , training loss: 4.388758
[INFO] Epoch: 32 , batch: 189 , training loss: 4.293684
[INFO] Epoch: 32 , batch: 190 , training loss: 4.329226
[INFO] Epoch: 32 , batch: 191 , training loss: 4.414642
[INFO] Epoch: 32 , batch: 192 , training loss: 4.242472
[INFO] Epoch: 32 , batch: 193 , training loss: 4.355989
[INFO] Epoch: 32 , batch: 194 , training loss: 4.307219
[INFO] Epoch: 32 , batch: 195 , training loss: 4.219785
[INFO] Epoch: 32 , batch: 196 , training loss: 4.106081
[INFO] Epoch: 32 , batch: 197 , training loss: 4.178969
[INFO] Epoch: 32 , batch: 198 , training loss: 4.096649
[INFO] Epoch: 32 , batch: 199 , training loss: 4.258779
[INFO] Epoch: 32 , batch: 200 , training loss: 4.148409
[INFO] Epoch: 32 , batch: 201 , training loss: 4.071609
[INFO] Epoch: 32 , batch: 202 , training loss: 4.048470
[INFO] Epoch: 32 , batch: 203 , training loss: 4.151656
[INFO] Epoch: 32 , batch: 204 , training loss: 4.283607
[INFO] Epoch: 32 , batch: 205 , training loss: 3.868770
[INFO] Epoch: 32 , batch: 206 , training loss: 3.794321
[INFO] Epoch: 32 , batch: 207 , training loss: 3.796843
[INFO] Epoch: 32 , batch: 208 , training loss: 4.117440
[INFO] Epoch: 32 , batch: 209 , training loss: 4.068749
[INFO] Epoch: 32 , batch: 210 , training loss: 4.072515
[INFO] Epoch: 32 , batch: 211 , training loss: 4.077797
[INFO] Epoch: 32 , batch: 212 , training loss: 4.177074
[INFO] Epoch: 32 , batch: 213 , training loss: 4.123137
[INFO] Epoch: 32 , batch: 214 , training loss: 4.235683
[INFO] Epoch: 32 , batch: 215 , training loss: 4.440040
[INFO] Epoch: 32 , batch: 216 , training loss: 4.138809
[INFO] Epoch: 32 , batch: 217 , training loss: 4.085000
[INFO] Epoch: 32 , batch: 218 , training loss: 4.070787
[INFO] Epoch: 32 , batch: 219 , training loss: 4.188413
[INFO] Epoch: 32 , batch: 220 , training loss: 3.979656
[INFO] Epoch: 32 , batch: 221 , training loss: 4.015662
[INFO] Epoch: 32 , batch: 222 , training loss: 4.162388
[INFO] Epoch: 32 , batch: 223 , training loss: 4.263031
[INFO] Epoch: 32 , batch: 224 , training loss: 4.288444
[INFO] Epoch: 32 , batch: 225 , training loss: 4.202385
[INFO] Epoch: 32 , batch: 226 , training loss: 4.302581
[INFO] Epoch: 32 , batch: 227 , training loss: 4.261904
[INFO] Epoch: 32 , batch: 228 , training loss: 4.299541
[INFO] Epoch: 32 , batch: 229 , training loss: 4.163016
[INFO] Epoch: 32 , batch: 230 , training loss: 4.024671
[INFO] Epoch: 32 , batch: 231 , training loss: 3.903419
[INFO] Epoch: 32 , batch: 232 , training loss: 4.040894
[INFO] Epoch: 32 , batch: 233 , training loss: 4.043886
[INFO] Epoch: 32 , batch: 234 , training loss: 3.749954
[INFO] Epoch: 32 , batch: 235 , training loss: 3.850730
[INFO] Epoch: 32 , batch: 236 , training loss: 3.953917
[INFO] Epoch: 32 , batch: 237 , training loss: 4.165173
[INFO] Epoch: 32 , batch: 238 , training loss: 3.937410
[INFO] Epoch: 32 , batch: 239 , training loss: 3.982217
[INFO] Epoch: 32 , batch: 240 , training loss: 4.052265
[INFO] Epoch: 32 , batch: 241 , training loss: 3.854910
[INFO] Epoch: 32 , batch: 242 , training loss: 3.883120
[INFO] Epoch: 32 , batch: 243 , training loss: 4.180666
[INFO] Epoch: 32 , batch: 244 , training loss: 4.092799
[INFO] Epoch: 32 , batch: 245 , training loss: 4.066294
[INFO] Epoch: 32 , batch: 246 , training loss: 3.774784
[INFO] Epoch: 32 , batch: 247 , training loss: 3.947765
[INFO] Epoch: 32 , batch: 248 , training loss: 4.018366
[INFO] Epoch: 32 , batch: 249 , training loss: 4.040710
[INFO] Epoch: 32 , batch: 250 , training loss: 3.797100
[INFO] Epoch: 32 , batch: 251 , training loss: 4.231607
[INFO] Epoch: 32 , batch: 252 , training loss: 3.925695
[INFO] Epoch: 32 , batch: 253 , training loss: 3.885584
[INFO] Epoch: 32 , batch: 254 , training loss: 4.132285
[INFO] Epoch: 32 , batch: 255 , training loss: 4.105083
[INFO] Epoch: 32 , batch: 256 , training loss: 4.133586
[INFO] Epoch: 32 , batch: 257 , training loss: 4.241388
[INFO] Epoch: 32 , batch: 258 , training loss: 4.306206
[INFO] Epoch: 32 , batch: 259 , training loss: 4.334947
[INFO] Epoch: 32 , batch: 260 , training loss: 4.076590
[INFO] Epoch: 32 , batch: 261 , training loss: 4.217102
[INFO] Epoch: 32 , batch: 262 , training loss: 4.395408
[INFO] Epoch: 32 , batch: 263 , training loss: 4.606025
[INFO] Epoch: 32 , batch: 264 , training loss: 3.899517
[INFO] Epoch: 32 , batch: 265 , training loss: 4.048532
[INFO] Epoch: 32 , batch: 266 , training loss: 4.461029
[INFO] Epoch: 32 , batch: 267 , training loss: 4.217213
[INFO] Epoch: 32 , batch: 268 , training loss: 4.146512
[INFO] Epoch: 32 , batch: 269 , training loss: 4.115856
[INFO] Epoch: 32 , batch: 270 , training loss: 4.145681
[INFO] Epoch: 32 , batch: 271 , training loss: 4.182613
[INFO] Epoch: 32 , batch: 272 , training loss: 4.159162
[INFO] Epoch: 32 , batch: 273 , training loss: 4.184657
[INFO] Epoch: 32 , batch: 274 , training loss: 4.245206
[INFO] Epoch: 32 , batch: 275 , training loss: 4.119081
[INFO] Epoch: 32 , batch: 276 , training loss: 4.176636
[INFO] Epoch: 32 , batch: 277 , training loss: 4.334858
[INFO] Epoch: 32 , batch: 278 , training loss: 4.022066
[INFO] Epoch: 32 , batch: 279 , training loss: 4.017752
[INFO] Epoch: 32 , batch: 280 , training loss: 3.962551
[INFO] Epoch: 32 , batch: 281 , training loss: 4.101641
[INFO] Epoch: 32 , batch: 282 , training loss: 4.025669
[INFO] Epoch: 32 , batch: 283 , training loss: 4.056771
[INFO] Epoch: 32 , batch: 284 , training loss: 4.069893
[INFO] Epoch: 32 , batch: 285 , training loss: 4.016770
[INFO] Epoch: 32 , batch: 286 , training loss: 4.005504
[INFO] Epoch: 32 , batch: 287 , training loss: 3.957157
[INFO] Epoch: 32 , batch: 288 , training loss: 3.976480
[INFO] Epoch: 32 , batch: 289 , training loss: 4.006600
[INFO] Epoch: 32 , batch: 290 , training loss: 3.806882
[INFO] Epoch: 32 , batch: 291 , training loss: 3.750745
[INFO] Epoch: 32 , batch: 292 , training loss: 3.892102
[INFO] Epoch: 32 , batch: 293 , training loss: 3.802721
[INFO] Epoch: 32 , batch: 294 , training loss: 4.452063
[INFO] Epoch: 32 , batch: 295 , training loss: 4.228628
[INFO] Epoch: 32 , batch: 296 , training loss: 4.161250
[INFO] Epoch: 32 , batch: 297 , training loss: 4.130478
[INFO] Epoch: 32 , batch: 298 , training loss: 3.969839
[INFO] Epoch: 32 , batch: 299 , training loss: 3.975595
[INFO] Epoch: 32 , batch: 300 , training loss: 3.939567
[INFO] Epoch: 32 , batch: 301 , training loss: 3.902110
[INFO] Epoch: 32 , batch: 302 , training loss: 4.073233
[INFO] Epoch: 32 , batch: 303 , training loss: 4.073862
[INFO] Epoch: 32 , batch: 304 , training loss: 4.243133
[INFO] Epoch: 32 , batch: 305 , training loss: 4.034185
[INFO] Epoch: 32 , batch: 306 , training loss: 4.173315
[INFO] Epoch: 32 , batch: 307 , training loss: 4.143767
[INFO] Epoch: 32 , batch: 308 , training loss: 4.013535
[INFO] Epoch: 32 , batch: 309 , training loss: 4.020015
[INFO] Epoch: 32 , batch: 310 , training loss: 3.907606
[INFO] Epoch: 32 , batch: 311 , training loss: 3.914001
[INFO] Epoch: 32 , batch: 312 , training loss: 3.834159
[INFO] Epoch: 32 , batch: 313 , training loss: 3.947526
[INFO] Epoch: 32 , batch: 314 , training loss: 4.017340
[INFO] Epoch: 32 , batch: 315 , training loss: 4.054298
[INFO] Epoch: 32 , batch: 316 , training loss: 4.293721
[INFO] Epoch: 32 , batch: 317 , training loss: 4.708233
[INFO] Epoch: 32 , batch: 318 , training loss: 4.859137
[INFO] Epoch: 32 , batch: 319 , training loss: 4.490026
[INFO] Epoch: 32 , batch: 320 , training loss: 4.011171
[INFO] Epoch: 32 , batch: 321 , training loss: 3.835933
[INFO] Epoch: 32 , batch: 322 , training loss: 3.966694
[INFO] Epoch: 32 , batch: 323 , training loss: 3.995406
[INFO] Epoch: 32 , batch: 324 , training loss: 3.948624
[INFO] Epoch: 32 , batch: 325 , training loss: 4.116207
[INFO] Epoch: 32 , batch: 326 , training loss: 4.160119
[INFO] Epoch: 32 , batch: 327 , training loss: 4.067777
[INFO] Epoch: 32 , batch: 328 , training loss: 4.056663
[INFO] Epoch: 32 , batch: 329 , training loss: 3.981203
[INFO] Epoch: 32 , batch: 330 , training loss: 3.986541
[INFO] Epoch: 32 , batch: 331 , training loss: 4.134079
[INFO] Epoch: 32 , batch: 332 , training loss: 3.922970
[INFO] Epoch: 32 , batch: 333 , training loss: 3.958156
[INFO] Epoch: 32 , batch: 334 , training loss: 3.944111
[INFO] Epoch: 32 , batch: 335 , training loss: 4.078630
[INFO] Epoch: 32 , batch: 336 , training loss: 4.112629
[INFO] Epoch: 32 , batch: 337 , training loss: 4.137722
[INFO] Epoch: 32 , batch: 338 , training loss: 4.346887
[INFO] Epoch: 32 , batch: 339 , training loss: 4.225527
[INFO] Epoch: 32 , batch: 340 , training loss: 4.347069
[INFO] Epoch: 32 , batch: 341 , training loss: 4.104946
[INFO] Epoch: 32 , batch: 342 , training loss: 3.882621
[INFO] Epoch: 32 , batch: 343 , training loss: 3.963903
[INFO] Epoch: 32 , batch: 344 , training loss: 3.830782
[INFO] Epoch: 32 , batch: 345 , training loss: 3.964371
[INFO] Epoch: 32 , batch: 346 , training loss: 4.002457
[INFO] Epoch: 32 , batch: 347 , training loss: 3.898925
[INFO] Epoch: 32 , batch: 348 , training loss: 4.001818
[INFO] Epoch: 32 , batch: 349 , training loss: 4.180470
[INFO] Epoch: 32 , batch: 350 , training loss: 3.942261
[INFO] Epoch: 32 , batch: 351 , training loss: 4.027688
[INFO] Epoch: 32 , batch: 352 , training loss: 4.038441
[INFO] Epoch: 32 , batch: 353 , training loss: 4.011254
[INFO] Epoch: 32 , batch: 354 , training loss: 4.127031
[INFO] Epoch: 32 , batch: 355 , training loss: 4.163936
[INFO] Epoch: 32 , batch: 356 , training loss: 3.998883
[INFO] Epoch: 32 , batch: 357 , training loss: 4.082907
[INFO] Epoch: 32 , batch: 358 , training loss: 3.961017
[INFO] Epoch: 32 , batch: 359 , training loss: 3.987855
[INFO] Epoch: 32 , batch: 360 , training loss: 4.070047
[INFO] Epoch: 32 , batch: 361 , training loss: 4.038354
[INFO] Epoch: 32 , batch: 362 , training loss: 4.141052
[INFO] Epoch: 32 , batch: 363 , training loss: 4.031641
[INFO] Epoch: 32 , batch: 364 , training loss: 4.046205
[INFO] Epoch: 32 , batch: 365 , training loss: 4.009367
[INFO] Epoch: 32 , batch: 366 , training loss: 4.124208
[INFO] Epoch: 32 , batch: 367 , training loss: 4.190948
[INFO] Epoch: 32 , batch: 368 , training loss: 4.596468
[INFO] Epoch: 32 , batch: 369 , training loss: 4.268145
[INFO] Epoch: 32 , batch: 370 , training loss: 4.029994
[INFO] Epoch: 32 , batch: 371 , training loss: 4.387372
[INFO] Epoch: 32 , batch: 372 , training loss: 4.684605
[INFO] Epoch: 32 , batch: 373 , training loss: 4.783076
[INFO] Epoch: 32 , batch: 374 , training loss: 4.853165
[INFO] Epoch: 32 , batch: 375 , training loss: 4.825750
[INFO] Epoch: 32 , batch: 376 , training loss: 4.758850
[INFO] Epoch: 32 , batch: 377 , training loss: 4.546206
[INFO] Epoch: 32 , batch: 378 , training loss: 4.629294
[INFO] Epoch: 32 , batch: 379 , training loss: 4.592578
[INFO] Epoch: 32 , batch: 380 , training loss: 4.715330
[INFO] Epoch: 32 , batch: 381 , training loss: 4.456269
[INFO] Epoch: 32 , batch: 382 , training loss: 4.662929
[INFO] Epoch: 32 , batch: 383 , training loss: 4.675256
[INFO] Epoch: 32 , batch: 384 , training loss: 4.691567
[INFO] Epoch: 32 , batch: 385 , training loss: 4.402978
[INFO] Epoch: 32 , batch: 386 , training loss: 4.683172
[INFO] Epoch: 32 , batch: 387 , training loss: 4.652192
[INFO] Epoch: 32 , batch: 388 , training loss: 4.436393
[INFO] Epoch: 32 , batch: 389 , training loss: 4.285909
[INFO] Epoch: 32 , batch: 390 , training loss: 4.278694
[INFO] Epoch: 32 , batch: 391 , training loss: 4.342364
[INFO] Epoch: 32 , batch: 392 , training loss: 4.674125
[INFO] Epoch: 32 , batch: 393 , training loss: 4.568749
[INFO] Epoch: 32 , batch: 394 , training loss: 4.607216
[INFO] Epoch: 32 , batch: 395 , training loss: 4.475104
[INFO] Epoch: 32 , batch: 396 , training loss: 4.239752
[INFO] Epoch: 32 , batch: 397 , training loss: 4.431557
[INFO] Epoch: 32 , batch: 398 , training loss: 4.263079
[INFO] Epoch: 32 , batch: 399 , training loss: 4.333380
[INFO] Epoch: 32 , batch: 400 , training loss: 4.312599
[INFO] Epoch: 32 , batch: 401 , training loss: 4.721073
[INFO] Epoch: 32 , batch: 402 , training loss: 4.439433
[INFO] Epoch: 32 , batch: 403 , training loss: 4.241543
[INFO] Epoch: 32 , batch: 404 , training loss: 4.437213
[INFO] Epoch: 32 , batch: 405 , training loss: 4.505058
[INFO] Epoch: 32 , batch: 406 , training loss: 4.391829
[INFO] Epoch: 32 , batch: 407 , training loss: 4.456161
[INFO] Epoch: 32 , batch: 408 , training loss: 4.409509
[INFO] Epoch: 32 , batch: 409 , training loss: 4.403765
[INFO] Epoch: 32 , batch: 410 , training loss: 4.445830
[INFO] Epoch: 32 , batch: 411 , training loss: 4.645227
[INFO] Epoch: 32 , batch: 412 , training loss: 4.476852
[INFO] Epoch: 32 , batch: 413 , training loss: 4.356532
[INFO] Epoch: 32 , batch: 414 , training loss: 4.383500
[INFO] Epoch: 32 , batch: 415 , training loss: 4.406213
[INFO] Epoch: 32 , batch: 416 , training loss: 4.497572
[INFO] Epoch: 32 , batch: 417 , training loss: 4.409038
[INFO] Epoch: 32 , batch: 418 , training loss: 4.437689
[INFO] Epoch: 32 , batch: 419 , training loss: 4.393272
[INFO] Epoch: 32 , batch: 420 , training loss: 4.359019
[INFO] Epoch: 32 , batch: 421 , training loss: 4.357005
[INFO] Epoch: 32 , batch: 422 , training loss: 4.217061
[INFO] Epoch: 32 , batch: 423 , training loss: 4.431448
[INFO] Epoch: 32 , batch: 424 , training loss: 4.605184
[INFO] Epoch: 32 , batch: 425 , training loss: 4.465781
[INFO] Epoch: 32 , batch: 426 , training loss: 4.206788
[INFO] Epoch: 32 , batch: 427 , training loss: 4.444865
[INFO] Epoch: 32 , batch: 428 , training loss: 4.300418
[INFO] Epoch: 32 , batch: 429 , training loss: 4.195198
[INFO] Epoch: 32 , batch: 430 , training loss: 4.454973
[INFO] Epoch: 32 , batch: 431 , training loss: 4.052640
[INFO] Epoch: 32 , batch: 432 , training loss: 4.098036
[INFO] Epoch: 32 , batch: 433 , training loss: 4.146390
[INFO] Epoch: 32 , batch: 434 , training loss: 4.015524
[INFO] Epoch: 32 , batch: 435 , training loss: 4.385190
[INFO] Epoch: 32 , batch: 436 , training loss: 4.452974
[INFO] Epoch: 32 , batch: 437 , training loss: 4.203784
[INFO] Epoch: 32 , batch: 438 , training loss: 4.059415
[INFO] Epoch: 32 , batch: 439 , training loss: 4.275760
[INFO] Epoch: 32 , batch: 440 , training loss: 4.407766
[INFO] Epoch: 32 , batch: 441 , training loss: 4.510571
[INFO] Epoch: 32 , batch: 442 , training loss: 4.273072
[INFO] Epoch: 32 , batch: 443 , training loss: 4.462404
[INFO] Epoch: 32 , batch: 444 , training loss: 4.104130
[INFO] Epoch: 32 , batch: 445 , training loss: 3.975127
[INFO] Epoch: 32 , batch: 446 , training loss: 3.948637
[INFO] Epoch: 32 , batch: 447 , training loss: 4.123076
[INFO] Epoch: 32 , batch: 448 , training loss: 4.222133
[INFO] Epoch: 32 , batch: 449 , training loss: 4.623086
[INFO] Epoch: 32 , batch: 450 , training loss: 4.664901
[INFO] Epoch: 32 , batch: 451 , training loss: 4.588901
[INFO] Epoch: 32 , batch: 452 , training loss: 4.384777
[INFO] Epoch: 32 , batch: 453 , training loss: 4.142516
[INFO] Epoch: 32 , batch: 454 , training loss: 4.286482
[INFO] Epoch: 32 , batch: 455 , training loss: 4.351180
[INFO] Epoch: 32 , batch: 456 , training loss: 4.346016
[INFO] Epoch: 32 , batch: 457 , training loss: 4.437295
[INFO] Epoch: 32 , batch: 458 , training loss: 4.167573
[INFO] Epoch: 32 , batch: 459 , training loss: 4.154056
[INFO] Epoch: 32 , batch: 460 , training loss: 4.241568
[INFO] Epoch: 32 , batch: 461 , training loss: 4.220195
[INFO] Epoch: 32 , batch: 462 , training loss: 4.296704
[INFO] Epoch: 32 , batch: 463 , training loss: 4.207792
[INFO] Epoch: 32 , batch: 464 , training loss: 4.394927
[INFO] Epoch: 32 , batch: 465 , training loss: 4.322286
[INFO] Epoch: 32 , batch: 466 , training loss: 4.412314
[INFO] Epoch: 32 , batch: 467 , training loss: 4.377840
[INFO] Epoch: 32 , batch: 468 , training loss: 4.364859
[INFO] Epoch: 32 , batch: 469 , training loss: 4.374217
[INFO] Epoch: 32 , batch: 470 , training loss: 4.195082
[INFO] Epoch: 32 , batch: 471 , training loss: 4.276605
[INFO] Epoch: 32 , batch: 472 , training loss: 4.330764
[INFO] Epoch: 32 , batch: 473 , training loss: 4.261511
[INFO] Epoch: 32 , batch: 474 , training loss: 4.056082
[INFO] Epoch: 32 , batch: 475 , training loss: 3.942125
[INFO] Epoch: 32 , batch: 476 , training loss: 4.300224
[INFO] Epoch: 32 , batch: 477 , training loss: 4.450788
[INFO] Epoch: 32 , batch: 478 , training loss: 4.469687
[INFO] Epoch: 32 , batch: 479 , training loss: 4.420966
[INFO] Epoch: 32 , batch: 480 , training loss: 4.510902
[INFO] Epoch: 32 , batch: 481 , training loss: 4.414703
[INFO] Epoch: 32 , batch: 482 , training loss: 4.526634
[INFO] Epoch: 32 , batch: 483 , training loss: 4.369541
[INFO] Epoch: 32 , batch: 484 , training loss: 4.170786
[INFO] Epoch: 32 , batch: 485 , training loss: 4.265189
[INFO] Epoch: 32 , batch: 486 , training loss: 4.166909
[INFO] Epoch: 32 , batch: 487 , training loss: 4.149886
[INFO] Epoch: 32 , batch: 488 , training loss: 4.342358
[INFO] Epoch: 32 , batch: 489 , training loss: 4.227044
[INFO] Epoch: 32 , batch: 490 , training loss: 4.266871
[INFO] Epoch: 32 , batch: 491 , training loss: 4.225431
[INFO] Epoch: 32 , batch: 492 , training loss: 4.178515
[INFO] Epoch: 32 , batch: 493 , training loss: 4.363875
[INFO] Epoch: 32 , batch: 494 , training loss: 4.289408
[INFO] Epoch: 32 , batch: 495 , training loss: 4.403528
[INFO] Epoch: 32 , batch: 496 , training loss: 4.297721
[INFO] Epoch: 32 , batch: 497 , training loss: 4.325183
[INFO] Epoch: 32 , batch: 498 , training loss: 4.339272
[INFO] Epoch: 32 , batch: 499 , training loss: 4.386666
[INFO] Epoch: 32 , batch: 500 , training loss: 4.553415
[INFO] Epoch: 32 , batch: 501 , training loss: 4.847067
[INFO] Epoch: 32 , batch: 502 , training loss: 4.920516
[INFO] Epoch: 32 , batch: 503 , training loss: 4.602448
[INFO] Epoch: 32 , batch: 504 , training loss: 4.736739
[INFO] Epoch: 32 , batch: 505 , training loss: 4.696270
[INFO] Epoch: 32 , batch: 506 , training loss: 4.657726
[INFO] Epoch: 32 , batch: 507 , training loss: 4.717671
[INFO] Epoch: 32 , batch: 508 , training loss: 4.681218
[INFO] Epoch: 32 , batch: 509 , training loss: 4.458991
[INFO] Epoch: 32 , batch: 510 , training loss: 4.549062
[INFO] Epoch: 32 , batch: 511 , training loss: 4.438438
[INFO] Epoch: 32 , batch: 512 , training loss: 4.538274
[INFO] Epoch: 32 , batch: 513 , training loss: 4.772294
[INFO] Epoch: 32 , batch: 514 , training loss: 4.441025
[INFO] Epoch: 32 , batch: 515 , training loss: 4.678920
[INFO] Epoch: 32 , batch: 516 , training loss: 4.506896
[INFO] Epoch: 32 , batch: 517 , training loss: 4.451907
[INFO] Epoch: 32 , batch: 518 , training loss: 4.415479
[INFO] Epoch: 32 , batch: 519 , training loss: 4.272675
[INFO] Epoch: 32 , batch: 520 , training loss: 4.496505
[INFO] Epoch: 32 , batch: 521 , training loss: 4.472355
[INFO] Epoch: 32 , batch: 522 , training loss: 4.540886
[INFO] Epoch: 32 , batch: 523 , training loss: 4.437069
[INFO] Epoch: 32 , batch: 524 , training loss: 4.735229
[INFO] Epoch: 32 , batch: 525 , training loss: 4.661012
[INFO] Epoch: 32 , batch: 526 , training loss: 4.417357
[INFO] Epoch: 32 , batch: 527 , training loss: 4.470653
[INFO] Epoch: 32 , batch: 528 , training loss: 4.465547
[INFO] Epoch: 32 , batch: 529 , training loss: 4.472491
[INFO] Epoch: 32 , batch: 530 , training loss: 4.298750
[INFO] Epoch: 32 , batch: 531 , training loss: 4.437344
[INFO] Epoch: 32 , batch: 532 , training loss: 4.336479
[INFO] Epoch: 32 , batch: 533 , training loss: 4.484242
[INFO] Epoch: 32 , batch: 534 , training loss: 4.462858
[INFO] Epoch: 32 , batch: 535 , training loss: 4.485386
[INFO] Epoch: 32 , batch: 536 , training loss: 4.305874
[INFO] Epoch: 32 , batch: 537 , training loss: 4.321083
[INFO] Epoch: 32 , batch: 538 , training loss: 4.391823
[INFO] Epoch: 32 , batch: 539 , training loss: 4.504086
[INFO] Epoch: 32 , batch: 540 , training loss: 4.979853
[INFO] Epoch: 32 , batch: 541 , training loss: 4.875197
[INFO] Epoch: 32 , batch: 542 , training loss: 4.772313
[INFO] Epoch: 33 , batch: 0 , training loss: 3.422563
[INFO] Epoch: 33 , batch: 1 , training loss: 3.455307
[INFO] Epoch: 33 , batch: 2 , training loss: 3.639312
[INFO] Epoch: 33 , batch: 3 , training loss: 3.442025
[INFO] Epoch: 33 , batch: 4 , training loss: 3.858222
[INFO] Epoch: 33 , batch: 5 , training loss: 3.454329
[INFO] Epoch: 33 , batch: 6 , training loss: 3.825216
[INFO] Epoch: 33 , batch: 7 , training loss: 3.797760
[INFO] Epoch: 33 , batch: 8 , training loss: 3.509188
[INFO] Epoch: 33 , batch: 9 , training loss: 3.746397
[INFO] Epoch: 33 , batch: 10 , training loss: 3.741917
[INFO] Epoch: 33 , batch: 11 , training loss: 3.681915
[INFO] Epoch: 33 , batch: 12 , training loss: 3.573072
[INFO] Epoch: 33 , batch: 13 , training loss: 3.623418
[INFO] Epoch: 33 , batch: 14 , training loss: 3.485699
[INFO] Epoch: 33 , batch: 15 , training loss: 3.697423
[INFO] Epoch: 33 , batch: 16 , training loss: 3.553240
[INFO] Epoch: 33 , batch: 17 , training loss: 3.743917
[INFO] Epoch: 33 , batch: 18 , training loss: 3.677502
[INFO] Epoch: 33 , batch: 19 , training loss: 3.415248
[INFO] Epoch: 33 , batch: 20 , training loss: 3.383588
[INFO] Epoch: 33 , batch: 21 , training loss: 3.551370
[INFO] Epoch: 33 , batch: 22 , training loss: 3.384345
[INFO] Epoch: 33 , batch: 23 , training loss: 3.621744
[INFO] Epoch: 33 , batch: 24 , training loss: 3.408938
[INFO] Epoch: 33 , batch: 25 , training loss: 3.610284
[INFO] Epoch: 33 , batch: 26 , training loss: 3.483863
[INFO] Epoch: 33 , batch: 27 , training loss: 3.429562
[INFO] Epoch: 33 , batch: 28 , training loss: 3.589186
[INFO] Epoch: 33 , batch: 29 , training loss: 3.399245
[INFO] Epoch: 33 , batch: 30 , training loss: 3.499256
[INFO] Epoch: 33 , batch: 31 , training loss: 3.500087
[INFO] Epoch: 33 , batch: 32 , training loss: 3.509449
[INFO] Epoch: 33 , batch: 33 , training loss: 3.567594
[INFO] Epoch: 33 , batch: 34 , training loss: 3.492079
[INFO] Epoch: 33 , batch: 35 , training loss: 3.470726
[INFO] Epoch: 33 , batch: 36 , training loss: 3.540882
[INFO] Epoch: 33 , batch: 37 , training loss: 3.452347
[INFO] Epoch: 33 , batch: 38 , training loss: 3.492808
[INFO] Epoch: 33 , batch: 39 , training loss: 3.330401
[INFO] Epoch: 33 , batch: 40 , training loss: 3.569538
[INFO] Epoch: 33 , batch: 41 , training loss: 3.509901
[INFO] Epoch: 33 , batch: 42 , training loss: 3.929453
[INFO] Epoch: 33 , batch: 43 , training loss: 3.568898
[INFO] Epoch: 33 , batch: 44 , training loss: 3.981179
[INFO] Epoch: 33 , batch: 45 , training loss: 3.885457
[INFO] Epoch: 33 , batch: 46 , training loss: 3.785727
[INFO] Epoch: 33 , batch: 47 , training loss: 3.548490
[INFO] Epoch: 33 , batch: 48 , training loss: 3.574733
[INFO] Epoch: 33 , batch: 49 , training loss: 3.758037
[INFO] Epoch: 33 , batch: 50 , training loss: 3.574311
[INFO] Epoch: 33 , batch: 51 , training loss: 3.782925
[INFO] Epoch: 33 , batch: 52 , training loss: 3.647765
[INFO] Epoch: 33 , batch: 53 , training loss: 3.767739
[INFO] Epoch: 33 , batch: 54 , training loss: 3.754316
[INFO] Epoch: 33 , batch: 55 , training loss: 3.859661
[INFO] Epoch: 33 , batch: 56 , training loss: 3.737408
[INFO] Epoch: 33 , batch: 57 , training loss: 3.597167
[INFO] Epoch: 33 , batch: 58 , training loss: 3.699987
[INFO] Epoch: 33 , batch: 59 , training loss: 3.768524
[INFO] Epoch: 33 , batch: 60 , training loss: 3.695596
[INFO] Epoch: 33 , batch: 61 , training loss: 3.773537
[INFO] Epoch: 33 , batch: 62 , training loss: 3.683649
[INFO] Epoch: 33 , batch: 63 , training loss: 3.855636
[INFO] Epoch: 33 , batch: 64 , training loss: 4.017691
[INFO] Epoch: 33 , batch: 65 , training loss: 3.719057
[INFO] Epoch: 33 , batch: 66 , training loss: 3.586526
[INFO] Epoch: 33 , batch: 67 , training loss: 3.669655
[INFO] Epoch: 33 , batch: 68 , training loss: 3.814146
[INFO] Epoch: 33 , batch: 69 , training loss: 3.712906
[INFO] Epoch: 33 , batch: 70 , training loss: 3.918600
[INFO] Epoch: 33 , batch: 71 , training loss: 3.821532
[INFO] Epoch: 33 , batch: 72 , training loss: 3.839396
[INFO] Epoch: 33 , batch: 73 , training loss: 3.807349
[INFO] Epoch: 33 , batch: 74 , training loss: 3.900565
[INFO] Epoch: 33 , batch: 75 , training loss: 3.809870
[INFO] Epoch: 33 , batch: 76 , training loss: 3.852034
[INFO] Epoch: 33 , batch: 77 , training loss: 3.834559
[INFO] Epoch: 33 , batch: 78 , training loss: 3.942883
[INFO] Epoch: 33 , batch: 79 , training loss: 3.768484
[INFO] Epoch: 33 , batch: 80 , training loss: 3.971238
[INFO] Epoch: 33 , batch: 81 , training loss: 3.921387
[INFO] Epoch: 33 , batch: 82 , training loss: 3.852851
[INFO] Epoch: 33 , batch: 83 , training loss: 3.990170
[INFO] Epoch: 33 , batch: 84 , training loss: 3.913564
[INFO] Epoch: 33 , batch: 85 , training loss: 3.995352
[INFO] Epoch: 33 , batch: 86 , training loss: 3.922810
[INFO] Epoch: 33 , batch: 87 , training loss: 3.910969
[INFO] Epoch: 33 , batch: 88 , training loss: 4.013690
[INFO] Epoch: 33 , batch: 89 , training loss: 3.834600
[INFO] Epoch: 33 , batch: 90 , training loss: 3.908992
[INFO] Epoch: 33 , batch: 91 , training loss: 3.851877
[INFO] Epoch: 33 , batch: 92 , training loss: 3.854066
[INFO] Epoch: 33 , batch: 93 , training loss: 3.962102
[INFO] Epoch: 33 , batch: 94 , training loss: 4.133736
[INFO] Epoch: 33 , batch: 95 , training loss: 3.869182
[INFO] Epoch: 33 , batch: 96 , training loss: 3.887812
[INFO] Epoch: 33 , batch: 97 , training loss: 3.781893
[INFO] Epoch: 33 , batch: 98 , training loss: 3.760676
[INFO] Epoch: 33 , batch: 99 , training loss: 3.843924
[INFO] Epoch: 33 , batch: 100 , training loss: 3.796240
[INFO] Epoch: 33 , batch: 101 , training loss: 3.832258
[INFO] Epoch: 33 , batch: 102 , training loss: 3.912431
[INFO] Epoch: 33 , batch: 103 , training loss: 3.719687
[INFO] Epoch: 33 , batch: 104 , training loss: 3.672610
[INFO] Epoch: 33 , batch: 105 , training loss: 3.980555
[INFO] Epoch: 33 , batch: 106 , training loss: 3.912086
[INFO] Epoch: 33 , batch: 107 , training loss: 3.797869
[INFO] Epoch: 33 , batch: 108 , training loss: 3.733686
[INFO] Epoch: 33 , batch: 109 , training loss: 3.681002
[INFO] Epoch: 33 , batch: 110 , training loss: 3.830221
[INFO] Epoch: 33 , batch: 111 , training loss: 3.901073
[INFO] Epoch: 33 , batch: 112 , training loss: 3.852241
[INFO] Epoch: 33 , batch: 113 , training loss: 3.872851
[INFO] Epoch: 33 , batch: 114 , training loss: 3.816213
[INFO] Epoch: 33 , batch: 115 , training loss: 3.841491
[INFO] Epoch: 33 , batch: 116 , training loss: 3.750086
[INFO] Epoch: 33 , batch: 117 , training loss: 4.003268
[INFO] Epoch: 33 , batch: 118 , training loss: 3.936886
[INFO] Epoch: 33 , batch: 119 , training loss: 4.059240
[INFO] Epoch: 33 , batch: 120 , training loss: 4.068371
[INFO] Epoch: 33 , batch: 121 , training loss: 3.934035
[INFO] Epoch: 33 , batch: 122 , training loss: 3.811651
[INFO] Epoch: 33 , batch: 123 , training loss: 3.821392
[INFO] Epoch: 33 , batch: 124 , training loss: 3.949528
[INFO] Epoch: 33 , batch: 125 , training loss: 3.776391
[INFO] Epoch: 33 , batch: 126 , training loss: 3.767511
[INFO] Epoch: 33 , batch: 127 , training loss: 3.768957
[INFO] Epoch: 33 , batch: 128 , training loss: 3.873962
[INFO] Epoch: 33 , batch: 129 , training loss: 3.891877
[INFO] Epoch: 33 , batch: 130 , training loss: 3.865984
[INFO] Epoch: 33 , batch: 131 , training loss: 3.873056
[INFO] Epoch: 33 , batch: 132 , training loss: 3.827342
[INFO] Epoch: 33 , batch: 133 , training loss: 3.829513
[INFO] Epoch: 33 , batch: 134 , training loss: 3.646305
[INFO] Epoch: 33 , batch: 135 , training loss: 3.689901
[INFO] Epoch: 33 , batch: 136 , training loss: 3.970996
[INFO] Epoch: 33 , batch: 137 , training loss: 3.869421
[INFO] Epoch: 33 , batch: 138 , training loss: 3.924620
[INFO] Epoch: 33 , batch: 139 , training loss: 4.511190
[INFO] Epoch: 33 , batch: 140 , training loss: 4.209307
[INFO] Epoch: 33 , batch: 141 , training loss: 4.014294
[INFO] Epoch: 33 , batch: 142 , training loss: 3.811023
[INFO] Epoch: 33 , batch: 143 , training loss: 3.909912
[INFO] Epoch: 33 , batch: 144 , training loss: 3.730584
[INFO] Epoch: 33 , batch: 145 , training loss: 3.777666
[INFO] Epoch: 33 , batch: 146 , training loss: 4.011997
[INFO] Epoch: 33 , batch: 147 , training loss: 3.672114
[INFO] Epoch: 33 , batch: 148 , training loss: 3.690046
[INFO] Epoch: 33 , batch: 149 , training loss: 3.763895
[INFO] Epoch: 33 , batch: 150 , training loss: 3.979844
[INFO] Epoch: 33 , batch: 151 , training loss: 3.864677
[INFO] Epoch: 33 , batch: 152 , training loss: 3.912976
[INFO] Epoch: 33 , batch: 153 , training loss: 3.876199
[INFO] Epoch: 33 , batch: 154 , training loss: 3.988917
[INFO] Epoch: 33 , batch: 155 , training loss: 4.223508
[INFO] Epoch: 33 , batch: 156 , training loss: 3.911296
[INFO] Epoch: 33 , batch: 157 , training loss: 3.936098
[INFO] Epoch: 33 , batch: 158 , training loss: 4.012899
[INFO] Epoch: 33 , batch: 159 , training loss: 3.891280
[INFO] Epoch: 33 , batch: 160 , training loss: 4.205627
[INFO] Epoch: 33 , batch: 161 , training loss: 4.332669
[INFO] Epoch: 33 , batch: 162 , training loss: 4.284860
[INFO] Epoch: 33 , batch: 163 , training loss: 4.414789
[INFO] Epoch: 33 , batch: 164 , training loss: 4.387362
[INFO] Epoch: 33 , batch: 165 , training loss: 4.322211
[INFO] Epoch: 33 , batch: 166 , training loss: 4.170158
[INFO] Epoch: 33 , batch: 167 , training loss: 4.235252
[INFO] Epoch: 33 , batch: 168 , training loss: 3.888095
[INFO] Epoch: 33 , batch: 169 , training loss: 3.868483
[INFO] Epoch: 33 , batch: 170 , training loss: 4.124691
[INFO] Epoch: 33 , batch: 171 , training loss: 3.502516
[INFO] Epoch: 33 , batch: 172 , training loss: 3.735720
[INFO] Epoch: 33 , batch: 173 , training loss: 4.059599
[INFO] Epoch: 33 , batch: 174 , training loss: 4.474198
[INFO] Epoch: 33 , batch: 175 , training loss: 4.826873
[INFO] Epoch: 33 , batch: 176 , training loss: 4.454062
[INFO] Epoch: 33 , batch: 177 , training loss: 4.127537
[INFO] Epoch: 33 , batch: 178 , training loss: 4.108197
[INFO] Epoch: 33 , batch: 179 , training loss: 4.145421
[INFO] Epoch: 33 , batch: 180 , training loss: 4.115572
[INFO] Epoch: 33 , batch: 181 , training loss: 4.396933
[INFO] Epoch: 33 , batch: 182 , training loss: 4.343178
[INFO] Epoch: 33 , batch: 183 , training loss: 4.318931
[INFO] Epoch: 33 , batch: 184 , training loss: 4.199680
[INFO] Epoch: 33 , batch: 185 , training loss: 4.155520
[INFO] Epoch: 33 , batch: 186 , training loss: 4.327991
[INFO] Epoch: 33 , batch: 187 , training loss: 4.425738
[INFO] Epoch: 33 , batch: 188 , training loss: 4.391789
[INFO] Epoch: 33 , batch: 189 , training loss: 4.304625
[INFO] Epoch: 33 , batch: 190 , training loss: 4.312755
[INFO] Epoch: 33 , batch: 191 , training loss: 4.436271
[INFO] Epoch: 33 , batch: 192 , training loss: 4.256447
[INFO] Epoch: 33 , batch: 193 , training loss: 4.385643
[INFO] Epoch: 33 , batch: 194 , training loss: 4.321375
[INFO] Epoch: 33 , batch: 195 , training loss: 4.225200
[INFO] Epoch: 33 , batch: 196 , training loss: 4.102114
[INFO] Epoch: 33 , batch: 197 , training loss: 4.200518
[INFO] Epoch: 33 , batch: 198 , training loss: 4.119206
[INFO] Epoch: 33 , batch: 199 , training loss: 4.253561
[INFO] Epoch: 33 , batch: 200 , training loss: 4.152204
[INFO] Epoch: 33 , batch: 201 , training loss: 4.083158
[INFO] Epoch: 33 , batch: 202 , training loss: 4.054843
[INFO] Epoch: 33 , batch: 203 , training loss: 4.147285
[INFO] Epoch: 33 , batch: 204 , training loss: 4.301962
[INFO] Epoch: 33 , batch: 205 , training loss: 3.869648
[INFO] Epoch: 33 , batch: 206 , training loss: 3.799076
[INFO] Epoch: 33 , batch: 207 , training loss: 3.807155
[INFO] Epoch: 33 , batch: 208 , training loss: 4.134459
[INFO] Epoch: 33 , batch: 209 , training loss: 4.085230
[INFO] Epoch: 33 , batch: 210 , training loss: 4.085150
[INFO] Epoch: 33 , batch: 211 , training loss: 4.087068
[INFO] Epoch: 33 , batch: 212 , training loss: 4.201904
[INFO] Epoch: 33 , batch: 213 , training loss: 4.134251
[INFO] Epoch: 33 , batch: 214 , training loss: 4.228909
[INFO] Epoch: 33 , batch: 215 , training loss: 4.446959
[INFO] Epoch: 33 , batch: 216 , training loss: 4.138466
[INFO] Epoch: 33 , batch: 217 , training loss: 4.096395
[INFO] Epoch: 33 , batch: 218 , training loss: 4.075611
[INFO] Epoch: 33 , batch: 219 , training loss: 4.195442
[INFO] Epoch: 33 , batch: 220 , training loss: 4.003500
[INFO] Epoch: 33 , batch: 221 , training loss: 4.027481
[INFO] Epoch: 33 , batch: 222 , training loss: 4.158647
[INFO] Epoch: 33 , batch: 223 , training loss: 4.259946
[INFO] Epoch: 33 , batch: 224 , training loss: 4.293960
[INFO] Epoch: 33 , batch: 225 , training loss: 4.205532
[INFO] Epoch: 33 , batch: 226 , training loss: 4.313226
[INFO] Epoch: 33 , batch: 227 , training loss: 4.266687
[INFO] Epoch: 33 , batch: 228 , training loss: 4.314267
[INFO] Epoch: 33 , batch: 229 , training loss: 4.172717
[INFO] Epoch: 33 , batch: 230 , training loss: 4.046477
[INFO] Epoch: 33 , batch: 231 , training loss: 3.901843
[INFO] Epoch: 33 , batch: 232 , training loss: 4.050856
[INFO] Epoch: 33 , batch: 233 , training loss: 4.072519
[INFO] Epoch: 33 , batch: 234 , training loss: 3.763121
[INFO] Epoch: 33 , batch: 235 , training loss: 3.855113
[INFO] Epoch: 33 , batch: 236 , training loss: 3.965224
[INFO] Epoch: 33 , batch: 237 , training loss: 4.176777
[INFO] Epoch: 33 , batch: 238 , training loss: 3.953099
[INFO] Epoch: 33 , batch: 239 , training loss: 3.991204
[INFO] Epoch: 33 , batch: 240 , training loss: 4.061773
[INFO] Epoch: 33 , batch: 241 , training loss: 3.871841
[INFO] Epoch: 33 , batch: 242 , training loss: 3.875358
[INFO] Epoch: 33 , batch: 243 , training loss: 4.182108
[INFO] Epoch: 33 , batch: 244 , training loss: 4.109195
[INFO] Epoch: 33 , batch: 245 , training loss: 4.082935
[INFO] Epoch: 33 , batch: 246 , training loss: 3.793900
[INFO] Epoch: 33 , batch: 247 , training loss: 3.951101
[INFO] Epoch: 33 , batch: 248 , training loss: 4.036760
[INFO] Epoch: 33 , batch: 249 , training loss: 4.063627
[INFO] Epoch: 33 , batch: 250 , training loss: 3.809499
[INFO] Epoch: 33 , batch: 251 , training loss: 4.248314
[INFO] Epoch: 33 , batch: 252 , training loss: 3.947067
[INFO] Epoch: 33 , batch: 253 , training loss: 3.890611
[INFO] Epoch: 33 , batch: 254 , training loss: 4.164327
[INFO] Epoch: 33 , batch: 255 , training loss: 4.112631
[INFO] Epoch: 33 , batch: 256 , training loss: 4.120144
[INFO] Epoch: 33 , batch: 257 , training loss: 4.278650
[INFO] Epoch: 33 , batch: 258 , training loss: 4.321790
[INFO] Epoch: 33 , batch: 259 , training loss: 4.358300
[INFO] Epoch: 33 , batch: 260 , training loss: 4.088504
[INFO] Epoch: 33 , batch: 261 , training loss: 4.251674
[INFO] Epoch: 33 , batch: 262 , training loss: 4.389636
[INFO] Epoch: 33 , batch: 263 , training loss: 4.597239
[INFO] Epoch: 33 , batch: 264 , training loss: 3.897905
[INFO] Epoch: 33 , batch: 265 , training loss: 4.056857
[INFO] Epoch: 33 , batch: 266 , training loss: 4.461848
[INFO] Epoch: 33 , batch: 267 , training loss: 4.233801
[INFO] Epoch: 33 , batch: 268 , training loss: 4.143993
[INFO] Epoch: 33 , batch: 269 , training loss: 4.128982
[INFO] Epoch: 33 , batch: 270 , training loss: 4.159175
[INFO] Epoch: 33 , batch: 271 , training loss: 4.181706
[INFO] Epoch: 33 , batch: 272 , training loss: 4.163879
[INFO] Epoch: 33 , batch: 273 , training loss: 4.185609
[INFO] Epoch: 33 , batch: 274 , training loss: 4.256316
[INFO] Epoch: 33 , batch: 275 , training loss: 4.125022
[INFO] Epoch: 33 , batch: 276 , training loss: 4.181077
[INFO] Epoch: 33 , batch: 277 , training loss: 4.343849
[INFO] Epoch: 33 , batch: 278 , training loss: 4.014735
[INFO] Epoch: 33 , batch: 279 , training loss: 4.029673
[INFO] Epoch: 33 , batch: 280 , training loss: 3.971331
[INFO] Epoch: 33 , batch: 281 , training loss: 4.117531
[INFO] Epoch: 33 , batch: 282 , training loss: 4.027670
[INFO] Epoch: 33 , batch: 283 , training loss: 4.064899
[INFO] Epoch: 33 , batch: 284 , training loss: 4.088474
[INFO] Epoch: 33 , batch: 285 , training loss: 4.026698
[INFO] Epoch: 33 , batch: 286 , training loss: 4.005648
[INFO] Epoch: 33 , batch: 287 , training loss: 3.967527
[INFO] Epoch: 33 , batch: 288 , training loss: 3.988982
[INFO] Epoch: 33 , batch: 289 , training loss: 3.994757
[INFO] Epoch: 33 , batch: 290 , training loss: 3.799128
[INFO] Epoch: 33 , batch: 291 , training loss: 3.765601
[INFO] Epoch: 33 , batch: 292 , training loss: 3.897197
[INFO] Epoch: 33 , batch: 293 , training loss: 3.801494
[INFO] Epoch: 33 , batch: 294 , training loss: 4.463052
[INFO] Epoch: 33 , batch: 295 , training loss: 4.235128
[INFO] Epoch: 33 , batch: 296 , training loss: 4.162750
[INFO] Epoch: 33 , batch: 297 , training loss: 4.123879
[INFO] Epoch: 33 , batch: 298 , training loss: 3.958369
[INFO] Epoch: 33 , batch: 299 , training loss: 3.984092
[INFO] Epoch: 33 , batch: 300 , training loss: 3.951445
[INFO] Epoch: 33 , batch: 301 , training loss: 3.919647
[INFO] Epoch: 33 , batch: 302 , training loss: 4.050603
[INFO] Epoch: 33 , batch: 303 , training loss: 4.063210
[INFO] Epoch: 33 , batch: 304 , training loss: 4.236929
[INFO] Epoch: 33 , batch: 305 , training loss: 4.039479
[INFO] Epoch: 33 , batch: 306 , training loss: 4.176773
[INFO] Epoch: 33 , batch: 307 , training loss: 4.151068
[INFO] Epoch: 33 , batch: 308 , training loss: 4.008439
[INFO] Epoch: 33 , batch: 309 , training loss: 3.995304
[INFO] Epoch: 33 , batch: 310 , training loss: 3.920179
[INFO] Epoch: 33 , batch: 311 , training loss: 3.897429
[INFO] Epoch: 33 , batch: 312 , training loss: 3.843780
[INFO] Epoch: 33 , batch: 313 , training loss: 3.947827
[INFO] Epoch: 33 , batch: 314 , training loss: 4.007417
[INFO] Epoch: 33 , batch: 315 , training loss: 4.044319
[INFO] Epoch: 33 , batch: 316 , training loss: 4.286257
[INFO] Epoch: 33 , batch: 317 , training loss: 4.712389
[INFO] Epoch: 33 , batch: 318 , training loss: 4.862647
[INFO] Epoch: 33 , batch: 319 , training loss: 4.479258
[INFO] Epoch: 33 , batch: 320 , training loss: 4.007651
[INFO] Epoch: 33 , batch: 321 , training loss: 3.849065
[INFO] Epoch: 33 , batch: 322 , training loss: 3.979837
[INFO] Epoch: 33 , batch: 323 , training loss: 3.993742
[INFO] Epoch: 33 , batch: 324 , training loss: 3.943196
[INFO] Epoch: 33 , batch: 325 , training loss: 4.114029
[INFO] Epoch: 33 , batch: 326 , training loss: 4.153387
[INFO] Epoch: 33 , batch: 327 , training loss: 4.056454
[INFO] Epoch: 33 , batch: 328 , training loss: 4.055220
[INFO] Epoch: 33 , batch: 329 , training loss: 3.987116
[INFO] Epoch: 33 , batch: 330 , training loss: 3.992903
[INFO] Epoch: 33 , batch: 331 , training loss: 4.137412
[INFO] Epoch: 33 , batch: 332 , training loss: 3.942657
[INFO] Epoch: 33 , batch: 333 , training loss: 3.959618
[INFO] Epoch: 33 , batch: 334 , training loss: 3.952236
[INFO] Epoch: 33 , batch: 335 , training loss: 4.088993
[INFO] Epoch: 33 , batch: 336 , training loss: 4.120296
[INFO] Epoch: 33 , batch: 337 , training loss: 4.135578
[INFO] Epoch: 33 , batch: 338 , training loss: 4.340408
[INFO] Epoch: 33 , batch: 339 , training loss: 4.211236
[INFO] Epoch: 33 , batch: 340 , training loss: 4.357162
[INFO] Epoch: 33 , batch: 341 , training loss: 4.114297
[INFO] Epoch: 33 , batch: 342 , training loss: 3.902011
[INFO] Epoch: 33 , batch: 343 , training loss: 3.978197
[INFO] Epoch: 33 , batch: 344 , training loss: 3.824466
[INFO] Epoch: 33 , batch: 345 , training loss: 3.954777
[INFO] Epoch: 33 , batch: 346 , training loss: 4.007031
[INFO] Epoch: 33 , batch: 347 , training loss: 3.896675
[INFO] Epoch: 33 , batch: 348 , training loss: 4.022362
[INFO] Epoch: 33 , batch: 349 , training loss: 4.180258
[INFO] Epoch: 33 , batch: 350 , training loss: 3.959299
[INFO] Epoch: 33 , batch: 351 , training loss: 4.025832
[INFO] Epoch: 33 , batch: 352 , training loss: 4.034013
[INFO] Epoch: 33 , batch: 353 , training loss: 4.026273
[INFO] Epoch: 33 , batch: 354 , training loss: 4.125422
[INFO] Epoch: 33 , batch: 355 , training loss: 4.161717
[INFO] Epoch: 33 , batch: 356 , training loss: 4.000154
[INFO] Epoch: 33 , batch: 357 , training loss: 4.078826
[INFO] Epoch: 33 , batch: 358 , training loss: 3.965246
[INFO] Epoch: 33 , batch: 359 , training loss: 3.990031
[INFO] Epoch: 33 , batch: 360 , training loss: 4.064371
[INFO] Epoch: 33 , batch: 361 , training loss: 4.048073
[INFO] Epoch: 33 , batch: 362 , training loss: 4.145968
[INFO] Epoch: 33 , batch: 363 , training loss: 4.029157
[INFO] Epoch: 33 , batch: 364 , training loss: 4.050376
[INFO] Epoch: 33 , batch: 365 , training loss: 4.010660
[INFO] Epoch: 33 , batch: 366 , training loss: 4.121173
[INFO] Epoch: 33 , batch: 367 , training loss: 4.191222
[INFO] Epoch: 33 , batch: 368 , training loss: 4.598950
[INFO] Epoch: 33 , batch: 369 , training loss: 4.260662
[INFO] Epoch: 33 , batch: 370 , training loss: 4.029214
[INFO] Epoch: 33 , batch: 371 , training loss: 4.385171
[INFO] Epoch: 33 , batch: 372 , training loss: 4.672820
[INFO] Epoch: 33 , batch: 373 , training loss: 4.782509
[INFO] Epoch: 33 , batch: 374 , training loss: 4.835618
[INFO] Epoch: 33 , batch: 375 , training loss: 4.822596
[INFO] Epoch: 33 , batch: 376 , training loss: 4.761271
[INFO] Epoch: 33 , batch: 377 , training loss: 4.532899
[INFO] Epoch: 33 , batch: 378 , training loss: 4.621294
[INFO] Epoch: 33 , batch: 379 , training loss: 4.580542
[INFO] Epoch: 33 , batch: 380 , training loss: 4.701484
[INFO] Epoch: 33 , batch: 381 , training loss: 4.441175
[INFO] Epoch: 33 , batch: 382 , training loss: 4.655654
[INFO] Epoch: 33 , batch: 383 , training loss: 4.667728
[INFO] Epoch: 33 , batch: 384 , training loss: 4.675742
[INFO] Epoch: 33 , batch: 385 , training loss: 4.411755
[INFO] Epoch: 33 , batch: 386 , training loss: 4.679609
[INFO] Epoch: 33 , batch: 387 , training loss: 4.638527
[INFO] Epoch: 33 , batch: 388 , training loss: 4.440492
[INFO] Epoch: 33 , batch: 389 , training loss: 4.289474
[INFO] Epoch: 33 , batch: 390 , training loss: 4.287914
[INFO] Epoch: 33 , batch: 391 , training loss: 4.338027
[INFO] Epoch: 33 , batch: 392 , training loss: 4.670103
[INFO] Epoch: 33 , batch: 393 , training loss: 4.552473
[INFO] Epoch: 33 , batch: 394 , training loss: 4.613941
[INFO] Epoch: 33 , batch: 395 , training loss: 4.470488
[INFO] Epoch: 33 , batch: 396 , training loss: 4.240799
[INFO] Epoch: 33 , batch: 397 , training loss: 4.438542
[INFO] Epoch: 33 , batch: 398 , training loss: 4.264273
[INFO] Epoch: 33 , batch: 399 , training loss: 4.333550
[INFO] Epoch: 33 , batch: 400 , training loss: 4.306206
[INFO] Epoch: 33 , batch: 401 , training loss: 4.711396
[INFO] Epoch: 33 , batch: 402 , training loss: 4.443147
[INFO] Epoch: 33 , batch: 403 , training loss: 4.230216
[INFO] Epoch: 33 , batch: 404 , training loss: 4.438148
[INFO] Epoch: 33 , batch: 405 , training loss: 4.490345
[INFO] Epoch: 33 , batch: 406 , training loss: 4.393692
[INFO] Epoch: 33 , batch: 407 , training loss: 4.460319
[INFO] Epoch: 33 , batch: 408 , training loss: 4.424804
[INFO] Epoch: 33 , batch: 409 , training loss: 4.391618
[INFO] Epoch: 33 , batch: 410 , training loss: 4.452662
[INFO] Epoch: 33 , batch: 411 , training loss: 4.628859
[INFO] Epoch: 33 , batch: 412 , training loss: 4.473293
[INFO] Epoch: 33 , batch: 413 , training loss: 4.354184
[INFO] Epoch: 33 , batch: 414 , training loss: 4.371102
[INFO] Epoch: 33 , batch: 415 , training loss: 4.413550
[INFO] Epoch: 33 , batch: 416 , training loss: 4.503589
[INFO] Epoch: 33 , batch: 417 , training loss: 4.414096
[INFO] Epoch: 33 , batch: 418 , training loss: 4.412921
[INFO] Epoch: 33 , batch: 419 , training loss: 4.389679
[INFO] Epoch: 33 , batch: 420 , training loss: 4.364891
[INFO] Epoch: 33 , batch: 421 , training loss: 4.354676
[INFO] Epoch: 33 , batch: 422 , training loss: 4.225794
[INFO] Epoch: 33 , batch: 423 , training loss: 4.433251
[INFO] Epoch: 33 , batch: 424 , training loss: 4.595459
[INFO] Epoch: 33 , batch: 425 , training loss: 4.460936
[INFO] Epoch: 33 , batch: 426 , training loss: 4.199613
[INFO] Epoch: 33 , batch: 427 , training loss: 4.442445
[INFO] Epoch: 33 , batch: 428 , training loss: 4.301726
[INFO] Epoch: 33 , batch: 429 , training loss: 4.180151
[INFO] Epoch: 33 , batch: 430 , training loss: 4.447801
[INFO] Epoch: 33 , batch: 431 , training loss: 4.052289
[INFO] Epoch: 33 , batch: 432 , training loss: 4.109423
[INFO] Epoch: 33 , batch: 433 , training loss: 4.140999
[INFO] Epoch: 33 , batch: 434 , training loss: 4.016634
[INFO] Epoch: 33 , batch: 435 , training loss: 4.380903
[INFO] Epoch: 33 , batch: 436 , training loss: 4.462516
[INFO] Epoch: 33 , batch: 437 , training loss: 4.207482
[INFO] Epoch: 33 , batch: 438 , training loss: 4.068829
[INFO] Epoch: 33 , batch: 439 , training loss: 4.268505
[INFO] Epoch: 33 , batch: 440 , training loss: 4.398641
[INFO] Epoch: 33 , batch: 441 , training loss: 4.501459
[INFO] Epoch: 33 , batch: 442 , training loss: 4.271279
[INFO] Epoch: 33 , batch: 443 , training loss: 4.463521
[INFO] Epoch: 33 , batch: 444 , training loss: 4.111748
[INFO] Epoch: 33 , batch: 445 , training loss: 3.991276
[INFO] Epoch: 33 , batch: 446 , training loss: 3.938039
[INFO] Epoch: 33 , batch: 447 , training loss: 4.125388
[INFO] Epoch: 33 , batch: 448 , training loss: 4.209690
[INFO] Epoch: 33 , batch: 449 , training loss: 4.624272
[INFO] Epoch: 33 , batch: 450 , training loss: 4.652796
[INFO] Epoch: 33 , batch: 451 , training loss: 4.586394
[INFO] Epoch: 33 , batch: 452 , training loss: 4.385381
[INFO] Epoch: 33 , batch: 453 , training loss: 4.158478
[INFO] Epoch: 33 , batch: 454 , training loss: 4.305715
[INFO] Epoch: 33 , batch: 455 , training loss: 4.347042
[INFO] Epoch: 33 , batch: 456 , training loss: 4.349625
[INFO] Epoch: 33 , batch: 457 , training loss: 4.437468
[INFO] Epoch: 33 , batch: 458 , training loss: 4.166888
[INFO] Epoch: 33 , batch: 459 , training loss: 4.158502
[INFO] Epoch: 33 , batch: 460 , training loss: 4.245920
[INFO] Epoch: 33 , batch: 461 , training loss: 4.217899
[INFO] Epoch: 33 , batch: 462 , training loss: 4.289355
[INFO] Epoch: 33 , batch: 463 , training loss: 4.192253
[INFO] Epoch: 33 , batch: 464 , training loss: 4.383961
[INFO] Epoch: 33 , batch: 465 , training loss: 4.310700
[INFO] Epoch: 33 , batch: 466 , training loss: 4.407748
[INFO] Epoch: 33 , batch: 467 , training loss: 4.371571
[INFO] Epoch: 33 , batch: 468 , training loss: 4.364358
[INFO] Epoch: 33 , batch: 469 , training loss: 4.369414
[INFO] Epoch: 33 , batch: 470 , training loss: 4.188360
[INFO] Epoch: 33 , batch: 471 , training loss: 4.274265
[INFO] Epoch: 33 , batch: 472 , training loss: 4.328153
[INFO] Epoch: 33 , batch: 473 , training loss: 4.247923
[INFO] Epoch: 33 , batch: 474 , training loss: 4.056063
[INFO] Epoch: 33 , batch: 475 , training loss: 3.931814
[INFO] Epoch: 33 , batch: 476 , training loss: 4.295651
[INFO] Epoch: 33 , batch: 477 , training loss: 4.439597
[INFO] Epoch: 33 , batch: 478 , training loss: 4.466788
[INFO] Epoch: 33 , batch: 479 , training loss: 4.432874
[INFO] Epoch: 33 , batch: 480 , training loss: 4.510205
[INFO] Epoch: 33 , batch: 481 , training loss: 4.398423
[INFO] Epoch: 33 , batch: 482 , training loss: 4.526312
[INFO] Epoch: 33 , batch: 483 , training loss: 4.372755
[INFO] Epoch: 33 , batch: 484 , training loss: 4.179835
[INFO] Epoch: 33 , batch: 485 , training loss: 4.272249
[INFO] Epoch: 33 , batch: 486 , training loss: 4.167733
[INFO] Epoch: 33 , batch: 487 , training loss: 4.145751
[INFO] Epoch: 33 , batch: 488 , training loss: 4.337691
[INFO] Epoch: 33 , batch: 489 , training loss: 4.221869
[INFO] Epoch: 33 , batch: 490 , training loss: 4.270684
[INFO] Epoch: 33 , batch: 491 , training loss: 4.224110
[INFO] Epoch: 33 , batch: 492 , training loss: 4.173526
[INFO] Epoch: 33 , batch: 493 , training loss: 4.356104
[INFO] Epoch: 33 , batch: 494 , training loss: 4.298730
[INFO] Epoch: 33 , batch: 495 , training loss: 4.405626
[INFO] Epoch: 33 , batch: 496 , training loss: 4.295235
[INFO] Epoch: 33 , batch: 497 , training loss: 4.334219
[INFO] Epoch: 33 , batch: 498 , training loss: 4.321655
[INFO] Epoch: 33 , batch: 499 , training loss: 4.383420
[INFO] Epoch: 33 , batch: 500 , training loss: 4.546732
[INFO] Epoch: 33 , batch: 501 , training loss: 4.854179
[INFO] Epoch: 33 , batch: 502 , training loss: 4.906535
[INFO] Epoch: 33 , batch: 503 , training loss: 4.601418
[INFO] Epoch: 33 , batch: 504 , training loss: 4.720455
[INFO] Epoch: 33 , batch: 505 , training loss: 4.691023
[INFO] Epoch: 33 , batch: 506 , training loss: 4.649385
[INFO] Epoch: 33 , batch: 507 , training loss: 4.722662
[INFO] Epoch: 33 , batch: 508 , training loss: 4.669972
[INFO] Epoch: 33 , batch: 509 , training loss: 4.458643
[INFO] Epoch: 33 , batch: 510 , training loss: 4.546233
[INFO] Epoch: 33 , batch: 511 , training loss: 4.431322
[INFO] Epoch: 33 , batch: 512 , training loss: 4.540074
[INFO] Epoch: 33 , batch: 513 , training loss: 4.775482
[INFO] Epoch: 33 , batch: 514 , training loss: 4.443693
[INFO] Epoch: 33 , batch: 515 , training loss: 4.659935
[INFO] Epoch: 33 , batch: 516 , training loss: 4.493722
[INFO] Epoch: 33 , batch: 517 , training loss: 4.443400
[INFO] Epoch: 33 , batch: 518 , training loss: 4.407575
[INFO] Epoch: 33 , batch: 519 , training loss: 4.267132
[INFO] Epoch: 33 , batch: 520 , training loss: 4.498902
[INFO] Epoch: 33 , batch: 521 , training loss: 4.460119
[INFO] Epoch: 33 , batch: 522 , training loss: 4.521657
[INFO] Epoch: 33 , batch: 523 , training loss: 4.431806
[INFO] Epoch: 33 , batch: 524 , training loss: 4.730350
[INFO] Epoch: 33 , batch: 525 , training loss: 4.656500
[INFO] Epoch: 33 , batch: 526 , training loss: 4.419939
[INFO] Epoch: 33 , batch: 527 , training loss: 4.457886
[INFO] Epoch: 33 , batch: 528 , training loss: 4.462432
[INFO] Epoch: 33 , batch: 529 , training loss: 4.452863
[INFO] Epoch: 33 , batch: 530 , training loss: 4.289836
[INFO] Epoch: 33 , batch: 531 , training loss: 4.434879
[INFO] Epoch: 33 , batch: 532 , training loss: 4.339332
[INFO] Epoch: 33 , batch: 533 , training loss: 4.488649
[INFO] Epoch: 33 , batch: 534 , training loss: 4.462006
[INFO] Epoch: 33 , batch: 535 , training loss: 4.480584
[INFO] Epoch: 33 , batch: 536 , training loss: 4.300167
[INFO] Epoch: 33 , batch: 537 , training loss: 4.309390
[INFO] Epoch: 33 , batch: 538 , training loss: 4.399023
[INFO] Epoch: 33 , batch: 539 , training loss: 4.498093
[INFO] Epoch: 33 , batch: 540 , training loss: 4.967406
[INFO] Epoch: 33 , batch: 541 , training loss: 4.858809
[INFO] Epoch: 33 , batch: 542 , training loss: 4.779679
[INFO] Epoch: 34 , batch: 0 , training loss: 3.370466
[INFO] Epoch: 34 , batch: 1 , training loss: 3.484302
[INFO] Epoch: 34 , batch: 2 , training loss: 3.609765
[INFO] Epoch: 34 , batch: 3 , training loss: 3.412039
[INFO] Epoch: 34 , batch: 4 , training loss: 3.847279
[INFO] Epoch: 34 , batch: 5 , training loss: 3.458424
[INFO] Epoch: 34 , batch: 6 , training loss: 3.849543
[INFO] Epoch: 34 , batch: 7 , training loss: 3.781335
[INFO] Epoch: 34 , batch: 8 , training loss: 3.493674
[INFO] Epoch: 34 , batch: 9 , training loss: 3.738145
[INFO] Epoch: 34 , batch: 10 , training loss: 3.724865
[INFO] Epoch: 34 , batch: 11 , training loss: 3.675067
[INFO] Epoch: 34 , batch: 12 , training loss: 3.556225
[INFO] Epoch: 34 , batch: 13 , training loss: 3.629663
[INFO] Epoch: 34 , batch: 14 , training loss: 3.480673
[INFO] Epoch: 34 , batch: 15 , training loss: 3.677349
[INFO] Epoch: 34 , batch: 16 , training loss: 3.522074
[INFO] Epoch: 34 , batch: 17 , training loss: 3.736696
[INFO] Epoch: 34 , batch: 18 , training loss: 3.659874
[INFO] Epoch: 34 , batch: 19 , training loss: 3.408477
[INFO] Epoch: 34 , batch: 20 , training loss: 3.390284
[INFO] Epoch: 34 , batch: 21 , training loss: 3.509373
[INFO] Epoch: 34 , batch: 22 , training loss: 3.383792
[INFO] Epoch: 34 , batch: 23 , training loss: 3.615066
[INFO] Epoch: 34 , batch: 24 , training loss: 3.416139
[INFO] Epoch: 34 , batch: 25 , training loss: 3.627315
[INFO] Epoch: 34 , batch: 26 , training loss: 3.476441
[INFO] Epoch: 34 , batch: 27 , training loss: 3.419520
[INFO] Epoch: 34 , batch: 28 , training loss: 3.584287
[INFO] Epoch: 34 , batch: 29 , training loss: 3.402243
[INFO] Epoch: 34 , batch: 30 , training loss: 3.474516
[INFO] Epoch: 34 , batch: 31 , training loss: 3.484797
[INFO] Epoch: 34 , batch: 32 , training loss: 3.483353
[INFO] Epoch: 34 , batch: 33 , training loss: 3.532550
[INFO] Epoch: 34 , batch: 34 , training loss: 3.513627
[INFO] Epoch: 34 , batch: 35 , training loss: 3.485880
[INFO] Epoch: 34 , batch: 36 , training loss: 3.522923
[INFO] Epoch: 34 , batch: 37 , training loss: 3.453599
[INFO] Epoch: 34 , batch: 38 , training loss: 3.485560
[INFO] Epoch: 34 , batch: 39 , training loss: 3.349993
[INFO] Epoch: 34 , batch: 40 , training loss: 3.559354
[INFO] Epoch: 34 , batch: 41 , training loss: 3.509788
[INFO] Epoch: 34 , batch: 42 , training loss: 3.899463
[INFO] Epoch: 34 , batch: 43 , training loss: 3.603276
[INFO] Epoch: 34 , batch: 44 , training loss: 3.961764
[INFO] Epoch: 34 , batch: 45 , training loss: 3.899237
[INFO] Epoch: 34 , batch: 46 , training loss: 3.773230
[INFO] Epoch: 34 , batch: 47 , training loss: 3.568511
[INFO] Epoch: 34 , batch: 48 , training loss: 3.570843
[INFO] Epoch: 34 , batch: 49 , training loss: 3.737739
[INFO] Epoch: 34 , batch: 50 , training loss: 3.558324
[INFO] Epoch: 34 , batch: 51 , training loss: 3.768249
[INFO] Epoch: 34 , batch: 52 , training loss: 3.655434
[INFO] Epoch: 34 , batch: 53 , training loss: 3.760655
[INFO] Epoch: 34 , batch: 54 , training loss: 3.763396
[INFO] Epoch: 34 , batch: 55 , training loss: 3.861038
[INFO] Epoch: 34 , batch: 56 , training loss: 3.711901
[INFO] Epoch: 34 , batch: 57 , training loss: 3.620739
[INFO] Epoch: 34 , batch: 58 , training loss: 3.694563
[INFO] Epoch: 34 , batch: 59 , training loss: 3.767134
[INFO] Epoch: 34 , batch: 60 , training loss: 3.706592
[INFO] Epoch: 34 , batch: 61 , training loss: 3.775154
[INFO] Epoch: 34 , batch: 62 , training loss: 3.692415
[INFO] Epoch: 34 , batch: 63 , training loss: 3.863483
[INFO] Epoch: 34 , batch: 64 , training loss: 4.021926
[INFO] Epoch: 34 , batch: 65 , training loss: 3.725057
[INFO] Epoch: 34 , batch: 66 , training loss: 3.593669
[INFO] Epoch: 34 , batch: 67 , training loss: 3.674254
[INFO] Epoch: 34 , batch: 68 , training loss: 3.819553
[INFO] Epoch: 34 , batch: 69 , training loss: 3.688202
[INFO] Epoch: 34 , batch: 70 , training loss: 3.929717
[INFO] Epoch: 34 , batch: 71 , training loss: 3.800922
[INFO] Epoch: 34 , batch: 72 , training loss: 3.882866
[INFO] Epoch: 34 , batch: 73 , training loss: 3.801577
[INFO] Epoch: 34 , batch: 74 , training loss: 3.926533
[INFO] Epoch: 34 , batch: 75 , training loss: 3.813977
[INFO] Epoch: 34 , batch: 76 , training loss: 3.829414
[INFO] Epoch: 34 , batch: 77 , training loss: 3.839364
[INFO] Epoch: 34 , batch: 78 , training loss: 3.943917
[INFO] Epoch: 34 , batch: 79 , training loss: 3.747998
[INFO] Epoch: 34 , batch: 80 , training loss: 3.953370
[INFO] Epoch: 34 , batch: 81 , training loss: 3.889151
[INFO] Epoch: 34 , batch: 82 , training loss: 3.869532
[INFO] Epoch: 34 , batch: 83 , training loss: 3.999007
[INFO] Epoch: 34 , batch: 84 , training loss: 3.924909
[INFO] Epoch: 34 , batch: 85 , training loss: 3.989215
[INFO] Epoch: 34 , batch: 86 , training loss: 3.926524
[INFO] Epoch: 34 , batch: 87 , training loss: 3.909315
[INFO] Epoch: 34 , batch: 88 , training loss: 4.051595
[INFO] Epoch: 34 , batch: 89 , training loss: 3.832053
[INFO] Epoch: 34 , batch: 90 , training loss: 3.910334
[INFO] Epoch: 34 , batch: 91 , training loss: 3.846397
[INFO] Epoch: 34 , batch: 92 , training loss: 3.848063
[INFO] Epoch: 34 , batch: 93 , training loss: 3.965183
[INFO] Epoch: 34 , batch: 94 , training loss: 4.090913
[INFO] Epoch: 34 , batch: 95 , training loss: 3.881358
[INFO] Epoch: 34 , batch: 96 , training loss: 3.893837
[INFO] Epoch: 34 , batch: 97 , training loss: 3.784029
[INFO] Epoch: 34 , batch: 98 , training loss: 3.768252
[INFO] Epoch: 34 , batch: 99 , training loss: 3.835229
[INFO] Epoch: 34 , batch: 100 , training loss: 3.790449
[INFO] Epoch: 34 , batch: 101 , training loss: 3.824286
[INFO] Epoch: 34 , batch: 102 , training loss: 3.926463
[INFO] Epoch: 34 , batch: 103 , training loss: 3.721152
[INFO] Epoch: 34 , batch: 104 , training loss: 3.671848
[INFO] Epoch: 34 , batch: 105 , training loss: 3.970185
[INFO] Epoch: 34 , batch: 106 , training loss: 3.920914
[INFO] Epoch: 34 , batch: 107 , training loss: 3.823660
[INFO] Epoch: 34 , batch: 108 , training loss: 3.752481
[INFO] Epoch: 34 , batch: 109 , training loss: 3.664480
[INFO] Epoch: 34 , batch: 110 , training loss: 3.852570
[INFO] Epoch: 34 , batch: 111 , training loss: 3.915727
[INFO] Epoch: 34 , batch: 112 , training loss: 3.825243
[INFO] Epoch: 34 , batch: 113 , training loss: 3.859215
[INFO] Epoch: 34 , batch: 114 , training loss: 3.807755
[INFO] Epoch: 34 , batch: 115 , training loss: 3.844264
[INFO] Epoch: 34 , batch: 116 , training loss: 3.733758
[INFO] Epoch: 34 , batch: 117 , training loss: 3.960575
[INFO] Epoch: 34 , batch: 118 , training loss: 3.940823
[INFO] Epoch: 34 , batch: 119 , training loss: 4.056454
[INFO] Epoch: 34 , batch: 120 , training loss: 4.072376
[INFO] Epoch: 34 , batch: 121 , training loss: 3.924925
[INFO] Epoch: 34 , batch: 122 , training loss: 3.831968
[INFO] Epoch: 34 , batch: 123 , training loss: 3.815517
[INFO] Epoch: 34 , batch: 124 , training loss: 3.924731
[INFO] Epoch: 34 , batch: 125 , training loss: 3.783052
[INFO] Epoch: 34 , batch: 126 , training loss: 3.771033
[INFO] Epoch: 34 , batch: 127 , training loss: 3.760077
[INFO] Epoch: 34 , batch: 128 , training loss: 3.871090
[INFO] Epoch: 34 , batch: 129 , training loss: 3.886715
[INFO] Epoch: 34 , batch: 130 , training loss: 3.866688
[INFO] Epoch: 34 , batch: 131 , training loss: 3.860708
[INFO] Epoch: 34 , batch: 132 , training loss: 3.847454
[INFO] Epoch: 34 , batch: 133 , training loss: 3.834526
[INFO] Epoch: 34 , batch: 134 , training loss: 3.633823
[INFO] Epoch: 34 , batch: 135 , training loss: 3.695813
[INFO] Epoch: 34 , batch: 136 , training loss: 3.973339
[INFO] Epoch: 34 , batch: 137 , training loss: 3.866807
[INFO] Epoch: 34 , batch: 138 , training loss: 3.948192
[INFO] Epoch: 34 , batch: 139 , training loss: 4.528480
[INFO] Epoch: 34 , batch: 140 , training loss: 4.260818
[INFO] Epoch: 34 , batch: 141 , training loss: 4.029667
[INFO] Epoch: 34 , batch: 142 , training loss: 3.807800
[INFO] Epoch: 34 , batch: 143 , training loss: 3.944989
[INFO] Epoch: 34 , batch: 144 , training loss: 3.740596
[INFO] Epoch: 34 , batch: 145 , training loss: 3.792680
[INFO] Epoch: 34 , batch: 146 , training loss: 4.011885
[INFO] Epoch: 34 , batch: 147 , training loss: 3.684122
[INFO] Epoch: 34 , batch: 148 , training loss: 3.688714
[INFO] Epoch: 34 , batch: 149 , training loss: 3.794676
[INFO] Epoch: 34 , batch: 150 , training loss: 4.009704
[INFO] Epoch: 34 , batch: 151 , training loss: 3.868598
[INFO] Epoch: 34 , batch: 152 , training loss: 3.923902
[INFO] Epoch: 34 , batch: 153 , training loss: 3.897067
[INFO] Epoch: 34 , batch: 154 , training loss: 4.002676
[INFO] Epoch: 34 , batch: 155 , training loss: 4.206834
[INFO] Epoch: 34 , batch: 156 , training loss: 3.906988
[INFO] Epoch: 34 , batch: 157 , training loss: 3.915833
[INFO] Epoch: 34 , batch: 158 , training loss: 4.070189
[INFO] Epoch: 34 , batch: 159 , training loss: 3.895204
[INFO] Epoch: 34 , batch: 160 , training loss: 4.188984
[INFO] Epoch: 34 , batch: 161 , training loss: 4.330157
[INFO] Epoch: 34 , batch: 162 , training loss: 4.280822
[INFO] Epoch: 34 , batch: 163 , training loss: 4.442199
[INFO] Epoch: 34 , batch: 164 , training loss: 4.400691
[INFO] Epoch: 34 , batch: 165 , training loss: 4.321442
[INFO] Epoch: 34 , batch: 166 , training loss: 4.167297
[INFO] Epoch: 34 , batch: 167 , training loss: 4.189680
[INFO] Epoch: 34 , batch: 168 , training loss: 3.922157
[INFO] Epoch: 34 , batch: 169 , training loss: 3.880642
[INFO] Epoch: 34 , batch: 170 , training loss: 4.099803
[INFO] Epoch: 34 , batch: 171 , training loss: 3.495282
[INFO] Epoch: 34 , batch: 172 , training loss: 3.715524
[INFO] Epoch: 34 , batch: 173 , training loss: 4.096272
[INFO] Epoch: 34 , batch: 174 , training loss: 4.502069
[INFO] Epoch: 34 , batch: 175 , training loss: 4.820934
[INFO] Epoch: 34 , batch: 176 , training loss: 4.471849
[INFO] Epoch: 34 , batch: 177 , training loss: 4.109457
[INFO] Epoch: 34 , batch: 178 , training loss: 4.098278
[INFO] Epoch: 34 , batch: 179 , training loss: 4.166736
[INFO] Epoch: 34 , batch: 180 , training loss: 4.117802
[INFO] Epoch: 34 , batch: 181 , training loss: 4.383469
[INFO] Epoch: 34 , batch: 182 , training loss: 4.357400
[INFO] Epoch: 34 , batch: 183 , training loss: 4.323352
[INFO] Epoch: 34 , batch: 184 , training loss: 4.209705
[INFO] Epoch: 34 , batch: 185 , training loss: 4.121619
[INFO] Epoch: 34 , batch: 186 , training loss: 4.327056
[INFO] Epoch: 34 , batch: 187 , training loss: 4.435657
[INFO] Epoch: 34 , batch: 188 , training loss: 4.412325
[INFO] Epoch: 34 , batch: 189 , training loss: 4.315277
[INFO] Epoch: 34 , batch: 190 , training loss: 4.305076
[INFO] Epoch: 34 , batch: 191 , training loss: 4.438017
[INFO] Epoch: 34 , batch: 192 , training loss: 4.265521
[INFO] Epoch: 34 , batch: 193 , training loss: 4.348419
[INFO] Epoch: 34 , batch: 194 , training loss: 4.289622
[INFO] Epoch: 34 , batch: 195 , training loss: 4.200602
[INFO] Epoch: 34 , batch: 196 , training loss: 4.087107
[INFO] Epoch: 34 , batch: 197 , training loss: 4.194424
[INFO] Epoch: 34 , batch: 198 , training loss: 4.097703
[INFO] Epoch: 34 , batch: 199 , training loss: 4.259015
[INFO] Epoch: 34 , batch: 200 , training loss: 4.155395
[INFO] Epoch: 34 , batch: 201 , training loss: 4.060885
[INFO] Epoch: 34 , batch: 202 , training loss: 4.053187
[INFO] Epoch: 34 , batch: 203 , training loss: 4.147286
[INFO] Epoch: 34 , batch: 204 , training loss: 4.276103
[INFO] Epoch: 34 , batch: 205 , training loss: 3.860429
[INFO] Epoch: 34 , batch: 206 , training loss: 3.799898
[INFO] Epoch: 34 , batch: 207 , training loss: 3.798932
[INFO] Epoch: 34 , batch: 208 , training loss: 4.133662
[INFO] Epoch: 34 , batch: 209 , training loss: 4.075031
[INFO] Epoch: 34 , batch: 210 , training loss: 4.061872
[INFO] Epoch: 34 , batch: 211 , training loss: 4.081906
[INFO] Epoch: 34 , batch: 212 , training loss: 4.185127
[INFO] Epoch: 34 , batch: 213 , training loss: 4.134006
[INFO] Epoch: 34 , batch: 214 , training loss: 4.213068
[INFO] Epoch: 34 , batch: 215 , training loss: 4.429918
[INFO] Epoch: 34 , batch: 216 , training loss: 4.125078
[INFO] Epoch: 34 , batch: 217 , training loss: 4.101472
[INFO] Epoch: 34 , batch: 218 , training loss: 4.073854
[INFO] Epoch: 34 , batch: 219 , training loss: 4.174228
[INFO] Epoch: 34 , batch: 220 , training loss: 3.997489
[INFO] Epoch: 34 , batch: 221 , training loss: 4.013305
[INFO] Epoch: 34 , batch: 222 , training loss: 4.169298
[INFO] Epoch: 34 , batch: 223 , training loss: 4.255142
[INFO] Epoch: 34 , batch: 224 , training loss: 4.293394
[INFO] Epoch: 34 , batch: 225 , training loss: 4.191013
[INFO] Epoch: 34 , batch: 226 , training loss: 4.297196
[INFO] Epoch: 34 , batch: 227 , training loss: 4.250919
[INFO] Epoch: 34 , batch: 228 , training loss: 4.322686
[INFO] Epoch: 34 , batch: 229 , training loss: 4.148466
[INFO] Epoch: 34 , batch: 230 , training loss: 4.034467
[INFO] Epoch: 34 , batch: 231 , training loss: 3.890379
[INFO] Epoch: 34 , batch: 232 , training loss: 4.045142
[INFO] Epoch: 34 , batch: 233 , training loss: 4.061347
[INFO] Epoch: 34 , batch: 234 , training loss: 3.743532
[INFO] Epoch: 34 , batch: 235 , training loss: 3.844045
[INFO] Epoch: 34 , batch: 236 , training loss: 3.949578
[INFO] Epoch: 34 , batch: 237 , training loss: 4.179416
[INFO] Epoch: 34 , batch: 238 , training loss: 3.952976
[INFO] Epoch: 34 , batch: 239 , training loss: 3.989467
[INFO] Epoch: 34 , batch: 240 , training loss: 4.048029
[INFO] Epoch: 34 , batch: 241 , training loss: 3.853666
[INFO] Epoch: 34 , batch: 242 , training loss: 3.872437
[INFO] Epoch: 34 , batch: 243 , training loss: 4.167969
[INFO] Epoch: 34 , batch: 244 , training loss: 4.097931
[INFO] Epoch: 34 , batch: 245 , training loss: 4.071468
[INFO] Epoch: 34 , batch: 246 , training loss: 3.779491
[INFO] Epoch: 34 , batch: 247 , training loss: 3.927831
[INFO] Epoch: 34 , batch: 248 , training loss: 4.018884
[INFO] Epoch: 34 , batch: 249 , training loss: 4.042593
[INFO] Epoch: 34 , batch: 250 , training loss: 3.799839
[INFO] Epoch: 34 , batch: 251 , training loss: 4.222478
[INFO] Epoch: 34 , batch: 252 , training loss: 3.929420
[INFO] Epoch: 34 , batch: 253 , training loss: 3.879055
[INFO] Epoch: 34 , batch: 254 , training loss: 4.141215
[INFO] Epoch: 34 , batch: 255 , training loss: 4.112469
[INFO] Epoch: 34 , batch: 256 , training loss: 4.121560
[INFO] Epoch: 34 , batch: 257 , training loss: 4.247828
[INFO] Epoch: 34 , batch: 258 , training loss: 4.311660
[INFO] Epoch: 34 , batch: 259 , training loss: 4.344122
[INFO] Epoch: 34 , batch: 260 , training loss: 4.080715
[INFO] Epoch: 34 , batch: 261 , training loss: 4.245307
[INFO] Epoch: 34 , batch: 262 , training loss: 4.376264
[INFO] Epoch: 34 , batch: 263 , training loss: 4.603634
[INFO] Epoch: 34 , batch: 264 , training loss: 3.898702
[INFO] Epoch: 34 , batch: 265 , training loss: 4.041212
[INFO] Epoch: 34 , batch: 266 , training loss: 4.467347
[INFO] Epoch: 34 , batch: 267 , training loss: 4.228938
[INFO] Epoch: 34 , batch: 268 , training loss: 4.145909
[INFO] Epoch: 34 , batch: 269 , training loss: 4.118454
[INFO] Epoch: 34 , batch: 270 , training loss: 4.148306
[INFO] Epoch: 34 , batch: 271 , training loss: 4.178413
[INFO] Epoch: 34 , batch: 272 , training loss: 4.150150
[INFO] Epoch: 34 , batch: 273 , training loss: 4.201264
[INFO] Epoch: 34 , batch: 274 , training loss: 4.230023
[INFO] Epoch: 34 , batch: 275 , training loss: 4.131463
[INFO] Epoch: 34 , batch: 276 , training loss: 4.159992
[INFO] Epoch: 34 , batch: 277 , training loss: 4.327293
[INFO] Epoch: 34 , batch: 278 , training loss: 4.012177
[INFO] Epoch: 34 , batch: 279 , training loss: 4.017450
[INFO] Epoch: 34 , batch: 280 , training loss: 3.971339
[INFO] Epoch: 34 , batch: 281 , training loss: 4.107369
[INFO] Epoch: 34 , batch: 282 , training loss: 4.013800
[INFO] Epoch: 34 , batch: 283 , training loss: 4.056705
[INFO] Epoch: 34 , batch: 284 , training loss: 4.075835
[INFO] Epoch: 34 , batch: 285 , training loss: 3.994055
[INFO] Epoch: 34 , batch: 286 , training loss: 3.993127
[INFO] Epoch: 34 , batch: 287 , training loss: 3.950961
[INFO] Epoch: 34 , batch: 288 , training loss: 3.967138
[INFO] Epoch: 34 , batch: 289 , training loss: 3.996959
[INFO] Epoch: 34 , batch: 290 , training loss: 3.791669
[INFO] Epoch: 34 , batch: 291 , training loss: 3.748378
[INFO] Epoch: 34 , batch: 292 , training loss: 3.878392
[INFO] Epoch: 34 , batch: 293 , training loss: 3.799542
[INFO] Epoch: 34 , batch: 294 , training loss: 4.451342
[INFO] Epoch: 34 , batch: 295 , training loss: 4.225483
[INFO] Epoch: 34 , batch: 296 , training loss: 4.150592
[INFO] Epoch: 34 , batch: 297 , training loss: 4.116631
[INFO] Epoch: 34 , batch: 298 , training loss: 3.962354
[INFO] Epoch: 34 , batch: 299 , training loss: 3.986849
[INFO] Epoch: 34 , batch: 300 , training loss: 3.942900
[INFO] Epoch: 34 , batch: 301 , training loss: 3.899327
[INFO] Epoch: 34 , batch: 302 , training loss: 4.050174
[INFO] Epoch: 34 , batch: 303 , training loss: 4.066314
[INFO] Epoch: 34 , batch: 304 , training loss: 4.239053
[INFO] Epoch: 34 , batch: 305 , training loss: 4.026400
[INFO] Epoch: 34 , batch: 306 , training loss: 4.168410
[INFO] Epoch: 34 , batch: 307 , training loss: 4.148170
[INFO] Epoch: 34 , batch: 308 , training loss: 3.989737
[INFO] Epoch: 34 , batch: 309 , training loss: 4.013484
[INFO] Epoch: 34 , batch: 310 , training loss: 3.899866
[INFO] Epoch: 34 , batch: 311 , training loss: 3.890939
[INFO] Epoch: 34 , batch: 312 , training loss: 3.831946
[INFO] Epoch: 34 , batch: 313 , training loss: 3.950212
[INFO] Epoch: 34 , batch: 314 , training loss: 3.991971
[INFO] Epoch: 34 , batch: 315 , training loss: 4.048330
[INFO] Epoch: 34 , batch: 316 , training loss: 4.279773
[INFO] Epoch: 34 , batch: 317 , training loss: 4.700030
[INFO] Epoch: 34 , batch: 318 , training loss: 4.875323
[INFO] Epoch: 34 , batch: 319 , training loss: 4.470114
[INFO] Epoch: 34 , batch: 320 , training loss: 4.006949
[INFO] Epoch: 34 , batch: 321 , training loss: 3.847696
[INFO] Epoch: 34 , batch: 322 , training loss: 3.983355
[INFO] Epoch: 34 , batch: 323 , training loss: 3.986815
[INFO] Epoch: 34 , batch: 324 , training loss: 3.946309
[INFO] Epoch: 34 , batch: 325 , training loss: 4.113844
[INFO] Epoch: 34 , batch: 326 , training loss: 4.158974
[INFO] Epoch: 34 , batch: 327 , training loss: 4.055919
[INFO] Epoch: 34 , batch: 328 , training loss: 4.058238
[INFO] Epoch: 34 , batch: 329 , training loss: 3.981435
[INFO] Epoch: 34 , batch: 330 , training loss: 3.982236
[INFO] Epoch: 34 , batch: 331 , training loss: 4.126191
[INFO] Epoch: 34 , batch: 332 , training loss: 3.920787
[INFO] Epoch: 34 , batch: 333 , training loss: 3.955255
[INFO] Epoch: 34 , batch: 334 , training loss: 3.939544
[INFO] Epoch: 34 , batch: 335 , training loss: 4.070200
[INFO] Epoch: 34 , batch: 336 , training loss: 4.119480
[INFO] Epoch: 34 , batch: 337 , training loss: 4.115290
[INFO] Epoch: 34 , batch: 338 , training loss: 4.340768
[INFO] Epoch: 34 , batch: 339 , training loss: 4.197577
[INFO] Epoch: 34 , batch: 340 , training loss: 4.334208
[INFO] Epoch: 34 , batch: 341 , training loss: 4.102112
[INFO] Epoch: 34 , batch: 342 , training loss: 3.891248
[INFO] Epoch: 34 , batch: 343 , training loss: 3.975468
[INFO] Epoch: 34 , batch: 344 , training loss: 3.820173
[INFO] Epoch: 34 , batch: 345 , training loss: 3.951592
[INFO] Epoch: 34 , batch: 346 , training loss: 4.008892
[INFO] Epoch: 34 , batch: 347 , training loss: 3.899544
[INFO] Epoch: 34 , batch: 348 , training loss: 4.018555
[INFO] Epoch: 34 , batch: 349 , training loss: 4.178735
[INFO] Epoch: 34 , batch: 350 , training loss: 3.942412
[INFO] Epoch: 34 , batch: 351 , training loss: 4.030191
[INFO] Epoch: 34 , batch: 352 , training loss: 4.035786
[INFO] Epoch: 34 , batch: 353 , training loss: 4.002123
[INFO] Epoch: 34 , batch: 354 , training loss: 4.127655
[INFO] Epoch: 34 , batch: 355 , training loss: 4.139853
[INFO] Epoch: 34 , batch: 356 , training loss: 3.997750
[INFO] Epoch: 34 , batch: 357 , training loss: 4.080318
[INFO] Epoch: 34 , batch: 358 , training loss: 3.951424
[INFO] Epoch: 34 , batch: 359 , training loss: 3.985796
[INFO] Epoch: 34 , batch: 360 , training loss: 4.076090
[INFO] Epoch: 34 , batch: 361 , training loss: 4.045404
[INFO] Epoch: 34 , batch: 362 , training loss: 4.137536
[INFO] Epoch: 34 , batch: 363 , training loss: 4.018040
[INFO] Epoch: 34 , batch: 364 , training loss: 4.047176
[INFO] Epoch: 34 , batch: 365 , training loss: 4.007675
[INFO] Epoch: 34 , batch: 366 , training loss: 4.132753
[INFO] Epoch: 34 , batch: 367 , training loss: 4.192319
[INFO] Epoch: 34 , batch: 368 , training loss: 4.595854
[INFO] Epoch: 34 , batch: 369 , training loss: 4.247325
[INFO] Epoch: 34 , batch: 370 , training loss: 4.029393
[INFO] Epoch: 34 , batch: 371 , training loss: 4.390628
[INFO] Epoch: 34 , batch: 372 , training loss: 4.697747
[INFO] Epoch: 34 , batch: 373 , training loss: 4.776750
[INFO] Epoch: 34 , batch: 374 , training loss: 4.844085
[INFO] Epoch: 34 , batch: 375 , training loss: 4.851766
[INFO] Epoch: 34 , batch: 376 , training loss: 4.759202
[INFO] Epoch: 34 , batch: 377 , training loss: 4.550386
[INFO] Epoch: 34 , batch: 378 , training loss: 4.613713
[INFO] Epoch: 34 , batch: 379 , training loss: 4.583995
[INFO] Epoch: 34 , batch: 380 , training loss: 4.697416
[INFO] Epoch: 34 , batch: 381 , training loss: 4.436628
[INFO] Epoch: 34 , batch: 382 , training loss: 4.652852
[INFO] Epoch: 34 , batch: 383 , training loss: 4.655556
[INFO] Epoch: 34 , batch: 384 , training loss: 4.673954
[INFO] Epoch: 34 , batch: 385 , training loss: 4.401960
[INFO] Epoch: 34 , batch: 386 , training loss: 4.677088
[INFO] Epoch: 34 , batch: 387 , training loss: 4.664149
[INFO] Epoch: 34 , batch: 388 , training loss: 4.450604
[INFO] Epoch: 34 , batch: 389 , training loss: 4.274765
[INFO] Epoch: 34 , batch: 390 , training loss: 4.276829
[INFO] Epoch: 34 , batch: 391 , training loss: 4.318045
[INFO] Epoch: 34 , batch: 392 , training loss: 4.669000
[INFO] Epoch: 34 , batch: 393 , training loss: 4.553320
[INFO] Epoch: 34 , batch: 394 , training loss: 4.610879
[INFO] Epoch: 34 , batch: 395 , training loss: 4.477894
[INFO] Epoch: 34 , batch: 396 , training loss: 4.236658
[INFO] Epoch: 34 , batch: 397 , training loss: 4.424468
[INFO] Epoch: 34 , batch: 398 , training loss: 4.258675
[INFO] Epoch: 34 , batch: 399 , training loss: 4.339178
[INFO] Epoch: 34 , batch: 400 , training loss: 4.302508
[INFO] Epoch: 34 , batch: 401 , training loss: 4.697682
[INFO] Epoch: 34 , batch: 402 , training loss: 4.442120
[INFO] Epoch: 34 , batch: 403 , training loss: 4.231918
[INFO] Epoch: 34 , batch: 404 , training loss: 4.437884
[INFO] Epoch: 34 , batch: 405 , training loss: 4.497859
[INFO] Epoch: 34 , batch: 406 , training loss: 4.386002
[INFO] Epoch: 34 , batch: 407 , training loss: 4.451290
[INFO] Epoch: 34 , batch: 408 , training loss: 4.412922
[INFO] Epoch: 34 , batch: 409 , training loss: 4.394717
[INFO] Epoch: 34 , batch: 410 , training loss: 4.452838
[INFO] Epoch: 34 , batch: 411 , training loss: 4.625933
[INFO] Epoch: 34 , batch: 412 , training loss: 4.483315
[INFO] Epoch: 34 , batch: 413 , training loss: 4.352380
[INFO] Epoch: 34 , batch: 414 , training loss: 4.375110
[INFO] Epoch: 34 , batch: 415 , training loss: 4.415557
[INFO] Epoch: 34 , batch: 416 , training loss: 4.502396
[INFO] Epoch: 34 , batch: 417 , training loss: 4.405458
[INFO] Epoch: 34 , batch: 418 , training loss: 4.421784
[INFO] Epoch: 34 , batch: 419 , training loss: 4.394938
[INFO] Epoch: 34 , batch: 420 , training loss: 4.362153
[INFO] Epoch: 34 , batch: 421 , training loss: 4.353283
[INFO] Epoch: 34 , batch: 422 , training loss: 4.224783
[INFO] Epoch: 34 , batch: 423 , training loss: 4.428780
[INFO] Epoch: 34 , batch: 424 , training loss: 4.606915
[INFO] Epoch: 34 , batch: 425 , training loss: 4.468033
[INFO] Epoch: 34 , batch: 426 , training loss: 4.196555
[INFO] Epoch: 34 , batch: 427 , training loss: 4.432672
[INFO] Epoch: 34 , batch: 428 , training loss: 4.302859
[INFO] Epoch: 34 , batch: 429 , training loss: 4.179224
[INFO] Epoch: 34 , batch: 430 , training loss: 4.440668
[INFO] Epoch: 34 , batch: 431 , training loss: 4.051565
[INFO] Epoch: 34 , batch: 432 , training loss: 4.100772
[INFO] Epoch: 34 , batch: 433 , training loss: 4.135339
[INFO] Epoch: 34 , batch: 434 , training loss: 4.009225
[INFO] Epoch: 34 , batch: 435 , training loss: 4.385135
[INFO] Epoch: 34 , batch: 436 , training loss: 4.455690
[INFO] Epoch: 34 , batch: 437 , training loss: 4.210124
[INFO] Epoch: 34 , batch: 438 , training loss: 4.066983
[INFO] Epoch: 34 , batch: 439 , training loss: 4.278604
[INFO] Epoch: 34 , batch: 440 , training loss: 4.400183
[INFO] Epoch: 34 , batch: 441 , training loss: 4.510434
[INFO] Epoch: 34 , batch: 442 , training loss: 4.281617
[INFO] Epoch: 34 , batch: 443 , training loss: 4.467203
[INFO] Epoch: 34 , batch: 444 , training loss: 4.099610
[INFO] Epoch: 34 , batch: 445 , training loss: 3.994974
[INFO] Epoch: 34 , batch: 446 , training loss: 3.939498
[INFO] Epoch: 34 , batch: 447 , training loss: 4.130217
[INFO] Epoch: 34 , batch: 448 , training loss: 4.211199
[INFO] Epoch: 34 , batch: 449 , training loss: 4.620246
[INFO] Epoch: 34 , batch: 450 , training loss: 4.657588
[INFO] Epoch: 34 , batch: 451 , training loss: 4.581819
[INFO] Epoch: 34 , batch: 452 , training loss: 4.383938
[INFO] Epoch: 34 , batch: 453 , training loss: 4.159653
[INFO] Epoch: 34 , batch: 454 , training loss: 4.308809
[INFO] Epoch: 34 , batch: 455 , training loss: 4.348962
[INFO] Epoch: 34 , batch: 456 , training loss: 4.346941
[INFO] Epoch: 34 , batch: 457 , training loss: 4.425789
[INFO] Epoch: 34 , batch: 458 , training loss: 4.165313
[INFO] Epoch: 34 , batch: 459 , training loss: 4.158373
[INFO] Epoch: 34 , batch: 460 , training loss: 4.244980
[INFO] Epoch: 34 , batch: 461 , training loss: 4.229285
[INFO] Epoch: 34 , batch: 462 , training loss: 4.292015
[INFO] Epoch: 34 , batch: 463 , training loss: 4.197116
[INFO] Epoch: 34 , batch: 464 , training loss: 4.379621
[INFO] Epoch: 34 , batch: 465 , training loss: 4.310919
[INFO] Epoch: 34 , batch: 466 , training loss: 4.407374
[INFO] Epoch: 34 , batch: 467 , training loss: 4.392622
[INFO] Epoch: 34 , batch: 468 , training loss: 4.351269
[INFO] Epoch: 34 , batch: 469 , training loss: 4.360828
[INFO] Epoch: 34 , batch: 470 , training loss: 4.206118
[INFO] Epoch: 34 , batch: 471 , training loss: 4.271445
[INFO] Epoch: 34 , batch: 472 , training loss: 4.326152
[INFO] Epoch: 34 , batch: 473 , training loss: 4.258672
[INFO] Epoch: 34 , batch: 474 , training loss: 4.047555
[INFO] Epoch: 34 , batch: 475 , training loss: 3.935619
[INFO] Epoch: 34 , batch: 476 , training loss: 4.304265
[INFO] Epoch: 34 , batch: 477 , training loss: 4.431968
[INFO] Epoch: 34 , batch: 478 , training loss: 4.481722
[INFO] Epoch: 34 , batch: 479 , training loss: 4.430544
[INFO] Epoch: 34 , batch: 480 , training loss: 4.517045
[INFO] Epoch: 34 , batch: 481 , training loss: 4.392908
[INFO] Epoch: 34 , batch: 482 , training loss: 4.516608
[INFO] Epoch: 34 , batch: 483 , training loss: 4.376152
[INFO] Epoch: 34 , batch: 484 , training loss: 4.177300
[INFO] Epoch: 34 , batch: 485 , training loss: 4.268274
[INFO] Epoch: 34 , batch: 486 , training loss: 4.169240
[INFO] Epoch: 34 , batch: 487 , training loss: 4.159910
[INFO] Epoch: 34 , batch: 488 , training loss: 4.351314
[INFO] Epoch: 34 , batch: 489 , training loss: 4.230766
[INFO] Epoch: 34 , batch: 490 , training loss: 4.264722
[INFO] Epoch: 34 , batch: 491 , training loss: 4.217243
[INFO] Epoch: 34 , batch: 492 , training loss: 4.175034
[INFO] Epoch: 34 , batch: 493 , training loss: 4.366657
[INFO] Epoch: 34 , batch: 494 , training loss: 4.286617
[INFO] Epoch: 34 , batch: 495 , training loss: 4.414430
[INFO] Epoch: 34 , batch: 496 , training loss: 4.295712
[INFO] Epoch: 34 , batch: 497 , training loss: 4.338424
[INFO] Epoch: 34 , batch: 498 , training loss: 4.353408
[INFO] Epoch: 34 , batch: 499 , training loss: 4.384497
[INFO] Epoch: 34 , batch: 500 , training loss: 4.549075
[INFO] Epoch: 34 , batch: 501 , training loss: 4.848078
[INFO] Epoch: 34 , batch: 502 , training loss: 4.890637
[INFO] Epoch: 34 , batch: 503 , training loss: 4.599475
[INFO] Epoch: 34 , batch: 504 , training loss: 4.719851
[INFO] Epoch: 34 , batch: 505 , training loss: 4.688879
[INFO] Epoch: 34 , batch: 506 , training loss: 4.653541
[INFO] Epoch: 34 , batch: 507 , training loss: 4.715896
[INFO] Epoch: 34 , batch: 508 , training loss: 4.673092
[INFO] Epoch: 34 , batch: 509 , training loss: 4.454031
[INFO] Epoch: 34 , batch: 510 , training loss: 4.525621
[INFO] Epoch: 34 , batch: 511 , training loss: 4.435545
[INFO] Epoch: 34 , batch: 512 , training loss: 4.533359
[INFO] Epoch: 34 , batch: 513 , training loss: 4.783532
[INFO] Epoch: 34 , batch: 514 , training loss: 4.446899
[INFO] Epoch: 34 , batch: 515 , training loss: 4.656115
[INFO] Epoch: 34 , batch: 516 , training loss: 4.483560
[INFO] Epoch: 34 , batch: 517 , training loss: 4.430282
[INFO] Epoch: 34 , batch: 518 , training loss: 4.402077
[INFO] Epoch: 34 , batch: 519 , training loss: 4.267765
[INFO] Epoch: 34 , batch: 520 , training loss: 4.494011
[INFO] Epoch: 34 , batch: 521 , training loss: 4.459283
[INFO] Epoch: 34 , batch: 522 , training loss: 4.530427
[INFO] Epoch: 34 , batch: 523 , training loss: 4.433987
[INFO] Epoch: 34 , batch: 524 , training loss: 4.723475
[INFO] Epoch: 34 , batch: 525 , training loss: 4.659348
[INFO] Epoch: 34 , batch: 526 , training loss: 4.418913
[INFO] Epoch: 34 , batch: 527 , training loss: 4.452434
[INFO] Epoch: 34 , batch: 528 , training loss: 4.462071
[INFO] Epoch: 34 , batch: 529 , training loss: 4.449615
[INFO] Epoch: 34 , batch: 530 , training loss: 4.290381
[INFO] Epoch: 34 , batch: 531 , training loss: 4.433194
[INFO] Epoch: 34 , batch: 532 , training loss: 4.331197
[INFO] Epoch: 34 , batch: 533 , training loss: 4.485017
[INFO] Epoch: 34 , batch: 534 , training loss: 4.454980
[INFO] Epoch: 34 , batch: 535 , training loss: 4.486192
[INFO] Epoch: 34 , batch: 536 , training loss: 4.300356
[INFO] Epoch: 34 , batch: 537 , training loss: 4.298746
[INFO] Epoch: 34 , batch: 538 , training loss: 4.391943
[INFO] Epoch: 34 , batch: 539 , training loss: 4.484670
[INFO] Epoch: 34 , batch: 540 , training loss: 4.963400
[INFO] Epoch: 34 , batch: 541 , training loss: 4.854758
[INFO] Epoch: 34 , batch: 542 , training loss: 4.782331
[INFO] Epoch: 35 , batch: 0 , training loss: 3.382806
[INFO] Epoch: 35 , batch: 1 , training loss: 3.457232
[INFO] Epoch: 35 , batch: 2 , training loss: 3.611787
[INFO] Epoch: 35 , batch: 3 , training loss: 3.426049
[INFO] Epoch: 35 , batch: 4 , training loss: 3.815784
[INFO] Epoch: 35 , batch: 5 , training loss: 3.438122
[INFO] Epoch: 35 , batch: 6 , training loss: 3.842815
[INFO] Epoch: 35 , batch: 7 , training loss: 3.788034
[INFO] Epoch: 35 , batch: 8 , training loss: 3.476049
[INFO] Epoch: 35 , batch: 9 , training loss: 3.737189
[INFO] Epoch: 35 , batch: 10 , training loss: 3.708945
[INFO] Epoch: 35 , batch: 11 , training loss: 3.636326
[INFO] Epoch: 35 , batch: 12 , training loss: 3.554735
[INFO] Epoch: 35 , batch: 13 , training loss: 3.618595
[INFO] Epoch: 35 , batch: 14 , training loss: 3.459954
[INFO] Epoch: 35 , batch: 15 , training loss: 3.659914
[INFO] Epoch: 35 , batch: 16 , training loss: 3.520357
[INFO] Epoch: 35 , batch: 17 , training loss: 3.736875
[INFO] Epoch: 35 , batch: 18 , training loss: 3.646848
[INFO] Epoch: 35 , batch: 19 , training loss: 3.394290
[INFO] Epoch: 35 , batch: 20 , training loss: 3.367185
[INFO] Epoch: 35 , batch: 21 , training loss: 3.503085
[INFO] Epoch: 35 , batch: 22 , training loss: 3.371966
[INFO] Epoch: 35 , batch: 23 , training loss: 3.622174
[INFO] Epoch: 35 , batch: 24 , training loss: 3.384042
[INFO] Epoch: 35 , batch: 25 , training loss: 3.571026
[INFO] Epoch: 35 , batch: 26 , training loss: 3.440125
[INFO] Epoch: 35 , batch: 27 , training loss: 3.412964
[INFO] Epoch: 35 , batch: 28 , training loss: 3.559998
[INFO] Epoch: 35 , batch: 29 , training loss: 3.388643
[INFO] Epoch: 35 , batch: 30 , training loss: 3.468026
[INFO] Epoch: 35 , batch: 31 , training loss: 3.480917
[INFO] Epoch: 35 , batch: 32 , training loss: 3.482646
[INFO] Epoch: 35 , batch: 33 , training loss: 3.535137
[INFO] Epoch: 35 , batch: 34 , training loss: 3.483114
[INFO] Epoch: 35 , batch: 35 , training loss: 3.485808
[INFO] Epoch: 35 , batch: 36 , training loss: 3.491239
[INFO] Epoch: 35 , batch: 37 , training loss: 3.431464
[INFO] Epoch: 35 , batch: 38 , training loss: 3.463037
[INFO] Epoch: 35 , batch: 39 , training loss: 3.337841
[INFO] Epoch: 35 , batch: 40 , training loss: 3.580449
[INFO] Epoch: 35 , batch: 41 , training loss: 3.453057
[INFO] Epoch: 35 , batch: 42 , training loss: 3.908988
[INFO] Epoch: 35 , batch: 43 , training loss: 3.608775
[INFO] Epoch: 35 , batch: 44 , training loss: 3.973841
[INFO] Epoch: 35 , batch: 45 , training loss: 3.874974
[INFO] Epoch: 35 , batch: 46 , training loss: 3.819578
[INFO] Epoch: 35 , batch: 47 , training loss: 3.554848
[INFO] Epoch: 35 , batch: 48 , training loss: 3.615103
[INFO] Epoch: 35 , batch: 49 , training loss: 3.739512
[INFO] Epoch: 35 , batch: 50 , training loss: 3.568055
[INFO] Epoch: 35 , batch: 51 , training loss: 3.769055
[INFO] Epoch: 35 , batch: 52 , training loss: 3.648271
[INFO] Epoch: 35 , batch: 53 , training loss: 3.768372
[INFO] Epoch: 35 , batch: 54 , training loss: 3.746126
[INFO] Epoch: 35 , batch: 55 , training loss: 3.846074
[INFO] Epoch: 35 , batch: 56 , training loss: 3.722307
[INFO] Epoch: 35 , batch: 57 , training loss: 3.622734
[INFO] Epoch: 35 , batch: 58 , training loss: 3.669629
[INFO] Epoch: 35 , batch: 59 , training loss: 3.751255
[INFO] Epoch: 35 , batch: 60 , training loss: 3.712438
[INFO] Epoch: 35 , batch: 61 , training loss: 3.747704
[INFO] Epoch: 35 , batch: 62 , training loss: 3.703444
[INFO] Epoch: 35 , batch: 63 , training loss: 3.890095
[INFO] Epoch: 35 , batch: 64 , training loss: 4.013500
[INFO] Epoch: 35 , batch: 65 , training loss: 3.719624
[INFO] Epoch: 35 , batch: 66 , training loss: 3.600842
[INFO] Epoch: 35 , batch: 67 , training loss: 3.669918
[INFO] Epoch: 35 , batch: 68 , training loss: 3.819485
[INFO] Epoch: 35 , batch: 69 , training loss: 3.700012
[INFO] Epoch: 35 , batch: 70 , training loss: 3.949044
[INFO] Epoch: 35 , batch: 71 , training loss: 3.833811
[INFO] Epoch: 35 , batch: 72 , training loss: 3.847080
[INFO] Epoch: 35 , batch: 73 , training loss: 3.789435
[INFO] Epoch: 35 , batch: 74 , training loss: 3.925569
[INFO] Epoch: 35 , batch: 75 , training loss: 3.792765
[INFO] Epoch: 35 , batch: 76 , training loss: 3.864570
[INFO] Epoch: 35 , batch: 77 , training loss: 3.835899
[INFO] Epoch: 35 , batch: 78 , training loss: 3.938963
[INFO] Epoch: 35 , batch: 79 , training loss: 3.755300
[INFO] Epoch: 35 , batch: 80 , training loss: 3.951413
[INFO] Epoch: 35 , batch: 81 , training loss: 3.910261
[INFO] Epoch: 35 , batch: 82 , training loss: 3.839594
[INFO] Epoch: 35 , batch: 83 , training loss: 3.991250
[INFO] Epoch: 35 , batch: 84 , training loss: 3.902268
[INFO] Epoch: 35 , batch: 85 , training loss: 3.988717
[INFO] Epoch: 35 , batch: 86 , training loss: 3.934530
[INFO] Epoch: 35 , batch: 87 , training loss: 3.921426
[INFO] Epoch: 35 , batch: 88 , training loss: 4.050335
[INFO] Epoch: 35 , batch: 89 , training loss: 3.844290
[INFO] Epoch: 35 , batch: 90 , training loss: 3.907811
[INFO] Epoch: 35 , batch: 91 , training loss: 3.868430
[INFO] Epoch: 35 , batch: 92 , training loss: 3.856831
[INFO] Epoch: 35 , batch: 93 , training loss: 3.972794
[INFO] Epoch: 35 , batch: 94 , training loss: 4.108860
[INFO] Epoch: 35 , batch: 95 , training loss: 3.851354
[INFO] Epoch: 35 , batch: 96 , training loss: 3.905932
[INFO] Epoch: 35 , batch: 97 , training loss: 3.782735
[INFO] Epoch: 35 , batch: 98 , training loss: 3.767121
[INFO] Epoch: 35 , batch: 99 , training loss: 3.850153
[INFO] Epoch: 35 , batch: 100 , training loss: 3.793312
[INFO] Epoch: 35 , batch: 101 , training loss: 3.831398
[INFO] Epoch: 35 , batch: 102 , training loss: 3.924592
[INFO] Epoch: 35 , batch: 103 , training loss: 3.723104
[INFO] Epoch: 35 , batch: 104 , training loss: 3.696764
[INFO] Epoch: 35 , batch: 105 , training loss: 3.950961
[INFO] Epoch: 35 , batch: 106 , training loss: 3.930000
[INFO] Epoch: 35 , batch: 107 , training loss: 3.823990
[INFO] Epoch: 35 , batch: 108 , training loss: 3.734863
[INFO] Epoch: 35 , batch: 109 , training loss: 3.671104
[INFO] Epoch: 35 , batch: 110 , training loss: 3.847045
[INFO] Epoch: 35 , batch: 111 , training loss: 3.911454
[INFO] Epoch: 35 , batch: 112 , training loss: 3.831824
[INFO] Epoch: 35 , batch: 113 , training loss: 3.835861
[INFO] Epoch: 35 , batch: 114 , training loss: 3.827068
[INFO] Epoch: 35 , batch: 115 , training loss: 3.856367
[INFO] Epoch: 35 , batch: 116 , training loss: 3.726727
[INFO] Epoch: 35 , batch: 117 , training loss: 3.974674
[INFO] Epoch: 35 , batch: 118 , training loss: 3.937784
[INFO] Epoch: 35 , batch: 119 , training loss: 4.067686
[INFO] Epoch: 35 , batch: 120 , training loss: 4.057924
[INFO] Epoch: 35 , batch: 121 , training loss: 3.930118
[INFO] Epoch: 35 , batch: 122 , training loss: 3.835702
[INFO] Epoch: 35 , batch: 123 , training loss: 3.831074
[INFO] Epoch: 35 , batch: 124 , training loss: 3.935107
[INFO] Epoch: 35 , batch: 125 , training loss: 3.764243
[INFO] Epoch: 35 , batch: 126 , training loss: 3.779922
[INFO] Epoch: 35 , batch: 127 , training loss: 3.782463
[INFO] Epoch: 35 , batch: 128 , training loss: 3.888688
[INFO] Epoch: 35 , batch: 129 , training loss: 3.894856
[INFO] Epoch: 35 , batch: 130 , training loss: 3.865741
[INFO] Epoch: 35 , batch: 131 , training loss: 3.882648
[INFO] Epoch: 35 , batch: 132 , training loss: 3.865545
[INFO] Epoch: 35 , batch: 133 , training loss: 3.862647
[INFO] Epoch: 35 , batch: 134 , training loss: 3.639873
[INFO] Epoch: 35 , batch: 135 , training loss: 3.697726
[INFO] Epoch: 35 , batch: 136 , training loss: 3.973130
[INFO] Epoch: 35 , batch: 137 , training loss: 3.891662
[INFO] Epoch: 35 , batch: 138 , training loss: 3.939564
[INFO] Epoch: 35 , batch: 139 , training loss: 4.519390
[INFO] Epoch: 35 , batch: 140 , training loss: 4.261250
[INFO] Epoch: 35 , batch: 141 , training loss: 4.068811
[INFO] Epoch: 35 , batch: 142 , training loss: 3.826442
[INFO] Epoch: 35 , batch: 143 , training loss: 3.922739
[INFO] Epoch: 35 , batch: 144 , training loss: 3.755791
[INFO] Epoch: 35 , batch: 145 , training loss: 3.815318
[INFO] Epoch: 35 , batch: 146 , training loss: 4.010792
[INFO] Epoch: 35 , batch: 147 , training loss: 3.713058
[INFO] Epoch: 35 , batch: 148 , training loss: 3.692576
[INFO] Epoch: 35 , batch: 149 , training loss: 3.812098
[INFO] Epoch: 35 , batch: 150 , training loss: 3.984699
[INFO] Epoch: 35 , batch: 151 , training loss: 3.859266
[INFO] Epoch: 35 , batch: 152 , training loss: 3.952267
[INFO] Epoch: 35 , batch: 153 , training loss: 3.907690
[INFO] Epoch: 35 , batch: 154 , training loss: 4.006582
[INFO] Epoch: 35 , batch: 155 , training loss: 4.227746
[INFO] Epoch: 35 , batch: 156 , training loss: 3.928587
[INFO] Epoch: 35 , batch: 157 , training loss: 3.907537
[INFO] Epoch: 35 , batch: 158 , training loss: 4.021620
[INFO] Epoch: 35 , batch: 159 , training loss: 3.924749
[INFO] Epoch: 35 , batch: 160 , training loss: 4.214130
[INFO] Epoch: 35 , batch: 161 , training loss: 4.329558
[INFO] Epoch: 35 , batch: 162 , training loss: 4.310101
[INFO] Epoch: 35 , batch: 163 , training loss: 4.453999
[INFO] Epoch: 35 , batch: 164 , training loss: 4.409055
[INFO] Epoch: 35 , batch: 165 , training loss: 4.309859
[INFO] Epoch: 35 , batch: 166 , training loss: 4.251975
[INFO] Epoch: 35 , batch: 167 , training loss: 4.180583
[INFO] Epoch: 35 , batch: 168 , training loss: 3.914718
[INFO] Epoch: 35 , batch: 169 , training loss: 3.878195
[INFO] Epoch: 35 , batch: 170 , training loss: 4.100529
[INFO] Epoch: 35 , batch: 171 , training loss: 3.500117
[INFO] Epoch: 35 , batch: 172 , training loss: 3.713307
[INFO] Epoch: 35 , batch: 173 , training loss: 4.085030
[INFO] Epoch: 35 , batch: 174 , training loss: 4.507161
[INFO] Epoch: 35 , batch: 175 , training loss: 4.832935
[INFO] Epoch: 35 , batch: 176 , training loss: 4.486538
[INFO] Epoch: 35 , batch: 177 , training loss: 4.118701
[INFO] Epoch: 35 , batch: 178 , training loss: 4.090687
[INFO] Epoch: 35 , batch: 179 , training loss: 4.170009
[INFO] Epoch: 35 , batch: 180 , training loss: 4.117429
[INFO] Epoch: 35 , batch: 181 , training loss: 4.390926
[INFO] Epoch: 35 , batch: 182 , training loss: 4.355568
[INFO] Epoch: 35 , batch: 183 , training loss: 4.311574
[INFO] Epoch: 35 , batch: 184 , training loss: 4.200208
[INFO] Epoch: 35 , batch: 185 , training loss: 4.146132
[INFO] Epoch: 35 , batch: 186 , training loss: 4.330546
[INFO] Epoch: 35 , batch: 187 , training loss: 4.410263
[INFO] Epoch: 35 , batch: 188 , training loss: 4.422426
[INFO] Epoch: 35 , batch: 189 , training loss: 4.321386
[INFO] Epoch: 35 , batch: 190 , training loss: 4.325119
[INFO] Epoch: 35 , batch: 191 , training loss: 4.421398
[INFO] Epoch: 35 , batch: 192 , training loss: 4.261373
[INFO] Epoch: 35 , batch: 193 , training loss: 4.345038
[INFO] Epoch: 35 , batch: 194 , training loss: 4.292833
[INFO] Epoch: 35 , batch: 195 , training loss: 4.212387
[INFO] Epoch: 35 , batch: 196 , training loss: 4.089224
[INFO] Epoch: 35 , batch: 197 , training loss: 4.190335
[INFO] Epoch: 35 , batch: 198 , training loss: 4.113367
[INFO] Epoch: 35 , batch: 199 , training loss: 4.248039
[INFO] Epoch: 35 , batch: 200 , training loss: 4.145409
[INFO] Epoch: 35 , batch: 201 , training loss: 4.085222
[INFO] Epoch: 35 , batch: 202 , training loss: 4.049147
[INFO] Epoch: 35 , batch: 203 , training loss: 4.149053
[INFO] Epoch: 35 , batch: 204 , training loss: 4.297527
[INFO] Epoch: 35 , batch: 205 , training loss: 3.871706
[INFO] Epoch: 35 , batch: 206 , training loss: 3.814658
[INFO] Epoch: 35 , batch: 207 , training loss: 3.800439
[INFO] Epoch: 35 , batch: 208 , training loss: 4.130629
[INFO] Epoch: 35 , batch: 209 , training loss: 4.066276
[INFO] Epoch: 35 , batch: 210 , training loss: 4.086473
[INFO] Epoch: 35 , batch: 211 , training loss: 4.065204
[INFO] Epoch: 35 , batch: 212 , training loss: 4.200316
[INFO] Epoch: 35 , batch: 213 , training loss: 4.135766
[INFO] Epoch: 35 , batch: 214 , training loss: 4.233780
[INFO] Epoch: 35 , batch: 215 , training loss: 4.432084
[INFO] Epoch: 35 , batch: 216 , training loss: 4.139262
[INFO] Epoch: 35 , batch: 217 , training loss: 4.099181
[INFO] Epoch: 35 , batch: 218 , training loss: 4.088115
[INFO] Epoch: 35 , batch: 219 , training loss: 4.185403
[INFO] Epoch: 35 , batch: 220 , training loss: 3.995989
[INFO] Epoch: 35 , batch: 221 , training loss: 4.022073
[INFO] Epoch: 35 , batch: 222 , training loss: 4.182556
[INFO] Epoch: 35 , batch: 223 , training loss: 4.259186
[INFO] Epoch: 35 , batch: 224 , training loss: 4.289298
[INFO] Epoch: 35 , batch: 225 , training loss: 4.189575
[INFO] Epoch: 35 , batch: 226 , training loss: 4.292322
[INFO] Epoch: 35 , batch: 227 , training loss: 4.272291
[INFO] Epoch: 35 , batch: 228 , training loss: 4.333258
[INFO] Epoch: 35 , batch: 229 , training loss: 4.171141
[INFO] Epoch: 35 , batch: 230 , training loss: 4.050199
[INFO] Epoch: 35 , batch: 231 , training loss: 3.901379
[INFO] Epoch: 35 , batch: 232 , training loss: 4.051359
[INFO] Epoch: 35 , batch: 233 , training loss: 4.058030
[INFO] Epoch: 35 , batch: 234 , training loss: 3.773095
[INFO] Epoch: 35 , batch: 235 , training loss: 3.862339
[INFO] Epoch: 35 , batch: 236 , training loss: 3.980387
[INFO] Epoch: 35 , batch: 237 , training loss: 4.185669
[INFO] Epoch: 35 , batch: 238 , training loss: 3.955248
[INFO] Epoch: 35 , batch: 239 , training loss: 3.989376
[INFO] Epoch: 35 , batch: 240 , training loss: 4.048380
[INFO] Epoch: 35 , batch: 241 , training loss: 3.864759
[INFO] Epoch: 35 , batch: 242 , training loss: 3.894424
[INFO] Epoch: 35 , batch: 243 , training loss: 4.198145
[INFO] Epoch: 35 , batch: 244 , training loss: 4.103490
[INFO] Epoch: 35 , batch: 245 , training loss: 4.088866
[INFO] Epoch: 35 , batch: 246 , training loss: 3.799228
[INFO] Epoch: 35 , batch: 247 , training loss: 3.946486
[INFO] Epoch: 35 , batch: 248 , training loss: 4.038565
[INFO] Epoch: 35 , batch: 249 , training loss: 4.057784
[INFO] Epoch: 35 , batch: 250 , training loss: 3.794473
[INFO] Epoch: 35 , batch: 251 , training loss: 4.240705
[INFO] Epoch: 35 , batch: 252 , training loss: 3.944901
[INFO] Epoch: 35 , batch: 253 , training loss: 3.884219
[INFO] Epoch: 35 , batch: 254 , training loss: 4.154229
[INFO] Epoch: 35 , batch: 255 , training loss: 4.103564
[INFO] Epoch: 35 , batch: 256 , training loss: 4.131677
[INFO] Epoch: 35 , batch: 257 , training loss: 4.272739
[INFO] Epoch: 35 , batch: 258 , training loss: 4.319602
[INFO] Epoch: 35 , batch: 259 , training loss: 4.352766
[INFO] Epoch: 35 , batch: 260 , training loss: 4.088675
[INFO] Epoch: 35 , batch: 261 , training loss: 4.226591
[INFO] Epoch: 35 , batch: 262 , training loss: 4.412340
[INFO] Epoch: 35 , batch: 263 , training loss: 4.610489
[INFO] Epoch: 35 , batch: 264 , training loss: 3.899786
[INFO] Epoch: 35 , batch: 265 , training loss: 4.047216
[INFO] Epoch: 35 , batch: 266 , training loss: 4.455479
[INFO] Epoch: 35 , batch: 267 , training loss: 4.227448
[INFO] Epoch: 35 , batch: 268 , training loss: 4.152713
[INFO] Epoch: 35 , batch: 269 , training loss: 4.130710
[INFO] Epoch: 35 , batch: 270 , training loss: 4.164734
[INFO] Epoch: 35 , batch: 271 , training loss: 4.209702
[INFO] Epoch: 35 , batch: 272 , training loss: 4.160976
[INFO] Epoch: 35 , batch: 273 , training loss: 4.200185
[INFO] Epoch: 35 , batch: 274 , training loss: 4.236262
[INFO] Epoch: 35 , batch: 275 , training loss: 4.127598
[INFO] Epoch: 35 , batch: 276 , training loss: 4.162615
[INFO] Epoch: 35 , batch: 277 , training loss: 4.343805
[INFO] Epoch: 35 , batch: 278 , training loss: 4.015749
[INFO] Epoch: 35 , batch: 279 , training loss: 4.012828
[INFO] Epoch: 35 , batch: 280 , training loss: 3.971172
[INFO] Epoch: 35 , batch: 281 , training loss: 4.122499
[INFO] Epoch: 35 , batch: 282 , training loss: 4.013919
[INFO] Epoch: 35 , batch: 283 , training loss: 4.066257
[INFO] Epoch: 35 , batch: 284 , training loss: 4.082970
[INFO] Epoch: 35 , batch: 285 , training loss: 4.001044
[INFO] Epoch: 35 , batch: 286 , training loss: 4.011640
[INFO] Epoch: 35 , batch: 287 , training loss: 3.953529
[INFO] Epoch: 35 , batch: 288 , training loss: 3.959131
[INFO] Epoch: 35 , batch: 289 , training loss: 3.992000
[INFO] Epoch: 35 , batch: 290 , training loss: 3.799301
[INFO] Epoch: 35 , batch: 291 , training loss: 3.741190
[INFO] Epoch: 35 , batch: 292 , training loss: 3.899767
[INFO] Epoch: 35 , batch: 293 , training loss: 3.793073
[INFO] Epoch: 35 , batch: 294 , training loss: 4.452064
[INFO] Epoch: 35 , batch: 295 , training loss: 4.237348
[INFO] Epoch: 35 , batch: 296 , training loss: 4.153759
[INFO] Epoch: 35 , batch: 297 , training loss: 4.146349
[INFO] Epoch: 35 , batch: 298 , training loss: 3.965551
[INFO] Epoch: 35 , batch: 299 , training loss: 3.989108
[INFO] Epoch: 35 , batch: 300 , training loss: 3.955245
[INFO] Epoch: 35 , batch: 301 , training loss: 3.910890
[INFO] Epoch: 35 , batch: 302 , training loss: 4.068066
[INFO] Epoch: 35 , batch: 303 , training loss: 4.066869
[INFO] Epoch: 35 , batch: 304 , training loss: 4.240815
[INFO] Epoch: 35 , batch: 305 , training loss: 4.040664
[INFO] Epoch: 35 , batch: 306 , training loss: 4.166043
[INFO] Epoch: 35 , batch: 307 , training loss: 4.141491
[INFO] Epoch: 35 , batch: 308 , training loss: 3.999449
[INFO] Epoch: 35 , batch: 309 , training loss: 3.999408
[INFO] Epoch: 35 , batch: 310 , training loss: 3.916402
[INFO] Epoch: 35 , batch: 311 , training loss: 3.896579
[INFO] Epoch: 35 , batch: 312 , training loss: 3.833526
[INFO] Epoch: 35 , batch: 313 , training loss: 3.953966
[INFO] Epoch: 35 , batch: 314 , training loss: 4.000138
[INFO] Epoch: 35 , batch: 315 , training loss: 4.045222
[INFO] Epoch: 35 , batch: 316 , training loss: 4.293579
[INFO] Epoch: 35 , batch: 317 , training loss: 4.702569
[INFO] Epoch: 35 , batch: 318 , training loss: 4.860401
[INFO] Epoch: 35 , batch: 319 , training loss: 4.463348
[INFO] Epoch: 35 , batch: 320 , training loss: 4.000866
[INFO] Epoch: 35 , batch: 321 , training loss: 3.840860
[INFO] Epoch: 35 , batch: 322 , training loss: 3.969089
[INFO] Epoch: 35 , batch: 323 , training loss: 3.987976
[INFO] Epoch: 35 , batch: 324 , training loss: 3.947150
[INFO] Epoch: 35 , batch: 325 , training loss: 4.104561
[INFO] Epoch: 35 , batch: 326 , training loss: 4.168840
[INFO] Epoch: 35 , batch: 327 , training loss: 4.062982
[INFO] Epoch: 35 , batch: 328 , training loss: 4.046380
[INFO] Epoch: 35 , batch: 329 , training loss: 3.980121
[INFO] Epoch: 35 , batch: 330 , training loss: 3.984864
[INFO] Epoch: 35 , batch: 331 , training loss: 4.137651
[INFO] Epoch: 35 , batch: 332 , training loss: 3.933355
[INFO] Epoch: 35 , batch: 333 , training loss: 3.958052
[INFO] Epoch: 35 , batch: 334 , training loss: 3.950260
[INFO] Epoch: 35 , batch: 335 , training loss: 4.084161
[INFO] Epoch: 35 , batch: 336 , training loss: 4.110415
[INFO] Epoch: 35 , batch: 337 , training loss: 4.125630
[INFO] Epoch: 35 , batch: 338 , training loss: 4.337198
[INFO] Epoch: 35 , batch: 339 , training loss: 4.219866
[INFO] Epoch: 35 , batch: 340 , training loss: 4.337546
[INFO] Epoch: 35 , batch: 341 , training loss: 4.103735
[INFO] Epoch: 35 , batch: 342 , training loss: 3.877609
[INFO] Epoch: 35 , batch: 343 , training loss: 3.966521
[INFO] Epoch: 35 , batch: 344 , training loss: 3.821683
[INFO] Epoch: 35 , batch: 345 , training loss: 3.943842
[INFO] Epoch: 35 , batch: 346 , training loss: 3.992668
[INFO] Epoch: 35 , batch: 347 , training loss: 3.906998
[INFO] Epoch: 35 , batch: 348 , training loss: 4.018240
[INFO] Epoch: 35 , batch: 349 , training loss: 4.160531
[INFO] Epoch: 35 , batch: 350 , training loss: 3.940205
[INFO] Epoch: 35 , batch: 351 , training loss: 4.034590
[INFO] Epoch: 35 , batch: 352 , training loss: 4.036238
[INFO] Epoch: 35 , batch: 353 , training loss: 3.999157
[INFO] Epoch: 35 , batch: 354 , training loss: 4.118563
[INFO] Epoch: 35 , batch: 355 , training loss: 4.148053
[INFO] Epoch: 35 , batch: 356 , training loss: 4.005225
[INFO] Epoch: 35 , batch: 357 , training loss: 4.072362
[INFO] Epoch: 35 , batch: 358 , training loss: 3.948872
[INFO] Epoch: 35 , batch: 359 , training loss: 3.984632
[INFO] Epoch: 35 , batch: 360 , training loss: 4.082482
[INFO] Epoch: 35 , batch: 361 , training loss: 4.032136
[INFO] Epoch: 35 , batch: 362 , training loss: 4.136248
[INFO] Epoch: 35 , batch: 363 , training loss: 4.013250
[INFO] Epoch: 35 , batch: 364 , training loss: 4.054286
[INFO] Epoch: 35 , batch: 365 , training loss: 4.016329
[INFO] Epoch: 35 , batch: 366 , training loss: 4.126660
[INFO] Epoch: 35 , batch: 367 , training loss: 4.198072
[INFO] Epoch: 35 , batch: 368 , training loss: 4.601921
[INFO] Epoch: 35 , batch: 369 , training loss: 4.236645
[INFO] Epoch: 35 , batch: 370 , training loss: 4.009256
[INFO] Epoch: 35 , batch: 371 , training loss: 4.403878
[INFO] Epoch: 35 , batch: 372 , training loss: 4.680672
[INFO] Epoch: 35 , batch: 373 , training loss: 4.761938
[INFO] Epoch: 35 , batch: 374 , training loss: 4.840412
[INFO] Epoch: 35 , batch: 375 , training loss: 4.822722
[INFO] Epoch: 35 , batch: 376 , training loss: 4.742212
[INFO] Epoch: 35 , batch: 377 , training loss: 4.528830
[INFO] Epoch: 35 , batch: 378 , training loss: 4.616855
[INFO] Epoch: 35 , batch: 379 , training loss: 4.589830
[INFO] Epoch: 35 , batch: 380 , training loss: 4.706768
[INFO] Epoch: 35 , batch: 381 , training loss: 4.429010
[INFO] Epoch: 35 , batch: 382 , training loss: 4.667174
[INFO] Epoch: 35 , batch: 383 , training loss: 4.657799
[INFO] Epoch: 35 , batch: 384 , training loss: 4.692286
[INFO] Epoch: 35 , batch: 385 , training loss: 4.392680
[INFO] Epoch: 35 , batch: 386 , training loss: 4.675036
[INFO] Epoch: 35 , batch: 387 , training loss: 4.633497
[INFO] Epoch: 35 , batch: 388 , training loss: 4.444562
[INFO] Epoch: 35 , batch: 389 , training loss: 4.283282
[INFO] Epoch: 35 , batch: 390 , training loss: 4.276053
[INFO] Epoch: 35 , batch: 391 , training loss: 4.332644
[INFO] Epoch: 35 , batch: 392 , training loss: 4.674588
[INFO] Epoch: 35 , batch: 393 , training loss: 4.537497
[INFO] Epoch: 35 , batch: 394 , training loss: 4.613286
[INFO] Epoch: 35 , batch: 395 , training loss: 4.485700
[INFO] Epoch: 35 , batch: 396 , training loss: 4.234139
[INFO] Epoch: 35 , batch: 397 , training loss: 4.411483
[INFO] Epoch: 35 , batch: 398 , training loss: 4.262618
[INFO] Epoch: 35 , batch: 399 , training loss: 4.329555
[INFO] Epoch: 35 , batch: 400 , training loss: 4.307701
[INFO] Epoch: 35 , batch: 401 , training loss: 4.703716
[INFO] Epoch: 35 , batch: 402 , training loss: 4.442427
[INFO] Epoch: 35 , batch: 403 , training loss: 4.254078
[INFO] Epoch: 35 , batch: 404 , training loss: 4.435968
[INFO] Epoch: 35 , batch: 405 , training loss: 4.483784
[INFO] Epoch: 35 , batch: 406 , training loss: 4.380671
[INFO] Epoch: 35 , batch: 407 , training loss: 4.450554
[INFO] Epoch: 35 , batch: 408 , training loss: 4.414490
[INFO] Epoch: 35 , batch: 409 , training loss: 4.387687
[INFO] Epoch: 35 , batch: 410 , training loss: 4.440850
[INFO] Epoch: 35 , batch: 411 , training loss: 4.617684
[INFO] Epoch: 35 , batch: 412 , training loss: 4.475777
[INFO] Epoch: 35 , batch: 413 , training loss: 4.356415
[INFO] Epoch: 35 , batch: 414 , training loss: 4.380990
[INFO] Epoch: 35 , batch: 415 , training loss: 4.405585
[INFO] Epoch: 35 , batch: 416 , training loss: 4.495885
[INFO] Epoch: 35 , batch: 417 , training loss: 4.403845
[INFO] Epoch: 35 , batch: 418 , training loss: 4.409645
[INFO] Epoch: 35 , batch: 419 , training loss: 4.404806
[INFO] Epoch: 35 , batch: 420 , training loss: 4.366850
[INFO] Epoch: 35 , batch: 421 , training loss: 4.353495
[INFO] Epoch: 35 , batch: 422 , training loss: 4.222762
[INFO] Epoch: 35 , batch: 423 , training loss: 4.431118
[INFO] Epoch: 35 , batch: 424 , training loss: 4.613279
[INFO] Epoch: 35 , batch: 425 , training loss: 4.462738
[INFO] Epoch: 35 , batch: 426 , training loss: 4.203200
[INFO] Epoch: 35 , batch: 427 , training loss: 4.433177
[INFO] Epoch: 35 , batch: 428 , training loss: 4.298033
[INFO] Epoch: 35 , batch: 429 , training loss: 4.175787
[INFO] Epoch: 35 , batch: 430 , training loss: 4.451064
[INFO] Epoch: 35 , batch: 431 , training loss: 4.046413
[INFO] Epoch: 35 , batch: 432 , training loss: 4.106864
[INFO] Epoch: 35 , batch: 433 , training loss: 4.146947
[INFO] Epoch: 35 , batch: 434 , training loss: 4.013310
[INFO] Epoch: 35 , batch: 435 , training loss: 4.383643
[INFO] Epoch: 35 , batch: 436 , training loss: 4.447068
[INFO] Epoch: 35 , batch: 437 , training loss: 4.203647
[INFO] Epoch: 35 , batch: 438 , training loss: 4.062552
[INFO] Epoch: 35 , batch: 439 , training loss: 4.270350
[INFO] Epoch: 35 , batch: 440 , training loss: 4.387921
[INFO] Epoch: 35 , batch: 441 , training loss: 4.512368
[INFO] Epoch: 35 , batch: 442 , training loss: 4.268619
[INFO] Epoch: 35 , batch: 443 , training loss: 4.468870
[INFO] Epoch: 35 , batch: 444 , training loss: 4.097249
[INFO] Epoch: 35 , batch: 445 , training loss: 3.999327
[INFO] Epoch: 35 , batch: 446 , training loss: 3.933893
[INFO] Epoch: 35 , batch: 447 , training loss: 4.126763
[INFO] Epoch: 35 , batch: 448 , training loss: 4.225741
[INFO] Epoch: 35 , batch: 449 , training loss: 4.618405
[INFO] Epoch: 35 , batch: 450 , training loss: 4.673135
[INFO] Epoch: 35 , batch: 451 , training loss: 4.573442
[INFO] Epoch: 35 , batch: 452 , training loss: 4.387835
[INFO] Epoch: 35 , batch: 453 , training loss: 4.154098
[INFO] Epoch: 35 , batch: 454 , training loss: 4.292608
[INFO] Epoch: 35 , batch: 455 , training loss: 4.338601
[INFO] Epoch: 35 , batch: 456 , training loss: 4.348690
[INFO] Epoch: 35 , batch: 457 , training loss: 4.433801
[INFO] Epoch: 35 , batch: 458 , training loss: 4.177857
[INFO] Epoch: 35 , batch: 459 , training loss: 4.164279
[INFO] Epoch: 35 , batch: 460 , training loss: 4.237544
[INFO] Epoch: 35 , batch: 461 , training loss: 4.226746
[INFO] Epoch: 35 , batch: 462 , training loss: 4.295950
[INFO] Epoch: 35 , batch: 463 , training loss: 4.203532
[INFO] Epoch: 35 , batch: 464 , training loss: 4.383320
[INFO] Epoch: 35 , batch: 465 , training loss: 4.316412
[INFO] Epoch: 35 , batch: 466 , training loss: 4.401841
[INFO] Epoch: 35 , batch: 467 , training loss: 4.376800
[INFO] Epoch: 35 , batch: 468 , training loss: 4.352187
[INFO] Epoch: 35 , batch: 469 , training loss: 4.360895
[INFO] Epoch: 35 , batch: 470 , training loss: 4.198745
[INFO] Epoch: 35 , batch: 471 , training loss: 4.273439
[INFO] Epoch: 35 , batch: 472 , training loss: 4.301736
[INFO] Epoch: 35 , batch: 473 , training loss: 4.245461
[INFO] Epoch: 35 , batch: 474 , training loss: 4.051519
[INFO] Epoch: 35 , batch: 475 , training loss: 3.934059
[INFO] Epoch: 35 , batch: 476 , training loss: 4.305738
[INFO] Epoch: 35 , batch: 477 , training loss: 4.441511
[INFO] Epoch: 35 , batch: 478 , training loss: 4.458675
[INFO] Epoch: 35 , batch: 479 , training loss: 4.426427
[INFO] Epoch: 35 , batch: 480 , training loss: 4.515887
[INFO] Epoch: 35 , batch: 481 , training loss: 4.400310
[INFO] Epoch: 35 , batch: 482 , training loss: 4.522209
[INFO] Epoch: 35 , batch: 483 , training loss: 4.373481
[INFO] Epoch: 35 , batch: 484 , training loss: 4.189151
[INFO] Epoch: 35 , batch: 485 , training loss: 4.265245
[INFO] Epoch: 35 , batch: 486 , training loss: 4.173198
[INFO] Epoch: 35 , batch: 487 , training loss: 4.160331
[INFO] Epoch: 35 , batch: 488 , training loss: 4.346160
[INFO] Epoch: 35 , batch: 489 , training loss: 4.231614
[INFO] Epoch: 35 , batch: 490 , training loss: 4.276043
[INFO] Epoch: 35 , batch: 491 , training loss: 4.220373
[INFO] Epoch: 35 , batch: 492 , training loss: 4.164264
[INFO] Epoch: 35 , batch: 493 , training loss: 4.370980
[INFO] Epoch: 35 , batch: 494 , training loss: 4.274993
[INFO] Epoch: 35 , batch: 495 , training loss: 4.403521
[INFO] Epoch: 35 , batch: 496 , training loss: 4.282771
[INFO] Epoch: 35 , batch: 497 , training loss: 4.339765
[INFO] Epoch: 35 , batch: 498 , training loss: 4.332488
[INFO] Epoch: 35 , batch: 499 , training loss: 4.380265
[INFO] Epoch: 35 , batch: 500 , training loss: 4.541024
[INFO] Epoch: 35 , batch: 501 , training loss: 4.848591
[INFO] Epoch: 35 , batch: 502 , training loss: 4.893407
[INFO] Epoch: 35 , batch: 503 , training loss: 4.580989
[INFO] Epoch: 35 , batch: 504 , training loss: 4.700803
[INFO] Epoch: 35 , batch: 505 , training loss: 4.678503
[INFO] Epoch: 35 , batch: 506 , training loss: 4.641076
[INFO] Epoch: 35 , batch: 507 , training loss: 4.716475
[INFO] Epoch: 35 , batch: 508 , training loss: 4.659228
[INFO] Epoch: 35 , batch: 509 , training loss: 4.446608
[INFO] Epoch: 35 , batch: 510 , training loss: 4.516757
[INFO] Epoch: 35 , batch: 511 , training loss: 4.428830
[INFO] Epoch: 35 , batch: 512 , training loss: 4.531627
[INFO] Epoch: 35 , batch: 513 , training loss: 4.764369
[INFO] Epoch: 35 , batch: 514 , training loss: 4.436283
[INFO] Epoch: 35 , batch: 515 , training loss: 4.643334
[INFO] Epoch: 35 , batch: 516 , training loss: 4.478811
[INFO] Epoch: 35 , batch: 517 , training loss: 4.434040
[INFO] Epoch: 35 , batch: 518 , training loss: 4.398325
[INFO] Epoch: 35 , batch: 519 , training loss: 4.256407
[INFO] Epoch: 35 , batch: 520 , training loss: 4.493953
[INFO] Epoch: 35 , batch: 521 , training loss: 4.451402
[INFO] Epoch: 35 , batch: 522 , training loss: 4.508080
[INFO] Epoch: 35 , batch: 523 , training loss: 4.426102
[INFO] Epoch: 35 , batch: 524 , training loss: 4.716999
[INFO] Epoch: 35 , batch: 525 , training loss: 4.653893
[INFO] Epoch: 35 , batch: 526 , training loss: 4.417338
[INFO] Epoch: 35 , batch: 527 , training loss: 4.452386
[INFO] Epoch: 35 , batch: 528 , training loss: 4.466742
[INFO] Epoch: 35 , batch: 529 , training loss: 4.441876
[INFO] Epoch: 35 , batch: 530 , training loss: 4.287941
[INFO] Epoch: 35 , batch: 531 , training loss: 4.434506
[INFO] Epoch: 35 , batch: 532 , training loss: 4.331746
[INFO] Epoch: 35 , batch: 533 , training loss: 4.474538
[INFO] Epoch: 35 , batch: 534 , training loss: 4.452970
[INFO] Epoch: 35 , batch: 535 , training loss: 4.482517
[INFO] Epoch: 35 , batch: 536 , training loss: 4.302962
[INFO] Epoch: 35 , batch: 537 , training loss: 4.296995
[INFO] Epoch: 35 , batch: 538 , training loss: 4.385893
[INFO] Epoch: 35 , batch: 539 , training loss: 4.494914
[INFO] Epoch: 35 , batch: 540 , training loss: 4.954763
[INFO] Epoch: 35 , batch: 541 , training loss: 4.844604
[INFO] Epoch: 35 , batch: 542 , training loss: 4.769231
[INFO] Epoch: 36 , batch: 0 , training loss: 3.347697
[INFO] Epoch: 36 , batch: 1 , training loss: 3.435604
[INFO] Epoch: 36 , batch: 2 , training loss: 3.597844
[INFO] Epoch: 36 , batch: 3 , training loss: 3.390657
[INFO] Epoch: 36 , batch: 4 , training loss: 3.825344
[INFO] Epoch: 36 , batch: 5 , training loss: 3.465655
[INFO] Epoch: 36 , batch: 6 , training loss: 3.832430
[INFO] Epoch: 36 , batch: 7 , training loss: 3.778607
[INFO] Epoch: 36 , batch: 8 , training loss: 3.456670
[INFO] Epoch: 36 , batch: 9 , training loss: 3.727149
[INFO] Epoch: 36 , batch: 10 , training loss: 3.708458
[INFO] Epoch: 36 , batch: 11 , training loss: 3.649401
[INFO] Epoch: 36 , batch: 12 , training loss: 3.555494
[INFO] Epoch: 36 , batch: 13 , training loss: 3.618820
[INFO] Epoch: 36 , batch: 14 , training loss: 3.460420
[INFO] Epoch: 36 , batch: 15 , training loss: 3.660965
[INFO] Epoch: 36 , batch: 16 , training loss: 3.520104
[INFO] Epoch: 36 , batch: 17 , training loss: 3.719270
[INFO] Epoch: 36 , batch: 18 , training loss: 3.638146
[INFO] Epoch: 36 , batch: 19 , training loss: 3.378766
[INFO] Epoch: 36 , batch: 20 , training loss: 3.351807
[INFO] Epoch: 36 , batch: 21 , training loss: 3.495490
[INFO] Epoch: 36 , batch: 22 , training loss: 3.358982
[INFO] Epoch: 36 , batch: 23 , training loss: 3.605147
[INFO] Epoch: 36 , batch: 24 , training loss: 3.367102
[INFO] Epoch: 36 , batch: 25 , training loss: 3.613070
[INFO] Epoch: 36 , batch: 26 , training loss: 3.439587
[INFO] Epoch: 36 , batch: 27 , training loss: 3.410178
[INFO] Epoch: 36 , batch: 28 , training loss: 3.576181
[INFO] Epoch: 36 , batch: 29 , training loss: 3.408948
[INFO] Epoch: 36 , batch: 30 , training loss: 3.460304
[INFO] Epoch: 36 , batch: 31 , training loss: 3.488965
[INFO] Epoch: 36 , batch: 32 , training loss: 3.486373
[INFO] Epoch: 36 , batch: 33 , training loss: 3.508884
[INFO] Epoch: 36 , batch: 34 , training loss: 3.503495
[INFO] Epoch: 36 , batch: 35 , training loss: 3.476111
[INFO] Epoch: 36 , batch: 36 , training loss: 3.510205
[INFO] Epoch: 36 , batch: 37 , training loss: 3.445450
[INFO] Epoch: 36 , batch: 38 , training loss: 3.463630
[INFO] Epoch: 36 , batch: 39 , training loss: 3.352700
[INFO] Epoch: 36 , batch: 40 , training loss: 3.561587
[INFO] Epoch: 36 , batch: 41 , training loss: 3.457449
[INFO] Epoch: 36 , batch: 42 , training loss: 3.835585
[INFO] Epoch: 36 , batch: 43 , training loss: 3.591193
[INFO] Epoch: 36 , batch: 44 , training loss: 4.000857
[INFO] Epoch: 36 , batch: 45 , training loss: 3.850290
[INFO] Epoch: 36 , batch: 46 , training loss: 3.860105
[INFO] Epoch: 36 , batch: 47 , training loss: 3.548938
[INFO] Epoch: 36 , batch: 48 , training loss: 3.547966
[INFO] Epoch: 36 , batch: 49 , training loss: 3.748870
[INFO] Epoch: 36 , batch: 50 , training loss: 3.585967
[INFO] Epoch: 36 , batch: 51 , training loss: 3.759110
[INFO] Epoch: 36 , batch: 52 , training loss: 3.625926
[INFO] Epoch: 36 , batch: 53 , training loss: 3.769064
[INFO] Epoch: 36 , batch: 54 , training loss: 3.722019
[INFO] Epoch: 36 , batch: 55 , training loss: 3.864420
[INFO] Epoch: 36 , batch: 56 , training loss: 3.713266
[INFO] Epoch: 36 , batch: 57 , training loss: 3.588247
[INFO] Epoch: 36 , batch: 58 , training loss: 3.674422
[INFO] Epoch: 36 , batch: 59 , training loss: 3.737891
[INFO] Epoch: 36 , batch: 60 , training loss: 3.698116
[INFO] Epoch: 36 , batch: 61 , training loss: 3.773993
[INFO] Epoch: 36 , batch: 62 , training loss: 3.646428
[INFO] Epoch: 36 , batch: 63 , training loss: 3.847542
[INFO] Epoch: 36 , batch: 64 , training loss: 4.006035
[INFO] Epoch: 36 , batch: 65 , training loss: 3.739501
[INFO] Epoch: 36 , batch: 66 , training loss: 3.581214
[INFO] Epoch: 36 , batch: 67 , training loss: 3.688413
[INFO] Epoch: 36 , batch: 68 , training loss: 3.819391
[INFO] Epoch: 36 , batch: 69 , training loss: 3.707792
[INFO] Epoch: 36 , batch: 70 , training loss: 3.915961
[INFO] Epoch: 36 , batch: 71 , training loss: 3.795275
[INFO] Epoch: 36 , batch: 72 , training loss: 3.836766
[INFO] Epoch: 36 , batch: 73 , training loss: 3.786218
[INFO] Epoch: 36 , batch: 74 , training loss: 3.877851
[INFO] Epoch: 36 , batch: 75 , training loss: 3.802639
[INFO] Epoch: 36 , batch: 76 , training loss: 3.835669
[INFO] Epoch: 36 , batch: 77 , training loss: 3.820398
[INFO] Epoch: 36 , batch: 78 , training loss: 3.933818
[INFO] Epoch: 36 , batch: 79 , training loss: 3.738476
[INFO] Epoch: 36 , batch: 80 , training loss: 3.934592
[INFO] Epoch: 36 , batch: 81 , training loss: 3.876712
[INFO] Epoch: 36 , batch: 82 , training loss: 3.858477
[INFO] Epoch: 36 , batch: 83 , training loss: 3.982785
[INFO] Epoch: 36 , batch: 84 , training loss: 3.903476
[INFO] Epoch: 36 , batch: 85 , training loss: 3.996249
[INFO] Epoch: 36 , batch: 86 , training loss: 3.933254
[INFO] Epoch: 36 , batch: 87 , training loss: 3.890075
[INFO] Epoch: 36 , batch: 88 , training loss: 4.011305
[INFO] Epoch: 36 , batch: 89 , training loss: 3.820532
[INFO] Epoch: 36 , batch: 90 , training loss: 3.905237
[INFO] Epoch: 36 , batch: 91 , training loss: 3.850874
[INFO] Epoch: 36 , batch: 92 , training loss: 3.837346
[INFO] Epoch: 36 , batch: 93 , training loss: 3.974612
[INFO] Epoch: 36 , batch: 94 , training loss: 4.127780
[INFO] Epoch: 36 , batch: 95 , training loss: 3.838777
[INFO] Epoch: 36 , batch: 96 , training loss: 3.887276
[INFO] Epoch: 36 , batch: 97 , training loss: 3.784104
[INFO] Epoch: 36 , batch: 98 , training loss: 3.729766
[INFO] Epoch: 36 , batch: 99 , training loss: 3.847342
[INFO] Epoch: 36 , batch: 100 , training loss: 3.762980
[INFO] Epoch: 36 , batch: 101 , training loss: 3.821667
[INFO] Epoch: 36 , batch: 102 , training loss: 3.919386
[INFO] Epoch: 36 , batch: 103 , training loss: 3.713101
[INFO] Epoch: 36 , batch: 104 , training loss: 3.666686
[INFO] Epoch: 36 , batch: 105 , training loss: 3.943955
[INFO] Epoch: 36 , batch: 106 , training loss: 3.912530
[INFO] Epoch: 36 , batch: 107 , training loss: 3.776786
[INFO] Epoch: 36 , batch: 108 , training loss: 3.701586
[INFO] Epoch: 36 , batch: 109 , training loss: 3.654314
[INFO] Epoch: 36 , batch: 110 , training loss: 3.832370
[INFO] Epoch: 36 , batch: 111 , training loss: 3.893043
[INFO] Epoch: 36 , batch: 112 , training loss: 3.809862
[INFO] Epoch: 36 , batch: 113 , training loss: 3.837297
[INFO] Epoch: 36 , batch: 114 , training loss: 3.803888
[INFO] Epoch: 36 , batch: 115 , training loss: 3.827486
[INFO] Epoch: 36 , batch: 116 , training loss: 3.738855
[INFO] Epoch: 36 , batch: 117 , training loss: 3.937817
[INFO] Epoch: 36 , batch: 118 , training loss: 3.919887
[INFO] Epoch: 36 , batch: 119 , training loss: 4.040043
[INFO] Epoch: 36 , batch: 120 , training loss: 4.055295
[INFO] Epoch: 36 , batch: 121 , training loss: 3.921066
[INFO] Epoch: 36 , batch: 122 , training loss: 3.816483
[INFO] Epoch: 36 , batch: 123 , training loss: 3.826768
[INFO] Epoch: 36 , batch: 124 , training loss: 3.933185
[INFO] Epoch: 36 , batch: 125 , training loss: 3.770993
[INFO] Epoch: 36 , batch: 126 , training loss: 3.753430
[INFO] Epoch: 36 , batch: 127 , training loss: 3.743036
[INFO] Epoch: 36 , batch: 128 , training loss: 3.883446
[INFO] Epoch: 36 , batch: 129 , training loss: 3.872672
[INFO] Epoch: 36 , batch: 130 , training loss: 3.841764
[INFO] Epoch: 36 , batch: 131 , training loss: 3.877450
[INFO] Epoch: 36 , batch: 132 , training loss: 3.856307
[INFO] Epoch: 36 , batch: 133 , training loss: 3.811893
[INFO] Epoch: 36 , batch: 134 , training loss: 3.644422
[INFO] Epoch: 36 , batch: 135 , training loss: 3.668944
[INFO] Epoch: 36 , batch: 136 , training loss: 3.937131
[INFO] Epoch: 36 , batch: 137 , training loss: 3.867945
[INFO] Epoch: 36 , batch: 138 , training loss: 3.915140
[INFO] Epoch: 36 , batch: 139 , training loss: 4.523641
[INFO] Epoch: 36 , batch: 140 , training loss: 4.232519
[INFO] Epoch: 36 , batch: 141 , training loss: 4.003197
[INFO] Epoch: 36 , batch: 142 , training loss: 3.810484
[INFO] Epoch: 36 , batch: 143 , training loss: 3.908244
[INFO] Epoch: 36 , batch: 144 , training loss: 3.739303
[INFO] Epoch: 36 , batch: 145 , training loss: 3.792232
[INFO] Epoch: 36 , batch: 146 , training loss: 3.985747
[INFO] Epoch: 36 , batch: 147 , training loss: 3.695921
[INFO] Epoch: 36 , batch: 148 , training loss: 3.657533
[INFO] Epoch: 36 , batch: 149 , training loss: 3.771255
[INFO] Epoch: 36 , batch: 150 , training loss: 3.940361
[INFO] Epoch: 36 , batch: 151 , training loss: 3.862028
[INFO] Epoch: 36 , batch: 152 , training loss: 3.927808
[INFO] Epoch: 36 , batch: 153 , training loss: 3.869599
[INFO] Epoch: 36 , batch: 154 , training loss: 3.972082
[INFO] Epoch: 36 , batch: 155 , training loss: 4.199867
[INFO] Epoch: 36 , batch: 156 , training loss: 3.914306
[INFO] Epoch: 36 , batch: 157 , training loss: 3.895231
[INFO] Epoch: 36 , batch: 158 , training loss: 4.020838
[INFO] Epoch: 36 , batch: 159 , training loss: 3.879195
[INFO] Epoch: 36 , batch: 160 , training loss: 4.156399
[INFO] Epoch: 36 , batch: 161 , training loss: 4.293835
[INFO] Epoch: 36 , batch: 162 , training loss: 4.269687
[INFO] Epoch: 36 , batch: 163 , training loss: 4.398180
[INFO] Epoch: 36 , batch: 164 , training loss: 4.408859
[INFO] Epoch: 36 , batch: 165 , training loss: 4.309690
[INFO] Epoch: 36 , batch: 166 , training loss: 4.179526
[INFO] Epoch: 36 , batch: 167 , training loss: 4.112023
[INFO] Epoch: 36 , batch: 168 , training loss: 3.873912
[INFO] Epoch: 36 , batch: 169 , training loss: 3.850642
[INFO] Epoch: 36 , batch: 170 , training loss: 4.059456
[INFO] Epoch: 36 , batch: 171 , training loss: 3.455933
[INFO] Epoch: 36 , batch: 172 , training loss: 3.663400
[INFO] Epoch: 36 , batch: 173 , training loss: 4.064764
[INFO] Epoch: 36 , batch: 174 , training loss: 4.457261
[INFO] Epoch: 36 , batch: 175 , training loss: 4.807719
[INFO] Epoch: 36 , batch: 176 , training loss: 4.430850
[INFO] Epoch: 36 , batch: 177 , training loss: 4.082219
[INFO] Epoch: 36 , batch: 178 , training loss: 4.072406
[INFO] Epoch: 36 , batch: 179 , training loss: 4.119152
[INFO] Epoch: 36 , batch: 180 , training loss: 4.106322
[INFO] Epoch: 36 , batch: 181 , training loss: 4.376618
[INFO] Epoch: 36 , batch: 182 , training loss: 4.326882
[INFO] Epoch: 36 , batch: 183 , training loss: 4.307624
[INFO] Epoch: 36 , batch: 184 , training loss: 4.160056
[INFO] Epoch: 36 , batch: 185 , training loss: 4.151780
[INFO] Epoch: 36 , batch: 186 , training loss: 4.294777
[INFO] Epoch: 36 , batch: 187 , training loss: 4.399833
[INFO] Epoch: 36 , batch: 188 , training loss: 4.388026
[INFO] Epoch: 36 , batch: 189 , training loss: 4.299517
[INFO] Epoch: 36 , batch: 190 , training loss: 4.315757
[INFO] Epoch: 36 , batch: 191 , training loss: 4.410585
[INFO] Epoch: 36 , batch: 192 , training loss: 4.227214
[INFO] Epoch: 36 , batch: 193 , training loss: 4.328542
[INFO] Epoch: 36 , batch: 194 , training loss: 4.290760
[INFO] Epoch: 36 , batch: 195 , training loss: 4.180606
[INFO] Epoch: 36 , batch: 196 , training loss: 4.086835
[INFO] Epoch: 36 , batch: 197 , training loss: 4.197368
[INFO] Epoch: 36 , batch: 198 , training loss: 4.083699
[INFO] Epoch: 36 , batch: 199 , training loss: 4.233073
[INFO] Epoch: 36 , batch: 200 , training loss: 4.118835
[INFO] Epoch: 36 , batch: 201 , training loss: 4.044366
[INFO] Epoch: 36 , batch: 202 , training loss: 4.043013
[INFO] Epoch: 36 , batch: 203 , training loss: 4.148882
[INFO] Epoch: 36 , batch: 204 , training loss: 4.274371
[INFO] Epoch: 36 , batch: 205 , training loss: 3.846838
[INFO] Epoch: 36 , batch: 206 , training loss: 3.803838
[INFO] Epoch: 36 , batch: 207 , training loss: 3.798428
[INFO] Epoch: 36 , batch: 208 , training loss: 4.099400
[INFO] Epoch: 36 , batch: 209 , training loss: 4.051426
[INFO] Epoch: 36 , batch: 210 , training loss: 4.069815
[INFO] Epoch: 36 , batch: 211 , training loss: 4.095263
[INFO] Epoch: 36 , batch: 212 , training loss: 4.191447
[INFO] Epoch: 36 , batch: 213 , training loss: 4.113606
[INFO] Epoch: 36 , batch: 214 , training loss: 4.215402
[INFO] Epoch: 36 , batch: 215 , training loss: 4.426924
[INFO] Epoch: 36 , batch: 216 , training loss: 4.124012
[INFO] Epoch: 36 , batch: 217 , training loss: 4.093839
[INFO] Epoch: 36 , batch: 218 , training loss: 4.084797
[INFO] Epoch: 36 , batch: 219 , training loss: 4.181704
[INFO] Epoch: 36 , batch: 220 , training loss: 3.977867
[INFO] Epoch: 36 , batch: 221 , training loss: 4.019657
[INFO] Epoch: 36 , batch: 222 , training loss: 4.152539
[INFO] Epoch: 36 , batch: 223 , training loss: 4.245062
[INFO] Epoch: 36 , batch: 224 , training loss: 4.272770
[INFO] Epoch: 36 , batch: 225 , training loss: 4.192758
[INFO] Epoch: 36 , batch: 226 , training loss: 4.283579
[INFO] Epoch: 36 , batch: 227 , training loss: 4.262954
[INFO] Epoch: 36 , batch: 228 , training loss: 4.290282
[INFO] Epoch: 36 , batch: 229 , training loss: 4.149733
[INFO] Epoch: 36 , batch: 230 , training loss: 4.024799
[INFO] Epoch: 36 , batch: 231 , training loss: 3.887218
[INFO] Epoch: 36 , batch: 232 , training loss: 4.026889
[INFO] Epoch: 36 , batch: 233 , training loss: 4.053592
[INFO] Epoch: 36 , batch: 234 , training loss: 3.738755
[INFO] Epoch: 36 , batch: 235 , training loss: 3.850663
[INFO] Epoch: 36 , batch: 236 , training loss: 3.949216
[INFO] Epoch: 36 , batch: 237 , training loss: 4.162825
[INFO] Epoch: 36 , batch: 238 , training loss: 3.949062
[INFO] Epoch: 36 , batch: 239 , training loss: 3.970499
[INFO] Epoch: 36 , batch: 240 , training loss: 4.053811
[INFO] Epoch: 36 , batch: 241 , training loss: 3.844251
[INFO] Epoch: 36 , batch: 242 , training loss: 3.864245
[INFO] Epoch: 36 , batch: 243 , training loss: 4.172324
[INFO] Epoch: 36 , batch: 244 , training loss: 4.108168
[INFO] Epoch: 36 , batch: 245 , training loss: 4.060581
[INFO] Epoch: 36 , batch: 246 , training loss: 3.781925
[INFO] Epoch: 36 , batch: 247 , training loss: 3.940397
[INFO] Epoch: 36 , batch: 248 , training loss: 4.012311
[INFO] Epoch: 36 , batch: 249 , training loss: 4.046304
[INFO] Epoch: 36 , batch: 250 , training loss: 3.787362
[INFO] Epoch: 36 , batch: 251 , training loss: 4.234299
[INFO] Epoch: 36 , batch: 252 , training loss: 3.939232
[INFO] Epoch: 36 , batch: 253 , training loss: 3.885076
[INFO] Epoch: 36 , batch: 254 , training loss: 4.146581
[INFO] Epoch: 36 , batch: 255 , training loss: 4.115910
[INFO] Epoch: 36 , batch: 256 , training loss: 4.116160
[INFO] Epoch: 36 , batch: 257 , training loss: 4.258092
[INFO] Epoch: 36 , batch: 258 , training loss: 4.310005
[INFO] Epoch: 36 , batch: 259 , training loss: 4.345480
[INFO] Epoch: 36 , batch: 260 , training loss: 4.099689
[INFO] Epoch: 36 , batch: 261 , training loss: 4.236156
[INFO] Epoch: 36 , batch: 262 , training loss: 4.396008
[INFO] Epoch: 36 , batch: 263 , training loss: 4.584424
[INFO] Epoch: 36 , batch: 264 , training loss: 3.898401
[INFO] Epoch: 36 , batch: 265 , training loss: 4.041873
[INFO] Epoch: 36 , batch: 266 , training loss: 4.454019
[INFO] Epoch: 36 , batch: 267 , training loss: 4.206991
[INFO] Epoch: 36 , batch: 268 , training loss: 4.122107
[INFO] Epoch: 36 , batch: 269 , training loss: 4.114193
[INFO] Epoch: 36 , batch: 270 , training loss: 4.154523
[INFO] Epoch: 36 , batch: 271 , training loss: 4.190837
[INFO] Epoch: 36 , batch: 272 , training loss: 4.155649
[INFO] Epoch: 36 , batch: 273 , training loss: 4.187854
[INFO] Epoch: 36 , batch: 274 , training loss: 4.242122
[INFO] Epoch: 36 , batch: 275 , training loss: 4.126406
[INFO] Epoch: 36 , batch: 276 , training loss: 4.162133
[INFO] Epoch: 36 , batch: 277 , training loss: 4.328780
[INFO] Epoch: 36 , batch: 278 , training loss: 4.005578
[INFO] Epoch: 36 , batch: 279 , training loss: 4.014648
[INFO] Epoch: 36 , batch: 280 , training loss: 3.972427
[INFO] Epoch: 36 , batch: 281 , training loss: 4.124348
[INFO] Epoch: 36 , batch: 282 , training loss: 4.023498
[INFO] Epoch: 36 , batch: 283 , training loss: 4.059515
[INFO] Epoch: 36 , batch: 284 , training loss: 4.078166
[INFO] Epoch: 36 , batch: 285 , training loss: 3.993035
[INFO] Epoch: 36 , batch: 286 , training loss: 3.999876
[INFO] Epoch: 36 , batch: 287 , training loss: 3.949948
[INFO] Epoch: 36 , batch: 288 , training loss: 3.961025
[INFO] Epoch: 36 , batch: 289 , training loss: 3.996751
[INFO] Epoch: 36 , batch: 290 , training loss: 3.812036
[INFO] Epoch: 36 , batch: 291 , training loss: 3.751275
[INFO] Epoch: 36 , batch: 292 , training loss: 3.908382
[INFO] Epoch: 36 , batch: 293 , training loss: 3.782228
[INFO] Epoch: 36 , batch: 294 , training loss: 4.460477
[INFO] Epoch: 36 , batch: 295 , training loss: 4.216995
[INFO] Epoch: 36 , batch: 296 , training loss: 4.159548
[INFO] Epoch: 36 , batch: 297 , training loss: 4.132775
[INFO] Epoch: 36 , batch: 298 , training loss: 3.968002
[INFO] Epoch: 36 , batch: 299 , training loss: 3.978244
[INFO] Epoch: 36 , batch: 300 , training loss: 3.945925
[INFO] Epoch: 36 , batch: 301 , training loss: 3.900567
[INFO] Epoch: 36 , batch: 302 , training loss: 4.074547
[INFO] Epoch: 36 , batch: 303 , training loss: 4.074430
[INFO] Epoch: 36 , batch: 304 , training loss: 4.216809
[INFO] Epoch: 36 , batch: 305 , training loss: 4.043685
[INFO] Epoch: 36 , batch: 306 , training loss: 4.163902
[INFO] Epoch: 36 , batch: 307 , training loss: 4.167532
[INFO] Epoch: 36 , batch: 308 , training loss: 3.986408
[INFO] Epoch: 36 , batch: 309 , training loss: 3.999049
[INFO] Epoch: 36 , batch: 310 , training loss: 3.920666
[INFO] Epoch: 36 , batch: 311 , training loss: 3.898632
[INFO] Epoch: 36 , batch: 312 , training loss: 3.834926
[INFO] Epoch: 36 , batch: 313 , training loss: 3.943685
[INFO] Epoch: 36 , batch: 314 , training loss: 3.996989
[INFO] Epoch: 36 , batch: 315 , training loss: 4.035238
[INFO] Epoch: 36 , batch: 316 , training loss: 4.297281
[INFO] Epoch: 36 , batch: 317 , training loss: 4.711551
[INFO] Epoch: 36 , batch: 318 , training loss: 4.860470
[INFO] Epoch: 36 , batch: 319 , training loss: 4.478205
[INFO] Epoch: 36 , batch: 320 , training loss: 4.002087
[INFO] Epoch: 36 , batch: 321 , training loss: 3.851122
[INFO] Epoch: 36 , batch: 322 , training loss: 3.975531
[INFO] Epoch: 36 , batch: 323 , training loss: 3.979770
[INFO] Epoch: 36 , batch: 324 , training loss: 3.942607
[INFO] Epoch: 36 , batch: 325 , training loss: 4.111026
[INFO] Epoch: 36 , batch: 326 , training loss: 4.166685
[INFO] Epoch: 36 , batch: 327 , training loss: 4.031343
[INFO] Epoch: 36 , batch: 328 , training loss: 4.044470
[INFO] Epoch: 36 , batch: 329 , training loss: 3.981983
[INFO] Epoch: 36 , batch: 330 , training loss: 3.981705
[INFO] Epoch: 36 , batch: 331 , training loss: 4.128304
[INFO] Epoch: 36 , batch: 332 , training loss: 3.936909
[INFO] Epoch: 36 , batch: 333 , training loss: 3.966337
[INFO] Epoch: 36 , batch: 334 , training loss: 3.942397
[INFO] Epoch: 36 , batch: 335 , training loss: 4.081556
[INFO] Epoch: 36 , batch: 336 , training loss: 4.098679
[INFO] Epoch: 36 , batch: 337 , training loss: 4.122432
[INFO] Epoch: 36 , batch: 338 , training loss: 4.331694
[INFO] Epoch: 36 , batch: 339 , training loss: 4.218165
[INFO] Epoch: 36 , batch: 340 , training loss: 4.328623
[INFO] Epoch: 36 , batch: 341 , training loss: 4.101554
[INFO] Epoch: 36 , batch: 342 , training loss: 3.879397
[INFO] Epoch: 36 , batch: 343 , training loss: 3.959047
[INFO] Epoch: 36 , batch: 344 , training loss: 3.836207
[INFO] Epoch: 36 , batch: 345 , training loss: 3.958690
[INFO] Epoch: 36 , batch: 346 , training loss: 3.990310
[INFO] Epoch: 36 , batch: 347 , training loss: 3.901040
[INFO] Epoch: 36 , batch: 348 , training loss: 4.006206
[INFO] Epoch: 36 , batch: 349 , training loss: 4.168535
[INFO] Epoch: 36 , batch: 350 , training loss: 3.939956
[INFO] Epoch: 36 , batch: 351 , training loss: 4.024673
[INFO] Epoch: 36 , batch: 352 , training loss: 4.043570
[INFO] Epoch: 36 , batch: 353 , training loss: 3.998294
[INFO] Epoch: 36 , batch: 354 , training loss: 4.118548
[INFO] Epoch: 36 , batch: 355 , training loss: 4.164772
[INFO] Epoch: 36 , batch: 356 , training loss: 3.985414
[INFO] Epoch: 36 , batch: 357 , training loss: 4.069292
[INFO] Epoch: 36 , batch: 358 , training loss: 3.955620
[INFO] Epoch: 36 , batch: 359 , training loss: 3.984333
[INFO] Epoch: 36 , batch: 360 , training loss: 4.072271
[INFO] Epoch: 36 , batch: 361 , training loss: 4.039825
[INFO] Epoch: 36 , batch: 362 , training loss: 4.128537
[INFO] Epoch: 36 , batch: 363 , training loss: 4.007435
[INFO] Epoch: 36 , batch: 364 , training loss: 4.063141
[INFO] Epoch: 36 , batch: 365 , training loss: 4.004276
[INFO] Epoch: 36 , batch: 366 , training loss: 4.133909
[INFO] Epoch: 36 , batch: 367 , training loss: 4.190586
[INFO] Epoch: 36 , batch: 368 , training loss: 4.587595
[INFO] Epoch: 36 , batch: 369 , training loss: 4.223778
[INFO] Epoch: 36 , batch: 370 , training loss: 4.010427
[INFO] Epoch: 36 , batch: 371 , training loss: 4.397316
[INFO] Epoch: 36 , batch: 372 , training loss: 4.676068
[INFO] Epoch: 36 , batch: 373 , training loss: 4.747190
[INFO] Epoch: 36 , batch: 374 , training loss: 4.845625
[INFO] Epoch: 36 , batch: 375 , training loss: 4.812582
[INFO] Epoch: 36 , batch: 376 , training loss: 4.721220
[INFO] Epoch: 36 , batch: 377 , training loss: 4.531255
[INFO] Epoch: 36 , batch: 378 , training loss: 4.600984
[INFO] Epoch: 36 , batch: 379 , training loss: 4.570059
[INFO] Epoch: 36 , batch: 380 , training loss: 4.686621
[INFO] Epoch: 36 , batch: 381 , training loss: 4.434183
[INFO] Epoch: 36 , batch: 382 , training loss: 4.655307
[INFO] Epoch: 36 , batch: 383 , training loss: 4.650467
[INFO] Epoch: 36 , batch: 384 , training loss: 4.688142
[INFO] Epoch: 36 , batch: 385 , training loss: 4.403086
[INFO] Epoch: 36 , batch: 386 , training loss: 4.678504
[INFO] Epoch: 36 , batch: 387 , training loss: 4.624424
[INFO] Epoch: 36 , batch: 388 , training loss: 4.427684
[INFO] Epoch: 36 , batch: 389 , training loss: 4.260357
[INFO] Epoch: 36 , batch: 390 , training loss: 4.255177
[INFO] Epoch: 36 , batch: 391 , training loss: 4.325854
[INFO] Epoch: 36 , batch: 392 , training loss: 4.653299
[INFO] Epoch: 36 , batch: 393 , training loss: 4.536644
[INFO] Epoch: 36 , batch: 394 , training loss: 4.602816
[INFO] Epoch: 36 , batch: 395 , training loss: 4.470741
[INFO] Epoch: 36 , batch: 396 , training loss: 4.239158
[INFO] Epoch: 36 , batch: 397 , training loss: 4.432541
[INFO] Epoch: 36 , batch: 398 , training loss: 4.253557
[INFO] Epoch: 36 , batch: 399 , training loss: 4.323301
[INFO] Epoch: 36 , batch: 400 , training loss: 4.283473
[INFO] Epoch: 36 , batch: 401 , training loss: 4.686196
[INFO] Epoch: 36 , batch: 402 , training loss: 4.441442
[INFO] Epoch: 36 , batch: 403 , training loss: 4.230021
[INFO] Epoch: 36 , batch: 404 , training loss: 4.439430
[INFO] Epoch: 36 , batch: 405 , training loss: 4.483735
[INFO] Epoch: 36 , batch: 406 , training loss: 4.369050
[INFO] Epoch: 36 , batch: 407 , training loss: 4.454050
[INFO] Epoch: 36 , batch: 408 , training loss: 4.411780
[INFO] Epoch: 36 , batch: 409 , training loss: 4.390240
[INFO] Epoch: 36 , batch: 410 , training loss: 4.434874
[INFO] Epoch: 36 , batch: 411 , training loss: 4.611815
[INFO] Epoch: 36 , batch: 412 , training loss: 4.473383
[INFO] Epoch: 36 , batch: 413 , training loss: 4.346109
[INFO] Epoch: 36 , batch: 414 , training loss: 4.380000
[INFO] Epoch: 36 , batch: 415 , training loss: 4.415967
[INFO] Epoch: 36 , batch: 416 , training loss: 4.493958
[INFO] Epoch: 36 , batch: 417 , training loss: 4.413276
[INFO] Epoch: 36 , batch: 418 , training loss: 4.416005
[INFO] Epoch: 36 , batch: 419 , training loss: 4.386716
[INFO] Epoch: 36 , batch: 420 , training loss: 4.355114
[INFO] Epoch: 36 , batch: 421 , training loss: 4.352758
[INFO] Epoch: 36 , batch: 422 , training loss: 4.215123
[INFO] Epoch: 36 , batch: 423 , training loss: 4.422595
[INFO] Epoch: 36 , batch: 424 , training loss: 4.599141
[INFO] Epoch: 36 , batch: 425 , training loss: 4.465829
[INFO] Epoch: 36 , batch: 426 , training loss: 4.198332
[INFO] Epoch: 36 , batch: 427 , training loss: 4.426364
[INFO] Epoch: 36 , batch: 428 , training loss: 4.305003
[INFO] Epoch: 36 , batch: 429 , training loss: 4.170329
[INFO] Epoch: 36 , batch: 430 , training loss: 4.450340
[INFO] Epoch: 36 , batch: 431 , training loss: 4.030752
[INFO] Epoch: 36 , batch: 432 , training loss: 4.104195
[INFO] Epoch: 36 , batch: 433 , training loss: 4.140981
[INFO] Epoch: 36 , batch: 434 , training loss: 4.006350
[INFO] Epoch: 36 , batch: 435 , training loss: 4.369277
[INFO] Epoch: 36 , batch: 436 , training loss: 4.450119
[INFO] Epoch: 36 , batch: 437 , training loss: 4.198124
[INFO] Epoch: 36 , batch: 438 , training loss: 4.058153
[INFO] Epoch: 36 , batch: 439 , training loss: 4.276148
[INFO] Epoch: 36 , batch: 440 , training loss: 4.387564
[INFO] Epoch: 36 , batch: 441 , training loss: 4.501807
[INFO] Epoch: 36 , batch: 442 , training loss: 4.278807
[INFO] Epoch: 36 , batch: 443 , training loss: 4.463881
[INFO] Epoch: 36 , batch: 444 , training loss: 4.092702
[INFO] Epoch: 36 , batch: 445 , training loss: 3.982519
[INFO] Epoch: 36 , batch: 446 , training loss: 3.945986
[INFO] Epoch: 36 , batch: 447 , training loss: 4.108268
[INFO] Epoch: 36 , batch: 448 , training loss: 4.204185
[INFO] Epoch: 36 , batch: 449 , training loss: 4.607141
[INFO] Epoch: 36 , batch: 450 , training loss: 4.654738
[INFO] Epoch: 36 , batch: 451 , training loss: 4.569790
[INFO] Epoch: 36 , batch: 452 , training loss: 4.377778
[INFO] Epoch: 36 , batch: 453 , training loss: 4.156288
[INFO] Epoch: 36 , batch: 454 , training loss: 4.289745
[INFO] Epoch: 36 , batch: 455 , training loss: 4.346193
[INFO] Epoch: 36 , batch: 456 , training loss: 4.347408
[INFO] Epoch: 36 , batch: 457 , training loss: 4.426537
[INFO] Epoch: 36 , batch: 458 , training loss: 4.170724
[INFO] Epoch: 36 , batch: 459 , training loss: 4.146948
[INFO] Epoch: 36 , batch: 460 , training loss: 4.232793
[INFO] Epoch: 36 , batch: 461 , training loss: 4.222998
[INFO] Epoch: 36 , batch: 462 , training loss: 4.293714
[INFO] Epoch: 36 , batch: 463 , training loss: 4.204488
[INFO] Epoch: 36 , batch: 464 , training loss: 4.375326
[INFO] Epoch: 36 , batch: 465 , training loss: 4.306671
[INFO] Epoch: 36 , batch: 466 , training loss: 4.394207
[INFO] Epoch: 36 , batch: 467 , training loss: 4.373456
[INFO] Epoch: 36 , batch: 468 , training loss: 4.348544
[INFO] Epoch: 36 , batch: 469 , training loss: 4.360900
[INFO] Epoch: 36 , batch: 470 , training loss: 4.188967
[INFO] Epoch: 36 , batch: 471 , training loss: 4.274254
[INFO] Epoch: 36 , batch: 472 , training loss: 4.308971
[INFO] Epoch: 36 , batch: 473 , training loss: 4.246007
[INFO] Epoch: 36 , batch: 474 , training loss: 4.054840
[INFO] Epoch: 36 , batch: 475 , training loss: 3.938465
[INFO] Epoch: 36 , batch: 476 , training loss: 4.291511
[INFO] Epoch: 36 , batch: 477 , training loss: 4.436985
[INFO] Epoch: 36 , batch: 478 , training loss: 4.453553
[INFO] Epoch: 36 , batch: 479 , training loss: 4.420119
[INFO] Epoch: 36 , batch: 480 , training loss: 4.522198
[INFO] Epoch: 36 , batch: 481 , training loss: 4.406141
[INFO] Epoch: 36 , batch: 482 , training loss: 4.505113
[INFO] Epoch: 36 , batch: 483 , training loss: 4.384718
[INFO] Epoch: 36 , batch: 484 , training loss: 4.176287
[INFO] Epoch: 36 , batch: 485 , training loss: 4.265555
[INFO] Epoch: 36 , batch: 486 , training loss: 4.162044
[INFO] Epoch: 36 , batch: 487 , training loss: 4.147280
[INFO] Epoch: 36 , batch: 488 , training loss: 4.342368
[INFO] Epoch: 36 , batch: 489 , training loss: 4.227957
[INFO] Epoch: 36 , batch: 490 , training loss: 4.271183
[INFO] Epoch: 36 , batch: 491 , training loss: 4.227889
[INFO] Epoch: 36 , batch: 492 , training loss: 4.168005
[INFO] Epoch: 36 , batch: 493 , training loss: 4.366530
[INFO] Epoch: 36 , batch: 494 , training loss: 4.284954
[INFO] Epoch: 36 , batch: 495 , training loss: 4.404724
[INFO] Epoch: 36 , batch: 496 , training loss: 4.288440
[INFO] Epoch: 36 , batch: 497 , training loss: 4.331495
[INFO] Epoch: 36 , batch: 498 , training loss: 4.337889
[INFO] Epoch: 36 , batch: 499 , training loss: 4.364688
[INFO] Epoch: 36 , batch: 500 , training loss: 4.522516
[INFO] Epoch: 36 , batch: 501 , training loss: 4.840195
[INFO] Epoch: 36 , batch: 502 , training loss: 4.875549
[INFO] Epoch: 36 , batch: 503 , training loss: 4.580401
[INFO] Epoch: 36 , batch: 504 , training loss: 4.694621
[INFO] Epoch: 36 , batch: 505 , training loss: 4.677280
[INFO] Epoch: 36 , batch: 506 , training loss: 4.634268
[INFO] Epoch: 36 , batch: 507 , training loss: 4.699783
[INFO] Epoch: 36 , batch: 508 , training loss: 4.667369
[INFO] Epoch: 36 , batch: 509 , training loss: 4.441212
[INFO] Epoch: 36 , batch: 510 , training loss: 4.516230
[INFO] Epoch: 36 , batch: 511 , training loss: 4.428929
[INFO] Epoch: 36 , batch: 512 , training loss: 4.537906
[INFO] Epoch: 36 , batch: 513 , training loss: 4.765659
[INFO] Epoch: 36 , batch: 514 , training loss: 4.428373
[INFO] Epoch: 36 , batch: 515 , training loss: 4.655699
[INFO] Epoch: 36 , batch: 516 , training loss: 4.479557
[INFO] Epoch: 36 , batch: 517 , training loss: 4.426978
[INFO] Epoch: 36 , batch: 518 , training loss: 4.389461
[INFO] Epoch: 36 , batch: 519 , training loss: 4.261188
[INFO] Epoch: 36 , batch: 520 , training loss: 4.497496
[INFO] Epoch: 36 , batch: 521 , training loss: 4.453649
[INFO] Epoch: 36 , batch: 522 , training loss: 4.512900
[INFO] Epoch: 36 , batch: 523 , training loss: 4.406009
[INFO] Epoch: 36 , batch: 524 , training loss: 4.710735
[INFO] Epoch: 36 , batch: 525 , training loss: 4.652254
[INFO] Epoch: 36 , batch: 526 , training loss: 4.414340
[INFO] Epoch: 36 , batch: 527 , training loss: 4.452711
[INFO] Epoch: 36 , batch: 528 , training loss: 4.455263
[INFO] Epoch: 36 , batch: 529 , training loss: 4.444492
[INFO] Epoch: 36 , batch: 530 , training loss: 4.275489
[INFO] Epoch: 36 , batch: 531 , training loss: 4.432048
[INFO] Epoch: 36 , batch: 532 , training loss: 4.333194
[INFO] Epoch: 36 , batch: 533 , training loss: 4.474710
[INFO] Epoch: 36 , batch: 534 , training loss: 4.448900
[INFO] Epoch: 36 , batch: 535 , training loss: 4.475444
[INFO] Epoch: 36 , batch: 536 , training loss: 4.300540
[INFO] Epoch: 36 , batch: 537 , training loss: 4.294382
[INFO] Epoch: 36 , batch: 538 , training loss: 4.388934
[INFO] Epoch: 36 , batch: 539 , training loss: 4.493761
[INFO] Epoch: 36 , batch: 540 , training loss: 4.943839
[INFO] Epoch: 36 , batch: 541 , training loss: 4.860249
[INFO] Epoch: 36 , batch: 542 , training loss: 4.773062
[INFO] Epoch: 37 , batch: 0 , training loss: 3.322136
[INFO] Epoch: 37 , batch: 1 , training loss: 3.420753
[INFO] Epoch: 37 , batch: 2 , training loss: 3.578780
[INFO] Epoch: 37 , batch: 3 , training loss: 3.371118
[INFO] Epoch: 37 , batch: 4 , training loss: 3.819342
[INFO] Epoch: 37 , batch: 5 , training loss: 3.415965
[INFO] Epoch: 37 , batch: 6 , training loss: 3.839946
[INFO] Epoch: 37 , batch: 7 , training loss: 3.767695
[INFO] Epoch: 37 , batch: 8 , training loss: 3.451891
[INFO] Epoch: 37 , batch: 9 , training loss: 3.707994
[INFO] Epoch: 37 , batch: 10 , training loss: 3.715353
[INFO] Epoch: 37 , batch: 11 , training loss: 3.646594
[INFO] Epoch: 37 , batch: 12 , training loss: 3.533385
[INFO] Epoch: 37 , batch: 13 , training loss: 3.584025
[INFO] Epoch: 37 , batch: 14 , training loss: 3.454489
[INFO] Epoch: 37 , batch: 15 , training loss: 3.654351
[INFO] Epoch: 37 , batch: 16 , training loss: 3.500645
[INFO] Epoch: 37 , batch: 17 , training loss: 3.674803
[INFO] Epoch: 37 , batch: 18 , training loss: 3.615279
[INFO] Epoch: 37 , batch: 19 , training loss: 3.391041
[INFO] Epoch: 37 , batch: 20 , training loss: 3.335237
[INFO] Epoch: 37 , batch: 21 , training loss: 3.484234
[INFO] Epoch: 37 , batch: 22 , training loss: 3.350037
[INFO] Epoch: 37 , batch: 23 , training loss: 3.610320
[INFO] Epoch: 37 , batch: 24 , training loss: 3.362557
[INFO] Epoch: 37 , batch: 25 , training loss: 3.568371
[INFO] Epoch: 37 , batch: 26 , training loss: 3.450859
[INFO] Epoch: 37 , batch: 27 , training loss: 3.389563
[INFO] Epoch: 37 , batch: 28 , training loss: 3.546221
[INFO] Epoch: 37 , batch: 29 , training loss: 3.392873
[INFO] Epoch: 37 , batch: 30 , training loss: 3.446135
[INFO] Epoch: 37 , batch: 31 , training loss: 3.477969
[INFO] Epoch: 37 , batch: 32 , training loss: 3.462980
[INFO] Epoch: 37 , batch: 33 , training loss: 3.538733
[INFO] Epoch: 37 , batch: 34 , training loss: 3.491989
[INFO] Epoch: 37 , batch: 35 , training loss: 3.476360
[INFO] Epoch: 37 , batch: 36 , training loss: 3.516544
[INFO] Epoch: 37 , batch: 37 , training loss: 3.408504
[INFO] Epoch: 37 , batch: 38 , training loss: 3.445554
[INFO] Epoch: 37 , batch: 39 , training loss: 3.361034
[INFO] Epoch: 37 , batch: 40 , training loss: 3.584469
[INFO] Epoch: 37 , batch: 41 , training loss: 3.447539
[INFO] Epoch: 37 , batch: 42 , training loss: 3.858818
[INFO] Epoch: 37 , batch: 43 , training loss: 3.595422
[INFO] Epoch: 37 , batch: 44 , training loss: 3.953945
[INFO] Epoch: 37 , batch: 45 , training loss: 3.830114
[INFO] Epoch: 37 , batch: 46 , training loss: 3.762292
[INFO] Epoch: 37 , batch: 47 , training loss: 3.552246
[INFO] Epoch: 37 , batch: 48 , training loss: 3.532559
[INFO] Epoch: 37 , batch: 49 , training loss: 3.727082
[INFO] Epoch: 37 , batch: 50 , training loss: 3.547344
[INFO] Epoch: 37 , batch: 51 , training loss: 3.765921
[INFO] Epoch: 37 , batch: 52 , training loss: 3.613930
[INFO] Epoch: 37 , batch: 53 , training loss: 3.741710
[INFO] Epoch: 37 , batch: 54 , training loss: 3.746973
[INFO] Epoch: 37 , batch: 55 , training loss: 3.832293
[INFO] Epoch: 37 , batch: 56 , training loss: 3.711069
[INFO] Epoch: 37 , batch: 57 , training loss: 3.580441
[INFO] Epoch: 37 , batch: 58 , training loss: 3.675098
[INFO] Epoch: 37 , batch: 59 , training loss: 3.722832
[INFO] Epoch: 37 , batch: 60 , training loss: 3.686686
[INFO] Epoch: 37 , batch: 61 , training loss: 3.749550
[INFO] Epoch: 37 , batch: 62 , training loss: 3.662335
[INFO] Epoch: 37 , batch: 63 , training loss: 3.851718
[INFO] Epoch: 37 , batch: 64 , training loss: 4.018336
[INFO] Epoch: 37 , batch: 65 , training loss: 3.717628
[INFO] Epoch: 37 , batch: 66 , training loss: 3.579112
[INFO] Epoch: 37 , batch: 67 , training loss: 3.664838
[INFO] Epoch: 37 , batch: 68 , training loss: 3.802082
[INFO] Epoch: 37 , batch: 69 , training loss: 3.691186
[INFO] Epoch: 37 , batch: 70 , training loss: 3.897475
[INFO] Epoch: 37 , batch: 71 , training loss: 3.789917
[INFO] Epoch: 37 , batch: 72 , training loss: 3.855948
[INFO] Epoch: 37 , batch: 73 , training loss: 3.783457
[INFO] Epoch: 37 , batch: 74 , training loss: 3.897156
[INFO] Epoch: 37 , batch: 75 , training loss: 3.801411
[INFO] Epoch: 37 , batch: 76 , training loss: 3.827003
[INFO] Epoch: 37 , batch: 77 , training loss: 3.827125
[INFO] Epoch: 37 , batch: 78 , training loss: 3.926181
[INFO] Epoch: 37 , batch: 79 , training loss: 3.762458
[INFO] Epoch: 37 , batch: 80 , training loss: 3.921995
[INFO] Epoch: 37 , batch: 81 , training loss: 3.875645
[INFO] Epoch: 37 , batch: 82 , training loss: 3.834291
[INFO] Epoch: 37 , batch: 83 , training loss: 3.975242
[INFO] Epoch: 37 , batch: 84 , training loss: 3.882479
[INFO] Epoch: 37 , batch: 85 , training loss: 3.972379
[INFO] Epoch: 37 , batch: 86 , training loss: 3.913610
[INFO] Epoch: 37 , batch: 87 , training loss: 3.891335
[INFO] Epoch: 37 , batch: 88 , training loss: 3.993876
[INFO] Epoch: 37 , batch: 89 , training loss: 3.837333
[INFO] Epoch: 37 , batch: 90 , training loss: 3.901198
[INFO] Epoch: 37 , batch: 91 , training loss: 3.854738
[INFO] Epoch: 37 , batch: 92 , training loss: 3.847590
[INFO] Epoch: 37 , batch: 93 , training loss: 3.957277
[INFO] Epoch: 37 , batch: 94 , training loss: 4.104949
[INFO] Epoch: 37 , batch: 95 , training loss: 3.860088
[INFO] Epoch: 37 , batch: 96 , training loss: 3.888872
[INFO] Epoch: 37 , batch: 97 , training loss: 3.770983
[INFO] Epoch: 37 , batch: 98 , training loss: 3.749070
[INFO] Epoch: 37 , batch: 99 , training loss: 3.846392
[INFO] Epoch: 37 , batch: 100 , training loss: 3.778677
[INFO] Epoch: 37 , batch: 101 , training loss: 3.833032
[INFO] Epoch: 37 , batch: 102 , training loss: 3.913952
[INFO] Epoch: 37 , batch: 103 , training loss: 3.709570
[INFO] Epoch: 37 , batch: 104 , training loss: 3.665561
[INFO] Epoch: 37 , batch: 105 , training loss: 3.943408
[INFO] Epoch: 37 , batch: 106 , training loss: 3.907745
[INFO] Epoch: 37 , batch: 107 , training loss: 3.781698
[INFO] Epoch: 37 , batch: 108 , training loss: 3.704342
[INFO] Epoch: 37 , batch: 109 , training loss: 3.648202
[INFO] Epoch: 37 , batch: 110 , training loss: 3.838287
[INFO] Epoch: 37 , batch: 111 , training loss: 3.895937
[INFO] Epoch: 37 , batch: 112 , training loss: 3.823086
[INFO] Epoch: 37 , batch: 113 , training loss: 3.832861
[INFO] Epoch: 37 , batch: 114 , training loss: 3.806494
[INFO] Epoch: 37 , batch: 115 , training loss: 3.815749
[INFO] Epoch: 37 , batch: 116 , training loss: 3.721834
[INFO] Epoch: 37 , batch: 117 , training loss: 3.948148
[INFO] Epoch: 37 , batch: 118 , training loss: 3.929750
[INFO] Epoch: 37 , batch: 119 , training loss: 4.012233
[INFO] Epoch: 37 , batch: 120 , training loss: 4.046356
[INFO] Epoch: 37 , batch: 121 , training loss: 3.904725
[INFO] Epoch: 37 , batch: 122 , training loss: 3.801811
[INFO] Epoch: 37 , batch: 123 , training loss: 3.834372
[INFO] Epoch: 37 , batch: 124 , training loss: 3.914388
[INFO] Epoch: 37 , batch: 125 , training loss: 3.740112
[INFO] Epoch: 37 , batch: 126 , training loss: 3.733694
[INFO] Epoch: 37 , batch: 127 , training loss: 3.742821
[INFO] Epoch: 37 , batch: 128 , training loss: 3.876621
[INFO] Epoch: 37 , batch: 129 , training loss: 3.867609
[INFO] Epoch: 37 , batch: 130 , training loss: 3.845587
[INFO] Epoch: 37 , batch: 131 , training loss: 3.870945
[INFO] Epoch: 37 , batch: 132 , training loss: 3.835867
[INFO] Epoch: 37 , batch: 133 , training loss: 3.816152
[INFO] Epoch: 37 , batch: 134 , training loss: 3.611800
[INFO] Epoch: 37 , batch: 135 , training loss: 3.665123
[INFO] Epoch: 37 , batch: 136 , training loss: 3.948266
[INFO] Epoch: 37 , batch: 137 , training loss: 3.882401
[INFO] Epoch: 37 , batch: 138 , training loss: 3.907351
[INFO] Epoch: 37 , batch: 139 , training loss: 4.469343
[INFO] Epoch: 37 , batch: 140 , training loss: 4.206212
[INFO] Epoch: 37 , batch: 141 , training loss: 3.997915
[INFO] Epoch: 37 , batch: 142 , training loss: 3.789548
[INFO] Epoch: 37 , batch: 143 , training loss: 3.908258
[INFO] Epoch: 37 , batch: 144 , training loss: 3.743725
[INFO] Epoch: 37 , batch: 145 , training loss: 3.780840
[INFO] Epoch: 37 , batch: 146 , training loss: 3.990296
[INFO] Epoch: 37 , batch: 147 , training loss: 3.679548
[INFO] Epoch: 37 , batch: 148 , training loss: 3.660235
[INFO] Epoch: 37 , batch: 149 , training loss: 3.767228
[INFO] Epoch: 37 , batch: 150 , training loss: 3.944574
[INFO] Epoch: 37 , batch: 151 , training loss: 3.842811
[INFO] Epoch: 37 , batch: 152 , training loss: 3.921901
[INFO] Epoch: 37 , batch: 153 , training loss: 3.861182
[INFO] Epoch: 37 , batch: 154 , training loss: 3.971304
[INFO] Epoch: 37 , batch: 155 , training loss: 4.173316
[INFO] Epoch: 37 , batch: 156 , training loss: 3.886086
[INFO] Epoch: 37 , batch: 157 , training loss: 3.896503
[INFO] Epoch: 37 , batch: 158 , training loss: 4.019457
[INFO] Epoch: 37 , batch: 159 , training loss: 3.861118
[INFO] Epoch: 37 , batch: 160 , training loss: 4.153990
[INFO] Epoch: 37 , batch: 161 , training loss: 4.274877
[INFO] Epoch: 37 , batch: 162 , training loss: 4.268033
[INFO] Epoch: 37 , batch: 163 , training loss: 4.400739
[INFO] Epoch: 37 , batch: 164 , training loss: 4.394552
[INFO] Epoch: 37 , batch: 165 , training loss: 4.310400
[INFO] Epoch: 37 , batch: 166 , training loss: 4.137080
[INFO] Epoch: 37 , batch: 167 , training loss: 4.105372
[INFO] Epoch: 37 , batch: 168 , training loss: 3.837717
[INFO] Epoch: 37 , batch: 169 , training loss: 3.805559
[INFO] Epoch: 37 , batch: 170 , training loss: 4.069438
[INFO] Epoch: 37 , batch: 171 , training loss: 3.424782
[INFO] Epoch: 37 , batch: 172 , training loss: 3.650671
[INFO] Epoch: 37 , batch: 173 , training loss: 4.023439
[INFO] Epoch: 37 , batch: 174 , training loss: 4.465601
[INFO] Epoch: 37 , batch: 175 , training loss: 4.810242
[INFO] Epoch: 37 , batch: 176 , training loss: 4.384715
[INFO] Epoch: 37 , batch: 177 , training loss: 4.091439
[INFO] Epoch: 37 , batch: 178 , training loss: 4.052037
[INFO] Epoch: 37 , batch: 179 , training loss: 4.132754
[INFO] Epoch: 37 , batch: 180 , training loss: 4.091972
[INFO] Epoch: 37 , batch: 181 , training loss: 4.364517
[INFO] Epoch: 37 , batch: 182 , training loss: 4.334869
[INFO] Epoch: 37 , batch: 183 , training loss: 4.293132
[INFO] Epoch: 37 , batch: 184 , training loss: 4.173693
[INFO] Epoch: 37 , batch: 185 , training loss: 4.143092
[INFO] Epoch: 37 , batch: 186 , training loss: 4.294222
[INFO] Epoch: 37 , batch: 187 , training loss: 4.405615
[INFO] Epoch: 37 , batch: 188 , training loss: 4.386203
[INFO] Epoch: 37 , batch: 189 , training loss: 4.286984
[INFO] Epoch: 37 , batch: 190 , training loss: 4.289952
[INFO] Epoch: 37 , batch: 191 , training loss: 4.419859
[INFO] Epoch: 37 , batch: 192 , training loss: 4.236617
[INFO] Epoch: 37 , batch: 193 , training loss: 4.334002
[INFO] Epoch: 37 , batch: 194 , training loss: 4.272759
[INFO] Epoch: 37 , batch: 195 , training loss: 4.171181
[INFO] Epoch: 37 , batch: 196 , training loss: 4.075363
[INFO] Epoch: 37 , batch: 197 , training loss: 4.159740
[INFO] Epoch: 37 , batch: 198 , training loss: 4.092036
[INFO] Epoch: 37 , batch: 199 , training loss: 4.217811
[INFO] Epoch: 37 , batch: 200 , training loss: 4.122831
[INFO] Epoch: 37 , batch: 201 , training loss: 4.057373
[INFO] Epoch: 37 , batch: 202 , training loss: 4.031862
[INFO] Epoch: 37 , batch: 203 , training loss: 4.135041
[INFO] Epoch: 37 , batch: 204 , training loss: 4.285081
[INFO] Epoch: 37 , batch: 205 , training loss: 3.853046
[INFO] Epoch: 37 , batch: 206 , training loss: 3.794854
[INFO] Epoch: 37 , batch: 207 , training loss: 3.786525
[INFO] Epoch: 37 , batch: 208 , training loss: 4.099764
[INFO] Epoch: 37 , batch: 209 , training loss: 4.041966
[INFO] Epoch: 37 , batch: 210 , training loss: 4.068069
[INFO] Epoch: 37 , batch: 211 , training loss: 4.069030
[INFO] Epoch: 37 , batch: 212 , training loss: 4.177150
[INFO] Epoch: 37 , batch: 213 , training loss: 4.112554
[INFO] Epoch: 37 , batch: 214 , training loss: 4.202645
[INFO] Epoch: 37 , batch: 215 , training loss: 4.426321
[INFO] Epoch: 37 , batch: 216 , training loss: 4.120630
[INFO] Epoch: 37 , batch: 217 , training loss: 4.079743
[INFO] Epoch: 37 , batch: 218 , training loss: 4.061601
[INFO] Epoch: 37 , batch: 219 , training loss: 4.170885
[INFO] Epoch: 37 , batch: 220 , training loss: 3.985381
[INFO] Epoch: 37 , batch: 221 , training loss: 4.016375
[INFO] Epoch: 37 , batch: 222 , training loss: 4.154588
[INFO] Epoch: 37 , batch: 223 , training loss: 4.237324
[INFO] Epoch: 37 , batch: 224 , training loss: 4.273756
[INFO] Epoch: 37 , batch: 225 , training loss: 4.186955
[INFO] Epoch: 37 , batch: 226 , training loss: 4.283818
[INFO] Epoch: 37 , batch: 227 , training loss: 4.249928
[INFO] Epoch: 37 , batch: 228 , training loss: 4.288423
[INFO] Epoch: 37 , batch: 229 , training loss: 4.168337
[INFO] Epoch: 37 , batch: 230 , training loss: 4.025679
[INFO] Epoch: 37 , batch: 231 , training loss: 3.878221
[INFO] Epoch: 37 , batch: 232 , training loss: 4.022798
[INFO] Epoch: 37 , batch: 233 , training loss: 4.034414
[INFO] Epoch: 37 , batch: 234 , training loss: 3.737045
[INFO] Epoch: 37 , batch: 235 , training loss: 3.836415
[INFO] Epoch: 37 , batch: 236 , training loss: 3.936247
[INFO] Epoch: 37 , batch: 237 , training loss: 4.155914
[INFO] Epoch: 37 , batch: 238 , training loss: 3.943997
[INFO] Epoch: 37 , batch: 239 , training loss: 3.984561
[INFO] Epoch: 37 , batch: 240 , training loss: 4.040532
[INFO] Epoch: 37 , batch: 241 , training loss: 3.853553
[INFO] Epoch: 37 , batch: 242 , training loss: 3.870478
[INFO] Epoch: 37 , batch: 243 , training loss: 4.178861
[INFO] Epoch: 37 , batch: 244 , training loss: 4.100916
[INFO] Epoch: 37 , batch: 245 , training loss: 4.071251
[INFO] Epoch: 37 , batch: 246 , training loss: 3.768360
[INFO] Epoch: 37 , batch: 247 , training loss: 3.935875
[INFO] Epoch: 37 , batch: 248 , training loss: 4.005835
[INFO] Epoch: 37 , batch: 249 , training loss: 4.041574
[INFO] Epoch: 37 , batch: 250 , training loss: 3.782860
[INFO] Epoch: 37 , batch: 251 , training loss: 4.237217
[INFO] Epoch: 37 , batch: 252 , training loss: 3.933319
[INFO] Epoch: 37 , batch: 253 , training loss: 3.871049
[INFO] Epoch: 37 , batch: 254 , training loss: 4.130996
[INFO] Epoch: 37 , batch: 255 , training loss: 4.117020
[INFO] Epoch: 37 , batch: 256 , training loss: 4.106095
[INFO] Epoch: 37 , batch: 257 , training loss: 4.264534
[INFO] Epoch: 37 , batch: 258 , training loss: 4.295744
[INFO] Epoch: 37 , batch: 259 , training loss: 4.328535
[INFO] Epoch: 37 , batch: 260 , training loss: 4.074324
[INFO] Epoch: 37 , batch: 261 , training loss: 4.221019
[INFO] Epoch: 37 , batch: 262 , training loss: 4.386058
[INFO] Epoch: 37 , batch: 263 , training loss: 4.593042
[INFO] Epoch: 37 , batch: 264 , training loss: 3.880014
[INFO] Epoch: 37 , batch: 265 , training loss: 4.045187
[INFO] Epoch: 37 , batch: 266 , training loss: 4.431820
[INFO] Epoch: 37 , batch: 267 , training loss: 4.211569
[INFO] Epoch: 37 , batch: 268 , training loss: 4.124965
[INFO] Epoch: 37 , batch: 269 , training loss: 4.122141
[INFO] Epoch: 37 , batch: 270 , training loss: 4.141294
[INFO] Epoch: 37 , batch: 271 , training loss: 4.165271
[INFO] Epoch: 37 , batch: 272 , training loss: 4.148929
[INFO] Epoch: 37 , batch: 273 , training loss: 4.182611
[INFO] Epoch: 37 , batch: 274 , training loss: 4.223035
[INFO] Epoch: 37 , batch: 275 , training loss: 4.106411
[INFO] Epoch: 37 , batch: 276 , training loss: 4.163078
[INFO] Epoch: 37 , batch: 277 , training loss: 4.320403
[INFO] Epoch: 37 , batch: 278 , training loss: 4.011727
[INFO] Epoch: 37 , batch: 279 , training loss: 4.014140
[INFO] Epoch: 37 , batch: 280 , training loss: 3.983657
[INFO] Epoch: 37 , batch: 281 , training loss: 4.123130
[INFO] Epoch: 37 , batch: 282 , training loss: 4.030899
[INFO] Epoch: 37 , batch: 283 , training loss: 4.050754
[INFO] Epoch: 37 , batch: 284 , training loss: 4.069771
[INFO] Epoch: 37 , batch: 285 , training loss: 3.994508
[INFO] Epoch: 37 , batch: 286 , training loss: 3.994582
[INFO] Epoch: 37 , batch: 287 , training loss: 3.950773
[INFO] Epoch: 37 , batch: 288 , training loss: 3.952347
[INFO] Epoch: 37 , batch: 289 , training loss: 4.006866
[INFO] Epoch: 37 , batch: 290 , training loss: 3.784560
[INFO] Epoch: 37 , batch: 291 , training loss: 3.747038
[INFO] Epoch: 37 , batch: 292 , training loss: 3.893867
[INFO] Epoch: 37 , batch: 293 , training loss: 3.796023
[INFO] Epoch: 37 , batch: 294 , training loss: 4.448884
[INFO] Epoch: 37 , batch: 295 , training loss: 4.228707
[INFO] Epoch: 37 , batch: 296 , training loss: 4.154973
[INFO] Epoch: 37 , batch: 297 , training loss: 4.130937
[INFO] Epoch: 37 , batch: 298 , training loss: 3.960634
[INFO] Epoch: 37 , batch: 299 , training loss: 3.993267
[INFO] Epoch: 37 , batch: 300 , training loss: 3.941375
[INFO] Epoch: 37 , batch: 301 , training loss: 3.896508
[INFO] Epoch: 37 , batch: 302 , training loss: 4.048375
[INFO] Epoch: 37 , batch: 303 , training loss: 4.068714
[INFO] Epoch: 37 , batch: 304 , training loss: 4.236224
[INFO] Epoch: 37 , batch: 305 , training loss: 4.040030
[INFO] Epoch: 37 , batch: 306 , training loss: 4.157215
[INFO] Epoch: 37 , batch: 307 , training loss: 4.167840
[INFO] Epoch: 37 , batch: 308 , training loss: 3.997486
[INFO] Epoch: 37 , batch: 309 , training loss: 3.992663
[INFO] Epoch: 37 , batch: 310 , training loss: 3.924845
[INFO] Epoch: 37 , batch: 311 , training loss: 3.905222
[INFO] Epoch: 37 , batch: 312 , training loss: 3.829368
[INFO] Epoch: 37 , batch: 313 , training loss: 3.958855
[INFO] Epoch: 37 , batch: 314 , training loss: 3.990933
[INFO] Epoch: 37 , batch: 315 , training loss: 4.057770
[INFO] Epoch: 37 , batch: 316 , training loss: 4.297910
[INFO] Epoch: 37 , batch: 317 , training loss: 4.717991
[INFO] Epoch: 37 , batch: 318 , training loss: 4.848847
[INFO] Epoch: 37 , batch: 319 , training loss: 4.478890
[INFO] Epoch: 37 , batch: 320 , training loss: 4.007124
[INFO] Epoch: 37 , batch: 321 , training loss: 3.837082
[INFO] Epoch: 37 , batch: 322 , training loss: 3.974605
[INFO] Epoch: 37 , batch: 323 , training loss: 3.997347
[INFO] Epoch: 37 , batch: 324 , training loss: 3.946741
[INFO] Epoch: 37 , batch: 325 , training loss: 4.118294
[INFO] Epoch: 37 , batch: 326 , training loss: 4.163381
[INFO] Epoch: 37 , batch: 327 , training loss: 4.055860
[INFO] Epoch: 37 , batch: 328 , training loss: 4.054938
[INFO] Epoch: 37 , batch: 329 , training loss: 3.985201
[INFO] Epoch: 37 , batch: 330 , training loss: 3.992628
[INFO] Epoch: 37 , batch: 331 , training loss: 4.120686
[INFO] Epoch: 37 , batch: 332 , training loss: 3.948758
[INFO] Epoch: 37 , batch: 333 , training loss: 3.967076
[INFO] Epoch: 37 , batch: 334 , training loss: 3.955381
[INFO] Epoch: 37 , batch: 335 , training loss: 4.098691
[INFO] Epoch: 37 , batch: 336 , training loss: 4.114004
[INFO] Epoch: 37 , batch: 337 , training loss: 4.114613
[INFO] Epoch: 37 , batch: 338 , training loss: 4.342663
[INFO] Epoch: 37 , batch: 339 , training loss: 4.207377
[INFO] Epoch: 37 , batch: 340 , training loss: 4.337532
[INFO] Epoch: 37 , batch: 341 , training loss: 4.107087
[INFO] Epoch: 37 , batch: 342 , training loss: 3.882822
[INFO] Epoch: 37 , batch: 343 , training loss: 3.965802
[INFO] Epoch: 37 , batch: 344 , training loss: 3.830332
[INFO] Epoch: 37 , batch: 345 , training loss: 3.959394
[INFO] Epoch: 37 , batch: 346 , training loss: 4.007571
[INFO] Epoch: 37 , batch: 347 , training loss: 3.892794
[INFO] Epoch: 37 , batch: 348 , training loss: 4.011109
[INFO] Epoch: 37 , batch: 349 , training loss: 4.165622
[INFO] Epoch: 37 , batch: 350 , training loss: 3.945598
[INFO] Epoch: 37 , batch: 351 , training loss: 4.034432
[INFO] Epoch: 37 , batch: 352 , training loss: 4.021008
[INFO] Epoch: 37 , batch: 353 , training loss: 4.000917
[INFO] Epoch: 37 , batch: 354 , training loss: 4.115006
[INFO] Epoch: 37 , batch: 355 , training loss: 4.153551
[INFO] Epoch: 37 , batch: 356 , training loss: 3.991955
[INFO] Epoch: 37 , batch: 357 , training loss: 4.091937
[INFO] Epoch: 37 , batch: 358 , training loss: 3.957870
[INFO] Epoch: 37 , batch: 359 , training loss: 3.993083
[INFO] Epoch: 37 , batch: 360 , training loss: 4.079768
[INFO] Epoch: 37 , batch: 361 , training loss: 4.044673
[INFO] Epoch: 37 , batch: 362 , training loss: 4.130334
[INFO] Epoch: 37 , batch: 363 , training loss: 4.020728
[INFO] Epoch: 37 , batch: 364 , training loss: 4.065288
[INFO] Epoch: 37 , batch: 365 , training loss: 4.020397
[INFO] Epoch: 37 , batch: 366 , training loss: 4.137460
[INFO] Epoch: 37 , batch: 367 , training loss: 4.190730
[INFO] Epoch: 37 , batch: 368 , training loss: 4.585032
[INFO] Epoch: 37 , batch: 369 , training loss: 4.226429
[INFO] Epoch: 37 , batch: 370 , training loss: 4.015682
[INFO] Epoch: 37 , batch: 371 , training loss: 4.420474
[INFO] Epoch: 37 , batch: 372 , training loss: 4.677126
[INFO] Epoch: 37 , batch: 373 , training loss: 4.765049
[INFO] Epoch: 37 , batch: 374 , training loss: 4.847229
[INFO] Epoch: 37 , batch: 375 , training loss: 4.809402
[INFO] Epoch: 37 , batch: 376 , training loss: 4.732354
[INFO] Epoch: 37 , batch: 377 , training loss: 4.524884
[INFO] Epoch: 37 , batch: 378 , training loss: 4.609942
[INFO] Epoch: 37 , batch: 379 , training loss: 4.580184
[INFO] Epoch: 37 , batch: 380 , training loss: 4.692321
[INFO] Epoch: 37 , batch: 381 , training loss: 4.443305
[INFO] Epoch: 37 , batch: 382 , training loss: 4.657463
[INFO] Epoch: 37 , batch: 383 , training loss: 4.661669
[INFO] Epoch: 37 , batch: 384 , training loss: 4.708702
[INFO] Epoch: 37 , batch: 385 , training loss: 4.387080
[INFO] Epoch: 37 , batch: 386 , training loss: 4.673445
[INFO] Epoch: 37 , batch: 387 , training loss: 4.618771
[INFO] Epoch: 37 , batch: 388 , training loss: 4.428243
[INFO] Epoch: 37 , batch: 389 , training loss: 4.271167
[INFO] Epoch: 37 , batch: 390 , training loss: 4.271240
[INFO] Epoch: 37 , batch: 391 , training loss: 4.308524
[INFO] Epoch: 37 , batch: 392 , training loss: 4.662841
[INFO] Epoch: 37 , batch: 393 , training loss: 4.542037
[INFO] Epoch: 37 , batch: 394 , training loss: 4.606637
[INFO] Epoch: 37 , batch: 395 , training loss: 4.465889
[INFO] Epoch: 37 , batch: 396 , training loss: 4.236888
[INFO] Epoch: 37 , batch: 397 , training loss: 4.412170
[INFO] Epoch: 37 , batch: 398 , training loss: 4.260857
[INFO] Epoch: 37 , batch: 399 , training loss: 4.316894
[INFO] Epoch: 37 , batch: 400 , training loss: 4.281046
[INFO] Epoch: 37 , batch: 401 , training loss: 4.701216
[INFO] Epoch: 37 , batch: 402 , training loss: 4.434580
[INFO] Epoch: 37 , batch: 403 , training loss: 4.244487
[INFO] Epoch: 37 , batch: 404 , training loss: 4.429492
[INFO] Epoch: 37 , batch: 405 , training loss: 4.490389
[INFO] Epoch: 37 , batch: 406 , training loss: 4.382547
[INFO] Epoch: 37 , batch: 407 , training loss: 4.447836
[INFO] Epoch: 37 , batch: 408 , training loss: 4.413069
[INFO] Epoch: 37 , batch: 409 , training loss: 4.386069
[INFO] Epoch: 37 , batch: 410 , training loss: 4.430202
[INFO] Epoch: 37 , batch: 411 , training loss: 4.618001
[INFO] Epoch: 37 , batch: 412 , training loss: 4.464989
[INFO] Epoch: 37 , batch: 413 , training loss: 4.354515
[INFO] Epoch: 37 , batch: 414 , training loss: 4.372097
[INFO] Epoch: 37 , batch: 415 , training loss: 4.414080
[INFO] Epoch: 37 , batch: 416 , training loss: 4.500726
[INFO] Epoch: 37 , batch: 417 , training loss: 4.399888
[INFO] Epoch: 37 , batch: 418 , training loss: 4.419783
[INFO] Epoch: 37 , batch: 419 , training loss: 4.399066
[INFO] Epoch: 37 , batch: 420 , training loss: 4.366458
[INFO] Epoch: 37 , batch: 421 , training loss: 4.350334
[INFO] Epoch: 37 , batch: 422 , training loss: 4.214940
[INFO] Epoch: 37 , batch: 423 , training loss: 4.435387
[INFO] Epoch: 37 , batch: 424 , training loss: 4.591396
[INFO] Epoch: 37 , batch: 425 , training loss: 4.462170
[INFO] Epoch: 37 , batch: 426 , training loss: 4.210456
[INFO] Epoch: 37 , batch: 427 , training loss: 4.426300
[INFO] Epoch: 37 , batch: 428 , training loss: 4.311687
[INFO] Epoch: 37 , batch: 429 , training loss: 4.174645
[INFO] Epoch: 37 , batch: 430 , training loss: 4.456447
[INFO] Epoch: 37 , batch: 431 , training loss: 4.037611
[INFO] Epoch: 37 , batch: 432 , training loss: 4.107539
[INFO] Epoch: 37 , batch: 433 , training loss: 4.133353
[INFO] Epoch: 37 , batch: 434 , training loss: 4.009004
[INFO] Epoch: 37 , batch: 435 , training loss: 4.377505
[INFO] Epoch: 37 , batch: 436 , training loss: 4.435866
[INFO] Epoch: 37 , batch: 437 , training loss: 4.203671
[INFO] Epoch: 37 , batch: 438 , training loss: 4.068000
[INFO] Epoch: 37 , batch: 439 , training loss: 4.273600
[INFO] Epoch: 37 , batch: 440 , training loss: 4.391700
[INFO] Epoch: 37 , batch: 441 , training loss: 4.501435
[INFO] Epoch: 37 , batch: 442 , training loss: 4.264144
[INFO] Epoch: 37 , batch: 443 , training loss: 4.458475
[INFO] Epoch: 37 , batch: 444 , training loss: 4.089953
[INFO] Epoch: 37 , batch: 445 , training loss: 3.986323
[INFO] Epoch: 37 , batch: 446 , training loss: 3.925105
[INFO] Epoch: 37 , batch: 447 , training loss: 4.114769
[INFO] Epoch: 37 , batch: 448 , training loss: 4.199477
[INFO] Epoch: 37 , batch: 449 , training loss: 4.614953
[INFO] Epoch: 37 , batch: 450 , training loss: 4.664210
[INFO] Epoch: 37 , batch: 451 , training loss: 4.569971
[INFO] Epoch: 37 , batch: 452 , training loss: 4.387547
[INFO] Epoch: 37 , batch: 453 , training loss: 4.165312
[INFO] Epoch: 37 , batch: 454 , training loss: 4.286443
[INFO] Epoch: 37 , batch: 455 , training loss: 4.344123
[INFO] Epoch: 37 , batch: 456 , training loss: 4.335027
[INFO] Epoch: 37 , batch: 457 , training loss: 4.425215
[INFO] Epoch: 37 , batch: 458 , training loss: 4.171003
[INFO] Epoch: 37 , batch: 459 , training loss: 4.146798
[INFO] Epoch: 37 , batch: 460 , training loss: 4.247921
[INFO] Epoch: 37 , batch: 461 , training loss: 4.223256
[INFO] Epoch: 37 , batch: 462 , training loss: 4.282359
[INFO] Epoch: 37 , batch: 463 , training loss: 4.198582
[INFO] Epoch: 37 , batch: 464 , training loss: 4.378101
[INFO] Epoch: 37 , batch: 465 , training loss: 4.305190
[INFO] Epoch: 37 , batch: 466 , training loss: 4.413363
[INFO] Epoch: 37 , batch: 467 , training loss: 4.376904
[INFO] Epoch: 37 , batch: 468 , training loss: 4.341928
[INFO] Epoch: 37 , batch: 469 , training loss: 4.369528
[INFO] Epoch: 37 , batch: 470 , training loss: 4.189096
[INFO] Epoch: 37 , batch: 471 , training loss: 4.278172
[INFO] Epoch: 37 , batch: 472 , training loss: 4.306136
[INFO] Epoch: 37 , batch: 473 , training loss: 4.258706
[INFO] Epoch: 37 , batch: 474 , training loss: 4.053963
[INFO] Epoch: 37 , batch: 475 , training loss: 3.931058
[INFO] Epoch: 37 , batch: 476 , training loss: 4.286602
[INFO] Epoch: 37 , batch: 477 , training loss: 4.422505
[INFO] Epoch: 37 , batch: 478 , training loss: 4.460854
[INFO] Epoch: 37 , batch: 479 , training loss: 4.414051
[INFO] Epoch: 37 , batch: 480 , training loss: 4.520958
[INFO] Epoch: 37 , batch: 481 , training loss: 4.404479
[INFO] Epoch: 37 , batch: 482 , training loss: 4.518064
[INFO] Epoch: 37 , batch: 483 , training loss: 4.376087
[INFO] Epoch: 37 , batch: 484 , training loss: 4.168380
[INFO] Epoch: 37 , batch: 485 , training loss: 4.273599
[INFO] Epoch: 37 , batch: 486 , training loss: 4.161699
[INFO] Epoch: 37 , batch: 487 , training loss: 4.151093
[INFO] Epoch: 37 , batch: 488 , training loss: 4.333581
[INFO] Epoch: 37 , batch: 489 , training loss: 4.230101
[INFO] Epoch: 37 , batch: 490 , training loss: 4.263160
[INFO] Epoch: 37 , batch: 491 , training loss: 4.222503
[INFO] Epoch: 37 , batch: 492 , training loss: 4.162358
[INFO] Epoch: 37 , batch: 493 , training loss: 4.367229
[INFO] Epoch: 37 , batch: 494 , training loss: 4.283067
[INFO] Epoch: 37 , batch: 495 , training loss: 4.407824
[INFO] Epoch: 37 , batch: 496 , training loss: 4.282753
[INFO] Epoch: 37 , batch: 497 , training loss: 4.330483
[INFO] Epoch: 37 , batch: 498 , training loss: 4.335318
[INFO] Epoch: 37 , batch: 499 , training loss: 4.373772
[INFO] Epoch: 37 , batch: 500 , training loss: 4.521447
[INFO] Epoch: 37 , batch: 501 , training loss: 4.833177
[INFO] Epoch: 37 , batch: 502 , training loss: 4.873835
[INFO] Epoch: 37 , batch: 503 , training loss: 4.581344
[INFO] Epoch: 37 , batch: 504 , training loss: 4.693327
[INFO] Epoch: 37 , batch: 505 , training loss: 4.686309
[INFO] Epoch: 37 , batch: 506 , training loss: 4.616701
[INFO] Epoch: 37 , batch: 507 , training loss: 4.698669
[INFO] Epoch: 37 , batch: 508 , training loss: 4.664783
[INFO] Epoch: 37 , batch: 509 , training loss: 4.418126
[INFO] Epoch: 37 , batch: 510 , training loss: 4.518400
[INFO] Epoch: 37 , batch: 511 , training loss: 4.426694
[INFO] Epoch: 37 , batch: 512 , training loss: 4.522487
[INFO] Epoch: 37 , batch: 513 , training loss: 4.754353
[INFO] Epoch: 37 , batch: 514 , training loss: 4.419267
[INFO] Epoch: 37 , batch: 515 , training loss: 4.646914
[INFO] Epoch: 37 , batch: 516 , training loss: 4.476648
[INFO] Epoch: 37 , batch: 517 , training loss: 4.434636
[INFO] Epoch: 37 , batch: 518 , training loss: 4.396138
[INFO] Epoch: 37 , batch: 519 , training loss: 4.256503
[INFO] Epoch: 37 , batch: 520 , training loss: 4.482986
[INFO] Epoch: 37 , batch: 521 , training loss: 4.461489
[INFO] Epoch: 37 , batch: 522 , training loss: 4.507603
[INFO] Epoch: 37 , batch: 523 , training loss: 4.417670
[INFO] Epoch: 37 , batch: 524 , training loss: 4.727256
[INFO] Epoch: 37 , batch: 525 , training loss: 4.647561
[INFO] Epoch: 37 , batch: 526 , training loss: 4.411849
[INFO] Epoch: 37 , batch: 527 , training loss: 4.453190
[INFO] Epoch: 37 , batch: 528 , training loss: 4.448341
[INFO] Epoch: 37 , batch: 529 , training loss: 4.440041
[INFO] Epoch: 37 , batch: 530 , training loss: 4.276791
[INFO] Epoch: 37 , batch: 531 , training loss: 4.435241
[INFO] Epoch: 37 , batch: 532 , training loss: 4.329939
[INFO] Epoch: 37 , batch: 533 , training loss: 4.475342
[INFO] Epoch: 37 , batch: 534 , training loss: 4.449314
[INFO] Epoch: 37 , batch: 535 , training loss: 4.483902
[INFO] Epoch: 37 , batch: 536 , training loss: 4.293553
[INFO] Epoch: 37 , batch: 537 , training loss: 4.304620
[INFO] Epoch: 37 , batch: 538 , training loss: 4.377695
[INFO] Epoch: 37 , batch: 539 , training loss: 4.478736
[INFO] Epoch: 37 , batch: 540 , training loss: 4.957021
[INFO] Epoch: 37 , batch: 541 , training loss: 4.837930
[INFO] Epoch: 37 , batch: 542 , training loss: 4.765275
[INFO] Epoch: 38 , batch: 0 , training loss: 3.344437
[INFO] Epoch: 38 , batch: 1 , training loss: 3.411015
[INFO] Epoch: 38 , batch: 2 , training loss: 3.583668
[INFO] Epoch: 38 , batch: 3 , training loss: 3.387194
[INFO] Epoch: 38 , batch: 4 , training loss: 3.824418
[INFO] Epoch: 38 , batch: 5 , training loss: 3.432687
[INFO] Epoch: 38 , batch: 6 , training loss: 3.825467
[INFO] Epoch: 38 , batch: 7 , training loss: 3.755578
[INFO] Epoch: 38 , batch: 8 , training loss: 3.437556
[INFO] Epoch: 38 , batch: 9 , training loss: 3.703851
[INFO] Epoch: 38 , batch: 10 , training loss: 3.711682
[INFO] Epoch: 38 , batch: 11 , training loss: 3.676210
[INFO] Epoch: 38 , batch: 12 , training loss: 3.542495
[INFO] Epoch: 38 , batch: 13 , training loss: 3.591587
[INFO] Epoch: 38 , batch: 14 , training loss: 3.446104
[INFO] Epoch: 38 , batch: 15 , training loss: 3.617935
[INFO] Epoch: 38 , batch: 16 , training loss: 3.522041
[INFO] Epoch: 38 , batch: 17 , training loss: 3.702896
[INFO] Epoch: 38 , batch: 18 , training loss: 3.626955
[INFO] Epoch: 38 , batch: 19 , training loss: 3.369522
[INFO] Epoch: 38 , batch: 20 , training loss: 3.356073
[INFO] Epoch: 38 , batch: 21 , training loss: 3.493119
[INFO] Epoch: 38 , batch: 22 , training loss: 3.348526
[INFO] Epoch: 38 , batch: 23 , training loss: 3.567181
[INFO] Epoch: 38 , batch: 24 , training loss: 3.369733
[INFO] Epoch: 38 , batch: 25 , training loss: 3.606669
[INFO] Epoch: 38 , batch: 26 , training loss: 3.461784
[INFO] Epoch: 38 , batch: 27 , training loss: 3.387204
[INFO] Epoch: 38 , batch: 28 , training loss: 3.533631
[INFO] Epoch: 38 , batch: 29 , training loss: 3.391094
[INFO] Epoch: 38 , batch: 30 , training loss: 3.456632
[INFO] Epoch: 38 , batch: 31 , training loss: 3.478027
[INFO] Epoch: 38 , batch: 32 , training loss: 3.468931
[INFO] Epoch: 38 , batch: 33 , training loss: 3.521761
[INFO] Epoch: 38 , batch: 34 , training loss: 3.472350
[INFO] Epoch: 38 , batch: 35 , training loss: 3.459597
[INFO] Epoch: 38 , batch: 36 , training loss: 3.502441
[INFO] Epoch: 38 , batch: 37 , training loss: 3.389344
[INFO] Epoch: 38 , batch: 38 , training loss: 3.440508
[INFO] Epoch: 38 , batch: 39 , training loss: 3.331621
[INFO] Epoch: 38 , batch: 40 , training loss: 3.579050
[INFO] Epoch: 38 , batch: 41 , training loss: 3.439088
[INFO] Epoch: 38 , batch: 42 , training loss: 3.855512
[INFO] Epoch: 38 , batch: 43 , training loss: 3.591873
[INFO] Epoch: 38 , batch: 44 , training loss: 3.972565
[INFO] Epoch: 38 , batch: 45 , training loss: 3.893416
[INFO] Epoch: 38 , batch: 46 , training loss: 3.812833
[INFO] Epoch: 38 , batch: 47 , training loss: 3.534537
[INFO] Epoch: 38 , batch: 48 , training loss: 3.538202
[INFO] Epoch: 38 , batch: 49 , training loss: 3.732656
[INFO] Epoch: 38 , batch: 50 , training loss: 3.541697
[INFO] Epoch: 38 , batch: 51 , training loss: 3.756635
[INFO] Epoch: 38 , batch: 52 , training loss: 3.622594
[INFO] Epoch: 38 , batch: 53 , training loss: 3.764466
[INFO] Epoch: 38 , batch: 54 , training loss: 3.729807
[INFO] Epoch: 38 , batch: 55 , training loss: 3.831465
[INFO] Epoch: 38 , batch: 56 , training loss: 3.724878
[INFO] Epoch: 38 , batch: 57 , training loss: 3.588961
[INFO] Epoch: 38 , batch: 58 , training loss: 3.677199
[INFO] Epoch: 38 , batch: 59 , training loss: 3.749889
[INFO] Epoch: 38 , batch: 60 , training loss: 3.685639
[INFO] Epoch: 38 , batch: 61 , training loss: 3.755049
[INFO] Epoch: 38 , batch: 62 , training loss: 3.650726
[INFO] Epoch: 38 , batch: 63 , training loss: 3.860319
[INFO] Epoch: 38 , batch: 64 , training loss: 4.017188
[INFO] Epoch: 38 , batch: 65 , training loss: 3.704467
[INFO] Epoch: 38 , batch: 66 , training loss: 3.600118
[INFO] Epoch: 38 , batch: 67 , training loss: 3.680943
[INFO] Epoch: 38 , batch: 68 , training loss: 3.809236
[INFO] Epoch: 38 , batch: 69 , training loss: 3.699205
[INFO] Epoch: 38 , batch: 70 , training loss: 3.895234
[INFO] Epoch: 38 , batch: 71 , training loss: 3.792060
[INFO] Epoch: 38 , batch: 72 , training loss: 3.838868
[INFO] Epoch: 38 , batch: 73 , training loss: 3.766657
[INFO] Epoch: 38 , batch: 74 , training loss: 3.913696
[INFO] Epoch: 38 , batch: 75 , training loss: 3.797350
[INFO] Epoch: 38 , batch: 76 , training loss: 3.841799
[INFO] Epoch: 38 , batch: 77 , training loss: 3.804278
[INFO] Epoch: 38 , batch: 78 , training loss: 3.934117
[INFO] Epoch: 38 , batch: 79 , training loss: 3.774213
[INFO] Epoch: 38 , batch: 80 , training loss: 3.935796
[INFO] Epoch: 38 , batch: 81 , training loss: 3.895092
[INFO] Epoch: 38 , batch: 82 , training loss: 3.838171
[INFO] Epoch: 38 , batch: 83 , training loss: 3.991100
[INFO] Epoch: 38 , batch: 84 , training loss: 3.907015
[INFO] Epoch: 38 , batch: 85 , training loss: 3.994659
[INFO] Epoch: 38 , batch: 86 , training loss: 3.903501
[INFO] Epoch: 38 , batch: 87 , training loss: 3.892591
[INFO] Epoch: 38 , batch: 88 , training loss: 3.999835
[INFO] Epoch: 38 , batch: 89 , training loss: 3.836095
[INFO] Epoch: 38 , batch: 90 , training loss: 3.896573
[INFO] Epoch: 38 , batch: 91 , training loss: 3.850672
[INFO] Epoch: 38 , batch: 92 , training loss: 3.839003
[INFO] Epoch: 38 , batch: 93 , training loss: 3.950306
[INFO] Epoch: 38 , batch: 94 , training loss: 4.105518
[INFO] Epoch: 38 , batch: 95 , training loss: 3.847882
[INFO] Epoch: 38 , batch: 96 , training loss: 3.856908
[INFO] Epoch: 38 , batch: 97 , training loss: 3.782687
[INFO] Epoch: 38 , batch: 98 , training loss: 3.734125
[INFO] Epoch: 38 , batch: 99 , training loss: 3.832837
[INFO] Epoch: 38 , batch: 100 , training loss: 3.785330
[INFO] Epoch: 38 , batch: 101 , training loss: 3.820379
[INFO] Epoch: 38 , batch: 102 , training loss: 3.939861
[INFO] Epoch: 38 , batch: 103 , training loss: 3.711228
[INFO] Epoch: 38 , batch: 104 , training loss: 3.687157
[INFO] Epoch: 38 , batch: 105 , training loss: 3.934561
[INFO] Epoch: 38 , batch: 106 , training loss: 3.927535
[INFO] Epoch: 38 , batch: 107 , training loss: 3.766726
[INFO] Epoch: 38 , batch: 108 , training loss: 3.714949
[INFO] Epoch: 38 , batch: 109 , training loss: 3.643887
[INFO] Epoch: 38 , batch: 110 , training loss: 3.829386
[INFO] Epoch: 38 , batch: 111 , training loss: 3.899126
[INFO] Epoch: 38 , batch: 112 , training loss: 3.825220
[INFO] Epoch: 38 , batch: 113 , training loss: 3.850401
[INFO] Epoch: 38 , batch: 114 , training loss: 3.809883
[INFO] Epoch: 38 , batch: 115 , training loss: 3.836689
[INFO] Epoch: 38 , batch: 116 , training loss: 3.738065
[INFO] Epoch: 38 , batch: 117 , training loss: 3.956939
[INFO] Epoch: 38 , batch: 118 , training loss: 3.941885
[INFO] Epoch: 38 , batch: 119 , training loss: 4.049141
[INFO] Epoch: 38 , batch: 120 , training loss: 4.040495
[INFO] Epoch: 38 , batch: 121 , training loss: 3.894810
[INFO] Epoch: 38 , batch: 122 , training loss: 3.827928
[INFO] Epoch: 38 , batch: 123 , training loss: 3.821046
[INFO] Epoch: 38 , batch: 124 , training loss: 3.920309
[INFO] Epoch: 38 , batch: 125 , training loss: 3.747831
[INFO] Epoch: 38 , batch: 126 , training loss: 3.742115
[INFO] Epoch: 38 , batch: 127 , training loss: 3.750252
[INFO] Epoch: 38 , batch: 128 , training loss: 3.862694
[INFO] Epoch: 38 , batch: 129 , training loss: 3.857750
[INFO] Epoch: 38 , batch: 130 , training loss: 3.846390
[INFO] Epoch: 38 , batch: 131 , training loss: 3.822895
[INFO] Epoch: 38 , batch: 132 , training loss: 3.825442
[INFO] Epoch: 38 , batch: 133 , training loss: 3.805780
[INFO] Epoch: 38 , batch: 134 , training loss: 3.613037
[INFO] Epoch: 38 , batch: 135 , training loss: 3.663759
[INFO] Epoch: 38 , batch: 136 , training loss: 3.932139
[INFO] Epoch: 38 , batch: 137 , training loss: 3.868213
[INFO] Epoch: 38 , batch: 138 , training loss: 3.911059
[INFO] Epoch: 38 , batch: 139 , training loss: 4.481160
[INFO] Epoch: 38 , batch: 140 , training loss: 4.207442
[INFO] Epoch: 38 , batch: 141 , training loss: 3.998827
[INFO] Epoch: 38 , batch: 142 , training loss: 3.789299
[INFO] Epoch: 38 , batch: 143 , training loss: 3.895142
[INFO] Epoch: 38 , batch: 144 , training loss: 3.711507
[INFO] Epoch: 38 , batch: 145 , training loss: 3.767082
[INFO] Epoch: 38 , batch: 146 , training loss: 3.977044
[INFO] Epoch: 38 , batch: 147 , training loss: 3.674167
[INFO] Epoch: 38 , batch: 148 , training loss: 3.692281
[INFO] Epoch: 38 , batch: 149 , training loss: 3.738476
[INFO] Epoch: 38 , batch: 150 , training loss: 3.994361
[INFO] Epoch: 38 , batch: 151 , training loss: 3.825142
[INFO] Epoch: 38 , batch: 152 , training loss: 3.904081
[INFO] Epoch: 38 , batch: 153 , training loss: 3.863573
[INFO] Epoch: 38 , batch: 154 , training loss: 3.965516
[INFO] Epoch: 38 , batch: 155 , training loss: 4.165997
[INFO] Epoch: 38 , batch: 156 , training loss: 3.893346
[INFO] Epoch: 38 , batch: 157 , training loss: 3.894578
[INFO] Epoch: 38 , batch: 158 , training loss: 4.010750
[INFO] Epoch: 38 , batch: 159 , training loss: 3.892753
[INFO] Epoch: 38 , batch: 160 , training loss: 4.163743
[INFO] Epoch: 38 , batch: 161 , training loss: 4.277812
[INFO] Epoch: 38 , batch: 162 , training loss: 4.262957
[INFO] Epoch: 38 , batch: 163 , training loss: 4.392846
[INFO] Epoch: 38 , batch: 164 , training loss: 4.368521
[INFO] Epoch: 38 , batch: 165 , training loss: 4.304546
[INFO] Epoch: 38 , batch: 166 , training loss: 4.104983
[INFO] Epoch: 38 , batch: 167 , training loss: 4.121202
[INFO] Epoch: 38 , batch: 168 , training loss: 3.827406
[INFO] Epoch: 38 , batch: 169 , training loss: 3.813308
[INFO] Epoch: 38 , batch: 170 , training loss: 4.042117
[INFO] Epoch: 38 , batch: 171 , training loss: 3.439871
[INFO] Epoch: 38 , batch: 172 , training loss: 3.654162
[INFO] Epoch: 38 , batch: 173 , training loss: 4.028347
[INFO] Epoch: 38 , batch: 174 , training loss: 4.453379
[INFO] Epoch: 38 , batch: 175 , training loss: 4.801365
[INFO] Epoch: 38 , batch: 176 , training loss: 4.440393
[INFO] Epoch: 38 , batch: 177 , training loss: 4.086707
[INFO] Epoch: 38 , batch: 178 , training loss: 4.058040
[INFO] Epoch: 38 , batch: 179 , training loss: 4.134066
[INFO] Epoch: 38 , batch: 180 , training loss: 4.094822
[INFO] Epoch: 38 , batch: 181 , training loss: 4.369153
[INFO] Epoch: 38 , batch: 182 , training loss: 4.330877
[INFO] Epoch: 38 , batch: 183 , training loss: 4.300890
[INFO] Epoch: 38 , batch: 184 , training loss: 4.168344
[INFO] Epoch: 38 , batch: 185 , training loss: 4.142477
[INFO] Epoch: 38 , batch: 186 , training loss: 4.303915
[INFO] Epoch: 38 , batch: 187 , training loss: 4.403492
[INFO] Epoch: 38 , batch: 188 , training loss: 4.413702
[INFO] Epoch: 38 , batch: 189 , training loss: 4.294580
[INFO] Epoch: 38 , batch: 190 , training loss: 4.306832
[INFO] Epoch: 38 , batch: 191 , training loss: 4.406670
[INFO] Epoch: 38 , batch: 192 , training loss: 4.224719
[INFO] Epoch: 38 , batch: 193 , training loss: 4.340901
[INFO] Epoch: 38 , batch: 194 , training loss: 4.278814
[INFO] Epoch: 38 , batch: 195 , training loss: 4.184333
[INFO] Epoch: 38 , batch: 196 , training loss: 4.085321
[INFO] Epoch: 38 , batch: 197 , training loss: 4.149451
[INFO] Epoch: 38 , batch: 198 , training loss: 4.071159
[INFO] Epoch: 38 , batch: 199 , training loss: 4.223907
[INFO] Epoch: 38 , batch: 200 , training loss: 4.145494
[INFO] Epoch: 38 , batch: 201 , training loss: 4.057607
[INFO] Epoch: 38 , batch: 202 , training loss: 4.032795
[INFO] Epoch: 38 , batch: 203 , training loss: 4.137197
[INFO] Epoch: 38 , batch: 204 , training loss: 4.290103
[INFO] Epoch: 38 , batch: 205 , training loss: 3.835088
[INFO] Epoch: 38 , batch: 206 , training loss: 3.794138
[INFO] Epoch: 38 , batch: 207 , training loss: 3.781760
[INFO] Epoch: 38 , batch: 208 , training loss: 4.110786
[INFO] Epoch: 38 , batch: 209 , training loss: 4.058869
[INFO] Epoch: 38 , batch: 210 , training loss: 4.081729
[INFO] Epoch: 38 , batch: 211 , training loss: 4.069401
[INFO] Epoch: 38 , batch: 212 , training loss: 4.184096
[INFO] Epoch: 38 , batch: 213 , training loss: 4.125821
[INFO] Epoch: 38 , batch: 214 , training loss: 4.223072
[INFO] Epoch: 38 , batch: 215 , training loss: 4.433756
[INFO] Epoch: 38 , batch: 216 , training loss: 4.112042
[INFO] Epoch: 38 , batch: 217 , training loss: 4.096650
[INFO] Epoch: 38 , batch: 218 , training loss: 4.062816
[INFO] Epoch: 38 , batch: 219 , training loss: 4.174760
[INFO] Epoch: 38 , batch: 220 , training loss: 3.981433
[INFO] Epoch: 38 , batch: 221 , training loss: 4.001816
[INFO] Epoch: 38 , batch: 222 , training loss: 4.149137
[INFO] Epoch: 38 , batch: 223 , training loss: 4.249941
[INFO] Epoch: 38 , batch: 224 , training loss: 4.284082
[INFO] Epoch: 38 , batch: 225 , training loss: 4.200285
[INFO] Epoch: 38 , batch: 226 , training loss: 4.287604
[INFO] Epoch: 38 , batch: 227 , training loss: 4.274434
[INFO] Epoch: 38 , batch: 228 , training loss: 4.313909
[INFO] Epoch: 38 , batch: 229 , training loss: 4.156065
[INFO] Epoch: 38 , batch: 230 , training loss: 4.027376
[INFO] Epoch: 38 , batch: 231 , training loss: 3.884004
[INFO] Epoch: 38 , batch: 232 , training loss: 4.039331
[INFO] Epoch: 38 , batch: 233 , training loss: 4.047340
[INFO] Epoch: 38 , batch: 234 , training loss: 3.756376
[INFO] Epoch: 38 , batch: 235 , training loss: 3.848500
[INFO] Epoch: 38 , batch: 236 , training loss: 3.948738
[INFO] Epoch: 38 , batch: 237 , training loss: 4.163630
[INFO] Epoch: 38 , batch: 238 , training loss: 3.937881
[INFO] Epoch: 38 , batch: 239 , training loss: 3.982450
[INFO] Epoch: 38 , batch: 240 , training loss: 4.047249
[INFO] Epoch: 38 , batch: 241 , training loss: 3.853842
[INFO] Epoch: 38 , batch: 242 , training loss: 3.865589
[INFO] Epoch: 38 , batch: 243 , training loss: 4.163118
[INFO] Epoch: 38 , batch: 244 , training loss: 4.113643
[INFO] Epoch: 38 , batch: 245 , training loss: 4.078827
[INFO] Epoch: 38 , batch: 246 , training loss: 3.773575
[INFO] Epoch: 38 , batch: 247 , training loss: 3.949552
[INFO] Epoch: 38 , batch: 248 , training loss: 4.010894
[INFO] Epoch: 38 , batch: 249 , training loss: 4.051693
[INFO] Epoch: 38 , batch: 250 , training loss: 3.799188
[INFO] Epoch: 38 , batch: 251 , training loss: 4.229219
[INFO] Epoch: 38 , batch: 252 , training loss: 3.943032
[INFO] Epoch: 38 , batch: 253 , training loss: 3.860732
[INFO] Epoch: 38 , batch: 254 , training loss: 4.143908
[INFO] Epoch: 38 , batch: 255 , training loss: 4.110538
[INFO] Epoch: 38 , batch: 256 , training loss: 4.117129
[INFO] Epoch: 38 , batch: 257 , training loss: 4.253967
[INFO] Epoch: 38 , batch: 258 , training loss: 4.293299
[INFO] Epoch: 38 , batch: 259 , training loss: 4.344273
[INFO] Epoch: 38 , batch: 260 , training loss: 4.088379
[INFO] Epoch: 38 , batch: 261 , training loss: 4.219451
[INFO] Epoch: 38 , batch: 262 , training loss: 4.381689
[INFO] Epoch: 38 , batch: 263 , training loss: 4.590994
[INFO] Epoch: 38 , batch: 264 , training loss: 3.887806
[INFO] Epoch: 38 , batch: 265 , training loss: 4.039849
[INFO] Epoch: 38 , batch: 266 , training loss: 4.437965
[INFO] Epoch: 38 , batch: 267 , training loss: 4.206614
[INFO] Epoch: 38 , batch: 268 , training loss: 4.139557
[INFO] Epoch: 38 , batch: 269 , training loss: 4.133210
[INFO] Epoch: 38 , batch: 270 , training loss: 4.154246
[INFO] Epoch: 38 , batch: 271 , training loss: 4.177303
[INFO] Epoch: 38 , batch: 272 , training loss: 4.136015
[INFO] Epoch: 38 , batch: 273 , training loss: 4.184403
[INFO] Epoch: 38 , batch: 274 , training loss: 4.251027
[INFO] Epoch: 38 , batch: 275 , training loss: 4.109485
[INFO] Epoch: 38 , batch: 276 , training loss: 4.164694
[INFO] Epoch: 38 , batch: 277 , training loss: 4.315397
[INFO] Epoch: 38 , batch: 278 , training loss: 4.018692
[INFO] Epoch: 38 , batch: 279 , training loss: 4.013124
[INFO] Epoch: 38 , batch: 280 , training loss: 3.966776
[INFO] Epoch: 38 , batch: 281 , training loss: 4.122672
[INFO] Epoch: 38 , batch: 282 , training loss: 4.033475
[INFO] Epoch: 38 , batch: 283 , training loss: 4.041334
[INFO] Epoch: 38 , batch: 284 , training loss: 4.056469
[INFO] Epoch: 38 , batch: 285 , training loss: 3.990520
[INFO] Epoch: 38 , batch: 286 , training loss: 3.987863
[INFO] Epoch: 38 , batch: 287 , training loss: 3.951761
[INFO] Epoch: 38 , batch: 288 , training loss: 3.945418
[INFO] Epoch: 38 , batch: 289 , training loss: 4.003850
[INFO] Epoch: 38 , batch: 290 , training loss: 3.796880
[INFO] Epoch: 38 , batch: 291 , training loss: 3.736104
[INFO] Epoch: 38 , batch: 292 , training loss: 3.895197
[INFO] Epoch: 38 , batch: 293 , training loss: 3.802830
[INFO] Epoch: 38 , batch: 294 , training loss: 4.442009
[INFO] Epoch: 38 , batch: 295 , training loss: 4.227340
[INFO] Epoch: 38 , batch: 296 , training loss: 4.168374
[INFO] Epoch: 38 , batch: 297 , training loss: 4.123847
[INFO] Epoch: 38 , batch: 298 , training loss: 3.960849
[INFO] Epoch: 38 , batch: 299 , training loss: 3.985813
[INFO] Epoch: 38 , batch: 300 , training loss: 3.943665
[INFO] Epoch: 38 , batch: 301 , training loss: 3.891562
[INFO] Epoch: 38 , batch: 302 , training loss: 4.057806
[INFO] Epoch: 38 , batch: 303 , training loss: 4.068294
[INFO] Epoch: 38 , batch: 304 , training loss: 4.225954
[INFO] Epoch: 38 , batch: 305 , training loss: 4.035211
[INFO] Epoch: 38 , batch: 306 , training loss: 4.152641
[INFO] Epoch: 38 , batch: 307 , training loss: 4.150895
[INFO] Epoch: 38 , batch: 308 , training loss: 3.988110
[INFO] Epoch: 38 , batch: 309 , training loss: 3.983823
[INFO] Epoch: 38 , batch: 310 , training loss: 3.918840
[INFO] Epoch: 38 , batch: 311 , training loss: 3.897134
[INFO] Epoch: 38 , batch: 312 , training loss: 3.835837
[INFO] Epoch: 38 , batch: 313 , training loss: 3.939172
[INFO] Epoch: 38 , batch: 314 , training loss: 3.980246
[INFO] Epoch: 38 , batch: 315 , training loss: 4.047050
[INFO] Epoch: 38 , batch: 316 , training loss: 4.294426
[INFO] Epoch: 38 , batch: 317 , training loss: 4.714564
[INFO] Epoch: 38 , batch: 318 , training loss: 4.845885
[INFO] Epoch: 38 , batch: 319 , training loss: 4.464925
[INFO] Epoch: 38 , batch: 320 , training loss: 4.010132
[INFO] Epoch: 38 , batch: 321 , training loss: 3.845457
[INFO] Epoch: 38 , batch: 322 , training loss: 3.967834
[INFO] Epoch: 38 , batch: 323 , training loss: 3.979170
[INFO] Epoch: 38 , batch: 324 , training loss: 3.949061
[INFO] Epoch: 38 , batch: 325 , training loss: 4.092662
[INFO] Epoch: 38 , batch: 326 , training loss: 4.172393
[INFO] Epoch: 38 , batch: 327 , training loss: 4.047773
[INFO] Epoch: 38 , batch: 328 , training loss: 4.054955
[INFO] Epoch: 38 , batch: 329 , training loss: 3.985177
[INFO] Epoch: 38 , batch: 330 , training loss: 3.981298
[INFO] Epoch: 38 , batch: 331 , training loss: 4.122081
[INFO] Epoch: 38 , batch: 332 , training loss: 3.937102
[INFO] Epoch: 38 , batch: 333 , training loss: 3.964019
[INFO] Epoch: 38 , batch: 334 , training loss: 3.945455
[INFO] Epoch: 38 , batch: 335 , training loss: 4.106415
[INFO] Epoch: 38 , batch: 336 , training loss: 4.108635
[INFO] Epoch: 38 , batch: 337 , training loss: 4.138854
[INFO] Epoch: 38 , batch: 338 , training loss: 4.358765
[INFO] Epoch: 38 , batch: 339 , training loss: 4.196589
[INFO] Epoch: 38 , batch: 340 , training loss: 4.338049
[INFO] Epoch: 38 , batch: 341 , training loss: 4.103329
[INFO] Epoch: 38 , batch: 342 , training loss: 3.890400
[INFO] Epoch: 38 , batch: 343 , training loss: 3.970536
[INFO] Epoch: 38 , batch: 344 , training loss: 3.831951
[INFO] Epoch: 38 , batch: 345 , training loss: 3.962411
[INFO] Epoch: 38 , batch: 346 , training loss: 3.992350
[INFO] Epoch: 38 , batch: 347 , training loss: 3.910985
[INFO] Epoch: 38 , batch: 348 , training loss: 4.014865
[INFO] Epoch: 38 , batch: 349 , training loss: 4.157883
[INFO] Epoch: 38 , batch: 350 , training loss: 3.940838
[INFO] Epoch: 38 , batch: 351 , training loss: 4.032992
[INFO] Epoch: 38 , batch: 352 , training loss: 4.031141
[INFO] Epoch: 38 , batch: 353 , training loss: 4.006186
[INFO] Epoch: 38 , batch: 354 , training loss: 4.125585
[INFO] Epoch: 38 , batch: 355 , training loss: 4.144575
[INFO] Epoch: 38 , batch: 356 , training loss: 3.989661
[INFO] Epoch: 38 , batch: 357 , training loss: 4.083869
[INFO] Epoch: 38 , batch: 358 , training loss: 3.965182
[INFO] Epoch: 38 , batch: 359 , training loss: 3.992136
[INFO] Epoch: 38 , batch: 360 , training loss: 4.088016
[INFO] Epoch: 38 , batch: 361 , training loss: 4.037136
[INFO] Epoch: 38 , batch: 362 , training loss: 4.133047
[INFO] Epoch: 38 , batch: 363 , training loss: 4.018933
[INFO] Epoch: 38 , batch: 364 , training loss: 4.062893
[INFO] Epoch: 38 , batch: 365 , training loss: 4.001326
[INFO] Epoch: 38 , batch: 366 , training loss: 4.125195
[INFO] Epoch: 38 , batch: 367 , training loss: 4.185774
[INFO] Epoch: 38 , batch: 368 , training loss: 4.591724
[INFO] Epoch: 38 , batch: 369 , training loss: 4.237600
[INFO] Epoch: 38 , batch: 370 , training loss: 4.004807
[INFO] Epoch: 38 , batch: 371 , training loss: 4.383650
[INFO] Epoch: 38 , batch: 372 , training loss: 4.689256
[INFO] Epoch: 38 , batch: 373 , training loss: 4.762290
[INFO] Epoch: 38 , batch: 374 , training loss: 4.838767
[INFO] Epoch: 38 , batch: 375 , training loss: 4.813898
[INFO] Epoch: 38 , batch: 376 , training loss: 4.749411
[INFO] Epoch: 38 , batch: 377 , training loss: 4.514023
[INFO] Epoch: 38 , batch: 378 , training loss: 4.592078
[INFO] Epoch: 38 , batch: 379 , training loss: 4.556832
[INFO] Epoch: 38 , batch: 380 , training loss: 4.697323
[INFO] Epoch: 38 , batch: 381 , training loss: 4.424309
[INFO] Epoch: 38 , batch: 382 , training loss: 4.643484
[INFO] Epoch: 38 , batch: 383 , training loss: 4.674383
[INFO] Epoch: 38 , batch: 384 , training loss: 4.676284
[INFO] Epoch: 38 , batch: 385 , training loss: 4.375080
[INFO] Epoch: 38 , batch: 386 , training loss: 4.663133
[INFO] Epoch: 38 , batch: 387 , training loss: 4.624703
[INFO] Epoch: 38 , batch: 388 , training loss: 4.420239
[INFO] Epoch: 38 , batch: 389 , training loss: 4.270699
[INFO] Epoch: 38 , batch: 390 , training loss: 4.256260
[INFO] Epoch: 38 , batch: 391 , training loss: 4.327727
[INFO] Epoch: 38 , batch: 392 , training loss: 4.659554
[INFO] Epoch: 38 , batch: 393 , training loss: 4.537561
[INFO] Epoch: 38 , batch: 394 , training loss: 4.605551
[INFO] Epoch: 38 , batch: 395 , training loss: 4.468688
[INFO] Epoch: 38 , batch: 396 , training loss: 4.234405
[INFO] Epoch: 38 , batch: 397 , training loss: 4.398512
[INFO] Epoch: 38 , batch: 398 , training loss: 4.266832
[INFO] Epoch: 38 , batch: 399 , training loss: 4.327936
[INFO] Epoch: 38 , batch: 400 , training loss: 4.292766
[INFO] Epoch: 38 , batch: 401 , training loss: 4.700282
[INFO] Epoch: 38 , batch: 402 , training loss: 4.444939
[INFO] Epoch: 38 , batch: 403 , training loss: 4.225976
[INFO] Epoch: 38 , batch: 404 , training loss: 4.439850
[INFO] Epoch: 38 , batch: 405 , training loss: 4.489691
[INFO] Epoch: 38 , batch: 406 , training loss: 4.385202
[INFO] Epoch: 38 , batch: 407 , training loss: 4.449618
[INFO] Epoch: 38 , batch: 408 , training loss: 4.403072
[INFO] Epoch: 38 , batch: 409 , training loss: 4.391908
[INFO] Epoch: 38 , batch: 410 , training loss: 4.439612
[INFO] Epoch: 38 , batch: 411 , training loss: 4.618959
[INFO] Epoch: 38 , batch: 412 , training loss: 4.454847
[INFO] Epoch: 38 , batch: 413 , training loss: 4.342678
[INFO] Epoch: 38 , batch: 414 , training loss: 4.384268
[INFO] Epoch: 38 , batch: 415 , training loss: 4.415854
[INFO] Epoch: 38 , batch: 416 , training loss: 4.492916
[INFO] Epoch: 38 , batch: 417 , training loss: 4.400022
[INFO] Epoch: 38 , batch: 418 , training loss: 4.410100
[INFO] Epoch: 38 , batch: 419 , training loss: 4.405611
[INFO] Epoch: 38 , batch: 420 , training loss: 4.354975
[INFO] Epoch: 38 , batch: 421 , training loss: 4.364922
[INFO] Epoch: 38 , batch: 422 , training loss: 4.218391
[INFO] Epoch: 38 , batch: 423 , training loss: 4.436684
[INFO] Epoch: 38 , batch: 424 , training loss: 4.595044
[INFO] Epoch: 38 , batch: 425 , training loss: 4.449584
[INFO] Epoch: 38 , batch: 426 , training loss: 4.215269
[INFO] Epoch: 38 , batch: 427 , training loss: 4.425906
[INFO] Epoch: 38 , batch: 428 , training loss: 4.309285
[INFO] Epoch: 38 , batch: 429 , training loss: 4.170823
[INFO] Epoch: 38 , batch: 430 , training loss: 4.445352
[INFO] Epoch: 38 , batch: 431 , training loss: 4.036720
[INFO] Epoch: 38 , batch: 432 , training loss: 4.090485
[INFO] Epoch: 38 , batch: 433 , training loss: 4.132854
[INFO] Epoch: 38 , batch: 434 , training loss: 4.001904
[INFO] Epoch: 38 , batch: 435 , training loss: 4.382519
[INFO] Epoch: 38 , batch: 436 , training loss: 4.451148
[INFO] Epoch: 38 , batch: 437 , training loss: 4.202579
[INFO] Epoch: 38 , batch: 438 , training loss: 4.073924
[INFO] Epoch: 38 , batch: 439 , training loss: 4.269094
[INFO] Epoch: 38 , batch: 440 , training loss: 4.387353
[INFO] Epoch: 38 , batch: 441 , training loss: 4.489588
[INFO] Epoch: 38 , batch: 442 , training loss: 4.265091
[INFO] Epoch: 38 , batch: 443 , training loss: 4.461194
[INFO] Epoch: 38 , batch: 444 , training loss: 4.084262
[INFO] Epoch: 38 , batch: 445 , training loss: 3.988260
[INFO] Epoch: 38 , batch: 446 , training loss: 3.934313
[INFO] Epoch: 38 , batch: 447 , training loss: 4.105645
[INFO] Epoch: 38 , batch: 448 , training loss: 4.203220
[INFO] Epoch: 38 , batch: 449 , training loss: 4.608767
[INFO] Epoch: 38 , batch: 450 , training loss: 4.665680
[INFO] Epoch: 38 , batch: 451 , training loss: 4.572366
[INFO] Epoch: 38 , batch: 452 , training loss: 4.385877
[INFO] Epoch: 38 , batch: 453 , training loss: 4.159842
[INFO] Epoch: 38 , batch: 454 , training loss: 4.286993
[INFO] Epoch: 38 , batch: 455 , training loss: 4.334772
[INFO] Epoch: 38 , batch: 456 , training loss: 4.350641
[INFO] Epoch: 38 , batch: 457 , training loss: 4.416381
[INFO] Epoch: 38 , batch: 458 , training loss: 4.166847
[INFO] Epoch: 38 , batch: 459 , training loss: 4.148158
[INFO] Epoch: 38 , batch: 460 , training loss: 4.236860
[INFO] Epoch: 38 , batch: 461 , training loss: 4.214733
[INFO] Epoch: 38 , batch: 462 , training loss: 4.286124
[INFO] Epoch: 38 , batch: 463 , training loss: 4.202625
[INFO] Epoch: 38 , batch: 464 , training loss: 4.380514
[INFO] Epoch: 38 , batch: 465 , training loss: 4.314263
[INFO] Epoch: 38 , batch: 466 , training loss: 4.400127
[INFO] Epoch: 38 , batch: 467 , training loss: 4.372904
[INFO] Epoch: 38 , batch: 468 , training loss: 4.343636
[INFO] Epoch: 38 , batch: 469 , training loss: 4.380153
[INFO] Epoch: 38 , batch: 470 , training loss: 4.190054
[INFO] Epoch: 38 , batch: 471 , training loss: 4.275652
[INFO] Epoch: 38 , batch: 472 , training loss: 4.323140
[INFO] Epoch: 38 , batch: 473 , training loss: 4.254746
[INFO] Epoch: 38 , batch: 474 , training loss: 4.050828
[INFO] Epoch: 38 , batch: 475 , training loss: 3.925223
[INFO] Epoch: 38 , batch: 476 , training loss: 4.291545
[INFO] Epoch: 38 , batch: 477 , training loss: 4.429987
[INFO] Epoch: 38 , batch: 478 , training loss: 4.471962
[INFO] Epoch: 38 , batch: 479 , training loss: 4.415292
[INFO] Epoch: 38 , batch: 480 , training loss: 4.520069
[INFO] Epoch: 38 , batch: 481 , training loss: 4.407393
[INFO] Epoch: 38 , batch: 482 , training loss: 4.511309
[INFO] Epoch: 38 , batch: 483 , training loss: 4.386912
[INFO] Epoch: 38 , batch: 484 , training loss: 4.172856
[INFO] Epoch: 38 , batch: 485 , training loss: 4.260623
[INFO] Epoch: 38 , batch: 486 , training loss: 4.160365
[INFO] Epoch: 38 , batch: 487 , training loss: 4.145508
[INFO] Epoch: 38 , batch: 488 , training loss: 4.335084
[INFO] Epoch: 38 , batch: 489 , training loss: 4.218264
[INFO] Epoch: 38 , batch: 490 , training loss: 4.279859
[INFO] Epoch: 38 , batch: 491 , training loss: 4.221596
[INFO] Epoch: 38 , batch: 492 , training loss: 4.170928
[INFO] Epoch: 38 , batch: 493 , training loss: 4.365990
[INFO] Epoch: 38 , batch: 494 , training loss: 4.290106
[INFO] Epoch: 38 , batch: 495 , training loss: 4.404816
[INFO] Epoch: 38 , batch: 496 , training loss: 4.280021
[INFO] Epoch: 38 , batch: 497 , training loss: 4.332961
[INFO] Epoch: 38 , batch: 498 , training loss: 4.335552
[INFO] Epoch: 38 , batch: 499 , training loss: 4.375234
[INFO] Epoch: 38 , batch: 500 , training loss: 4.541284
[INFO] Epoch: 38 , batch: 501 , training loss: 4.812668
[INFO] Epoch: 38 , batch: 502 , training loss: 4.859774
[INFO] Epoch: 38 , batch: 503 , training loss: 4.568303
[INFO] Epoch: 38 , batch: 504 , training loss: 4.675679
[INFO] Epoch: 38 , batch: 505 , training loss: 4.678304
[INFO] Epoch: 38 , batch: 506 , training loss: 4.618068
[INFO] Epoch: 38 , batch: 507 , training loss: 4.692616
[INFO] Epoch: 38 , batch: 508 , training loss: 4.648179
[INFO] Epoch: 38 , batch: 509 , training loss: 4.426729
[INFO] Epoch: 38 , batch: 510 , training loss: 4.510769
[INFO] Epoch: 38 , batch: 511 , training loss: 4.418056
[INFO] Epoch: 38 , batch: 512 , training loss: 4.524262
[INFO] Epoch: 38 , batch: 513 , training loss: 4.772211
[INFO] Epoch: 38 , batch: 514 , training loss: 4.429035
[INFO] Epoch: 38 , batch: 515 , training loss: 4.655427
[INFO] Epoch: 38 , batch: 516 , training loss: 4.474750
[INFO] Epoch: 38 , batch: 517 , training loss: 4.429805
[INFO] Epoch: 38 , batch: 518 , training loss: 4.385869
[INFO] Epoch: 38 , batch: 519 , training loss: 4.244437
[INFO] Epoch: 38 , batch: 520 , training loss: 4.478132
[INFO] Epoch: 38 , batch: 521 , training loss: 4.456952
[INFO] Epoch: 38 , batch: 522 , training loss: 4.506984
[INFO] Epoch: 38 , batch: 523 , training loss: 4.413245
[INFO] Epoch: 38 , batch: 524 , training loss: 4.710374
[INFO] Epoch: 38 , batch: 525 , training loss: 4.656419
[INFO] Epoch: 38 , batch: 526 , training loss: 4.411324
[INFO] Epoch: 38 , batch: 527 , training loss: 4.452336
[INFO] Epoch: 38 , batch: 528 , training loss: 4.454438
[INFO] Epoch: 38 , batch: 529 , training loss: 4.435609
[INFO] Epoch: 38 , batch: 530 , training loss: 4.281308
[INFO] Epoch: 38 , batch: 531 , training loss: 4.422546
[INFO] Epoch: 38 , batch: 532 , training loss: 4.337731
[INFO] Epoch: 38 , batch: 533 , training loss: 4.481735
[INFO] Epoch: 38 , batch: 534 , training loss: 4.452415
[INFO] Epoch: 38 , batch: 535 , training loss: 4.479034
[INFO] Epoch: 38 , batch: 536 , training loss: 4.296983
[INFO] Epoch: 38 , batch: 537 , training loss: 4.295725
[INFO] Epoch: 38 , batch: 538 , training loss: 4.381424
[INFO] Epoch: 38 , batch: 539 , training loss: 4.488527
[INFO] Epoch: 38 , batch: 540 , training loss: 4.990343
[INFO] Epoch: 38 , batch: 541 , training loss: 4.846283
[INFO] Epoch: 38 , batch: 542 , training loss: 4.760823
[INFO] Epoch: 39 , batch: 0 , training loss: 3.295232
[INFO] Epoch: 39 , batch: 1 , training loss: 3.412691
[INFO] Epoch: 39 , batch: 2 , training loss: 3.564164
[INFO] Epoch: 39 , batch: 3 , training loss: 3.396703
[INFO] Epoch: 39 , batch: 4 , training loss: 3.782510
[INFO] Epoch: 39 , batch: 5 , training loss: 3.420102
[INFO] Epoch: 39 , batch: 6 , training loss: 3.824363
[INFO] Epoch: 39 , batch: 7 , training loss: 3.766048
[INFO] Epoch: 39 , batch: 8 , training loss: 3.475787
[INFO] Epoch: 39 , batch: 9 , training loss: 3.725199
[INFO] Epoch: 39 , batch: 10 , training loss: 3.719119
[INFO] Epoch: 39 , batch: 11 , training loss: 3.674740
[INFO] Epoch: 39 , batch: 12 , training loss: 3.560560
[INFO] Epoch: 39 , batch: 13 , training loss: 3.606660
[INFO] Epoch: 39 , batch: 14 , training loss: 3.473763
[INFO] Epoch: 39 , batch: 15 , training loss: 3.665972
[INFO] Epoch: 39 , batch: 16 , training loss: 3.544309
[INFO] Epoch: 39 , batch: 17 , training loss: 3.706898
[INFO] Epoch: 39 , batch: 18 , training loss: 3.637785
[INFO] Epoch: 39 , batch: 19 , training loss: 3.409456
[INFO] Epoch: 39 , batch: 20 , training loss: 3.394056
[INFO] Epoch: 39 , batch: 21 , training loss: 3.504030
[INFO] Epoch: 39 , batch: 22 , training loss: 3.370382
[INFO] Epoch: 39 , batch: 23 , training loss: 3.615564
[INFO] Epoch: 39 , batch: 24 , training loss: 3.387488
[INFO] Epoch: 39 , batch: 25 , training loss: 3.619589
[INFO] Epoch: 39 , batch: 26 , training loss: 3.481761
[INFO] Epoch: 39 , batch: 27 , training loss: 3.407688
[INFO] Epoch: 39 , batch: 28 , training loss: 3.567928
[INFO] Epoch: 39 , batch: 29 , training loss: 3.435013
[INFO] Epoch: 39 , batch: 30 , training loss: 3.483976
[INFO] Epoch: 39 , batch: 31 , training loss: 3.518654
[INFO] Epoch: 39 , batch: 32 , training loss: 3.512380
[INFO] Epoch: 39 , batch: 33 , training loss: 3.530237
[INFO] Epoch: 39 , batch: 34 , training loss: 3.479776
[INFO] Epoch: 39 , batch: 35 , training loss: 3.529320
[INFO] Epoch: 39 , batch: 36 , training loss: 3.574791
[INFO] Epoch: 39 , batch: 37 , training loss: 3.400543
[INFO] Epoch: 39 , batch: 38 , training loss: 3.451188
[INFO] Epoch: 39 , batch: 39 , training loss: 3.361934
[INFO] Epoch: 39 , batch: 40 , training loss: 3.569358
[INFO] Epoch: 39 , batch: 41 , training loss: 3.456058
[INFO] Epoch: 39 , batch: 42 , training loss: 3.886648
[INFO] Epoch: 39 , batch: 43 , training loss: 3.616009
[INFO] Epoch: 39 , batch: 44 , training loss: 3.955533
[INFO] Epoch: 39 , batch: 45 , training loss: 3.915862
[INFO] Epoch: 39 , batch: 46 , training loss: 3.836645
[INFO] Epoch: 39 , batch: 47 , training loss: 3.561262
[INFO] Epoch: 39 , batch: 48 , training loss: 3.562562
[INFO] Epoch: 39 , batch: 49 , training loss: 3.761380
[INFO] Epoch: 39 , batch: 50 , training loss: 3.532725
[INFO] Epoch: 39 , batch: 51 , training loss: 3.776002
[INFO] Epoch: 39 , batch: 52 , training loss: 3.609218
[INFO] Epoch: 39 , batch: 53 , training loss: 3.746621
[INFO] Epoch: 39 , batch: 54 , training loss: 3.714506
[INFO] Epoch: 39 , batch: 55 , training loss: 3.833524
[INFO] Epoch: 39 , batch: 56 , training loss: 3.702654
[INFO] Epoch: 39 , batch: 57 , training loss: 3.584565
[INFO] Epoch: 39 , batch: 58 , training loss: 3.661056
[INFO] Epoch: 39 , batch: 59 , training loss: 3.762058
[INFO] Epoch: 39 , batch: 60 , training loss: 3.679957
[INFO] Epoch: 39 , batch: 61 , training loss: 3.756432
[INFO] Epoch: 39 , batch: 62 , training loss: 3.680413
[INFO] Epoch: 39 , batch: 63 , training loss: 3.844793
[INFO] Epoch: 39 , batch: 64 , training loss: 4.037644
[INFO] Epoch: 39 , batch: 65 , training loss: 3.720259
[INFO] Epoch: 39 , batch: 66 , training loss: 3.577526
[INFO] Epoch: 39 , batch: 67 , training loss: 3.668199
[INFO] Epoch: 39 , batch: 68 , training loss: 3.831770
[INFO] Epoch: 39 , batch: 69 , training loss: 3.711641
[INFO] Epoch: 39 , batch: 70 , training loss: 3.926578
[INFO] Epoch: 39 , batch: 71 , training loss: 3.815701
[INFO] Epoch: 39 , batch: 72 , training loss: 3.849248
[INFO] Epoch: 39 , batch: 73 , training loss: 3.793120
[INFO] Epoch: 39 , batch: 74 , training loss: 3.912263
[INFO] Epoch: 39 , batch: 75 , training loss: 3.792708
[INFO] Epoch: 39 , batch: 76 , training loss: 3.840607
[INFO] Epoch: 39 , batch: 77 , training loss: 3.819862
[INFO] Epoch: 39 , batch: 78 , training loss: 3.938596
[INFO] Epoch: 39 , batch: 79 , training loss: 3.755087
[INFO] Epoch: 39 , batch: 80 , training loss: 3.943244
[INFO] Epoch: 39 , batch: 81 , training loss: 3.878781
[INFO] Epoch: 39 , batch: 82 , training loss: 3.837319
[INFO] Epoch: 39 , batch: 83 , training loss: 3.989137
[INFO] Epoch: 39 , batch: 84 , training loss: 3.909003
[INFO] Epoch: 39 , batch: 85 , training loss: 3.988718
[INFO] Epoch: 39 , batch: 86 , training loss: 3.913794
[INFO] Epoch: 39 , batch: 87 , training loss: 3.902754
[INFO] Epoch: 39 , batch: 88 , training loss: 4.005631
[INFO] Epoch: 39 , batch: 89 , training loss: 3.821850
[INFO] Epoch: 39 , batch: 90 , training loss: 3.922872
[INFO] Epoch: 39 , batch: 91 , training loss: 3.877376
[INFO] Epoch: 39 , batch: 92 , training loss: 3.847742
[INFO] Epoch: 39 , batch: 93 , training loss: 3.978595
[INFO] Epoch: 39 , batch: 94 , training loss: 4.105364
[INFO] Epoch: 39 , batch: 95 , training loss: 3.847497
[INFO] Epoch: 39 , batch: 96 , training loss: 3.885469
[INFO] Epoch: 39 , batch: 97 , training loss: 3.777155
[INFO] Epoch: 39 , batch: 98 , training loss: 3.743202
[INFO] Epoch: 39 , batch: 99 , training loss: 3.841360
[INFO] Epoch: 39 , batch: 100 , training loss: 3.788395
[INFO] Epoch: 39 , batch: 101 , training loss: 3.829361
[INFO] Epoch: 39 , batch: 102 , training loss: 3.917093
[INFO] Epoch: 39 , batch: 103 , training loss: 3.750387
[INFO] Epoch: 39 , batch: 104 , training loss: 3.683659
[INFO] Epoch: 39 , batch: 105 , training loss: 3.980589
[INFO] Epoch: 39 , batch: 106 , training loss: 3.927696
[INFO] Epoch: 39 , batch: 107 , training loss: 3.764690
[INFO] Epoch: 39 , batch: 108 , training loss: 3.714699
[INFO] Epoch: 39 , batch: 109 , training loss: 3.640197
[INFO] Epoch: 39 , batch: 110 , training loss: 3.859684
[INFO] Epoch: 39 , batch: 111 , training loss: 3.908186
[INFO] Epoch: 39 , batch: 112 , training loss: 3.833490
[INFO] Epoch: 39 , batch: 113 , training loss: 3.859291
[INFO] Epoch: 39 , batch: 114 , training loss: 3.824320
[INFO] Epoch: 39 , batch: 115 , training loss: 3.825387
[INFO] Epoch: 39 , batch: 116 , training loss: 3.750767
[INFO] Epoch: 39 , batch: 117 , training loss: 3.998113
[INFO] Epoch: 39 , batch: 118 , training loss: 3.943321
[INFO] Epoch: 39 , batch: 119 , training loss: 4.026913
[INFO] Epoch: 39 , batch: 120 , training loss: 4.060424
[INFO] Epoch: 39 , batch: 121 , training loss: 3.898212
[INFO] Epoch: 39 , batch: 122 , training loss: 3.803380
[INFO] Epoch: 39 , batch: 123 , training loss: 3.839968
[INFO] Epoch: 39 , batch: 124 , training loss: 3.922578
[INFO] Epoch: 39 , batch: 125 , training loss: 3.748528
[INFO] Epoch: 39 , batch: 126 , training loss: 3.734031
[INFO] Epoch: 39 , batch: 127 , training loss: 3.755968
[INFO] Epoch: 39 , batch: 128 , training loss: 3.897907
[INFO] Epoch: 39 , batch: 129 , training loss: 3.877729
[INFO] Epoch: 39 , batch: 130 , training loss: 3.838999
[INFO] Epoch: 39 , batch: 131 , training loss: 3.861241
[INFO] Epoch: 39 , batch: 132 , training loss: 3.852001
[INFO] Epoch: 39 , batch: 133 , training loss: 3.824671
[INFO] Epoch: 39 , batch: 134 , training loss: 3.640567
[INFO] Epoch: 39 , batch: 135 , training loss: 3.646768
[INFO] Epoch: 39 , batch: 136 , training loss: 3.936193
[INFO] Epoch: 39 , batch: 137 , training loss: 3.876467
[INFO] Epoch: 39 , batch: 138 , training loss: 3.915504
[INFO] Epoch: 39 , batch: 139 , training loss: 4.503613
[INFO] Epoch: 39 , batch: 140 , training loss: 4.220205
[INFO] Epoch: 39 , batch: 141 , training loss: 4.029150
[INFO] Epoch: 39 , batch: 142 , training loss: 3.806083
[INFO] Epoch: 39 , batch: 143 , training loss: 3.911316
[INFO] Epoch: 39 , batch: 144 , training loss: 3.723274
[INFO] Epoch: 39 , batch: 145 , training loss: 3.796931
[INFO] Epoch: 39 , batch: 146 , training loss: 3.983188
[INFO] Epoch: 39 , batch: 147 , training loss: 3.664549
[INFO] Epoch: 39 , batch: 148 , training loss: 3.685823
[INFO] Epoch: 39 , batch: 149 , training loss: 3.757651
[INFO] Epoch: 39 , batch: 150 , training loss: 3.969829
[INFO] Epoch: 39 , batch: 151 , training loss: 3.840682
[INFO] Epoch: 39 , batch: 152 , training loss: 3.908818
[INFO] Epoch: 39 , batch: 153 , training loss: 3.871313
[INFO] Epoch: 39 , batch: 154 , training loss: 3.972567
[INFO] Epoch: 39 , batch: 155 , training loss: 4.194349
[INFO] Epoch: 39 , batch: 156 , training loss: 3.885779
[INFO] Epoch: 39 , batch: 157 , training loss: 3.895575
[INFO] Epoch: 39 , batch: 158 , training loss: 4.019216
[INFO] Epoch: 39 , batch: 159 , training loss: 3.885054
[INFO] Epoch: 39 , batch: 160 , training loss: 4.207265
[INFO] Epoch: 39 , batch: 161 , training loss: 4.300744
[INFO] Epoch: 39 , batch: 162 , training loss: 4.265488
[INFO] Epoch: 39 , batch: 163 , training loss: 4.409420
[INFO] Epoch: 39 , batch: 164 , training loss: 4.416746
[INFO] Epoch: 39 , batch: 165 , training loss: 4.305803
[INFO] Epoch: 39 , batch: 166 , training loss: 4.149711
[INFO] Epoch: 39 , batch: 167 , training loss: 4.177479
[INFO] Epoch: 39 , batch: 168 , training loss: 3.885133
[INFO] Epoch: 39 , batch: 169 , training loss: 3.884537
[INFO] Epoch: 39 , batch: 170 , training loss: 4.121821
[INFO] Epoch: 39 , batch: 171 , training loss: 3.487272
[INFO] Epoch: 39 , batch: 172 , training loss: 3.697383
[INFO] Epoch: 39 , batch: 173 , training loss: 4.054767
[INFO] Epoch: 39 , batch: 174 , training loss: 4.454799
[INFO] Epoch: 39 , batch: 175 , training loss: 4.799160
[INFO] Epoch: 39 , batch: 176 , training loss: 4.436562
[INFO] Epoch: 39 , batch: 177 , training loss: 4.117158
[INFO] Epoch: 39 , batch: 178 , training loss: 4.089740
[INFO] Epoch: 39 , batch: 179 , training loss: 4.168499
[INFO] Epoch: 39 , batch: 180 , training loss: 4.086107
[INFO] Epoch: 39 , batch: 181 , training loss: 4.393874
[INFO] Epoch: 39 , batch: 182 , training loss: 4.332913
[INFO] Epoch: 39 , batch: 183 , training loss: 4.282069
[INFO] Epoch: 39 , batch: 184 , training loss: 4.171638
[INFO] Epoch: 39 , batch: 185 , training loss: 4.123468
[INFO] Epoch: 39 , batch: 186 , training loss: 4.314366
[INFO] Epoch: 39 , batch: 187 , training loss: 4.433028
[INFO] Epoch: 39 , batch: 188 , training loss: 4.389716
[INFO] Epoch: 39 , batch: 189 , training loss: 4.305245
[INFO] Epoch: 39 , batch: 190 , training loss: 4.304941
[INFO] Epoch: 39 , batch: 191 , training loss: 4.438829
[INFO] Epoch: 39 , batch: 192 , training loss: 4.244313
[INFO] Epoch: 39 , batch: 193 , training loss: 4.349414
[INFO] Epoch: 39 , batch: 194 , training loss: 4.300519
[INFO] Epoch: 39 , batch: 195 , training loss: 4.209188
[INFO] Epoch: 39 , batch: 196 , training loss: 4.090068
[INFO] Epoch: 39 , batch: 197 , training loss: 4.159334
[INFO] Epoch: 39 , batch: 198 , training loss: 4.094383
[INFO] Epoch: 39 , batch: 199 , training loss: 4.238783
[INFO] Epoch: 39 , batch: 200 , training loss: 4.133373
[INFO] Epoch: 39 , batch: 201 , training loss: 4.061234
[INFO] Epoch: 39 , batch: 202 , training loss: 4.036615
[INFO] Epoch: 39 , batch: 203 , training loss: 4.162016
[INFO] Epoch: 39 , batch: 204 , training loss: 4.309927
[INFO] Epoch: 39 , batch: 205 , training loss: 3.857121
[INFO] Epoch: 39 , batch: 206 , training loss: 3.809512
[INFO] Epoch: 39 , batch: 207 , training loss: 3.797375
[INFO] Epoch: 39 , batch: 208 , training loss: 4.113595
[INFO] Epoch: 39 , batch: 209 , training loss: 4.070419
[INFO] Epoch: 39 , batch: 210 , training loss: 4.096293
[INFO] Epoch: 39 , batch: 211 , training loss: 4.080583
[INFO] Epoch: 39 , batch: 212 , training loss: 4.204072
[INFO] Epoch: 39 , batch: 213 , training loss: 4.136781
[INFO] Epoch: 39 , batch: 214 , training loss: 4.206864
[INFO] Epoch: 39 , batch: 215 , training loss: 4.453619
[INFO] Epoch: 39 , batch: 216 , training loss: 4.121294
[INFO] Epoch: 39 , batch: 217 , training loss: 4.079073
[INFO] Epoch: 39 , batch: 218 , training loss: 4.058883
[INFO] Epoch: 39 , batch: 219 , training loss: 4.184468
[INFO] Epoch: 39 , batch: 220 , training loss: 3.987533
[INFO] Epoch: 39 , batch: 221 , training loss: 4.024800
[INFO] Epoch: 39 , batch: 222 , training loss: 4.152727
[INFO] Epoch: 39 , batch: 223 , training loss: 4.255201
[INFO] Epoch: 39 , batch: 224 , training loss: 4.299724
[INFO] Epoch: 39 , batch: 225 , training loss: 4.186535
[INFO] Epoch: 39 , batch: 226 , training loss: 4.300992
[INFO] Epoch: 39 , batch: 227 , training loss: 4.268657
[INFO] Epoch: 39 , batch: 228 , training loss: 4.311531
[INFO] Epoch: 39 , batch: 229 , training loss: 4.143114
[INFO] Epoch: 39 , batch: 230 , training loss: 4.029898
[INFO] Epoch: 39 , batch: 231 , training loss: 3.885383
[INFO] Epoch: 39 , batch: 232 , training loss: 4.034309
[INFO] Epoch: 39 , batch: 233 , training loss: 4.046000
[INFO] Epoch: 39 , batch: 234 , training loss: 3.758574
[INFO] Epoch: 39 , batch: 235 , training loss: 3.846695
[INFO] Epoch: 39 , batch: 236 , training loss: 3.941399
[INFO] Epoch: 39 , batch: 237 , training loss: 4.173034
[INFO] Epoch: 39 , batch: 238 , training loss: 3.948128
[INFO] Epoch: 39 , batch: 239 , training loss: 3.985061
[INFO] Epoch: 39 , batch: 240 , training loss: 4.058771
[INFO] Epoch: 39 , batch: 241 , training loss: 3.849215
[INFO] Epoch: 39 , batch: 242 , training loss: 3.862364
[INFO] Epoch: 39 , batch: 243 , training loss: 4.168444
[INFO] Epoch: 39 , batch: 244 , training loss: 4.122661
[INFO] Epoch: 39 , batch: 245 , training loss: 4.063222
[INFO] Epoch: 39 , batch: 246 , training loss: 3.786492
[INFO] Epoch: 39 , batch: 247 , training loss: 3.945816
[INFO] Epoch: 39 , batch: 248 , training loss: 4.004298
[INFO] Epoch: 39 , batch: 249 , training loss: 4.046655
[INFO] Epoch: 39 , batch: 250 , training loss: 3.805580
[INFO] Epoch: 39 , batch: 251 , training loss: 4.239879
[INFO] Epoch: 39 , batch: 252 , training loss: 3.933288
[INFO] Epoch: 39 , batch: 253 , training loss: 3.869998
[INFO] Epoch: 39 , batch: 254 , training loss: 4.169830
[INFO] Epoch: 39 , batch: 255 , training loss: 4.111641
[INFO] Epoch: 39 , batch: 256 , training loss: 4.125110
[INFO] Epoch: 39 , batch: 257 , training loss: 4.279199
[INFO] Epoch: 39 , batch: 258 , training loss: 4.295515
[INFO] Epoch: 39 , batch: 259 , training loss: 4.343675
[INFO] Epoch: 39 , batch: 260 , training loss: 4.072649
[INFO] Epoch: 39 , batch: 261 , training loss: 4.217138
[INFO] Epoch: 39 , batch: 262 , training loss: 4.391283
[INFO] Epoch: 39 , batch: 263 , training loss: 4.593054
[INFO] Epoch: 39 , batch: 264 , training loss: 3.887982
[INFO] Epoch: 39 , batch: 265 , training loss: 4.047323
[INFO] Epoch: 39 , batch: 266 , training loss: 4.444072
[INFO] Epoch: 39 , batch: 267 , training loss: 4.205973
[INFO] Epoch: 39 , batch: 268 , training loss: 4.145621
[INFO] Epoch: 39 , batch: 269 , training loss: 4.117384
[INFO] Epoch: 39 , batch: 270 , training loss: 4.151185
[INFO] Epoch: 39 , batch: 271 , training loss: 4.173285
[INFO] Epoch: 39 , batch: 272 , training loss: 4.146290
[INFO] Epoch: 39 , batch: 273 , training loss: 4.190581
[INFO] Epoch: 39 , batch: 274 , training loss: 4.233137
[INFO] Epoch: 39 , batch: 275 , training loss: 4.115740
[INFO] Epoch: 39 , batch: 276 , training loss: 4.171185
[INFO] Epoch: 39 , batch: 277 , training loss: 4.309980
[INFO] Epoch: 39 , batch: 278 , training loss: 4.004746
[INFO] Epoch: 39 , batch: 279 , training loss: 4.020957
[INFO] Epoch: 39 , batch: 280 , training loss: 3.969831
[INFO] Epoch: 39 , batch: 281 , training loss: 4.107326
[INFO] Epoch: 39 , batch: 282 , training loss: 4.028070
[INFO] Epoch: 39 , batch: 283 , training loss: 4.048293
[INFO] Epoch: 39 , batch: 284 , training loss: 4.057952
[INFO] Epoch: 39 , batch: 285 , training loss: 3.997626
[INFO] Epoch: 39 , batch: 286 , training loss: 3.996846
[INFO] Epoch: 39 , batch: 287 , training loss: 3.945753
[INFO] Epoch: 39 , batch: 288 , training loss: 3.955655
[INFO] Epoch: 39 , batch: 289 , training loss: 3.998486
[INFO] Epoch: 39 , batch: 290 , training loss: 3.795774
[INFO] Epoch: 39 , batch: 291 , training loss: 3.736635
[INFO] Epoch: 39 , batch: 292 , training loss: 3.882827
[INFO] Epoch: 39 , batch: 293 , training loss: 3.812079
[INFO] Epoch: 39 , batch: 294 , training loss: 4.455164
[INFO] Epoch: 39 , batch: 295 , training loss: 4.219190
[INFO] Epoch: 39 , batch: 296 , training loss: 4.154615
[INFO] Epoch: 39 , batch: 297 , training loss: 4.122604
[INFO] Epoch: 39 , batch: 298 , training loss: 3.941948
[INFO] Epoch: 39 , batch: 299 , training loss: 3.983338
[INFO] Epoch: 39 , batch: 300 , training loss: 3.943308
[INFO] Epoch: 39 , batch: 301 , training loss: 3.901954
[INFO] Epoch: 39 , batch: 302 , training loss: 4.068379
[INFO] Epoch: 39 , batch: 303 , training loss: 4.068481
[INFO] Epoch: 39 , batch: 304 , training loss: 4.227541
[INFO] Epoch: 39 , batch: 305 , training loss: 4.040294
[INFO] Epoch: 39 , batch: 306 , training loss: 4.152201
[INFO] Epoch: 39 , batch: 307 , training loss: 4.143225
[INFO] Epoch: 39 , batch: 308 , training loss: 3.994323
[INFO] Epoch: 39 , batch: 309 , training loss: 3.994058
[INFO] Epoch: 39 , batch: 310 , training loss: 3.919309
[INFO] Epoch: 39 , batch: 311 , training loss: 3.902339
[INFO] Epoch: 39 , batch: 312 , training loss: 3.838202
[INFO] Epoch: 39 , batch: 313 , training loss: 3.951431
[INFO] Epoch: 39 , batch: 314 , training loss: 3.977830
[INFO] Epoch: 39 , batch: 315 , training loss: 4.044703
[INFO] Epoch: 39 , batch: 316 , training loss: 4.287162
[INFO] Epoch: 39 , batch: 317 , training loss: 4.715097
[INFO] Epoch: 39 , batch: 318 , training loss: 4.858600
[INFO] Epoch: 39 , batch: 319 , training loss: 4.471061
[INFO] Epoch: 39 , batch: 320 , training loss: 4.000380
[INFO] Epoch: 39 , batch: 321 , training loss: 3.850635
[INFO] Epoch: 39 , batch: 322 , training loss: 3.976775
[INFO] Epoch: 39 , batch: 323 , training loss: 3.987449
[INFO] Epoch: 39 , batch: 324 , training loss: 3.931768
[INFO] Epoch: 39 , batch: 325 , training loss: 4.098326
[INFO] Epoch: 39 , batch: 326 , training loss: 4.157447
[INFO] Epoch: 39 , batch: 327 , training loss: 4.047058
[INFO] Epoch: 39 , batch: 328 , training loss: 4.050937
[INFO] Epoch: 39 , batch: 329 , training loss: 3.977543
[INFO] Epoch: 39 , batch: 330 , training loss: 3.989682
[INFO] Epoch: 39 , batch: 331 , training loss: 4.134097
[INFO] Epoch: 39 , batch: 332 , training loss: 3.935601
[INFO] Epoch: 39 , batch: 333 , training loss: 3.960140
[INFO] Epoch: 39 , batch: 334 , training loss: 3.925095
[INFO] Epoch: 39 , batch: 335 , training loss: 4.080997
[INFO] Epoch: 39 , batch: 336 , training loss: 4.123695
[INFO] Epoch: 39 , batch: 337 , training loss: 4.134047
[INFO] Epoch: 39 , batch: 338 , training loss: 4.358867
[INFO] Epoch: 39 , batch: 339 , training loss: 4.199165
[INFO] Epoch: 39 , batch: 340 , training loss: 4.337555
[INFO] Epoch: 39 , batch: 341 , training loss: 4.109990
[INFO] Epoch: 39 , batch: 342 , training loss: 3.874666
[INFO] Epoch: 39 , batch: 343 , training loss: 3.968853
[INFO] Epoch: 39 , batch: 344 , training loss: 3.834191
[INFO] Epoch: 39 , batch: 345 , training loss: 3.961837
[INFO] Epoch: 39 , batch: 346 , training loss: 4.002694
[INFO] Epoch: 39 , batch: 347 , training loss: 3.905978
[INFO] Epoch: 39 , batch: 348 , training loss: 4.002295
[INFO] Epoch: 39 , batch: 349 , training loss: 4.158076
[INFO] Epoch: 39 , batch: 350 , training loss: 3.945392
[INFO] Epoch: 39 , batch: 351 , training loss: 4.035133
[INFO] Epoch: 39 , batch: 352 , training loss: 4.044586
[INFO] Epoch: 39 , batch: 353 , training loss: 4.002025
[INFO] Epoch: 39 , batch: 354 , training loss: 4.117550
[INFO] Epoch: 39 , batch: 355 , training loss: 4.156004
[INFO] Epoch: 39 , batch: 356 , training loss: 4.008888
[INFO] Epoch: 39 , batch: 357 , training loss: 4.086563
[INFO] Epoch: 39 , batch: 358 , training loss: 3.965029
[INFO] Epoch: 39 , batch: 359 , training loss: 3.983907
[INFO] Epoch: 39 , batch: 360 , training loss: 4.074509
[INFO] Epoch: 39 , batch: 361 , training loss: 4.040419
[INFO] Epoch: 39 , batch: 362 , training loss: 4.130250
[INFO] Epoch: 39 , batch: 363 , training loss: 4.022760
[INFO] Epoch: 39 , batch: 364 , training loss: 4.064744
[INFO] Epoch: 39 , batch: 365 , training loss: 4.009022
[INFO] Epoch: 39 , batch: 366 , training loss: 4.129353
[INFO] Epoch: 39 , batch: 367 , training loss: 4.195219
[INFO] Epoch: 39 , batch: 368 , training loss: 4.601506
[INFO] Epoch: 39 , batch: 369 , training loss: 4.230867
[INFO] Epoch: 39 , batch: 370 , training loss: 4.015765
[INFO] Epoch: 39 , batch: 371 , training loss: 4.368539
[INFO] Epoch: 39 , batch: 372 , training loss: 4.690391
[INFO] Epoch: 39 , batch: 373 , training loss: 4.761914
[INFO] Epoch: 39 , batch: 374 , training loss: 4.876215
[INFO] Epoch: 39 , batch: 375 , training loss: 4.838013
[INFO] Epoch: 39 , batch: 376 , training loss: 4.737804
[INFO] Epoch: 39 , batch: 377 , training loss: 4.499979
[INFO] Epoch: 39 , batch: 378 , training loss: 4.587379
[INFO] Epoch: 39 , batch: 379 , training loss: 4.573836
[INFO] Epoch: 39 , batch: 380 , training loss: 4.712461
[INFO] Epoch: 39 , batch: 381 , training loss: 4.450629
[INFO] Epoch: 39 , batch: 382 , training loss: 4.666121
[INFO] Epoch: 39 , batch: 383 , training loss: 4.673708
[INFO] Epoch: 39 , batch: 384 , training loss: 4.679031
[INFO] Epoch: 39 , batch: 385 , training loss: 4.377377
[INFO] Epoch: 39 , batch: 386 , training loss: 4.659624
[INFO] Epoch: 39 , batch: 387 , training loss: 4.605369
[INFO] Epoch: 39 , batch: 388 , training loss: 4.421127
[INFO] Epoch: 39 , batch: 389 , training loss: 4.254519
[INFO] Epoch: 39 , batch: 390 , training loss: 4.260495
[INFO] Epoch: 39 , batch: 391 , training loss: 4.312983
[INFO] Epoch: 39 , batch: 392 , training loss: 4.660793
[INFO] Epoch: 39 , batch: 393 , training loss: 4.552499
[INFO] Epoch: 39 , batch: 394 , training loss: 4.602454
[INFO] Epoch: 39 , batch: 395 , training loss: 4.474321
[INFO] Epoch: 39 , batch: 396 , training loss: 4.248144
[INFO] Epoch: 39 , batch: 397 , training loss: 4.401468
[INFO] Epoch: 39 , batch: 398 , training loss: 4.264335
[INFO] Epoch: 39 , batch: 399 , training loss: 4.323784
[INFO] Epoch: 39 , batch: 400 , training loss: 4.298528
[INFO] Epoch: 39 , batch: 401 , training loss: 4.698521
[INFO] Epoch: 39 , batch: 402 , training loss: 4.443157
[INFO] Epoch: 39 , batch: 403 , training loss: 4.242190
[INFO] Epoch: 39 , batch: 404 , training loss: 4.432074
[INFO] Epoch: 39 , batch: 405 , training loss: 4.488415
[INFO] Epoch: 39 , batch: 406 , training loss: 4.386123
[INFO] Epoch: 39 , batch: 407 , training loss: 4.453327
[INFO] Epoch: 39 , batch: 408 , training loss: 4.419350
[INFO] Epoch: 39 , batch: 409 , training loss: 4.374601
[INFO] Epoch: 39 , batch: 410 , training loss: 4.425072
[INFO] Epoch: 39 , batch: 411 , training loss: 4.636595
[INFO] Epoch: 39 , batch: 412 , training loss: 4.462181
[INFO] Epoch: 39 , batch: 413 , training loss: 4.354022
[INFO] Epoch: 39 , batch: 414 , training loss: 4.382267
[INFO] Epoch: 39 , batch: 415 , training loss: 4.414692
[INFO] Epoch: 39 , batch: 416 , training loss: 4.498305
[INFO] Epoch: 39 , batch: 417 , training loss: 4.421574
[INFO] Epoch: 39 , batch: 418 , training loss: 4.417380
[INFO] Epoch: 39 , batch: 419 , training loss: 4.402636
[INFO] Epoch: 39 , batch: 420 , training loss: 4.366627
[INFO] Epoch: 39 , batch: 421 , training loss: 4.367033
[INFO] Epoch: 39 , batch: 422 , training loss: 4.221550
[INFO] Epoch: 39 , batch: 423 , training loss: 4.437696
[INFO] Epoch: 39 , batch: 424 , training loss: 4.609520
[INFO] Epoch: 39 , batch: 425 , training loss: 4.456994
[INFO] Epoch: 39 , batch: 426 , training loss: 4.203362
[INFO] Epoch: 39 , batch: 427 , training loss: 4.446890
[INFO] Epoch: 39 , batch: 428 , training loss: 4.329976
[INFO] Epoch: 39 , batch: 429 , training loss: 4.181965
[INFO] Epoch: 39 , batch: 430 , training loss: 4.443938
[INFO] Epoch: 39 , batch: 431 , training loss: 4.048353
[INFO] Epoch: 39 , batch: 432 , training loss: 4.109532
[INFO] Epoch: 39 , batch: 433 , training loss: 4.136815
[INFO] Epoch: 39 , batch: 434 , training loss: 4.013649
[INFO] Epoch: 39 , batch: 435 , training loss: 4.379490
[INFO] Epoch: 39 , batch: 436 , training loss: 4.455447
[INFO] Epoch: 39 , batch: 437 , training loss: 4.216171
[INFO] Epoch: 39 , batch: 438 , training loss: 4.081536
[INFO] Epoch: 39 , batch: 439 , training loss: 4.277288
[INFO] Epoch: 39 , batch: 440 , training loss: 4.395048
[INFO] Epoch: 39 , batch: 441 , training loss: 4.498057
[INFO] Epoch: 39 , batch: 442 , training loss: 4.275279
[INFO] Epoch: 39 , batch: 443 , training loss: 4.471064
[INFO] Epoch: 39 , batch: 444 , training loss: 4.089919
[INFO] Epoch: 39 , batch: 445 , training loss: 3.978541
[INFO] Epoch: 39 , batch: 446 , training loss: 3.939031
[INFO] Epoch: 39 , batch: 447 , training loss: 4.124116
[INFO] Epoch: 39 , batch: 448 , training loss: 4.208211
[INFO] Epoch: 39 , batch: 449 , training loss: 4.614271
[INFO] Epoch: 39 , batch: 450 , training loss: 4.664606
[INFO] Epoch: 39 , batch: 451 , training loss: 4.586235
[INFO] Epoch: 39 , batch: 452 , training loss: 4.386411
[INFO] Epoch: 39 , batch: 453 , training loss: 4.160440
[INFO] Epoch: 39 , batch: 454 , training loss: 4.305468
[INFO] Epoch: 39 , batch: 455 , training loss: 4.362679
[INFO] Epoch: 39 , batch: 456 , training loss: 4.340802
[INFO] Epoch: 39 , batch: 457 , training loss: 4.431412
[INFO] Epoch: 39 , batch: 458 , training loss: 4.172026
[INFO] Epoch: 39 , batch: 459 , training loss: 4.155191
[INFO] Epoch: 39 , batch: 460 , training loss: 4.247437
[INFO] Epoch: 39 , batch: 461 , training loss: 4.218121
[INFO] Epoch: 39 , batch: 462 , training loss: 4.290123
[INFO] Epoch: 39 , batch: 463 , training loss: 4.207374
[INFO] Epoch: 39 , batch: 464 , training loss: 4.379737
[INFO] Epoch: 39 , batch: 465 , training loss: 4.309710
[INFO] Epoch: 39 , batch: 466 , training loss: 4.414546
[INFO] Epoch: 39 , batch: 467 , training loss: 4.380486
[INFO] Epoch: 39 , batch: 468 , training loss: 4.346810
[INFO] Epoch: 39 , batch: 469 , training loss: 4.376591
[INFO] Epoch: 39 , batch: 470 , training loss: 4.192246
[INFO] Epoch: 39 , batch: 471 , training loss: 4.274325
[INFO] Epoch: 39 , batch: 472 , training loss: 4.320870
[INFO] Epoch: 39 , batch: 473 , training loss: 4.252958
[INFO] Epoch: 39 , batch: 474 , training loss: 4.066096
[INFO] Epoch: 39 , batch: 475 , training loss: 3.930019
[INFO] Epoch: 39 , batch: 476 , training loss: 4.292994
[INFO] Epoch: 39 , batch: 477 , training loss: 4.445459
[INFO] Epoch: 39 , batch: 478 , training loss: 4.456548
[INFO] Epoch: 39 , batch: 479 , training loss: 4.436771
[INFO] Epoch: 39 , batch: 480 , training loss: 4.518436
[INFO] Epoch: 39 , batch: 481 , training loss: 4.408573
[INFO] Epoch: 39 , batch: 482 , training loss: 4.509635
[INFO] Epoch: 39 , batch: 483 , training loss: 4.386709
[INFO] Epoch: 39 , batch: 484 , training loss: 4.177188
[INFO] Epoch: 39 , batch: 485 , training loss: 4.276532
[INFO] Epoch: 39 , batch: 486 , training loss: 4.156431
[INFO] Epoch: 39 , batch: 487 , training loss: 4.163375
[INFO] Epoch: 39 , batch: 488 , training loss: 4.335935
[INFO] Epoch: 39 , batch: 489 , training loss: 4.223695
[INFO] Epoch: 39 , batch: 490 , training loss: 4.270588
[INFO] Epoch: 39 , batch: 491 , training loss: 4.213063
[INFO] Epoch: 39 , batch: 492 , training loss: 4.176229
[INFO] Epoch: 39 , batch: 493 , training loss: 4.366483
[INFO] Epoch: 39 , batch: 494 , training loss: 4.284285
[INFO] Epoch: 39 , batch: 495 , training loss: 4.407903
[INFO] Epoch: 39 , batch: 496 , training loss: 4.289196
[INFO] Epoch: 39 , batch: 497 , training loss: 4.335812
[INFO] Epoch: 39 , batch: 498 , training loss: 4.336247
[INFO] Epoch: 39 , batch: 499 , training loss: 4.378319
[INFO] Epoch: 39 , batch: 500 , training loss: 4.533927
[INFO] Epoch: 39 , batch: 501 , training loss: 4.831963
[INFO] Epoch: 39 , batch: 502 , training loss: 4.903738
[INFO] Epoch: 39 , batch: 503 , training loss: 4.584489
[INFO] Epoch: 39 , batch: 504 , training loss: 4.708063
[INFO] Epoch: 39 , batch: 505 , training loss: 4.684737
[INFO] Epoch: 39 , batch: 506 , training loss: 4.615355
[INFO] Epoch: 39 , batch: 507 , training loss: 4.697031
[INFO] Epoch: 39 , batch: 508 , training loss: 4.645542
[INFO] Epoch: 39 , batch: 509 , training loss: 4.450694
[INFO] Epoch: 39 , batch: 510 , training loss: 4.524973
[INFO] Epoch: 39 , batch: 511 , training loss: 4.432381
[INFO] Epoch: 39 , batch: 512 , training loss: 4.545016
[INFO] Epoch: 39 , batch: 513 , training loss: 4.765957
[INFO] Epoch: 39 , batch: 514 , training loss: 4.426648
[INFO] Epoch: 39 , batch: 515 , training loss: 4.643888
[INFO] Epoch: 39 , batch: 516 , training loss: 4.477180
[INFO] Epoch: 39 , batch: 517 , training loss: 4.439598
[INFO] Epoch: 39 , batch: 518 , training loss: 4.402812
[INFO] Epoch: 39 , batch: 519 , training loss: 4.243891
[INFO] Epoch: 39 , batch: 520 , training loss: 4.478288
[INFO] Epoch: 39 , batch: 521 , training loss: 4.465929
[INFO] Epoch: 39 , batch: 522 , training loss: 4.517277
[INFO] Epoch: 39 , batch: 523 , training loss: 4.422969
[INFO] Epoch: 39 , batch: 524 , training loss: 4.714959
[INFO] Epoch: 39 , batch: 525 , training loss: 4.658064
[INFO] Epoch: 39 , batch: 526 , training loss: 4.418505
[INFO] Epoch: 39 , batch: 527 , training loss: 4.463820
[INFO] Epoch: 39 , batch: 528 , training loss: 4.468632
[INFO] Epoch: 39 , batch: 529 , training loss: 4.447885
[INFO] Epoch: 39 , batch: 530 , training loss: 4.298586
[INFO] Epoch: 39 , batch: 531 , training loss: 4.433246
[INFO] Epoch: 39 , batch: 532 , training loss: 4.341828
[INFO] Epoch: 39 , batch: 533 , training loss: 4.479083
[INFO] Epoch: 39 , batch: 534 , training loss: 4.452339
[INFO] Epoch: 39 , batch: 535 , training loss: 4.493258
[INFO] Epoch: 39 , batch: 536 , training loss: 4.312820
[INFO] Epoch: 39 , batch: 537 , training loss: 4.307133
[INFO] Epoch: 39 , batch: 538 , training loss: 4.387691
[INFO] Epoch: 39 , batch: 539 , training loss: 4.498811
[INFO] Epoch: 39 , batch: 540 , training loss: 4.978522
[INFO] Epoch: 39 , batch: 541 , training loss: 4.842644
[INFO] Epoch: 39 , batch: 542 , training loss: 4.762820
[INFO] Epoch: 40 , batch: 0 , training loss: 3.306667
[INFO] Epoch: 40 , batch: 1 , training loss: 3.394078
[INFO] Epoch: 40 , batch: 2 , training loss: 3.608904
[INFO] Epoch: 40 , batch: 3 , training loss: 3.384607
[INFO] Epoch: 40 , batch: 4 , training loss: 3.820415
[INFO] Epoch: 40 , batch: 5 , training loss: 3.433624
[INFO] Epoch: 40 , batch: 6 , training loss: 3.831528
[INFO] Epoch: 40 , batch: 7 , training loss: 3.780389
[INFO] Epoch: 40 , batch: 8 , training loss: 3.459815
[INFO] Epoch: 40 , batch: 9 , training loss: 3.720296
[INFO] Epoch: 40 , batch: 10 , training loss: 3.717219
[INFO] Epoch: 40 , batch: 11 , training loss: 3.685037
[INFO] Epoch: 40 , batch: 12 , training loss: 3.569792
[INFO] Epoch: 40 , batch: 13 , training loss: 3.630434
[INFO] Epoch: 40 , batch: 14 , training loss: 3.458466
[INFO] Epoch: 40 , batch: 15 , training loss: 3.691622
[INFO] Epoch: 40 , batch: 16 , training loss: 3.554333
[INFO] Epoch: 40 , batch: 17 , training loss: 3.695273
[INFO] Epoch: 40 , batch: 18 , training loss: 3.664178
[INFO] Epoch: 40 , batch: 19 , training loss: 3.403286
[INFO] Epoch: 40 , batch: 20 , training loss: 3.423828
[INFO] Epoch: 40 , batch: 21 , training loss: 3.536613
[INFO] Epoch: 40 , batch: 22 , training loss: 3.387316
[INFO] Epoch: 40 , batch: 23 , training loss: 3.634714
[INFO] Epoch: 40 , batch: 24 , training loss: 3.408121
[INFO] Epoch: 40 , batch: 25 , training loss: 3.624302
[INFO] Epoch: 40 , batch: 26 , training loss: 3.505493
[INFO] Epoch: 40 , batch: 27 , training loss: 3.420920
[INFO] Epoch: 40 , batch: 28 , training loss: 3.597557
[INFO] Epoch: 40 , batch: 29 , training loss: 3.437614
[INFO] Epoch: 40 , batch: 30 , training loss: 3.471544
[INFO] Epoch: 40 , batch: 31 , training loss: 3.508857
[INFO] Epoch: 40 , batch: 32 , training loss: 3.509269
[INFO] Epoch: 40 , batch: 33 , training loss: 3.542997
[INFO] Epoch: 40 , batch: 34 , training loss: 3.495595
[INFO] Epoch: 40 , batch: 35 , training loss: 3.511798
[INFO] Epoch: 40 , batch: 36 , training loss: 3.544575
[INFO] Epoch: 40 , batch: 37 , training loss: 3.436620
[INFO] Epoch: 40 , batch: 38 , training loss: 3.434819
[INFO] Epoch: 40 , batch: 39 , training loss: 3.351371
[INFO] Epoch: 40 , batch: 40 , training loss: 3.583584
[INFO] Epoch: 40 , batch: 41 , training loss: 3.466582
[INFO] Epoch: 40 , batch: 42 , training loss: 3.874167
[INFO] Epoch: 40 , batch: 43 , training loss: 3.610161
[INFO] Epoch: 40 , batch: 44 , training loss: 4.001378
[INFO] Epoch: 40 , batch: 45 , training loss: 3.862708
[INFO] Epoch: 40 , batch: 46 , training loss: 3.840394
[INFO] Epoch: 40 , batch: 47 , training loss: 3.587485
[INFO] Epoch: 40 , batch: 48 , training loss: 3.574251
[INFO] Epoch: 40 , batch: 49 , training loss: 3.745098
[INFO] Epoch: 40 , batch: 50 , training loss: 3.591761
[INFO] Epoch: 40 , batch: 51 , training loss: 3.820266
[INFO] Epoch: 40 , batch: 52 , training loss: 3.619028
[INFO] Epoch: 40 , batch: 53 , training loss: 3.752938
[INFO] Epoch: 40 , batch: 54 , training loss: 3.759154
[INFO] Epoch: 40 , batch: 55 , training loss: 3.871469
[INFO] Epoch: 40 , batch: 56 , training loss: 3.732580
[INFO] Epoch: 40 , batch: 57 , training loss: 3.632680
[INFO] Epoch: 40 , batch: 58 , training loss: 3.674802
[INFO] Epoch: 40 , batch: 59 , training loss: 3.763369
[INFO] Epoch: 40 , batch: 60 , training loss: 3.699830
[INFO] Epoch: 40 , batch: 61 , training loss: 3.794211
[INFO] Epoch: 40 , batch: 62 , training loss: 3.665790
[INFO] Epoch: 40 , batch: 63 , training loss: 3.874378
[INFO] Epoch: 40 , batch: 64 , training loss: 4.023860
[INFO] Epoch: 40 , batch: 65 , training loss: 3.726610
[INFO] Epoch: 40 , batch: 66 , training loss: 3.607429
[INFO] Epoch: 40 , batch: 67 , training loss: 3.695301
[INFO] Epoch: 40 , batch: 68 , training loss: 3.822611
[INFO] Epoch: 40 , batch: 69 , training loss: 3.705235
[INFO] Epoch: 40 , batch: 70 , training loss: 3.912438
[INFO] Epoch: 40 , batch: 71 , training loss: 3.788851
[INFO] Epoch: 40 , batch: 72 , training loss: 3.873775
[INFO] Epoch: 40 , batch: 73 , training loss: 3.815552
[INFO] Epoch: 40 , batch: 74 , training loss: 3.920717
[INFO] Epoch: 40 , batch: 75 , training loss: 3.806090
[INFO] Epoch: 40 , batch: 76 , training loss: 3.849982
[INFO] Epoch: 40 , batch: 77 , training loss: 3.832819
[INFO] Epoch: 40 , batch: 78 , training loss: 3.917889
[INFO] Epoch: 40 , batch: 79 , training loss: 3.760189
[INFO] Epoch: 40 , batch: 80 , training loss: 3.954488
[INFO] Epoch: 40 , batch: 81 , training loss: 3.873192
[INFO] Epoch: 40 , batch: 82 , training loss: 3.844623
[INFO] Epoch: 40 , batch: 83 , training loss: 3.990988
[INFO] Epoch: 40 , batch: 84 , training loss: 3.887504
[INFO] Epoch: 40 , batch: 85 , training loss: 3.974917
[INFO] Epoch: 40 , batch: 86 , training loss: 3.910191
[INFO] Epoch: 40 , batch: 87 , training loss: 3.887579
[INFO] Epoch: 40 , batch: 88 , training loss: 3.982094
[INFO] Epoch: 40 , batch: 89 , training loss: 3.823823
[INFO] Epoch: 40 , batch: 90 , training loss: 3.906715
[INFO] Epoch: 40 , batch: 91 , training loss: 3.876030
[INFO] Epoch: 40 , batch: 92 , training loss: 3.845108
[INFO] Epoch: 40 , batch: 93 , training loss: 3.936081
[INFO] Epoch: 40 , batch: 94 , training loss: 4.068480
[INFO] Epoch: 40 , batch: 95 , training loss: 3.851929
[INFO] Epoch: 40 , batch: 96 , training loss: 3.865817
[INFO] Epoch: 40 , batch: 97 , training loss: 3.783569
[INFO] Epoch: 40 , batch: 98 , training loss: 3.740475
[INFO] Epoch: 40 , batch: 99 , training loss: 3.865504
[INFO] Epoch: 40 , batch: 100 , training loss: 3.781851
[INFO] Epoch: 40 , batch: 101 , training loss: 3.831156
[INFO] Epoch: 40 , batch: 102 , training loss: 3.909067
[INFO] Epoch: 40 , batch: 103 , training loss: 3.716793
[INFO] Epoch: 40 , batch: 104 , training loss: 3.698033
[INFO] Epoch: 40 , batch: 105 , training loss: 3.963909
[INFO] Epoch: 40 , batch: 106 , training loss: 3.923405
[INFO] Epoch: 40 , batch: 107 , training loss: 3.772954
[INFO] Epoch: 40 , batch: 108 , training loss: 3.716881
[INFO] Epoch: 40 , batch: 109 , training loss: 3.646534
[INFO] Epoch: 40 , batch: 110 , training loss: 3.842498
[INFO] Epoch: 40 , batch: 111 , training loss: 3.900486
[INFO] Epoch: 40 , batch: 112 , training loss: 3.811054
[INFO] Epoch: 40 , batch: 113 , training loss: 3.835932
[INFO] Epoch: 40 , batch: 114 , training loss: 3.806648
[INFO] Epoch: 40 , batch: 115 , training loss: 3.821903
[INFO] Epoch: 40 , batch: 116 , training loss: 3.737159
[INFO] Epoch: 40 , batch: 117 , training loss: 3.988034
[INFO] Epoch: 40 , batch: 118 , training loss: 3.944773
[INFO] Epoch: 40 , batch: 119 , training loss: 4.033768
[INFO] Epoch: 40 , batch: 120 , training loss: 4.056682
[INFO] Epoch: 40 , batch: 121 , training loss: 3.897229
[INFO] Epoch: 40 , batch: 122 , training loss: 3.812864
[INFO] Epoch: 40 , batch: 123 , training loss: 3.807729
[INFO] Epoch: 40 , batch: 124 , training loss: 3.935690
[INFO] Epoch: 40 , batch: 125 , training loss: 3.751960
[INFO] Epoch: 40 , batch: 126 , training loss: 3.749025
[INFO] Epoch: 40 , batch: 127 , training loss: 3.742560
[INFO] Epoch: 40 , batch: 128 , training loss: 3.864109
[INFO] Epoch: 40 , batch: 129 , training loss: 3.877107
[INFO] Epoch: 40 , batch: 130 , training loss: 3.840624
[INFO] Epoch: 40 , batch: 131 , training loss: 3.856399
[INFO] Epoch: 40 , batch: 132 , training loss: 3.827023
[INFO] Epoch: 40 , batch: 133 , training loss: 3.825835
[INFO] Epoch: 40 , batch: 134 , training loss: 3.635341
[INFO] Epoch: 40 , batch: 135 , training loss: 3.654972
[INFO] Epoch: 40 , batch: 136 , training loss: 3.935576
[INFO] Epoch: 40 , batch: 137 , training loss: 3.856069
[INFO] Epoch: 40 , batch: 138 , training loss: 3.904338
[INFO] Epoch: 40 , batch: 139 , training loss: 4.464630
[INFO] Epoch: 40 , batch: 140 , training loss: 4.207843
[INFO] Epoch: 40 , batch: 141 , training loss: 3.992164
[INFO] Epoch: 40 , batch: 142 , training loss: 3.810417
[INFO] Epoch: 40 , batch: 143 , training loss: 3.923840
[INFO] Epoch: 40 , batch: 144 , training loss: 3.725825
[INFO] Epoch: 40 , batch: 145 , training loss: 3.777504
[INFO] Epoch: 40 , batch: 146 , training loss: 3.981815
[INFO] Epoch: 40 , batch: 147 , training loss: 3.682656
[INFO] Epoch: 40 , batch: 148 , training loss: 3.682390
[INFO] Epoch: 40 , batch: 149 , training loss: 3.767078
[INFO] Epoch: 40 , batch: 150 , training loss: 3.981947
[INFO] Epoch: 40 , batch: 151 , training loss: 3.825995
[INFO] Epoch: 40 , batch: 152 , training loss: 3.888328
[INFO] Epoch: 40 , batch: 153 , training loss: 3.868490
[INFO] Epoch: 40 , batch: 154 , training loss: 3.968885
[INFO] Epoch: 40 , batch: 155 , training loss: 4.182494
[INFO] Epoch: 40 , batch: 156 , training loss: 3.898131
[INFO] Epoch: 40 , batch: 157 , training loss: 3.904272
[INFO] Epoch: 40 , batch: 158 , training loss: 4.014127
[INFO] Epoch: 40 , batch: 159 , training loss: 3.913154
[INFO] Epoch: 40 , batch: 160 , training loss: 4.170450
[INFO] Epoch: 40 , batch: 161 , training loss: 4.275486
[INFO] Epoch: 40 , batch: 162 , training loss: 4.270278
[INFO] Epoch: 40 , batch: 163 , training loss: 4.387712
[INFO] Epoch: 40 , batch: 164 , training loss: 4.388893
[INFO] Epoch: 40 , batch: 165 , training loss: 4.303329
[INFO] Epoch: 40 , batch: 166 , training loss: 4.128748
[INFO] Epoch: 40 , batch: 167 , training loss: 4.109529
[INFO] Epoch: 40 , batch: 168 , training loss: 3.842076
[INFO] Epoch: 40 , batch: 169 , training loss: 3.850391
[INFO] Epoch: 40 , batch: 170 , training loss: 4.068577
[INFO] Epoch: 40 , batch: 171 , training loss: 3.459648
[INFO] Epoch: 40 , batch: 172 , training loss: 3.695060
[INFO] Epoch: 40 , batch: 173 , training loss: 4.007980
[INFO] Epoch: 40 , batch: 174 , training loss: 4.440025
[INFO] Epoch: 40 , batch: 175 , training loss: 4.825844
[INFO] Epoch: 40 , batch: 176 , training loss: 4.452288
[INFO] Epoch: 40 , batch: 177 , training loss: 4.080974
[INFO] Epoch: 40 , batch: 178 , training loss: 4.063834
[INFO] Epoch: 40 , batch: 179 , training loss: 4.132586
[INFO] Epoch: 40 , batch: 180 , training loss: 4.086443
[INFO] Epoch: 40 , batch: 181 , training loss: 4.358649
[INFO] Epoch: 40 , batch: 182 , training loss: 4.313779
[INFO] Epoch: 40 , batch: 183 , training loss: 4.291175
[INFO] Epoch: 40 , batch: 184 , training loss: 4.167062
[INFO] Epoch: 40 , batch: 185 , training loss: 4.112113
[INFO] Epoch: 40 , batch: 186 , training loss: 4.294630
[INFO] Epoch: 40 , batch: 187 , training loss: 4.400299
[INFO] Epoch: 40 , batch: 188 , training loss: 4.402365
[INFO] Epoch: 40 , batch: 189 , training loss: 4.290328
[INFO] Epoch: 40 , batch: 190 , training loss: 4.313270
[INFO] Epoch: 40 , batch: 191 , training loss: 4.405086
[INFO] Epoch: 40 , batch: 192 , training loss: 4.253831
[INFO] Epoch: 40 , batch: 193 , training loss: 4.342020
[INFO] Epoch: 40 , batch: 194 , training loss: 4.275085
[INFO] Epoch: 40 , batch: 195 , training loss: 4.183734
[INFO] Epoch: 40 , batch: 196 , training loss: 4.074808
[INFO] Epoch: 40 , batch: 197 , training loss: 4.176171
[INFO] Epoch: 40 , batch: 198 , training loss: 4.066215
[INFO] Epoch: 40 , batch: 199 , training loss: 4.228410
[INFO] Epoch: 40 , batch: 200 , training loss: 4.135547
[INFO] Epoch: 40 , batch: 201 , training loss: 4.055548
[INFO] Epoch: 40 , batch: 202 , training loss: 4.026851
[INFO] Epoch: 40 , batch: 203 , training loss: 4.155823
[INFO] Epoch: 40 , batch: 204 , training loss: 4.291327
[INFO] Epoch: 40 , batch: 205 , training loss: 3.829417
[INFO] Epoch: 40 , batch: 206 , training loss: 3.782312
[INFO] Epoch: 40 , batch: 207 , training loss: 3.794984
[INFO] Epoch: 40 , batch: 208 , training loss: 4.112099
[INFO] Epoch: 40 , batch: 209 , training loss: 4.046853
[INFO] Epoch: 40 , batch: 210 , training loss: 4.067358
[INFO] Epoch: 40 , batch: 211 , training loss: 4.075014
[INFO] Epoch: 40 , batch: 212 , training loss: 4.188548
[INFO] Epoch: 40 , batch: 213 , training loss: 4.111845
[INFO] Epoch: 40 , batch: 214 , training loss: 4.208986
[INFO] Epoch: 40 , batch: 215 , training loss: 4.433200
[INFO] Epoch: 40 , batch: 216 , training loss: 4.105177
[INFO] Epoch: 40 , batch: 217 , training loss: 4.086243
[INFO] Epoch: 40 , batch: 218 , training loss: 4.057873
[INFO] Epoch: 40 , batch: 219 , training loss: 4.174999
[INFO] Epoch: 40 , batch: 220 , training loss: 3.986361
[INFO] Epoch: 40 , batch: 221 , training loss: 4.008729
[INFO] Epoch: 40 , batch: 222 , training loss: 4.146298
[INFO] Epoch: 40 , batch: 223 , training loss: 4.244080
[INFO] Epoch: 40 , batch: 224 , training loss: 4.268641
[INFO] Epoch: 40 , batch: 225 , training loss: 4.185497
[INFO] Epoch: 40 , batch: 226 , training loss: 4.269066
[INFO] Epoch: 40 , batch: 227 , training loss: 4.266058
[INFO] Epoch: 40 , batch: 228 , training loss: 4.294377
[INFO] Epoch: 40 , batch: 229 , training loss: 4.150143
[INFO] Epoch: 40 , batch: 230 , training loss: 4.031560
[INFO] Epoch: 40 , batch: 231 , training loss: 3.885010
[INFO] Epoch: 40 , batch: 232 , training loss: 4.020605
[INFO] Epoch: 40 , batch: 233 , training loss: 4.043207
[INFO] Epoch: 40 , batch: 234 , training loss: 3.754395
[INFO] Epoch: 40 , batch: 235 , training loss: 3.827819
[INFO] Epoch: 40 , batch: 236 , training loss: 3.950444
[INFO] Epoch: 40 , batch: 237 , training loss: 4.161104
[INFO] Epoch: 40 , batch: 238 , training loss: 3.928323
[INFO] Epoch: 40 , batch: 239 , training loss: 3.979283
[INFO] Epoch: 40 , batch: 240 , training loss: 4.055448
[INFO] Epoch: 40 , batch: 241 , training loss: 3.836493
[INFO] Epoch: 40 , batch: 242 , training loss: 3.846931
[INFO] Epoch: 40 , batch: 243 , training loss: 4.168005
[INFO] Epoch: 40 , batch: 244 , training loss: 4.092150
[INFO] Epoch: 40 , batch: 245 , training loss: 4.059422
[INFO] Epoch: 40 , batch: 246 , training loss: 3.768005
[INFO] Epoch: 40 , batch: 247 , training loss: 3.940508
[INFO] Epoch: 40 , batch: 248 , training loss: 4.007288
[INFO] Epoch: 40 , batch: 249 , training loss: 4.017048
[INFO] Epoch: 40 , batch: 250 , training loss: 3.787824
[INFO] Epoch: 40 , batch: 251 , training loss: 4.221411
[INFO] Epoch: 40 , batch: 252 , training loss: 3.932132
[INFO] Epoch: 40 , batch: 253 , training loss: 3.866329
[INFO] Epoch: 40 , batch: 254 , training loss: 4.142274
[INFO] Epoch: 40 , batch: 255 , training loss: 4.105082
[INFO] Epoch: 40 , batch: 256 , training loss: 4.114516
[INFO] Epoch: 40 , batch: 257 , training loss: 4.252009
[INFO] Epoch: 40 , batch: 258 , training loss: 4.297750
[INFO] Epoch: 40 , batch: 259 , training loss: 4.315494
[INFO] Epoch: 40 , batch: 260 , training loss: 4.080941
[INFO] Epoch: 40 , batch: 261 , training loss: 4.203779
[INFO] Epoch: 40 , batch: 262 , training loss: 4.388075
[INFO] Epoch: 40 , batch: 263 , training loss: 4.577867
[INFO] Epoch: 40 , batch: 264 , training loss: 3.894012
[INFO] Epoch: 40 , batch: 265 , training loss: 4.030634
[INFO] Epoch: 40 , batch: 266 , training loss: 4.432528
[INFO] Epoch: 40 , batch: 267 , training loss: 4.204961
[INFO] Epoch: 40 , batch: 268 , training loss: 4.123236
[INFO] Epoch: 40 , batch: 269 , training loss: 4.125333
[INFO] Epoch: 40 , batch: 270 , training loss: 4.146368
[INFO] Epoch: 40 , batch: 271 , training loss: 4.174285
[INFO] Epoch: 40 , batch: 272 , training loss: 4.142494
[INFO] Epoch: 40 , batch: 273 , training loss: 4.185632
[INFO] Epoch: 40 , batch: 274 , training loss: 4.232952
[INFO] Epoch: 40 , batch: 275 , training loss: 4.123676
[INFO] Epoch: 40 , batch: 276 , training loss: 4.160059
[INFO] Epoch: 40 , batch: 277 , training loss: 4.322814
[INFO] Epoch: 40 , batch: 278 , training loss: 4.000833
[INFO] Epoch: 40 , batch: 279 , training loss: 4.003516
[INFO] Epoch: 40 , batch: 280 , training loss: 3.967029
[INFO] Epoch: 40 , batch: 281 , training loss: 4.103158
[INFO] Epoch: 40 , batch: 282 , training loss: 4.007537
[INFO] Epoch: 40 , batch: 283 , training loss: 4.046893
[INFO] Epoch: 40 , batch: 284 , training loss: 4.045112
[INFO] Epoch: 40 , batch: 285 , training loss: 4.003928
[INFO] Epoch: 40 , batch: 286 , training loss: 3.999465
[INFO] Epoch: 40 , batch: 287 , training loss: 3.952781
[INFO] Epoch: 40 , batch: 288 , training loss: 3.946821
[INFO] Epoch: 40 , batch: 289 , training loss: 3.993061
[INFO] Epoch: 40 , batch: 290 , training loss: 3.771822
[INFO] Epoch: 40 , batch: 291 , training loss: 3.726346
[INFO] Epoch: 40 , batch: 292 , training loss: 3.896158
[INFO] Epoch: 40 , batch: 293 , training loss: 3.804429
[INFO] Epoch: 40 , batch: 294 , training loss: 4.434349
[INFO] Epoch: 40 , batch: 295 , training loss: 4.228025
[INFO] Epoch: 40 , batch: 296 , training loss: 4.159801
[INFO] Epoch: 40 , batch: 297 , training loss: 4.117415
[INFO] Epoch: 40 , batch: 298 , training loss: 3.932979
[INFO] Epoch: 40 , batch: 299 , training loss: 3.967115
[INFO] Epoch: 40 , batch: 300 , training loss: 3.947858
[INFO] Epoch: 40 , batch: 301 , training loss: 3.912505
[INFO] Epoch: 40 , batch: 302 , training loss: 4.061494
[INFO] Epoch: 40 , batch: 303 , training loss: 4.057495
[INFO] Epoch: 40 , batch: 304 , training loss: 4.235579
[INFO] Epoch: 40 , batch: 305 , training loss: 4.030344
[INFO] Epoch: 40 , batch: 306 , training loss: 4.160594
[INFO] Epoch: 40 , batch: 307 , training loss: 4.144053
[INFO] Epoch: 40 , batch: 308 , training loss: 4.002310
[INFO] Epoch: 40 , batch: 309 , training loss: 3.999479
[INFO] Epoch: 40 , batch: 310 , training loss: 3.908066
[INFO] Epoch: 40 , batch: 311 , training loss: 3.895074
[INFO] Epoch: 40 , batch: 312 , training loss: 3.826363
[INFO] Epoch: 40 , batch: 313 , training loss: 3.938978
[INFO] Epoch: 40 , batch: 314 , training loss: 3.989743
[INFO] Epoch: 40 , batch: 315 , training loss: 4.031140
[INFO] Epoch: 40 , batch: 316 , training loss: 4.287028
[INFO] Epoch: 40 , batch: 317 , training loss: 4.707044
[INFO] Epoch: 40 , batch: 318 , training loss: 4.869465
[INFO] Epoch: 40 , batch: 319 , training loss: 4.470099
[INFO] Epoch: 40 , batch: 320 , training loss: 4.004845
[INFO] Epoch: 40 , batch: 321 , training loss: 3.845277
[INFO] Epoch: 40 , batch: 322 , training loss: 3.950840
[INFO] Epoch: 40 , batch: 323 , training loss: 3.985384
[INFO] Epoch: 40 , batch: 324 , training loss: 3.914503
[INFO] Epoch: 40 , batch: 325 , training loss: 4.085506
[INFO] Epoch: 40 , batch: 326 , training loss: 4.160112
[INFO] Epoch: 40 , batch: 327 , training loss: 4.046359
[INFO] Epoch: 40 , batch: 328 , training loss: 4.044486
[INFO] Epoch: 40 , batch: 329 , training loss: 3.974224
[INFO] Epoch: 40 , batch: 330 , training loss: 3.978262
[INFO] Epoch: 40 , batch: 331 , training loss: 4.131435
[INFO] Epoch: 40 , batch: 332 , training loss: 3.936485
[INFO] Epoch: 40 , batch: 333 , training loss: 3.964475
[INFO] Epoch: 40 , batch: 334 , training loss: 3.944476
[INFO] Epoch: 40 , batch: 335 , training loss: 4.098738
[INFO] Epoch: 40 , batch: 336 , training loss: 4.111737
[INFO] Epoch: 40 , batch: 337 , training loss: 4.115272
[INFO] Epoch: 40 , batch: 338 , training loss: 4.338361
[INFO] Epoch: 40 , batch: 339 , training loss: 4.208316
[INFO] Epoch: 40 , batch: 340 , training loss: 4.331917
[INFO] Epoch: 40 , batch: 341 , training loss: 4.099254
[INFO] Epoch: 40 , batch: 342 , training loss: 3.864960
[INFO] Epoch: 40 , batch: 343 , training loss: 3.962317
[INFO] Epoch: 40 , batch: 344 , training loss: 3.824568
[INFO] Epoch: 40 , batch: 345 , training loss: 3.967666
[INFO] Epoch: 40 , batch: 346 , training loss: 3.989315
[INFO] Epoch: 40 , batch: 347 , training loss: 3.894561
[INFO] Epoch: 40 , batch: 348 , training loss: 3.990305
[INFO] Epoch: 40 , batch: 349 , training loss: 4.157609
[INFO] Epoch: 40 , batch: 350 , training loss: 3.943730
[INFO] Epoch: 40 , batch: 351 , training loss: 4.025885
[INFO] Epoch: 40 , batch: 352 , training loss: 4.025936
[INFO] Epoch: 40 , batch: 353 , training loss: 3.998253
[INFO] Epoch: 40 , batch: 354 , training loss: 4.122678
[INFO] Epoch: 40 , batch: 355 , training loss: 4.152843
[INFO] Epoch: 40 , batch: 356 , training loss: 3.974892
[INFO] Epoch: 40 , batch: 357 , training loss: 4.072532
[INFO] Epoch: 40 , batch: 358 , training loss: 3.961722
[INFO] Epoch: 40 , batch: 359 , training loss: 3.974679
[INFO] Epoch: 40 , batch: 360 , training loss: 4.064354
[INFO] Epoch: 40 , batch: 361 , training loss: 4.029874
[INFO] Epoch: 40 , batch: 362 , training loss: 4.128196
[INFO] Epoch: 40 , batch: 363 , training loss: 4.008069
[INFO] Epoch: 40 , batch: 364 , training loss: 4.048462
[INFO] Epoch: 40 , batch: 365 , training loss: 4.007081
[INFO] Epoch: 40 , batch: 366 , training loss: 4.117389
[INFO] Epoch: 40 , batch: 367 , training loss: 4.198828
[INFO] Epoch: 40 , batch: 368 , training loss: 4.577771
[INFO] Epoch: 40 , batch: 369 , training loss: 4.238698
[INFO] Epoch: 40 , batch: 370 , training loss: 4.009734
[INFO] Epoch: 40 , batch: 371 , training loss: 4.383737
[INFO] Epoch: 40 , batch: 372 , training loss: 4.695111
[INFO] Epoch: 40 , batch: 373 , training loss: 4.737496
[INFO] Epoch: 40 , batch: 374 , training loss: 4.832623
[INFO] Epoch: 40 , batch: 375 , training loss: 4.823458
[INFO] Epoch: 40 , batch: 376 , training loss: 4.732133
[INFO] Epoch: 40 , batch: 377 , training loss: 4.520926
[INFO] Epoch: 40 , batch: 378 , training loss: 4.567994
[INFO] Epoch: 40 , batch: 379 , training loss: 4.565647
[INFO] Epoch: 40 , batch: 380 , training loss: 4.701294
[INFO] Epoch: 40 , batch: 381 , training loss: 4.436935
[INFO] Epoch: 40 , batch: 382 , training loss: 4.664272
[INFO] Epoch: 40 , batch: 383 , training loss: 4.656154
[INFO] Epoch: 40 , batch: 384 , training loss: 4.665933
[INFO] Epoch: 40 , batch: 385 , training loss: 4.366319
[INFO] Epoch: 40 , batch: 386 , training loss: 4.652782
[INFO] Epoch: 40 , batch: 387 , training loss: 4.600054
[INFO] Epoch: 40 , batch: 388 , training loss: 4.398151
[INFO] Epoch: 40 , batch: 389 , training loss: 4.246896
[INFO] Epoch: 40 , batch: 390 , training loss: 4.252129
[INFO] Epoch: 40 , batch: 391 , training loss: 4.297370
[INFO] Epoch: 40 , batch: 392 , training loss: 4.653372
[INFO] Epoch: 40 , batch: 393 , training loss: 4.535374
[INFO] Epoch: 40 , batch: 394 , training loss: 4.601091
[INFO] Epoch: 40 , batch: 395 , training loss: 4.461824
[INFO] Epoch: 40 , batch: 396 , training loss: 4.229647
[INFO] Epoch: 40 , batch: 397 , training loss: 4.389142
[INFO] Epoch: 40 , batch: 398 , training loss: 4.252715
[INFO] Epoch: 40 , batch: 399 , training loss: 4.309158
[INFO] Epoch: 40 , batch: 400 , training loss: 4.304968
[INFO] Epoch: 40 , batch: 401 , training loss: 4.698894
[INFO] Epoch: 40 , batch: 402 , training loss: 4.432395
[INFO] Epoch: 40 , batch: 403 , training loss: 4.245728
[INFO] Epoch: 40 , batch: 404 , training loss: 4.426464
[INFO] Epoch: 40 , batch: 405 , training loss: 4.469769
[INFO] Epoch: 40 , batch: 406 , training loss: 4.371184
[INFO] Epoch: 40 , batch: 407 , training loss: 4.439590
[INFO] Epoch: 40 , batch: 408 , training loss: 4.390691
[INFO] Epoch: 40 , batch: 409 , training loss: 4.369159
[INFO] Epoch: 40 , batch: 410 , training loss: 4.419766
[INFO] Epoch: 40 , batch: 411 , training loss: 4.619075
[INFO] Epoch: 40 , batch: 412 , training loss: 4.446728
[INFO] Epoch: 40 , batch: 413 , training loss: 4.357385
[INFO] Epoch: 40 , batch: 414 , training loss: 4.374235
[INFO] Epoch: 40 , batch: 415 , training loss: 4.410771
[INFO] Epoch: 40 , batch: 416 , training loss: 4.480751
[INFO] Epoch: 40 , batch: 417 , training loss: 4.395917
[INFO] Epoch: 40 , batch: 418 , training loss: 4.413643
[INFO] Epoch: 40 , batch: 419 , training loss: 4.388454
[INFO] Epoch: 40 , batch: 420 , training loss: 4.353661
[INFO] Epoch: 40 , batch: 421 , training loss: 4.354596
[INFO] Epoch: 40 , batch: 422 , training loss: 4.212496
[INFO] Epoch: 40 , batch: 423 , training loss: 4.421760
[INFO] Epoch: 40 , batch: 424 , training loss: 4.583146
[INFO] Epoch: 40 , batch: 425 , training loss: 4.451529
[INFO] Epoch: 40 , batch: 426 , training loss: 4.194023
[INFO] Epoch: 40 , batch: 427 , training loss: 4.417381
[INFO] Epoch: 40 , batch: 428 , training loss: 4.312609
[INFO] Epoch: 40 , batch: 429 , training loss: 4.172336
[INFO] Epoch: 40 , batch: 430 , training loss: 4.440704
[INFO] Epoch: 40 , batch: 431 , training loss: 4.034019
[INFO] Epoch: 40 , batch: 432 , training loss: 4.104147
[INFO] Epoch: 40 , batch: 433 , training loss: 4.129967
[INFO] Epoch: 40 , batch: 434 , training loss: 4.010976
[INFO] Epoch: 40 , batch: 435 , training loss: 4.370695
[INFO] Epoch: 40 , batch: 436 , training loss: 4.435493
[INFO] Epoch: 40 , batch: 437 , training loss: 4.197461
[INFO] Epoch: 40 , batch: 438 , training loss: 4.063177
[INFO] Epoch: 40 , batch: 439 , training loss: 4.265696
[INFO] Epoch: 40 , batch: 440 , training loss: 4.380702
[INFO] Epoch: 40 , batch: 441 , training loss: 4.504326
[INFO] Epoch: 40 , batch: 442 , training loss: 4.267991
[INFO] Epoch: 40 , batch: 443 , training loss: 4.461039
[INFO] Epoch: 40 , batch: 444 , training loss: 4.084683
[INFO] Epoch: 40 , batch: 445 , training loss: 3.973128
[INFO] Epoch: 40 , batch: 446 , training loss: 3.938145
[INFO] Epoch: 40 , batch: 447 , training loss: 4.111134
[INFO] Epoch: 40 , batch: 448 , training loss: 4.198692
[INFO] Epoch: 40 , batch: 449 , training loss: 4.601999
[INFO] Epoch: 40 , batch: 450 , training loss: 4.640300
[INFO] Epoch: 40 , batch: 451 , training loss: 4.587660
[INFO] Epoch: 40 , batch: 452 , training loss: 4.373221
[INFO] Epoch: 40 , batch: 453 , training loss: 4.156725
[INFO] Epoch: 40 , batch: 454 , training loss: 4.298552
[INFO] Epoch: 40 , batch: 455 , training loss: 4.349751
[INFO] Epoch: 40 , batch: 456 , training loss: 4.334156
[INFO] Epoch: 40 , batch: 457 , training loss: 4.418487
[INFO] Epoch: 40 , batch: 458 , training loss: 4.156340
[INFO] Epoch: 40 , batch: 459 , training loss: 4.144656
[INFO] Epoch: 40 , batch: 460 , training loss: 4.226956
[INFO] Epoch: 40 , batch: 461 , training loss: 4.218513
[INFO] Epoch: 40 , batch: 462 , training loss: 4.281137
[INFO] Epoch: 40 , batch: 463 , training loss: 4.195125
[INFO] Epoch: 40 , batch: 464 , training loss: 4.361212
[INFO] Epoch: 40 , batch: 465 , training loss: 4.300908
[INFO] Epoch: 40 , batch: 466 , training loss: 4.406568
[INFO] Epoch: 40 , batch: 467 , training loss: 4.375614
[INFO] Epoch: 40 , batch: 468 , training loss: 4.343928
[INFO] Epoch: 40 , batch: 469 , training loss: 4.356332
[INFO] Epoch: 40 , batch: 470 , training loss: 4.179833
[INFO] Epoch: 40 , batch: 471 , training loss: 4.271179
[INFO] Epoch: 40 , batch: 472 , training loss: 4.316428
[INFO] Epoch: 40 , batch: 473 , training loss: 4.246975
[INFO] Epoch: 40 , batch: 474 , training loss: 4.048132
[INFO] Epoch: 40 , batch: 475 , training loss: 3.931810
[INFO] Epoch: 40 , batch: 476 , training loss: 4.297291
[INFO] Epoch: 40 , batch: 477 , training loss: 4.442629
[INFO] Epoch: 40 , batch: 478 , training loss: 4.472109
[INFO] Epoch: 40 , batch: 479 , training loss: 4.428522
[INFO] Epoch: 40 , batch: 480 , training loss: 4.518277
[INFO] Epoch: 40 , batch: 481 , training loss: 4.413739
[INFO] Epoch: 40 , batch: 482 , training loss: 4.504771
[INFO] Epoch: 40 , batch: 483 , training loss: 4.378996
[INFO] Epoch: 40 , batch: 484 , training loss: 4.168403
[INFO] Epoch: 40 , batch: 485 , training loss: 4.267710
[INFO] Epoch: 40 , batch: 486 , training loss: 4.155040
[INFO] Epoch: 40 , batch: 487 , training loss: 4.158410
[INFO] Epoch: 40 , batch: 488 , training loss: 4.333783
[INFO] Epoch: 40 , batch: 489 , training loss: 4.209844
[INFO] Epoch: 40 , batch: 490 , training loss: 4.270541
[INFO] Epoch: 40 , batch: 491 , training loss: 4.208493
[INFO] Epoch: 40 , batch: 492 , training loss: 4.169282
[INFO] Epoch: 40 , batch: 493 , training loss: 4.370485
[INFO] Epoch: 40 , batch: 494 , training loss: 4.283100
[INFO] Epoch: 40 , batch: 495 , training loss: 4.408529
[INFO] Epoch: 40 , batch: 496 , training loss: 4.296119
[INFO] Epoch: 40 , batch: 497 , training loss: 4.322032
[INFO] Epoch: 40 , batch: 498 , training loss: 4.336901
[INFO] Epoch: 40 , batch: 499 , training loss: 4.378802
[INFO] Epoch: 40 , batch: 500 , training loss: 4.534122
[INFO] Epoch: 40 , batch: 501 , training loss: 4.821751
[INFO] Epoch: 40 , batch: 502 , training loss: 4.865707
[INFO] Epoch: 40 , batch: 503 , training loss: 4.543197
[INFO] Epoch: 40 , batch: 504 , training loss: 4.669586
[INFO] Epoch: 40 , batch: 505 , training loss: 4.664382
[INFO] Epoch: 40 , batch: 506 , training loss: 4.594903
[INFO] Epoch: 40 , batch: 507 , training loss: 4.669250
[INFO] Epoch: 40 , batch: 508 , training loss: 4.627570
[INFO] Epoch: 40 , batch: 509 , training loss: 4.412920
[INFO] Epoch: 40 , batch: 510 , training loss: 4.498984
[INFO] Epoch: 40 , batch: 511 , training loss: 4.405440
[INFO] Epoch: 40 , batch: 512 , training loss: 4.522618
[INFO] Epoch: 40 , batch: 513 , training loss: 4.744089
[INFO] Epoch: 40 , batch: 514 , training loss: 4.404362
[INFO] Epoch: 40 , batch: 515 , training loss: 4.633315
[INFO] Epoch: 40 , batch: 516 , training loss: 4.474238
[INFO] Epoch: 40 , batch: 517 , training loss: 4.410883
[INFO] Epoch: 40 , batch: 518 , training loss: 4.370970
[INFO] Epoch: 40 , batch: 519 , training loss: 4.232577
[INFO] Epoch: 40 , batch: 520 , training loss: 4.477458
[INFO] Epoch: 40 , batch: 521 , training loss: 4.447173
[INFO] Epoch: 40 , batch: 522 , training loss: 4.506603
[INFO] Epoch: 40 , batch: 523 , training loss: 4.408514
[INFO] Epoch: 40 , batch: 524 , training loss: 4.705238
[INFO] Epoch: 40 , batch: 525 , training loss: 4.646434
[INFO] Epoch: 40 , batch: 526 , training loss: 4.390797
[INFO] Epoch: 40 , batch: 527 , training loss: 4.445724
[INFO] Epoch: 40 , batch: 528 , training loss: 4.445624
[INFO] Epoch: 40 , batch: 529 , training loss: 4.428175
[INFO] Epoch: 40 , batch: 530 , training loss: 4.266870
[INFO] Epoch: 40 , batch: 531 , training loss: 4.417762
[INFO] Epoch: 40 , batch: 532 , training loss: 4.324885
[INFO] Epoch: 40 , batch: 533 , training loss: 4.474195
[INFO] Epoch: 40 , batch: 534 , training loss: 4.436402
[INFO] Epoch: 40 , batch: 535 , training loss: 4.467771
[INFO] Epoch: 40 , batch: 536 , training loss: 4.287992
[INFO] Epoch: 40 , batch: 537 , training loss: 4.297132
[INFO] Epoch: 40 , batch: 538 , training loss: 4.381938
[INFO] Epoch: 40 , batch: 539 , training loss: 4.473704
[INFO] Epoch: 40 , batch: 540 , training loss: 4.967205
[INFO] Epoch: 40 , batch: 541 , training loss: 4.829493
[INFO] Epoch: 40 , batch: 542 , training loss: 4.757211
[INFO] Epoch: 41 , batch: 0 , training loss: 3.292491
[INFO] Epoch: 41 , batch: 1 , training loss: 3.373543
[INFO] Epoch: 41 , batch: 2 , training loss: 3.596832
[INFO] Epoch: 41 , batch: 3 , training loss: 3.392581
[INFO] Epoch: 41 , batch: 4 , training loss: 3.768527
[INFO] Epoch: 41 , batch: 5 , training loss: 3.407337
[INFO] Epoch: 41 , batch: 6 , training loss: 3.830857
[INFO] Epoch: 41 , batch: 7 , training loss: 3.740443
[INFO] Epoch: 41 , batch: 8 , training loss: 3.453608
[INFO] Epoch: 41 , batch: 9 , training loss: 3.707444
[INFO] Epoch: 41 , batch: 10 , training loss: 3.699071
[INFO] Epoch: 41 , batch: 11 , training loss: 3.669714
[INFO] Epoch: 41 , batch: 12 , training loss: 3.551620
[INFO] Epoch: 41 , batch: 13 , training loss: 3.609628
[INFO] Epoch: 41 , batch: 14 , training loss: 3.461070
[INFO] Epoch: 41 , batch: 15 , training loss: 3.661925
[INFO] Epoch: 41 , batch: 16 , training loss: 3.531540
[INFO] Epoch: 41 , batch: 17 , training loss: 3.682149
[INFO] Epoch: 41 , batch: 18 , training loss: 3.621792
[INFO] Epoch: 41 , batch: 19 , training loss: 3.385501
[INFO] Epoch: 41 , batch: 20 , training loss: 3.403363
[INFO] Epoch: 41 , batch: 21 , training loss: 3.511298
[INFO] Epoch: 41 , batch: 22 , training loss: 3.350432
[INFO] Epoch: 41 , batch: 23 , training loss: 3.595584
[INFO] Epoch: 41 , batch: 24 , training loss: 3.381597
[INFO] Epoch: 41 , batch: 25 , training loss: 3.611621
[INFO] Epoch: 41 , batch: 26 , training loss: 3.494778
[INFO] Epoch: 41 , batch: 27 , training loss: 3.389588
[INFO] Epoch: 41 , batch: 28 , training loss: 3.565016
[INFO] Epoch: 41 , batch: 29 , training loss: 3.422889
[INFO] Epoch: 41 , batch: 30 , training loss: 3.469940
[INFO] Epoch: 41 , batch: 31 , training loss: 3.524736
[INFO] Epoch: 41 , batch: 32 , training loss: 3.508376
[INFO] Epoch: 41 , batch: 33 , training loss: 3.534269
[INFO] Epoch: 41 , batch: 34 , training loss: 3.488112
[INFO] Epoch: 41 , batch: 35 , training loss: 3.502001
[INFO] Epoch: 41 , batch: 36 , training loss: 3.512711
[INFO] Epoch: 41 , batch: 37 , training loss: 3.435936
[INFO] Epoch: 41 , batch: 38 , training loss: 3.470038
[INFO] Epoch: 41 , batch: 39 , training loss: 3.352509
[INFO] Epoch: 41 , batch: 40 , training loss: 3.573085
[INFO] Epoch: 41 , batch: 41 , training loss: 3.462430
[INFO] Epoch: 41 , batch: 42 , training loss: 3.888454
[INFO] Epoch: 41 , batch: 43 , training loss: 3.584977
[INFO] Epoch: 41 , batch: 44 , training loss: 3.971199
[INFO] Epoch: 41 , batch: 45 , training loss: 3.830317
[INFO] Epoch: 41 , batch: 46 , training loss: 3.749021
[INFO] Epoch: 41 , batch: 47 , training loss: 3.550779
[INFO] Epoch: 41 , batch: 48 , training loss: 3.535704
[INFO] Epoch: 41 , batch: 49 , training loss: 3.738955
[INFO] Epoch: 41 , batch: 50 , training loss: 3.540505
[INFO] Epoch: 41 , batch: 51 , training loss: 3.748491
[INFO] Epoch: 41 , batch: 52 , training loss: 3.621434
[INFO] Epoch: 41 , batch: 53 , training loss: 3.745267
[INFO] Epoch: 41 , batch: 54 , training loss: 3.739363
[INFO] Epoch: 41 , batch: 55 , training loss: 3.824771
[INFO] Epoch: 41 , batch: 56 , training loss: 3.726579
[INFO] Epoch: 41 , batch: 57 , training loss: 3.596818
[INFO] Epoch: 41 , batch: 58 , training loss: 3.682029
[INFO] Epoch: 41 , batch: 59 , training loss: 3.763820
[INFO] Epoch: 41 , batch: 60 , training loss: 3.689830
[INFO] Epoch: 41 , batch: 61 , training loss: 3.789374
[INFO] Epoch: 41 , batch: 62 , training loss: 3.664427
[INFO] Epoch: 41 , batch: 63 , training loss: 3.868743
[INFO] Epoch: 41 , batch: 64 , training loss: 4.017706
[INFO] Epoch: 41 , batch: 65 , training loss: 3.715752
[INFO] Epoch: 41 , batch: 66 , training loss: 3.595329
[INFO] Epoch: 41 , batch: 67 , training loss: 3.667531
[INFO] Epoch: 41 , batch: 68 , training loss: 3.810237
[INFO] Epoch: 41 , batch: 69 , training loss: 3.670910
[INFO] Epoch: 41 , batch: 70 , training loss: 3.914083
[INFO] Epoch: 41 , batch: 71 , training loss: 3.818796
[INFO] Epoch: 41 , batch: 72 , training loss: 3.842416
[INFO] Epoch: 41 , batch: 73 , training loss: 3.769900
[INFO] Epoch: 41 , batch: 74 , training loss: 3.880424
[INFO] Epoch: 41 , batch: 75 , training loss: 3.799233
[INFO] Epoch: 41 , batch: 76 , training loss: 3.828352
[INFO] Epoch: 41 , batch: 77 , training loss: 3.812717
[INFO] Epoch: 41 , batch: 78 , training loss: 3.932672
[INFO] Epoch: 41 , batch: 79 , training loss: 3.790552
[INFO] Epoch: 41 , batch: 80 , training loss: 3.924845
[INFO] Epoch: 41 , batch: 81 , training loss: 3.870713
[INFO] Epoch: 41 , batch: 82 , training loss: 3.840097
[INFO] Epoch: 41 , batch: 83 , training loss: 3.964697
[INFO] Epoch: 41 , batch: 84 , training loss: 3.906338
[INFO] Epoch: 41 , batch: 85 , training loss: 3.993521
[INFO] Epoch: 41 , batch: 86 , training loss: 3.889018
[INFO] Epoch: 41 , batch: 87 , training loss: 3.911763
[INFO] Epoch: 41 , batch: 88 , training loss: 4.024715
[INFO] Epoch: 41 , batch: 89 , training loss: 3.824090
[INFO] Epoch: 41 , batch: 90 , training loss: 3.904636
[INFO] Epoch: 41 , batch: 91 , training loss: 3.864331
[INFO] Epoch: 41 , batch: 92 , training loss: 3.836182
[INFO] Epoch: 41 , batch: 93 , training loss: 3.933859
[INFO] Epoch: 41 , batch: 94 , training loss: 4.088067
[INFO] Epoch: 41 , batch: 95 , training loss: 3.843107
[INFO] Epoch: 41 , batch: 96 , training loss: 3.866149
[INFO] Epoch: 41 , batch: 97 , training loss: 3.757785
[INFO] Epoch: 41 , batch: 98 , training loss: 3.752801
[INFO] Epoch: 41 , batch: 99 , training loss: 3.844244
[INFO] Epoch: 41 , batch: 100 , training loss: 3.778919
[INFO] Epoch: 41 , batch: 101 , training loss: 3.798477
[INFO] Epoch: 41 , batch: 102 , training loss: 3.912173
[INFO] Epoch: 41 , batch: 103 , training loss: 3.717009
[INFO] Epoch: 41 , batch: 104 , training loss: 3.684761
[INFO] Epoch: 41 , batch: 105 , training loss: 3.945216
[INFO] Epoch: 41 , batch: 106 , training loss: 3.920989
[INFO] Epoch: 41 , batch: 107 , training loss: 3.758816
[INFO] Epoch: 41 , batch: 108 , training loss: 3.719548
[INFO] Epoch: 41 , batch: 109 , training loss: 3.643315
[INFO] Epoch: 41 , batch: 110 , training loss: 3.837120
[INFO] Epoch: 41 , batch: 111 , training loss: 3.902929
[INFO] Epoch: 41 , batch: 112 , training loss: 3.783230
[INFO] Epoch: 41 , batch: 113 , training loss: 3.832901
[INFO] Epoch: 41 , batch: 114 , training loss: 3.806432
[INFO] Epoch: 41 , batch: 115 , training loss: 3.815587
[INFO] Epoch: 41 , batch: 116 , training loss: 3.737192
[INFO] Epoch: 41 , batch: 117 , training loss: 3.985651
[INFO] Epoch: 41 , batch: 118 , training loss: 3.939786
[INFO] Epoch: 41 , batch: 119 , training loss: 4.036629
[INFO] Epoch: 41 , batch: 120 , training loss: 4.047549
[INFO] Epoch: 41 , batch: 121 , training loss: 3.870667
[INFO] Epoch: 41 , batch: 122 , training loss: 3.810229
[INFO] Epoch: 41 , batch: 123 , training loss: 3.841019
[INFO] Epoch: 41 , batch: 124 , training loss: 3.921159
[INFO] Epoch: 41 , batch: 125 , training loss: 3.755103
[INFO] Epoch: 41 , batch: 126 , training loss: 3.744068
[INFO] Epoch: 41 , batch: 127 , training loss: 3.724195
[INFO] Epoch: 41 , batch: 128 , training loss: 3.883507
[INFO] Epoch: 41 , batch: 129 , training loss: 3.851855
[INFO] Epoch: 41 , batch: 130 , training loss: 3.818697
[INFO] Epoch: 41 , batch: 131 , training loss: 3.818563
[INFO] Epoch: 41 , batch: 132 , training loss: 3.825540
[INFO] Epoch: 41 , batch: 133 , training loss: 3.814072
[INFO] Epoch: 41 , batch: 134 , training loss: 3.625276
[INFO] Epoch: 41 , batch: 135 , training loss: 3.638214
[INFO] Epoch: 41 , batch: 136 , training loss: 3.940507
[INFO] Epoch: 41 , batch: 137 , training loss: 3.864615
[INFO] Epoch: 41 , batch: 138 , training loss: 3.894071
[INFO] Epoch: 41 , batch: 139 , training loss: 4.504102
[INFO] Epoch: 41 , batch: 140 , training loss: 4.186597
[INFO] Epoch: 41 , batch: 141 , training loss: 4.009863
[INFO] Epoch: 41 , batch: 142 , training loss: 3.795114
[INFO] Epoch: 41 , batch: 143 , training loss: 3.914659
[INFO] Epoch: 41 , batch: 144 , training loss: 3.720116
[INFO] Epoch: 41 , batch: 145 , training loss: 3.757439
[INFO] Epoch: 41 , batch: 146 , training loss: 3.956475
[INFO] Epoch: 41 , batch: 147 , training loss: 3.665005
[INFO] Epoch: 41 , batch: 148 , training loss: 3.679063
[INFO] Epoch: 41 , batch: 149 , training loss: 3.766215
[INFO] Epoch: 41 , batch: 150 , training loss: 3.966673
[INFO] Epoch: 41 , batch: 151 , training loss: 3.843686
[INFO] Epoch: 41 , batch: 152 , training loss: 3.878163
[INFO] Epoch: 41 , batch: 153 , training loss: 3.862803
[INFO] Epoch: 41 , batch: 154 , training loss: 3.945010
[INFO] Epoch: 41 , batch: 155 , training loss: 4.193449
[INFO] Epoch: 41 , batch: 156 , training loss: 3.887351
[INFO] Epoch: 41 , batch: 157 , training loss: 3.907870
[INFO] Epoch: 41 , batch: 158 , training loss: 4.006708
[INFO] Epoch: 41 , batch: 159 , training loss: 3.867961
[INFO] Epoch: 41 , batch: 160 , training loss: 4.143963
[INFO] Epoch: 41 , batch: 161 , training loss: 4.290579
[INFO] Epoch: 41 , batch: 162 , training loss: 4.233486
[INFO] Epoch: 41 , batch: 163 , training loss: 4.351220
[INFO] Epoch: 41 , batch: 164 , training loss: 4.354925
[INFO] Epoch: 41 , batch: 165 , training loss: 4.295876
[INFO] Epoch: 41 , batch: 166 , training loss: 4.120440
[INFO] Epoch: 41 , batch: 167 , training loss: 4.105184
[INFO] Epoch: 41 , batch: 168 , training loss: 3.827071
[INFO] Epoch: 41 , batch: 169 , training loss: 3.775472
[INFO] Epoch: 41 , batch: 170 , training loss: 4.046726
[INFO] Epoch: 41 , batch: 171 , training loss: 3.440317
[INFO] Epoch: 41 , batch: 172 , training loss: 3.646973
[INFO] Epoch: 41 , batch: 173 , training loss: 4.024076
[INFO] Epoch: 41 , batch: 174 , training loss: 4.445283
[INFO] Epoch: 41 , batch: 175 , training loss: 4.792120
[INFO] Epoch: 41 , batch: 176 , training loss: 4.418018
[INFO] Epoch: 41 , batch: 177 , training loss: 4.073088
[INFO] Epoch: 41 , batch: 178 , training loss: 4.066702
[INFO] Epoch: 41 , batch: 179 , training loss: 4.138996
[INFO] Epoch: 41 , batch: 180 , training loss: 4.084325
[INFO] Epoch: 41 , batch: 181 , training loss: 4.339367
[INFO] Epoch: 41 , batch: 182 , training loss: 4.322203
[INFO] Epoch: 41 , batch: 183 , training loss: 4.268755
[INFO] Epoch: 41 , batch: 184 , training loss: 4.164620
[INFO] Epoch: 41 , batch: 185 , training loss: 4.110533
[INFO] Epoch: 41 , batch: 186 , training loss: 4.283185
[INFO] Epoch: 41 , batch: 187 , training loss: 4.378746
[INFO] Epoch: 41 , batch: 188 , training loss: 4.369741
[INFO] Epoch: 41 , batch: 189 , training loss: 4.304156
[INFO] Epoch: 41 , batch: 190 , training loss: 4.316896
[INFO] Epoch: 41 , batch: 191 , training loss: 4.389664
[INFO] Epoch: 41 , batch: 192 , training loss: 4.240767
[INFO] Epoch: 41 , batch: 193 , training loss: 4.337519
[INFO] Epoch: 41 , batch: 194 , training loss: 4.278091
[INFO] Epoch: 41 , batch: 195 , training loss: 4.168524
[INFO] Epoch: 41 , batch: 196 , training loss: 4.062014
[INFO] Epoch: 41 , batch: 197 , training loss: 4.146700
[INFO] Epoch: 41 , batch: 198 , training loss: 4.067809
[INFO] Epoch: 41 , batch: 199 , training loss: 4.225029
[INFO] Epoch: 41 , batch: 200 , training loss: 4.106711
[INFO] Epoch: 41 , batch: 201 , training loss: 4.034470
[INFO] Epoch: 41 , batch: 202 , training loss: 4.019896
[INFO] Epoch: 41 , batch: 203 , training loss: 4.139333
[INFO] Epoch: 41 , batch: 204 , training loss: 4.277100
[INFO] Epoch: 41 , batch: 205 , training loss: 3.835944
[INFO] Epoch: 41 , batch: 206 , training loss: 3.771430
[INFO] Epoch: 41 , batch: 207 , training loss: 3.781319
[INFO] Epoch: 41 , batch: 208 , training loss: 4.102271
[INFO] Epoch: 41 , batch: 209 , training loss: 4.046264
[INFO] Epoch: 41 , batch: 210 , training loss: 4.062162
[INFO] Epoch: 41 , batch: 211 , training loss: 4.073634
[INFO] Epoch: 41 , batch: 212 , training loss: 4.173540
[INFO] Epoch: 41 , batch: 213 , training loss: 4.092866
[INFO] Epoch: 41 , batch: 214 , training loss: 4.191251
[INFO] Epoch: 41 , batch: 215 , training loss: 4.405685
[INFO] Epoch: 41 , batch: 216 , training loss: 4.098823
[INFO] Epoch: 41 , batch: 217 , training loss: 4.075772
[INFO] Epoch: 41 , batch: 218 , training loss: 4.042983
[INFO] Epoch: 41 , batch: 219 , training loss: 4.168730
[INFO] Epoch: 41 , batch: 220 , training loss: 3.977061
[INFO] Epoch: 41 , batch: 221 , training loss: 3.992613
[INFO] Epoch: 41 , batch: 222 , training loss: 4.141805
[INFO] Epoch: 41 , batch: 223 , training loss: 4.238293
[INFO] Epoch: 41 , batch: 224 , training loss: 4.273336
[INFO] Epoch: 41 , batch: 225 , training loss: 4.164064
[INFO] Epoch: 41 , batch: 226 , training loss: 4.257025
[INFO] Epoch: 41 , batch: 227 , training loss: 4.240943
[INFO] Epoch: 41 , batch: 228 , training loss: 4.285428
[INFO] Epoch: 41 , batch: 229 , training loss: 4.147987
[INFO] Epoch: 41 , batch: 230 , training loss: 4.012619
[INFO] Epoch: 41 , batch: 231 , training loss: 3.870234
[INFO] Epoch: 41 , batch: 232 , training loss: 4.015497
[INFO] Epoch: 41 , batch: 233 , training loss: 4.023698
[INFO] Epoch: 41 , batch: 234 , training loss: 3.731566
[INFO] Epoch: 41 , batch: 235 , training loss: 3.825848
[INFO] Epoch: 41 , batch: 236 , training loss: 3.930376
[INFO] Epoch: 41 , batch: 237 , training loss: 4.154991
[INFO] Epoch: 41 , batch: 238 , training loss: 3.920786
[INFO] Epoch: 41 , batch: 239 , training loss: 3.961198
[INFO] Epoch: 41 , batch: 240 , training loss: 4.039130
[INFO] Epoch: 41 , batch: 241 , training loss: 3.830276
[INFO] Epoch: 41 , batch: 242 , training loss: 3.850998
[INFO] Epoch: 41 , batch: 243 , training loss: 4.152544
[INFO] Epoch: 41 , batch: 244 , training loss: 4.080878
[INFO] Epoch: 41 , batch: 245 , training loss: 4.057934
[INFO] Epoch: 41 , batch: 246 , training loss: 3.766625
[INFO] Epoch: 41 , batch: 247 , training loss: 3.935106
[INFO] Epoch: 41 , batch: 248 , training loss: 4.008101
[INFO] Epoch: 41 , batch: 249 , training loss: 4.026105
[INFO] Epoch: 41 , batch: 250 , training loss: 3.761213
[INFO] Epoch: 41 , batch: 251 , training loss: 4.224824
[INFO] Epoch: 41 , batch: 252 , training loss: 3.913955
[INFO] Epoch: 41 , batch: 253 , training loss: 3.852145
[INFO] Epoch: 41 , batch: 254 , training loss: 4.116797
[INFO] Epoch: 41 , batch: 255 , training loss: 4.100475
[INFO] Epoch: 41 , batch: 256 , training loss: 4.122683
[INFO] Epoch: 41 , batch: 257 , training loss: 4.227468
[INFO] Epoch: 41 , batch: 258 , training loss: 4.293694
[INFO] Epoch: 41 , batch: 259 , training loss: 4.314631
[INFO] Epoch: 41 , batch: 260 , training loss: 4.066348
[INFO] Epoch: 41 , batch: 261 , training loss: 4.205133
[INFO] Epoch: 41 , batch: 262 , training loss: 4.378292
[INFO] Epoch: 41 , batch: 263 , training loss: 4.575665
[INFO] Epoch: 41 , batch: 264 , training loss: 3.889017
[INFO] Epoch: 41 , batch: 265 , training loss: 4.038830
[INFO] Epoch: 41 , batch: 266 , training loss: 4.424602
[INFO] Epoch: 41 , batch: 267 , training loss: 4.197978
[INFO] Epoch: 41 , batch: 268 , training loss: 4.119376
[INFO] Epoch: 41 , batch: 269 , training loss: 4.110114
[INFO] Epoch: 41 , batch: 270 , training loss: 4.122409
[INFO] Epoch: 41 , batch: 271 , training loss: 4.149635
[INFO] Epoch: 41 , batch: 272 , training loss: 4.125809
[INFO] Epoch: 41 , batch: 273 , training loss: 4.171441
[INFO] Epoch: 41 , batch: 274 , training loss: 4.227542
[INFO] Epoch: 41 , batch: 275 , training loss: 4.097390
[INFO] Epoch: 41 , batch: 276 , training loss: 4.166203
[INFO] Epoch: 41 , batch: 277 , training loss: 4.303851
[INFO] Epoch: 41 , batch: 278 , training loss: 3.997685
[INFO] Epoch: 41 , batch: 279 , training loss: 3.997619
[INFO] Epoch: 41 , batch: 280 , training loss: 3.947269
[INFO] Epoch: 41 , batch: 281 , training loss: 4.093525
[INFO] Epoch: 41 , batch: 282 , training loss: 4.006061
[INFO] Epoch: 41 , batch: 283 , training loss: 4.034827
[INFO] Epoch: 41 , batch: 284 , training loss: 4.027595
[INFO] Epoch: 41 , batch: 285 , training loss: 3.990670
[INFO] Epoch: 41 , batch: 286 , training loss: 3.998321
[INFO] Epoch: 41 , batch: 287 , training loss: 3.931909
[INFO] Epoch: 41 , batch: 288 , training loss: 3.936682
[INFO] Epoch: 41 , batch: 289 , training loss: 3.992178
[INFO] Epoch: 41 , batch: 290 , training loss: 3.781444
[INFO] Epoch: 41 , batch: 291 , training loss: 3.741271
[INFO] Epoch: 41 , batch: 292 , training loss: 3.879414
[INFO] Epoch: 41 , batch: 293 , training loss: 3.775502
[INFO] Epoch: 41 , batch: 294 , training loss: 4.431311
[INFO] Epoch: 41 , batch: 295 , training loss: 4.219641
[INFO] Epoch: 41 , batch: 296 , training loss: 4.160325
[INFO] Epoch: 41 , batch: 297 , training loss: 4.114529
[INFO] Epoch: 41 , batch: 298 , training loss: 3.932986
[INFO] Epoch: 41 , batch: 299 , training loss: 3.967699
[INFO] Epoch: 41 , batch: 300 , training loss: 3.928584
[INFO] Epoch: 41 , batch: 301 , training loss: 3.894583
[INFO] Epoch: 41 , batch: 302 , training loss: 4.048552
[INFO] Epoch: 41 , batch: 303 , training loss: 4.050459
[INFO] Epoch: 41 , batch: 304 , training loss: 4.210695
[INFO] Epoch: 41 , batch: 305 , training loss: 4.023253
[INFO] Epoch: 41 , batch: 306 , training loss: 4.150587
[INFO] Epoch: 41 , batch: 307 , training loss: 4.146836
[INFO] Epoch: 41 , batch: 308 , training loss: 3.999263
[INFO] Epoch: 41 , batch: 309 , training loss: 3.988344
[INFO] Epoch: 41 , batch: 310 , training loss: 3.917328
[INFO] Epoch: 41 , batch: 311 , training loss: 3.887156
[INFO] Epoch: 41 , batch: 312 , training loss: 3.818798
[INFO] Epoch: 41 , batch: 313 , training loss: 3.940745
[INFO] Epoch: 41 , batch: 314 , training loss: 3.980232
[INFO] Epoch: 41 , batch: 315 , training loss: 4.038489
[INFO] Epoch: 41 , batch: 316 , training loss: 4.293972
[INFO] Epoch: 41 , batch: 317 , training loss: 4.678795
[INFO] Epoch: 41 , batch: 318 , training loss: 4.840459
[INFO] Epoch: 41 , batch: 319 , training loss: 4.463449
[INFO] Epoch: 41 , batch: 320 , training loss: 4.001909
[INFO] Epoch: 41 , batch: 321 , training loss: 3.826460
[INFO] Epoch: 41 , batch: 322 , training loss: 3.959191
[INFO] Epoch: 41 , batch: 323 , training loss: 3.969524
[INFO] Epoch: 41 , batch: 324 , training loss: 3.920171
[INFO] Epoch: 41 , batch: 325 , training loss: 4.092297
[INFO] Epoch: 41 , batch: 326 , training loss: 4.153402
[INFO] Epoch: 41 , batch: 327 , training loss: 4.043746
[INFO] Epoch: 41 , batch: 328 , training loss: 4.052695
[INFO] Epoch: 41 , batch: 329 , training loss: 3.957959
[INFO] Epoch: 41 , batch: 330 , training loss: 3.972067
[INFO] Epoch: 41 , batch: 331 , training loss: 4.114299
[INFO] Epoch: 41 , batch: 332 , training loss: 3.940473
[INFO] Epoch: 41 , batch: 333 , training loss: 3.963285
[INFO] Epoch: 41 , batch: 334 , training loss: 3.938178
[INFO] Epoch: 41 , batch: 335 , training loss: 4.083901
[INFO] Epoch: 41 , batch: 336 , training loss: 4.107765
[INFO] Epoch: 41 , batch: 337 , training loss: 4.111552
[INFO] Epoch: 41 , batch: 338 , training loss: 4.329350
[INFO] Epoch: 41 , batch: 339 , training loss: 4.195616
[INFO] Epoch: 41 , batch: 340 , training loss: 4.333982
[INFO] Epoch: 41 , batch: 341 , training loss: 4.081380
[INFO] Epoch: 41 , batch: 342 , training loss: 3.855674
[INFO] Epoch: 41 , batch: 343 , training loss: 3.959609
[INFO] Epoch: 41 , batch: 344 , training loss: 3.830213
[INFO] Epoch: 41 , batch: 345 , training loss: 3.956025
[INFO] Epoch: 41 , batch: 346 , training loss: 3.984523
[INFO] Epoch: 41 , batch: 347 , training loss: 3.888745
[INFO] Epoch: 41 , batch: 348 , training loss: 3.998256
[INFO] Epoch: 41 , batch: 349 , training loss: 4.156858
[INFO] Epoch: 41 , batch: 350 , training loss: 3.948453
[INFO] Epoch: 41 , batch: 351 , training loss: 4.030371
[INFO] Epoch: 41 , batch: 352 , training loss: 4.018383
[INFO] Epoch: 41 , batch: 353 , training loss: 4.000125
[INFO] Epoch: 41 , batch: 354 , training loss: 4.108965
[INFO] Epoch: 41 , batch: 355 , training loss: 4.144158
[INFO] Epoch: 41 , batch: 356 , training loss: 3.970136
[INFO] Epoch: 41 , batch: 357 , training loss: 4.063672
[INFO] Epoch: 41 , batch: 358 , training loss: 3.963071
[INFO] Epoch: 41 , batch: 359 , training loss: 3.967008
[INFO] Epoch: 41 , batch: 360 , training loss: 4.073689
[INFO] Epoch: 41 , batch: 361 , training loss: 4.026273
[INFO] Epoch: 41 , batch: 362 , training loss: 4.116706
[INFO] Epoch: 41 , batch: 363 , training loss: 3.989879
[INFO] Epoch: 41 , batch: 364 , training loss: 4.045477
[INFO] Epoch: 41 , batch: 365 , training loss: 3.985712
[INFO] Epoch: 41 , batch: 366 , training loss: 4.111317
[INFO] Epoch: 41 , batch: 367 , training loss: 4.184057
[INFO] Epoch: 41 , batch: 368 , training loss: 4.575264
[INFO] Epoch: 41 , batch: 369 , training loss: 4.230313
[INFO] Epoch: 41 , batch: 370 , training loss: 4.001065
[INFO] Epoch: 41 , batch: 371 , training loss: 4.383262
[INFO] Epoch: 41 , batch: 372 , training loss: 4.666027
[INFO] Epoch: 41 , batch: 373 , training loss: 4.725791
[INFO] Epoch: 41 , batch: 374 , training loss: 4.825474
[INFO] Epoch: 41 , batch: 375 , training loss: 4.825964
[INFO] Epoch: 41 , batch: 376 , training loss: 4.740373
[INFO] Epoch: 41 , batch: 377 , training loss: 4.486011
[INFO] Epoch: 41 , batch: 378 , training loss: 4.569272
[INFO] Epoch: 41 , batch: 379 , training loss: 4.573757
[INFO] Epoch: 41 , batch: 380 , training loss: 4.676175
[INFO] Epoch: 41 , batch: 381 , training loss: 4.403442
[INFO] Epoch: 41 , batch: 382 , training loss: 4.671351
[INFO] Epoch: 41 , batch: 383 , training loss: 4.650415
[INFO] Epoch: 41 , batch: 384 , training loss: 4.668633
[INFO] Epoch: 41 , batch: 385 , training loss: 4.362217
[INFO] Epoch: 41 , batch: 386 , training loss: 4.638324
[INFO] Epoch: 41 , batch: 387 , training loss: 4.589020
[INFO] Epoch: 41 , batch: 388 , training loss: 4.402211
[INFO] Epoch: 41 , batch: 389 , training loss: 4.240304
[INFO] Epoch: 41 , batch: 390 , training loss: 4.239616
[INFO] Epoch: 41 , batch: 391 , training loss: 4.301845
[INFO] Epoch: 41 , batch: 392 , training loss: 4.626338
[INFO] Epoch: 41 , batch: 393 , training loss: 4.518954
[INFO] Epoch: 41 , batch: 394 , training loss: 4.589993
[INFO] Epoch: 41 , batch: 395 , training loss: 4.454241
[INFO] Epoch: 41 , batch: 396 , training loss: 4.237526
[INFO] Epoch: 41 , batch: 397 , training loss: 4.391347
[INFO] Epoch: 41 , batch: 398 , training loss: 4.235039
[INFO] Epoch: 41 , batch: 399 , training loss: 4.301683
[INFO] Epoch: 41 , batch: 400 , training loss: 4.293137
[INFO] Epoch: 41 , batch: 401 , training loss: 4.680020
[INFO] Epoch: 41 , batch: 402 , training loss: 4.419961
[INFO] Epoch: 41 , batch: 403 , training loss: 4.223241
[INFO] Epoch: 41 , batch: 404 , training loss: 4.424361
[INFO] Epoch: 41 , batch: 405 , training loss: 4.478842
[INFO] Epoch: 41 , batch: 406 , training loss: 4.378276
[INFO] Epoch: 41 , batch: 407 , training loss: 4.434227
[INFO] Epoch: 41 , batch: 408 , training loss: 4.384336
[INFO] Epoch: 41 , batch: 409 , training loss: 4.376684
[INFO] Epoch: 41 , batch: 410 , training loss: 4.413848
[INFO] Epoch: 41 , batch: 411 , training loss: 4.620992
[INFO] Epoch: 41 , batch: 412 , training loss: 4.452669
[INFO] Epoch: 41 , batch: 413 , training loss: 4.358170
[INFO] Epoch: 41 , batch: 414 , training loss: 4.375694
[INFO] Epoch: 41 , batch: 415 , training loss: 4.413816
[INFO] Epoch: 41 , batch: 416 , training loss: 4.494627
[INFO] Epoch: 41 , batch: 417 , training loss: 4.384938
[INFO] Epoch: 41 , batch: 418 , training loss: 4.406501
[INFO] Epoch: 41 , batch: 419 , training loss: 4.374546
[INFO] Epoch: 41 , batch: 420 , training loss: 4.353841
[INFO] Epoch: 41 , batch: 421 , training loss: 4.343475
[INFO] Epoch: 41 , batch: 422 , training loss: 4.211572
[INFO] Epoch: 41 , batch: 423 , training loss: 4.412709
[INFO] Epoch: 41 , batch: 424 , training loss: 4.587779
[INFO] Epoch: 41 , batch: 425 , training loss: 4.456803
[INFO] Epoch: 41 , batch: 426 , training loss: 4.199717
[INFO] Epoch: 41 , batch: 427 , training loss: 4.442571
[INFO] Epoch: 41 , batch: 428 , training loss: 4.306253
[INFO] Epoch: 41 , batch: 429 , training loss: 4.167977
[INFO] Epoch: 41 , batch: 430 , training loss: 4.431454
[INFO] Epoch: 41 , batch: 431 , training loss: 4.036945
[INFO] Epoch: 41 , batch: 432 , training loss: 4.097675
[INFO] Epoch: 41 , batch: 433 , training loss: 4.127412
[INFO] Epoch: 41 , batch: 434 , training loss: 4.012568
[INFO] Epoch: 41 , batch: 435 , training loss: 4.380866
[INFO] Epoch: 41 , batch: 436 , training loss: 4.442550
[INFO] Epoch: 41 , batch: 437 , training loss: 4.181605
[INFO] Epoch: 41 , batch: 438 , training loss: 4.065109
[INFO] Epoch: 41 , batch: 439 , training loss: 4.274312
[INFO] Epoch: 41 , batch: 440 , training loss: 4.382573
[INFO] Epoch: 41 , batch: 441 , training loss: 4.506090
[INFO] Epoch: 41 , batch: 442 , training loss: 4.265849
[INFO] Epoch: 41 , batch: 443 , training loss: 4.448021
[INFO] Epoch: 41 , batch: 444 , training loss: 4.094024
[INFO] Epoch: 41 , batch: 445 , training loss: 3.975241
[INFO] Epoch: 41 , batch: 446 , training loss: 3.939533
[INFO] Epoch: 41 , batch: 447 , training loss: 4.116754
[INFO] Epoch: 41 , batch: 448 , training loss: 4.195504
[INFO] Epoch: 41 , batch: 449 , training loss: 4.596815
[INFO] Epoch: 41 , batch: 450 , training loss: 4.654048
[INFO] Epoch: 41 , batch: 451 , training loss: 4.583732
[INFO] Epoch: 41 , batch: 452 , training loss: 4.375237
[INFO] Epoch: 41 , batch: 453 , training loss: 4.146188
[INFO] Epoch: 41 , batch: 454 , training loss: 4.278085
[INFO] Epoch: 41 , batch: 455 , training loss: 4.353148
[INFO] Epoch: 41 , batch: 456 , training loss: 4.328754
[INFO] Epoch: 41 , batch: 457 , training loss: 4.412680
[INFO] Epoch: 41 , batch: 458 , training loss: 4.165596
[INFO] Epoch: 41 , batch: 459 , training loss: 4.134394
[INFO] Epoch: 41 , batch: 460 , training loss: 4.242605
[INFO] Epoch: 41 , batch: 461 , training loss: 4.220725
[INFO] Epoch: 41 , batch: 462 , training loss: 4.272182
[INFO] Epoch: 41 , batch: 463 , training loss: 4.192059
[INFO] Epoch: 41 , batch: 464 , training loss: 4.380275
[INFO] Epoch: 41 , batch: 465 , training loss: 4.297874
[INFO] Epoch: 41 , batch: 466 , training loss: 4.399348
[INFO] Epoch: 41 , batch: 467 , training loss: 4.370431
[INFO] Epoch: 41 , batch: 468 , training loss: 4.346207
[INFO] Epoch: 41 , batch: 469 , training loss: 4.362309
[INFO] Epoch: 41 , batch: 470 , training loss: 4.174072
[INFO] Epoch: 41 , batch: 471 , training loss: 4.273559
[INFO] Epoch: 41 , batch: 472 , training loss: 4.318642
[INFO] Epoch: 41 , batch: 473 , training loss: 4.247081
[INFO] Epoch: 41 , batch: 474 , training loss: 4.053726
[INFO] Epoch: 41 , batch: 475 , training loss: 3.926855
[INFO] Epoch: 41 , batch: 476 , training loss: 4.283840
[INFO] Epoch: 41 , batch: 477 , training loss: 4.431533
[INFO] Epoch: 41 , batch: 478 , training loss: 4.454310
[INFO] Epoch: 41 , batch: 479 , training loss: 4.419308
[INFO] Epoch: 41 , batch: 480 , training loss: 4.525107
[INFO] Epoch: 41 , batch: 481 , training loss: 4.412061
[INFO] Epoch: 41 , batch: 482 , training loss: 4.502785
[INFO] Epoch: 41 , batch: 483 , training loss: 4.375048
[INFO] Epoch: 41 , batch: 484 , training loss: 4.169767
[INFO] Epoch: 41 , batch: 485 , training loss: 4.249210
[INFO] Epoch: 41 , batch: 486 , training loss: 4.159731
[INFO] Epoch: 41 , batch: 487 , training loss: 4.164379
[INFO] Epoch: 41 , batch: 488 , training loss: 4.355315
[INFO] Epoch: 41 , batch: 489 , training loss: 4.222770
[INFO] Epoch: 41 , batch: 490 , training loss: 4.261984
[INFO] Epoch: 41 , batch: 491 , training loss: 4.200105
[INFO] Epoch: 41 , batch: 492 , training loss: 4.166109
[INFO] Epoch: 41 , batch: 493 , training loss: 4.357760
[INFO] Epoch: 41 , batch: 494 , training loss: 4.287490
[INFO] Epoch: 41 , batch: 495 , training loss: 4.392360
[INFO] Epoch: 41 , batch: 496 , training loss: 4.296670
[INFO] Epoch: 41 , batch: 497 , training loss: 4.328223
[INFO] Epoch: 41 , batch: 498 , training loss: 4.314244
[INFO] Epoch: 41 , batch: 499 , training loss: 4.383716
[INFO] Epoch: 41 , batch: 500 , training loss: 4.523121
[INFO] Epoch: 41 , batch: 501 , training loss: 4.824103
[INFO] Epoch: 41 , batch: 502 , training loss: 4.886539
[INFO] Epoch: 41 , batch: 503 , training loss: 4.563176
[INFO] Epoch: 41 , batch: 504 , training loss: 4.686666
[INFO] Epoch: 41 , batch: 505 , training loss: 4.674354
[INFO] Epoch: 41 , batch: 506 , training loss: 4.617713
[INFO] Epoch: 41 , batch: 507 , training loss: 4.692859
[INFO] Epoch: 41 , batch: 508 , training loss: 4.636018
[INFO] Epoch: 41 , batch: 509 , training loss: 4.423764
[INFO] Epoch: 41 , batch: 510 , training loss: 4.506572
[INFO] Epoch: 41 , batch: 511 , training loss: 4.421571
[INFO] Epoch: 41 , batch: 512 , training loss: 4.531539
[INFO] Epoch: 41 , batch: 513 , training loss: 4.754079
[INFO] Epoch: 41 , batch: 514 , training loss: 4.409803
[INFO] Epoch: 41 , batch: 515 , training loss: 4.638780
[INFO] Epoch: 41 , batch: 516 , training loss: 4.461904
[INFO] Epoch: 41 , batch: 517 , training loss: 4.412916
[INFO] Epoch: 41 , batch: 518 , training loss: 4.372104
[INFO] Epoch: 41 , batch: 519 , training loss: 4.219300
[INFO] Epoch: 41 , batch: 520 , training loss: 4.466665
[INFO] Epoch: 41 , batch: 521 , training loss: 4.458266
[INFO] Epoch: 41 , batch: 522 , training loss: 4.505208
[INFO] Epoch: 41 , batch: 523 , training loss: 4.405785
[INFO] Epoch: 41 , batch: 524 , training loss: 4.703526
[INFO] Epoch: 41 , batch: 525 , training loss: 4.637982
[INFO] Epoch: 41 , batch: 526 , training loss: 4.401079
[INFO] Epoch: 41 , batch: 527 , training loss: 4.453575
[INFO] Epoch: 41 , batch: 528 , training loss: 4.439519
[INFO] Epoch: 41 , batch: 529 , training loss: 4.424111
[INFO] Epoch: 41 , batch: 530 , training loss: 4.270492
[INFO] Epoch: 41 , batch: 531 , training loss: 4.412518
[INFO] Epoch: 41 , batch: 532 , training loss: 4.319213
[INFO] Epoch: 41 , batch: 533 , training loss: 4.452414
[INFO] Epoch: 41 , batch: 534 , training loss: 4.442581
[INFO] Epoch: 41 , batch: 535 , training loss: 4.453776
[INFO] Epoch: 41 , batch: 536 , training loss: 4.278950
[INFO] Epoch: 41 , batch: 537 , training loss: 4.300769
[INFO] Epoch: 41 , batch: 538 , training loss: 4.372716
[INFO] Epoch: 41 , batch: 539 , training loss: 4.474250
[INFO] Epoch: 41 , batch: 540 , training loss: 4.936350
[INFO] Epoch: 41 , batch: 541 , training loss: 4.809918
[INFO] Epoch: 41 , batch: 542 , training loss: 4.749578
[INFO] Epoch: 42 , batch: 0 , training loss: 3.315451
[INFO] Epoch: 42 , batch: 1 , training loss: 3.404147
[INFO] Epoch: 42 , batch: 2 , training loss: 3.616232
[INFO] Epoch: 42 , batch: 3 , training loss: 3.420903
[INFO] Epoch: 42 , batch: 4 , training loss: 3.796460
[INFO] Epoch: 42 , batch: 5 , training loss: 3.413323
[INFO] Epoch: 42 , batch: 6 , training loss: 3.818967
[INFO] Epoch: 42 , batch: 7 , training loss: 3.748062
[INFO] Epoch: 42 , batch: 8 , training loss: 3.440659
[INFO] Epoch: 42 , batch: 9 , training loss: 3.719660
[INFO] Epoch: 42 , batch: 10 , training loss: 3.707840
[INFO] Epoch: 42 , batch: 11 , training loss: 3.669256
[INFO] Epoch: 42 , batch: 12 , training loss: 3.551062
[INFO] Epoch: 42 , batch: 13 , training loss: 3.626986
[INFO] Epoch: 42 , batch: 14 , training loss: 3.479621
[INFO] Epoch: 42 , batch: 15 , training loss: 3.661079
[INFO] Epoch: 42 , batch: 16 , training loss: 3.550365
[INFO] Epoch: 42 , batch: 17 , training loss: 3.703013
[INFO] Epoch: 42 , batch: 18 , training loss: 3.629751
[INFO] Epoch: 42 , batch: 19 , training loss: 3.395604
[INFO] Epoch: 42 , batch: 20 , training loss: 3.395201
[INFO] Epoch: 42 , batch: 21 , training loss: 3.505899
[INFO] Epoch: 42 , batch: 22 , training loss: 3.388303
[INFO] Epoch: 42 , batch: 23 , training loss: 3.590088
[INFO] Epoch: 42 , batch: 24 , training loss: 3.382077
[INFO] Epoch: 42 , batch: 25 , training loss: 3.614656
[INFO] Epoch: 42 , batch: 26 , training loss: 3.480038
[INFO] Epoch: 42 , batch: 27 , training loss: 3.422940
[INFO] Epoch: 42 , batch: 28 , training loss: 3.584332
[INFO] Epoch: 42 , batch: 29 , training loss: 3.396369
[INFO] Epoch: 42 , batch: 30 , training loss: 3.476368
[INFO] Epoch: 42 , batch: 31 , training loss: 3.511411
[INFO] Epoch: 42 , batch: 32 , training loss: 3.514618
[INFO] Epoch: 42 , batch: 33 , training loss: 3.531672
[INFO] Epoch: 42 , batch: 34 , training loss: 3.492537
[INFO] Epoch: 42 , batch: 35 , training loss: 3.498484
[INFO] Epoch: 42 , batch: 36 , training loss: 3.518128
[INFO] Epoch: 42 , batch: 37 , training loss: 3.430756
[INFO] Epoch: 42 , batch: 38 , training loss: 3.450894
[INFO] Epoch: 42 , batch: 39 , training loss: 3.349027
[INFO] Epoch: 42 , batch: 40 , training loss: 3.565922
[INFO] Epoch: 42 , batch: 41 , training loss: 3.461401
[INFO] Epoch: 42 , batch: 42 , training loss: 3.903428
[INFO] Epoch: 42 , batch: 43 , training loss: 3.613618
[INFO] Epoch: 42 , batch: 44 , training loss: 3.963784
[INFO] Epoch: 42 , batch: 45 , training loss: 3.915464
[INFO] Epoch: 42 , batch: 46 , training loss: 3.831343
[INFO] Epoch: 42 , batch: 47 , training loss: 3.572940
[INFO] Epoch: 42 , batch: 48 , training loss: 3.524240
[INFO] Epoch: 42 , batch: 49 , training loss: 3.727426
[INFO] Epoch: 42 , batch: 50 , training loss: 3.548077
[INFO] Epoch: 42 , batch: 51 , training loss: 3.763629
[INFO] Epoch: 42 , batch: 52 , training loss: 3.623457
[INFO] Epoch: 42 , batch: 53 , training loss: 3.748065
[INFO] Epoch: 42 , batch: 54 , training loss: 3.755886
[INFO] Epoch: 42 , batch: 55 , training loss: 3.841622
[INFO] Epoch: 42 , batch: 56 , training loss: 3.726027
[INFO] Epoch: 42 , batch: 57 , training loss: 3.613908
[INFO] Epoch: 42 , batch: 58 , training loss: 3.668404
[INFO] Epoch: 42 , batch: 59 , training loss: 3.750756
[INFO] Epoch: 42 , batch: 60 , training loss: 3.693668
[INFO] Epoch: 42 , batch: 61 , training loss: 3.770055
[INFO] Epoch: 42 , batch: 62 , training loss: 3.657270
[INFO] Epoch: 42 , batch: 63 , training loss: 3.871529
[INFO] Epoch: 42 , batch: 64 , training loss: 4.015114
[INFO] Epoch: 42 , batch: 65 , training loss: 3.728083
[INFO] Epoch: 42 , batch: 66 , training loss: 3.581023
[INFO] Epoch: 42 , batch: 67 , training loss: 3.665575
[INFO] Epoch: 42 , batch: 68 , training loss: 3.799508
[INFO] Epoch: 42 , batch: 69 , training loss: 3.678023
[INFO] Epoch: 42 , batch: 70 , training loss: 3.917600
[INFO] Epoch: 42 , batch: 71 , training loss: 3.799807
[INFO] Epoch: 42 , batch: 72 , training loss: 3.854070
[INFO] Epoch: 42 , batch: 73 , training loss: 3.803295
[INFO] Epoch: 42 , batch: 74 , training loss: 3.918618
[INFO] Epoch: 42 , batch: 75 , training loss: 3.769337
[INFO] Epoch: 42 , batch: 76 , training loss: 3.814193
[INFO] Epoch: 42 , batch: 77 , training loss: 3.801144
[INFO] Epoch: 42 , batch: 78 , training loss: 3.931718
[INFO] Epoch: 42 , batch: 79 , training loss: 3.741946
[INFO] Epoch: 42 , batch: 80 , training loss: 3.920595
[INFO] Epoch: 42 , batch: 81 , training loss: 3.882038
[INFO] Epoch: 42 , batch: 82 , training loss: 3.833836
[INFO] Epoch: 42 , batch: 83 , training loss: 3.985862
[INFO] Epoch: 42 , batch: 84 , training loss: 3.921698
[INFO] Epoch: 42 , batch: 85 , training loss: 4.003418
[INFO] Epoch: 42 , batch: 86 , training loss: 3.916991
[INFO] Epoch: 42 , batch: 87 , training loss: 3.899190
[INFO] Epoch: 42 , batch: 88 , training loss: 3.985975
[INFO] Epoch: 42 , batch: 89 , training loss: 3.810945
[INFO] Epoch: 42 , batch: 90 , training loss: 3.885414
[INFO] Epoch: 42 , batch: 91 , training loss: 3.850614
[INFO] Epoch: 42 , batch: 92 , training loss: 3.823355
[INFO] Epoch: 42 , batch: 93 , training loss: 3.932714
[INFO] Epoch: 42 , batch: 94 , training loss: 4.084052
[INFO] Epoch: 42 , batch: 95 , training loss: 3.842552
[INFO] Epoch: 42 , batch: 96 , training loss: 3.874155
[INFO] Epoch: 42 , batch: 97 , training loss: 3.753018
[INFO] Epoch: 42 , batch: 98 , training loss: 3.748064
[INFO] Epoch: 42 , batch: 99 , training loss: 3.849681
[INFO] Epoch: 42 , batch: 100 , training loss: 3.788871
[INFO] Epoch: 42 , batch: 101 , training loss: 3.805859
[INFO] Epoch: 42 , batch: 102 , training loss: 3.896226
[INFO] Epoch: 42 , batch: 103 , training loss: 3.713828
[INFO] Epoch: 42 , batch: 104 , training loss: 3.670252
[INFO] Epoch: 42 , batch: 105 , training loss: 3.933903
[INFO] Epoch: 42 , batch: 106 , training loss: 3.910839
[INFO] Epoch: 42 , batch: 107 , training loss: 3.776784
[INFO] Epoch: 42 , batch: 108 , training loss: 3.694610
[INFO] Epoch: 42 , batch: 109 , training loss: 3.645198
[INFO] Epoch: 42 , batch: 110 , training loss: 3.844948
[INFO] Epoch: 42 , batch: 111 , training loss: 3.888362
[INFO] Epoch: 42 , batch: 112 , training loss: 3.784108
[INFO] Epoch: 42 , batch: 113 , training loss: 3.834285
[INFO] Epoch: 42 , batch: 114 , training loss: 3.805603
[INFO] Epoch: 42 , batch: 115 , training loss: 3.830104
[INFO] Epoch: 42 , batch: 116 , training loss: 3.723472
[INFO] Epoch: 42 , batch: 117 , training loss: 3.977589
[INFO] Epoch: 42 , batch: 118 , training loss: 3.922436
[INFO] Epoch: 42 , batch: 119 , training loss: 4.047797
[INFO] Epoch: 42 , batch: 120 , training loss: 4.034466
[INFO] Epoch: 42 , batch: 121 , training loss: 3.878531
[INFO] Epoch: 42 , batch: 122 , training loss: 3.802798
[INFO] Epoch: 42 , batch: 123 , training loss: 3.799142
[INFO] Epoch: 42 , batch: 124 , training loss: 3.919725
[INFO] Epoch: 42 , batch: 125 , training loss: 3.743484
[INFO] Epoch: 42 , batch: 126 , training loss: 3.723066
[INFO] Epoch: 42 , batch: 127 , training loss: 3.729780
[INFO] Epoch: 42 , batch: 128 , training loss: 3.883793
[INFO] Epoch: 42 , batch: 129 , training loss: 3.845655
[INFO] Epoch: 42 , batch: 130 , training loss: 3.810644
[INFO] Epoch: 42 , batch: 131 , training loss: 3.828273
[INFO] Epoch: 42 , batch: 132 , training loss: 3.818903
[INFO] Epoch: 42 , batch: 133 , training loss: 3.807163
[INFO] Epoch: 42 , batch: 134 , training loss: 3.617264
[INFO] Epoch: 42 , batch: 135 , training loss: 3.647333
[INFO] Epoch: 42 , batch: 136 , training loss: 3.945550
[INFO] Epoch: 42 , batch: 137 , training loss: 3.859679
[INFO] Epoch: 42 , batch: 138 , training loss: 3.895284
[INFO] Epoch: 42 , batch: 139 , training loss: 4.477020
[INFO] Epoch: 42 , batch: 140 , training loss: 4.182980
[INFO] Epoch: 42 , batch: 141 , training loss: 3.984782
[INFO] Epoch: 42 , batch: 142 , training loss: 3.780816
[INFO] Epoch: 42 , batch: 143 , training loss: 3.913945
[INFO] Epoch: 42 , batch: 144 , training loss: 3.723475
[INFO] Epoch: 42 , batch: 145 , training loss: 3.777916
[INFO] Epoch: 42 , batch: 146 , training loss: 3.959213
[INFO] Epoch: 42 , batch: 147 , training loss: 3.653111
[INFO] Epoch: 42 , batch: 148 , training loss: 3.673656
[INFO] Epoch: 42 , batch: 149 , training loss: 3.773724
[INFO] Epoch: 42 , batch: 150 , training loss: 3.948915
[INFO] Epoch: 42 , batch: 151 , training loss: 3.835859
[INFO] Epoch: 42 , batch: 152 , training loss: 3.891804
[INFO] Epoch: 42 , batch: 153 , training loss: 3.847205
[INFO] Epoch: 42 , batch: 154 , training loss: 3.954983
[INFO] Epoch: 42 , batch: 155 , training loss: 4.191691
[INFO] Epoch: 42 , batch: 156 , training loss: 3.884180
[INFO] Epoch: 42 , batch: 157 , training loss: 3.883231
[INFO] Epoch: 42 , batch: 158 , training loss: 3.978115
[INFO] Epoch: 42 , batch: 159 , training loss: 3.871881
[INFO] Epoch: 42 , batch: 160 , training loss: 4.123647
[INFO] Epoch: 42 , batch: 161 , training loss: 4.255457
[INFO] Epoch: 42 , batch: 162 , training loss: 4.233755
[INFO] Epoch: 42 , batch: 163 , training loss: 4.346297
[INFO] Epoch: 42 , batch: 164 , training loss: 4.360014
[INFO] Epoch: 42 , batch: 165 , training loss: 4.269324
[INFO] Epoch: 42 , batch: 166 , training loss: 4.116881
[INFO] Epoch: 42 , batch: 167 , training loss: 4.087086
[INFO] Epoch: 42 , batch: 168 , training loss: 3.782657
[INFO] Epoch: 42 , batch: 169 , training loss: 3.762312
[INFO] Epoch: 42 , batch: 170 , training loss: 4.033275
[INFO] Epoch: 42 , batch: 171 , training loss: 3.419588
[INFO] Epoch: 42 , batch: 172 , training loss: 3.620342
[INFO] Epoch: 42 , batch: 173 , training loss: 3.975328
[INFO] Epoch: 42 , batch: 174 , training loss: 4.419586
[INFO] Epoch: 42 , batch: 175 , training loss: 4.769921
[INFO] Epoch: 42 , batch: 176 , training loss: 4.389207
[INFO] Epoch: 42 , batch: 177 , training loss: 4.061410
[INFO] Epoch: 42 , batch: 178 , training loss: 4.027384
[INFO] Epoch: 42 , batch: 179 , training loss: 4.116203
[INFO] Epoch: 42 , batch: 180 , training loss: 4.057611
[INFO] Epoch: 42 , batch: 181 , training loss: 4.333851
[INFO] Epoch: 42 , batch: 182 , training loss: 4.305155
[INFO] Epoch: 42 , batch: 183 , training loss: 4.262087
[INFO] Epoch: 42 , batch: 184 , training loss: 4.169106
[INFO] Epoch: 42 , batch: 185 , training loss: 4.108989
[INFO] Epoch: 42 , batch: 186 , training loss: 4.278491
[INFO] Epoch: 42 , batch: 187 , training loss: 4.392434
[INFO] Epoch: 42 , batch: 188 , training loss: 4.382349
[INFO] Epoch: 42 , batch: 189 , training loss: 4.281494
[INFO] Epoch: 42 , batch: 190 , training loss: 4.290126
[INFO] Epoch: 42 , batch: 191 , training loss: 4.393024
[INFO] Epoch: 42 , batch: 192 , training loss: 4.220533
[INFO] Epoch: 42 , batch: 193 , training loss: 4.336391
[INFO] Epoch: 42 , batch: 194 , training loss: 4.274842
[INFO] Epoch: 42 , batch: 195 , training loss: 4.150112
[INFO] Epoch: 42 , batch: 196 , training loss: 4.061085
[INFO] Epoch: 42 , batch: 197 , training loss: 4.185732
[INFO] Epoch: 42 , batch: 198 , training loss: 4.064663
[INFO] Epoch: 42 , batch: 199 , training loss: 4.201253
[INFO] Epoch: 42 , batch: 200 , training loss: 4.109909
[INFO] Epoch: 42 , batch: 201 , training loss: 4.033187
[INFO] Epoch: 42 , batch: 202 , training loss: 4.011965
[INFO] Epoch: 42 , batch: 203 , training loss: 4.135172
[INFO] Epoch: 42 , batch: 204 , training loss: 4.279833
[INFO] Epoch: 42 , batch: 205 , training loss: 3.837274
[INFO] Epoch: 42 , batch: 206 , training loss: 3.779311
[INFO] Epoch: 42 , batch: 207 , training loss: 3.778442
[INFO] Epoch: 42 , batch: 208 , training loss: 4.105179
[INFO] Epoch: 42 , batch: 209 , training loss: 4.043233
[INFO] Epoch: 42 , batch: 210 , training loss: 4.069756
[INFO] Epoch: 42 , batch: 211 , training loss: 4.076821
[INFO] Epoch: 42 , batch: 212 , training loss: 4.174540
[INFO] Epoch: 42 , batch: 213 , training loss: 4.084370
[INFO] Epoch: 42 , batch: 214 , training loss: 4.207902
[INFO] Epoch: 42 , batch: 215 , training loss: 4.433772
[INFO] Epoch: 42 , batch: 216 , training loss: 4.111606
[INFO] Epoch: 42 , batch: 217 , training loss: 4.082910
[INFO] Epoch: 42 , batch: 218 , training loss: 4.027144
[INFO] Epoch: 42 , batch: 219 , training loss: 4.151537
[INFO] Epoch: 42 , batch: 220 , training loss: 3.969677
[INFO] Epoch: 42 , batch: 221 , training loss: 4.007551
[INFO] Epoch: 42 , batch: 222 , training loss: 4.133223
[INFO] Epoch: 42 , batch: 223 , training loss: 4.235014
[INFO] Epoch: 42 , batch: 224 , training loss: 4.246806
[INFO] Epoch: 42 , batch: 225 , training loss: 4.163513
[INFO] Epoch: 42 , batch: 226 , training loss: 4.272761
[INFO] Epoch: 42 , batch: 227 , training loss: 4.234494
[INFO] Epoch: 42 , batch: 228 , training loss: 4.288824
[INFO] Epoch: 42 , batch: 229 , training loss: 4.142614
[INFO] Epoch: 42 , batch: 230 , training loss: 4.005641
[INFO] Epoch: 42 , batch: 231 , training loss: 3.879620
[INFO] Epoch: 42 , batch: 232 , training loss: 4.002295
[INFO] Epoch: 42 , batch: 233 , training loss: 4.017474
[INFO] Epoch: 42 , batch: 234 , training loss: 3.734097
[INFO] Epoch: 42 , batch: 235 , training loss: 3.828270
[INFO] Epoch: 42 , batch: 236 , training loss: 3.930604
[INFO] Epoch: 42 , batch: 237 , training loss: 4.134454
[INFO] Epoch: 42 , batch: 238 , training loss: 3.931888
[INFO] Epoch: 42 , batch: 239 , training loss: 3.957966
[INFO] Epoch: 42 , batch: 240 , training loss: 4.023432
[INFO] Epoch: 42 , batch: 241 , training loss: 3.830848
[INFO] Epoch: 42 , batch: 242 , training loss: 3.849501
[INFO] Epoch: 42 , batch: 243 , training loss: 4.137642
[INFO] Epoch: 42 , batch: 244 , training loss: 4.106234
[INFO] Epoch: 42 , batch: 245 , training loss: 4.059293
[INFO] Epoch: 42 , batch: 246 , training loss: 3.772477
[INFO] Epoch: 42 , batch: 247 , training loss: 3.933377
[INFO] Epoch: 42 , batch: 248 , training loss: 3.989449
[INFO] Epoch: 42 , batch: 249 , training loss: 4.030194
[INFO] Epoch: 42 , batch: 250 , training loss: 3.778706
[INFO] Epoch: 42 , batch: 251 , training loss: 4.211006
[INFO] Epoch: 42 , batch: 252 , training loss: 3.924153
[INFO] Epoch: 42 , batch: 253 , training loss: 3.867541
[INFO] Epoch: 42 , batch: 254 , training loss: 4.111362
[INFO] Epoch: 42 , batch: 255 , training loss: 4.087251
[INFO] Epoch: 42 , batch: 256 , training loss: 4.111732
[INFO] Epoch: 42 , batch: 257 , training loss: 4.239702
[INFO] Epoch: 42 , batch: 258 , training loss: 4.274226
[INFO] Epoch: 42 , batch: 259 , training loss: 4.309772
[INFO] Epoch: 42 , batch: 260 , training loss: 4.061033
[INFO] Epoch: 42 , batch: 261 , training loss: 4.180372
[INFO] Epoch: 42 , batch: 262 , training loss: 4.382272
[INFO] Epoch: 42 , batch: 263 , training loss: 4.568623
[INFO] Epoch: 42 , batch: 264 , training loss: 3.900397
[INFO] Epoch: 42 , batch: 265 , training loss: 4.039845
[INFO] Epoch: 42 , batch: 266 , training loss: 4.433289
[INFO] Epoch: 42 , batch: 267 , training loss: 4.209130
[INFO] Epoch: 42 , batch: 268 , training loss: 4.116715
[INFO] Epoch: 42 , batch: 269 , training loss: 4.101910
[INFO] Epoch: 42 , batch: 270 , training loss: 4.129015
[INFO] Epoch: 42 , batch: 271 , training loss: 4.146285
[INFO] Epoch: 42 , batch: 272 , training loss: 4.125831
[INFO] Epoch: 42 , batch: 273 , training loss: 4.180899
[INFO] Epoch: 42 , batch: 274 , training loss: 4.221821
[INFO] Epoch: 42 , batch: 275 , training loss: 4.099848
[INFO] Epoch: 42 , batch: 276 , training loss: 4.148592
[INFO] Epoch: 42 , batch: 277 , training loss: 4.302715
[INFO] Epoch: 42 , batch: 278 , training loss: 3.987865
[INFO] Epoch: 42 , batch: 279 , training loss: 3.994279
[INFO] Epoch: 42 , batch: 280 , training loss: 3.952834
[INFO] Epoch: 42 , batch: 281 , training loss: 4.085660
[INFO] Epoch: 42 , batch: 282 , training loss: 4.017778
[INFO] Epoch: 42 , batch: 283 , training loss: 4.039586
[INFO] Epoch: 42 , batch: 284 , training loss: 4.036074
[INFO] Epoch: 42 , batch: 285 , training loss: 3.976979
[INFO] Epoch: 42 , batch: 286 , training loss: 3.967778
[INFO] Epoch: 42 , batch: 287 , training loss: 3.933968
[INFO] Epoch: 42 , batch: 288 , training loss: 3.935557
[INFO] Epoch: 42 , batch: 289 , training loss: 3.973044
[INFO] Epoch: 42 , batch: 290 , training loss: 3.780398
[INFO] Epoch: 42 , batch: 291 , training loss: 3.724641
[INFO] Epoch: 42 , batch: 292 , training loss: 3.878529
[INFO] Epoch: 42 , batch: 293 , training loss: 3.788475
[INFO] Epoch: 42 , batch: 294 , training loss: 4.436522
[INFO] Epoch: 42 , batch: 295 , training loss: 4.214489
[INFO] Epoch: 42 , batch: 296 , training loss: 4.144209
[INFO] Epoch: 42 , batch: 297 , training loss: 4.102390
[INFO] Epoch: 42 , batch: 298 , training loss: 3.940562
[INFO] Epoch: 42 , batch: 299 , training loss: 3.964565
[INFO] Epoch: 42 , batch: 300 , training loss: 3.934394
[INFO] Epoch: 42 , batch: 301 , training loss: 3.895604
[INFO] Epoch: 42 , batch: 302 , training loss: 4.064492
[INFO] Epoch: 42 , batch: 303 , training loss: 4.063612
[INFO] Epoch: 42 , batch: 304 , training loss: 4.212108
[INFO] Epoch: 42 , batch: 305 , training loss: 4.030715
[INFO] Epoch: 42 , batch: 306 , training loss: 4.143995
[INFO] Epoch: 42 , batch: 307 , training loss: 4.134411
[INFO] Epoch: 42 , batch: 308 , training loss: 3.992357
[INFO] Epoch: 42 , batch: 309 , training loss: 3.980061
[INFO] Epoch: 42 , batch: 310 , training loss: 3.922982
[INFO] Epoch: 42 , batch: 311 , training loss: 3.890234
[INFO] Epoch: 42 , batch: 312 , training loss: 3.824495
[INFO] Epoch: 42 , batch: 313 , training loss: 3.930641
[INFO] Epoch: 42 , batch: 314 , training loss: 3.980606
[INFO] Epoch: 42 , batch: 315 , training loss: 4.032218
[INFO] Epoch: 42 , batch: 316 , training loss: 4.267147
[INFO] Epoch: 42 , batch: 317 , training loss: 4.677991
[INFO] Epoch: 42 , batch: 318 , training loss: 4.829862
[INFO] Epoch: 42 , batch: 319 , training loss: 4.471252
[INFO] Epoch: 42 , batch: 320 , training loss: 4.000463
[INFO] Epoch: 42 , batch: 321 , training loss: 3.839061
[INFO] Epoch: 42 , batch: 322 , training loss: 3.949805
[INFO] Epoch: 42 , batch: 323 , training loss: 3.972027
[INFO] Epoch: 42 , batch: 324 , training loss: 3.922750
[INFO] Epoch: 42 , batch: 325 , training loss: 4.085415
[INFO] Epoch: 42 , batch: 326 , training loss: 4.153646
[INFO] Epoch: 42 , batch: 327 , training loss: 4.038426
[INFO] Epoch: 42 , batch: 328 , training loss: 4.058218
[INFO] Epoch: 42 , batch: 329 , training loss: 3.959039
[INFO] Epoch: 42 , batch: 330 , training loss: 3.973260
[INFO] Epoch: 42 , batch: 331 , training loss: 4.128044
[INFO] Epoch: 42 , batch: 332 , training loss: 3.936990
[INFO] Epoch: 42 , batch: 333 , training loss: 3.949625
[INFO] Epoch: 42 , batch: 334 , training loss: 3.927036
[INFO] Epoch: 42 , batch: 335 , training loss: 4.083147
[INFO] Epoch: 42 , batch: 336 , training loss: 4.090469
[INFO] Epoch: 42 , batch: 337 , training loss: 4.129901
[INFO] Epoch: 42 , batch: 338 , training loss: 4.342479
[INFO] Epoch: 42 , batch: 339 , training loss: 4.187870
[INFO] Epoch: 42 , batch: 340 , training loss: 4.320197
[INFO] Epoch: 42 , batch: 341 , training loss: 4.106765
[INFO] Epoch: 42 , batch: 342 , training loss: 3.858745
[INFO] Epoch: 42 , batch: 343 , training loss: 3.949530
[INFO] Epoch: 42 , batch: 344 , training loss: 3.832040
[INFO] Epoch: 42 , batch: 345 , training loss: 3.957527
[INFO] Epoch: 42 , batch: 346 , training loss: 3.987908
[INFO] Epoch: 42 , batch: 347 , training loss: 3.891295
[INFO] Epoch: 42 , batch: 348 , training loss: 3.994632
[INFO] Epoch: 42 , batch: 349 , training loss: 4.159269
[INFO] Epoch: 42 , batch: 350 , training loss: 3.939802
[INFO] Epoch: 42 , batch: 351 , training loss: 4.023734
[INFO] Epoch: 42 , batch: 352 , training loss: 4.023867
[INFO] Epoch: 42 , batch: 353 , training loss: 4.001535
[INFO] Epoch: 42 , batch: 354 , training loss: 4.110724
[INFO] Epoch: 42 , batch: 355 , training loss: 4.143055
[INFO] Epoch: 42 , batch: 356 , training loss: 3.978820
[INFO] Epoch: 42 , batch: 357 , training loss: 4.062896
[INFO] Epoch: 42 , batch: 358 , training loss: 3.955927
[INFO] Epoch: 42 , batch: 359 , training loss: 3.952712
[INFO] Epoch: 42 , batch: 360 , training loss: 4.063835
[INFO] Epoch: 42 , batch: 361 , training loss: 4.046937
[INFO] Epoch: 42 , batch: 362 , training loss: 4.127048
[INFO] Epoch: 42 , batch: 363 , training loss: 4.004682
[INFO] Epoch: 42 , batch: 364 , training loss: 4.034934
[INFO] Epoch: 42 , batch: 365 , training loss: 3.998505
[INFO] Epoch: 42 , batch: 366 , training loss: 4.120054
[INFO] Epoch: 42 , batch: 367 , training loss: 4.175248
[INFO] Epoch: 42 , batch: 368 , training loss: 4.562840
[INFO] Epoch: 42 , batch: 369 , training loss: 4.230627
[INFO] Epoch: 42 , batch: 370 , training loss: 4.002917
[INFO] Epoch: 42 , batch: 371 , training loss: 4.377616
[INFO] Epoch: 42 , batch: 372 , training loss: 4.636881
[INFO] Epoch: 42 , batch: 373 , training loss: 4.719509
[INFO] Epoch: 42 , batch: 374 , training loss: 4.821633
[INFO] Epoch: 42 , batch: 375 , training loss: 4.826708
[INFO] Epoch: 42 , batch: 376 , training loss: 4.734960
[INFO] Epoch: 42 , batch: 377 , training loss: 4.506559
[INFO] Epoch: 42 , batch: 378 , training loss: 4.571759
[INFO] Epoch: 42 , batch: 379 , training loss: 4.577860
[INFO] Epoch: 42 , batch: 380 , training loss: 4.704766
[INFO] Epoch: 42 , batch: 381 , training loss: 4.423719
[INFO] Epoch: 42 , batch: 382 , training loss: 4.659252
[INFO] Epoch: 42 , batch: 383 , training loss: 4.648609
[INFO] Epoch: 42 , batch: 384 , training loss: 4.649285
[INFO] Epoch: 42 , batch: 385 , training loss: 4.367272
[INFO] Epoch: 42 , batch: 386 , training loss: 4.636878
[INFO] Epoch: 42 , batch: 387 , training loss: 4.605766
[INFO] Epoch: 42 , batch: 388 , training loss: 4.392237
[INFO] Epoch: 42 , batch: 389 , training loss: 4.246179
[INFO] Epoch: 42 , batch: 390 , training loss: 4.233967
[INFO] Epoch: 42 , batch: 391 , training loss: 4.291752
[INFO] Epoch: 42 , batch: 392 , training loss: 4.645657
[INFO] Epoch: 42 , batch: 393 , training loss: 4.522670
[INFO] Epoch: 42 , batch: 394 , training loss: 4.587940
[INFO] Epoch: 42 , batch: 395 , training loss: 4.440248
[INFO] Epoch: 42 , batch: 396 , training loss: 4.233536
[INFO] Epoch: 42 , batch: 397 , training loss: 4.394506
[INFO] Epoch: 42 , batch: 398 , training loss: 4.247356
[INFO] Epoch: 42 , batch: 399 , training loss: 4.310379
[INFO] Epoch: 42 , batch: 400 , training loss: 4.287736
[INFO] Epoch: 42 , batch: 401 , training loss: 4.694745
[INFO] Epoch: 42 , batch: 402 , training loss: 4.413402
[INFO] Epoch: 42 , batch: 403 , training loss: 4.220320
[INFO] Epoch: 42 , batch: 404 , training loss: 4.425375
[INFO] Epoch: 42 , batch: 405 , training loss: 4.476243
[INFO] Epoch: 42 , batch: 406 , training loss: 4.375945
[INFO] Epoch: 42 , batch: 407 , training loss: 4.422507
[INFO] Epoch: 42 , batch: 408 , training loss: 4.404779
[INFO] Epoch: 42 , batch: 409 , training loss: 4.381897
[INFO] Epoch: 42 , batch: 410 , training loss: 4.415442
[INFO] Epoch: 42 , batch: 411 , training loss: 4.623388
[INFO] Epoch: 42 , batch: 412 , training loss: 4.444949
[INFO] Epoch: 42 , batch: 413 , training loss: 4.342463
[INFO] Epoch: 42 , batch: 414 , training loss: 4.373085
[INFO] Epoch: 42 , batch: 415 , training loss: 4.410693
[INFO] Epoch: 42 , batch: 416 , training loss: 4.476860
[INFO] Epoch: 42 , batch: 417 , training loss: 4.382070
[INFO] Epoch: 42 , batch: 418 , training loss: 4.405777
[INFO] Epoch: 42 , batch: 419 , training loss: 4.371716
[INFO] Epoch: 42 , batch: 420 , training loss: 4.346220
[INFO] Epoch: 42 , batch: 421 , training loss: 4.358887
[INFO] Epoch: 42 , batch: 422 , training loss: 4.213761
[INFO] Epoch: 42 , batch: 423 , training loss: 4.405975
[INFO] Epoch: 42 , batch: 424 , training loss: 4.569743
[INFO] Epoch: 42 , batch: 425 , training loss: 4.450354
[INFO] Epoch: 42 , batch: 426 , training loss: 4.187555
[INFO] Epoch: 42 , batch: 427 , training loss: 4.413936
[INFO] Epoch: 42 , batch: 428 , training loss: 4.300531
[INFO] Epoch: 42 , batch: 429 , training loss: 4.158613
[INFO] Epoch: 42 , batch: 430 , training loss: 4.433939
[INFO] Epoch: 42 , batch: 431 , training loss: 4.032458
[INFO] Epoch: 42 , batch: 432 , training loss: 4.092275
[INFO] Epoch: 42 , batch: 433 , training loss: 4.120207
[INFO] Epoch: 42 , batch: 434 , training loss: 4.009491
[INFO] Epoch: 42 , batch: 435 , training loss: 4.374156
[INFO] Epoch: 42 , batch: 436 , training loss: 4.449728
[INFO] Epoch: 42 , batch: 437 , training loss: 4.202127
[INFO] Epoch: 42 , batch: 438 , training loss: 4.064946
[INFO] Epoch: 42 , batch: 439 , training loss: 4.280494
[INFO] Epoch: 42 , batch: 440 , training loss: 4.386560
[INFO] Epoch: 42 , batch: 441 , training loss: 4.489964
[INFO] Epoch: 42 , batch: 442 , training loss: 4.255379
[INFO] Epoch: 42 , batch: 443 , training loss: 4.455559
[INFO] Epoch: 42 , batch: 444 , training loss: 4.087447
[INFO] Epoch: 42 , batch: 445 , training loss: 3.973097
[INFO] Epoch: 42 , batch: 446 , training loss: 3.947113
[INFO] Epoch: 42 , batch: 447 , training loss: 4.107669
[INFO] Epoch: 42 , batch: 448 , training loss: 4.202805
[INFO] Epoch: 42 , batch: 449 , training loss: 4.587029
[INFO] Epoch: 42 , batch: 450 , training loss: 4.645182
[INFO] Epoch: 42 , batch: 451 , training loss: 4.574377
[INFO] Epoch: 42 , batch: 452 , training loss: 4.366241
[INFO] Epoch: 42 , batch: 453 , training loss: 4.149993
[INFO] Epoch: 42 , batch: 454 , training loss: 4.277001
[INFO] Epoch: 42 , batch: 455 , training loss: 4.351140
[INFO] Epoch: 42 , batch: 456 , training loss: 4.326266
[INFO] Epoch: 42 , batch: 457 , training loss: 4.418360
[INFO] Epoch: 42 , batch: 458 , training loss: 4.163556
[INFO] Epoch: 42 , batch: 459 , training loss: 4.142080
[INFO] Epoch: 42 , batch: 460 , training loss: 4.230526
[INFO] Epoch: 42 , batch: 461 , training loss: 4.217770
[INFO] Epoch: 42 , batch: 462 , training loss: 4.277584
[INFO] Epoch: 42 , batch: 463 , training loss: 4.207209
[INFO] Epoch: 42 , batch: 464 , training loss: 4.380969
[INFO] Epoch: 42 , batch: 465 , training loss: 4.294531
[INFO] Epoch: 42 , batch: 466 , training loss: 4.391364
[INFO] Epoch: 42 , batch: 467 , training loss: 4.371897
[INFO] Epoch: 42 , batch: 468 , training loss: 4.340319
[INFO] Epoch: 42 , batch: 469 , training loss: 4.361075
[INFO] Epoch: 42 , batch: 470 , training loss: 4.172447
[INFO] Epoch: 42 , batch: 471 , training loss: 4.275754
[INFO] Epoch: 42 , batch: 472 , training loss: 4.314535
[INFO] Epoch: 42 , batch: 473 , training loss: 4.257470
[INFO] Epoch: 42 , batch: 474 , training loss: 4.044110
[INFO] Epoch: 42 , batch: 475 , training loss: 3.923427
[INFO] Epoch: 42 , batch: 476 , training loss: 4.271162
[INFO] Epoch: 42 , batch: 477 , training loss: 4.434875
[INFO] Epoch: 42 , batch: 478 , training loss: 4.453633
[INFO] Epoch: 42 , batch: 479 , training loss: 4.412150
[INFO] Epoch: 42 , batch: 480 , training loss: 4.525938
[INFO] Epoch: 42 , batch: 481 , training loss: 4.406796
[INFO] Epoch: 42 , batch: 482 , training loss: 4.498629
[INFO] Epoch: 42 , batch: 483 , training loss: 4.368874
[INFO] Epoch: 42 , batch: 484 , training loss: 4.174901
[INFO] Epoch: 42 , batch: 485 , training loss: 4.256426
[INFO] Epoch: 42 , batch: 486 , training loss: 4.153041
[INFO] Epoch: 42 , batch: 487 , training loss: 4.161104
[INFO] Epoch: 42 , batch: 488 , training loss: 4.330200
[INFO] Epoch: 42 , batch: 489 , training loss: 4.222641
[INFO] Epoch: 42 , batch: 490 , training loss: 4.247910
[INFO] Epoch: 42 , batch: 491 , training loss: 4.206574
[INFO] Epoch: 42 , batch: 492 , training loss: 4.165390
[INFO] Epoch: 42 , batch: 493 , training loss: 4.366005
[INFO] Epoch: 42 , batch: 494 , training loss: 4.276286
[INFO] Epoch: 42 , batch: 495 , training loss: 4.393943
[INFO] Epoch: 42 , batch: 496 , training loss: 4.293059
[INFO] Epoch: 42 , batch: 497 , training loss: 4.327075
[INFO] Epoch: 42 , batch: 498 , training loss: 4.309922
[INFO] Epoch: 42 , batch: 499 , training loss: 4.371950
[INFO] Epoch: 42 , batch: 500 , training loss: 4.525165
[INFO] Epoch: 42 , batch: 501 , training loss: 4.798753
[INFO] Epoch: 42 , batch: 502 , training loss: 4.885907
[INFO] Epoch: 42 , batch: 503 , training loss: 4.567099
[INFO] Epoch: 42 , batch: 504 , training loss: 4.682693
[INFO] Epoch: 42 , batch: 505 , training loss: 4.657967
[INFO] Epoch: 42 , batch: 506 , training loss: 4.615829
[INFO] Epoch: 42 , batch: 507 , training loss: 4.691338
[INFO] Epoch: 42 , batch: 508 , training loss: 4.628091
[INFO] Epoch: 42 , batch: 509 , training loss: 4.427800
[INFO] Epoch: 42 , batch: 510 , training loss: 4.503890
[INFO] Epoch: 42 , batch: 511 , training loss: 4.408335
[INFO] Epoch: 42 , batch: 512 , training loss: 4.531583
[INFO] Epoch: 42 , batch: 513 , training loss: 4.767126
[INFO] Epoch: 42 , batch: 514 , training loss: 4.419055
[INFO] Epoch: 42 , batch: 515 , training loss: 4.646810
[INFO] Epoch: 42 , batch: 516 , training loss: 4.466594
[INFO] Epoch: 42 , batch: 517 , training loss: 4.403708
[INFO] Epoch: 42 , batch: 518 , training loss: 4.371921
[INFO] Epoch: 42 , batch: 519 , training loss: 4.221222
[INFO] Epoch: 42 , batch: 520 , training loss: 4.459765
[INFO] Epoch: 42 , batch: 521 , training loss: 4.440739
[INFO] Epoch: 42 , batch: 522 , training loss: 4.498782
[INFO] Epoch: 42 , batch: 523 , training loss: 4.402884
[INFO] Epoch: 42 , batch: 524 , training loss: 4.703386
[INFO] Epoch: 42 , batch: 525 , training loss: 4.636516
[INFO] Epoch: 42 , batch: 526 , training loss: 4.396144
[INFO] Epoch: 42 , batch: 527 , training loss: 4.447662
[INFO] Epoch: 42 , batch: 528 , training loss: 4.444859
[INFO] Epoch: 42 , batch: 529 , training loss: 4.433980
[INFO] Epoch: 42 , batch: 530 , training loss: 4.272136
[INFO] Epoch: 42 , batch: 531 , training loss: 4.402618
[INFO] Epoch: 42 , batch: 532 , training loss: 4.313763
[INFO] Epoch: 42 , batch: 533 , training loss: 4.457109
[INFO] Epoch: 42 , batch: 534 , training loss: 4.425987
[INFO] Epoch: 42 , batch: 535 , training loss: 4.458620
[INFO] Epoch: 42 , batch: 536 , training loss: 4.266722
[INFO] Epoch: 42 , batch: 537 , training loss: 4.282840
[INFO] Epoch: 42 , batch: 538 , training loss: 4.361191
[INFO] Epoch: 42 , batch: 539 , training loss: 4.477232
[INFO] Epoch: 42 , batch: 540 , training loss: 4.952796
[INFO] Epoch: 42 , batch: 541 , training loss: 4.805575
[INFO] Epoch: 42 , batch: 542 , training loss: 4.740159
[INFO] Epoch: 43 , batch: 0 , training loss: 3.302167
[INFO] Epoch: 43 , batch: 1 , training loss: 3.378987
[INFO] Epoch: 43 , batch: 2 , training loss: 3.600763
[INFO] Epoch: 43 , batch: 3 , training loss: 3.385721
[INFO] Epoch: 43 , batch: 4 , training loss: 3.794980
[INFO] Epoch: 43 , batch: 5 , training loss: 3.432337
[INFO] Epoch: 43 , batch: 6 , training loss: 3.787292
[INFO] Epoch: 43 , batch: 7 , training loss: 3.765345
[INFO] Epoch: 43 , batch: 8 , training loss: 3.429999
[INFO] Epoch: 43 , batch: 9 , training loss: 3.717743
[INFO] Epoch: 43 , batch: 10 , training loss: 3.716525
[INFO] Epoch: 43 , batch: 11 , training loss: 3.671782
[INFO] Epoch: 43 , batch: 12 , training loss: 3.542517
[INFO] Epoch: 43 , batch: 13 , training loss: 3.612542
[INFO] Epoch: 43 , batch: 14 , training loss: 3.470668
[INFO] Epoch: 43 , batch: 15 , training loss: 3.683980
[INFO] Epoch: 43 , batch: 16 , training loss: 3.554207
[INFO] Epoch: 43 , batch: 17 , training loss: 3.693146
[INFO] Epoch: 43 , batch: 18 , training loss: 3.628060
[INFO] Epoch: 43 , batch: 19 , training loss: 3.376374
[INFO] Epoch: 43 , batch: 20 , training loss: 3.387346
[INFO] Epoch: 43 , batch: 21 , training loss: 3.494127
[INFO] Epoch: 43 , batch: 22 , training loss: 3.341997
[INFO] Epoch: 43 , batch: 23 , training loss: 3.609384
[INFO] Epoch: 43 , batch: 24 , training loss: 3.407598
[INFO] Epoch: 43 , batch: 25 , training loss: 3.604939
[INFO] Epoch: 43 , batch: 26 , training loss: 3.483793
[INFO] Epoch: 43 , batch: 27 , training loss: 3.408532
[INFO] Epoch: 43 , batch: 28 , training loss: 3.580771
[INFO] Epoch: 43 , batch: 29 , training loss: 3.417302
[INFO] Epoch: 43 , batch: 30 , training loss: 3.467515
[INFO] Epoch: 43 , batch: 31 , training loss: 3.504297
[INFO] Epoch: 43 , batch: 32 , training loss: 3.518758
[INFO] Epoch: 43 , batch: 33 , training loss: 3.540750
[INFO] Epoch: 43 , batch: 34 , training loss: 3.471530
[INFO] Epoch: 43 , batch: 35 , training loss: 3.476829
[INFO] Epoch: 43 , batch: 36 , training loss: 3.527654
[INFO] Epoch: 43 , batch: 37 , training loss: 3.458018
[INFO] Epoch: 43 , batch: 38 , training loss: 3.459241
[INFO] Epoch: 43 , batch: 39 , training loss: 3.345944
[INFO] Epoch: 43 , batch: 40 , training loss: 3.558763
[INFO] Epoch: 43 , batch: 41 , training loss: 3.422091
[INFO] Epoch: 43 , batch: 42 , training loss: 3.889490
[INFO] Epoch: 43 , batch: 43 , training loss: 3.612684
[INFO] Epoch: 43 , batch: 44 , training loss: 3.978596
[INFO] Epoch: 43 , batch: 45 , training loss: 3.910029
[INFO] Epoch: 43 , batch: 46 , training loss: 3.784255
[INFO] Epoch: 43 , batch: 47 , training loss: 3.560672
[INFO] Epoch: 43 , batch: 48 , training loss: 3.550688
[INFO] Epoch: 43 , batch: 49 , training loss: 3.721912
[INFO] Epoch: 43 , batch: 50 , training loss: 3.546073
[INFO] Epoch: 43 , batch: 51 , training loss: 3.753098
[INFO] Epoch: 43 , batch: 52 , training loss: 3.625110
[INFO] Epoch: 43 , batch: 53 , training loss: 3.733042
[INFO] Epoch: 43 , batch: 54 , training loss: 3.742296
[INFO] Epoch: 43 , batch: 55 , training loss: 3.830682
[INFO] Epoch: 43 , batch: 56 , training loss: 3.721515
[INFO] Epoch: 43 , batch: 57 , training loss: 3.587362
[INFO] Epoch: 43 , batch: 58 , training loss: 3.649871
[INFO] Epoch: 43 , batch: 59 , training loss: 3.752489
[INFO] Epoch: 43 , batch: 60 , training loss: 3.711876
[INFO] Epoch: 43 , batch: 61 , training loss: 3.770409
[INFO] Epoch: 43 , batch: 62 , training loss: 3.655932
[INFO] Epoch: 43 , batch: 63 , training loss: 3.855934
[INFO] Epoch: 43 , batch: 64 , training loss: 4.011910
[INFO] Epoch: 43 , batch: 65 , training loss: 3.701158
[INFO] Epoch: 43 , batch: 66 , training loss: 3.587383
[INFO] Epoch: 43 , batch: 67 , training loss: 3.660505
[INFO] Epoch: 43 , batch: 68 , training loss: 3.796575
[INFO] Epoch: 43 , batch: 69 , training loss: 3.667621
[INFO] Epoch: 43 , batch: 70 , training loss: 3.903924
[INFO] Epoch: 43 , batch: 71 , training loss: 3.802465
[INFO] Epoch: 43 , batch: 72 , training loss: 3.851107
[INFO] Epoch: 43 , batch: 73 , training loss: 3.757726
[INFO] Epoch: 43 , batch: 74 , training loss: 3.901125
[INFO] Epoch: 43 , batch: 75 , training loss: 3.773160
[INFO] Epoch: 43 , batch: 76 , training loss: 3.812539
[INFO] Epoch: 43 , batch: 77 , training loss: 3.783751
[INFO] Epoch: 43 , batch: 78 , training loss: 3.928228
[INFO] Epoch: 43 , batch: 79 , training loss: 3.758405
[INFO] Epoch: 43 , batch: 80 , training loss: 3.939194
[INFO] Epoch: 43 , batch: 81 , training loss: 3.867242
[INFO] Epoch: 43 , batch: 82 , training loss: 3.827928
[INFO] Epoch: 43 , batch: 83 , training loss: 3.960939
[INFO] Epoch: 43 , batch: 84 , training loss: 3.886168
[INFO] Epoch: 43 , batch: 85 , training loss: 3.992243
[INFO] Epoch: 43 , batch: 86 , training loss: 3.921932
[INFO] Epoch: 43 , batch: 87 , training loss: 3.897071
[INFO] Epoch: 43 , batch: 88 , training loss: 4.009386
[INFO] Epoch: 43 , batch: 89 , training loss: 3.809940
[INFO] Epoch: 43 , batch: 90 , training loss: 3.914105
[INFO] Epoch: 43 , batch: 91 , training loss: 3.860165
[INFO] Epoch: 43 , batch: 92 , training loss: 3.834022
[INFO] Epoch: 43 , batch: 93 , training loss: 3.943982
[INFO] Epoch: 43 , batch: 94 , training loss: 4.075910
[INFO] Epoch: 43 , batch: 95 , training loss: 3.860466
[INFO] Epoch: 43 , batch: 96 , training loss: 3.863884
[INFO] Epoch: 43 , batch: 97 , training loss: 3.766536
[INFO] Epoch: 43 , batch: 98 , training loss: 3.729949
[INFO] Epoch: 43 , batch: 99 , training loss: 3.844843
[INFO] Epoch: 43 , batch: 100 , training loss: 3.774657
[INFO] Epoch: 43 , batch: 101 , training loss: 3.796500
[INFO] Epoch: 43 , batch: 102 , training loss: 3.918096
[INFO] Epoch: 43 , batch: 103 , training loss: 3.735036
[INFO] Epoch: 43 , batch: 104 , training loss: 3.690074
[INFO] Epoch: 43 , batch: 105 , training loss: 3.949646
[INFO] Epoch: 43 , batch: 106 , training loss: 3.910400
[INFO] Epoch: 43 , batch: 107 , training loss: 3.766449
[INFO] Epoch: 43 , batch: 108 , training loss: 3.715040
[INFO] Epoch: 43 , batch: 109 , training loss: 3.651135
[INFO] Epoch: 43 , batch: 110 , training loss: 3.817559
[INFO] Epoch: 43 , batch: 111 , training loss: 3.888453
[INFO] Epoch: 43 , batch: 112 , training loss: 3.814838
[INFO] Epoch: 43 , batch: 113 , training loss: 3.838027
[INFO] Epoch: 43 , batch: 114 , training loss: 3.814310
[INFO] Epoch: 43 , batch: 115 , training loss: 3.798025
[INFO] Epoch: 43 , batch: 116 , training loss: 3.738725
[INFO] Epoch: 43 , batch: 117 , training loss: 3.974134
[INFO] Epoch: 43 , batch: 118 , training loss: 3.911265
[INFO] Epoch: 43 , batch: 119 , training loss: 4.033745
[INFO] Epoch: 43 , batch: 120 , training loss: 4.048615
[INFO] Epoch: 43 , batch: 121 , training loss: 3.897604
[INFO] Epoch: 43 , batch: 122 , training loss: 3.813077
[INFO] Epoch: 43 , batch: 123 , training loss: 3.824779
[INFO] Epoch: 43 , batch: 124 , training loss: 3.910744
[INFO] Epoch: 43 , batch: 125 , training loss: 3.763471
[INFO] Epoch: 43 , batch: 126 , training loss: 3.727863
[INFO] Epoch: 43 , batch: 127 , training loss: 3.745750
[INFO] Epoch: 43 , batch: 128 , training loss: 3.870128
[INFO] Epoch: 43 , batch: 129 , training loss: 3.834298
[INFO] Epoch: 43 , batch: 130 , training loss: 3.826443
[INFO] Epoch: 43 , batch: 131 , training loss: 3.836265
[INFO] Epoch: 43 , batch: 132 , training loss: 3.823632
[INFO] Epoch: 43 , batch: 133 , training loss: 3.796768
[INFO] Epoch: 43 , batch: 134 , training loss: 3.641719
[INFO] Epoch: 43 , batch: 135 , training loss: 3.649833
[INFO] Epoch: 43 , batch: 136 , training loss: 3.962063
[INFO] Epoch: 43 , batch: 137 , training loss: 3.881353
[INFO] Epoch: 43 , batch: 138 , training loss: 3.887729
[INFO] Epoch: 43 , batch: 139 , training loss: 4.478457
[INFO] Epoch: 43 , batch: 140 , training loss: 4.176048
[INFO] Epoch: 43 , batch: 141 , training loss: 4.020373
[INFO] Epoch: 43 , batch: 142 , training loss: 3.806890
[INFO] Epoch: 43 , batch: 143 , training loss: 3.923259
[INFO] Epoch: 43 , batch: 144 , training loss: 3.720798
[INFO] Epoch: 43 , batch: 145 , training loss: 3.794881
[INFO] Epoch: 43 , batch: 146 , training loss: 3.967633
[INFO] Epoch: 43 , batch: 147 , training loss: 3.664925
[INFO] Epoch: 43 , batch: 148 , training loss: 3.686749
[INFO] Epoch: 43 , batch: 149 , training loss: 3.768841
[INFO] Epoch: 43 , batch: 150 , training loss: 3.954216
[INFO] Epoch: 43 , batch: 151 , training loss: 3.847548
[INFO] Epoch: 43 , batch: 152 , training loss: 3.912226
[INFO] Epoch: 43 , batch: 153 , training loss: 3.878363
[INFO] Epoch: 43 , batch: 154 , training loss: 3.959040
[INFO] Epoch: 43 , batch: 155 , training loss: 4.200602
[INFO] Epoch: 43 , batch: 156 , training loss: 3.922493
[INFO] Epoch: 43 , batch: 157 , training loss: 3.916021
[INFO] Epoch: 43 , batch: 158 , training loss: 4.015220
[INFO] Epoch: 43 , batch: 159 , training loss: 3.876098
[INFO] Epoch: 43 , batch: 160 , training loss: 4.121899
[INFO] Epoch: 43 , batch: 161 , training loss: 4.282370
[INFO] Epoch: 43 , batch: 162 , training loss: 4.235988
[INFO] Epoch: 43 , batch: 163 , training loss: 4.377005
[INFO] Epoch: 43 , batch: 164 , training loss: 4.384710
[INFO] Epoch: 43 , batch: 165 , training loss: 4.313067
[INFO] Epoch: 43 , batch: 166 , training loss: 4.153921
[INFO] Epoch: 43 , batch: 167 , training loss: 4.111917
[INFO] Epoch: 43 , batch: 168 , training loss: 3.842712
[INFO] Epoch: 43 , batch: 169 , training loss: 3.803092
[INFO] Epoch: 43 , batch: 170 , training loss: 4.033465
[INFO] Epoch: 43 , batch: 171 , training loss: 3.421558
[INFO] Epoch: 43 , batch: 172 , training loss: 3.657640
[INFO] Epoch: 43 , batch: 173 , training loss: 4.055812
[INFO] Epoch: 43 , batch: 174 , training loss: 4.460997
[INFO] Epoch: 43 , batch: 175 , training loss: 4.775522
[INFO] Epoch: 43 , batch: 176 , training loss: 4.390501
[INFO] Epoch: 43 , batch: 177 , training loss: 4.111360
[INFO] Epoch: 43 , batch: 178 , training loss: 4.042145
[INFO] Epoch: 43 , batch: 179 , training loss: 4.123877
[INFO] Epoch: 43 , batch: 180 , training loss: 4.085340
[INFO] Epoch: 43 , batch: 181 , training loss: 4.354815
[INFO] Epoch: 43 , batch: 182 , training loss: 4.335085
[INFO] Epoch: 43 , batch: 183 , training loss: 4.255835
[INFO] Epoch: 43 , batch: 184 , training loss: 4.154940
[INFO] Epoch: 43 , batch: 185 , training loss: 4.081192
[INFO] Epoch: 43 , batch: 186 , training loss: 4.276506
[INFO] Epoch: 43 , batch: 187 , training loss: 4.388842
[INFO] Epoch: 43 , batch: 188 , training loss: 4.380327
[INFO] Epoch: 43 , batch: 189 , training loss: 4.271877
[INFO] Epoch: 43 , batch: 190 , training loss: 4.315056
[INFO] Epoch: 43 , batch: 191 , training loss: 4.393444
[INFO] Epoch: 43 , batch: 192 , training loss: 4.231075
[INFO] Epoch: 43 , batch: 193 , training loss: 4.328640
[INFO] Epoch: 43 , batch: 194 , training loss: 4.264815
[INFO] Epoch: 43 , batch: 195 , training loss: 4.176868
[INFO] Epoch: 43 , batch: 196 , training loss: 4.052398
[INFO] Epoch: 43 , batch: 197 , training loss: 4.164415
[INFO] Epoch: 43 , batch: 198 , training loss: 4.069976
[INFO] Epoch: 43 , batch: 199 , training loss: 4.234976
[INFO] Epoch: 43 , batch: 200 , training loss: 4.115501
[INFO] Epoch: 43 , batch: 201 , training loss: 4.034893
[INFO] Epoch: 43 , batch: 202 , training loss: 4.026132
[INFO] Epoch: 43 , batch: 203 , training loss: 4.136367
[INFO] Epoch: 43 , batch: 204 , training loss: 4.287328
[INFO] Epoch: 43 , batch: 205 , training loss: 3.811499
[INFO] Epoch: 43 , batch: 206 , training loss: 3.774933
[INFO] Epoch: 43 , batch: 207 , training loss: 3.781112
[INFO] Epoch: 43 , batch: 208 , training loss: 4.108856
[INFO] Epoch: 43 , batch: 209 , training loss: 4.049207
[INFO] Epoch: 43 , batch: 210 , training loss: 4.078566
[INFO] Epoch: 43 , batch: 211 , training loss: 4.069113
[INFO] Epoch: 43 , batch: 212 , training loss: 4.176089
[INFO] Epoch: 43 , batch: 213 , training loss: 4.116212
[INFO] Epoch: 43 , batch: 214 , training loss: 4.174195
[INFO] Epoch: 43 , batch: 215 , training loss: 4.428629
[INFO] Epoch: 43 , batch: 216 , training loss: 4.113131
[INFO] Epoch: 43 , batch: 217 , training loss: 4.075637
[INFO] Epoch: 43 , batch: 218 , training loss: 4.042037
[INFO] Epoch: 43 , batch: 219 , training loss: 4.151703
[INFO] Epoch: 43 , batch: 220 , training loss: 3.960583
[INFO] Epoch: 43 , batch: 221 , training loss: 3.988519
[INFO] Epoch: 43 , batch: 222 , training loss: 4.123802
[INFO] Epoch: 43 , batch: 223 , training loss: 4.230533
[INFO] Epoch: 43 , batch: 224 , training loss: 4.271096
[INFO] Epoch: 43 , batch: 225 , training loss: 4.175472
[INFO] Epoch: 43 , batch: 226 , training loss: 4.268917
[INFO] Epoch: 43 , batch: 227 , training loss: 4.245075
[INFO] Epoch: 43 , batch: 228 , training loss: 4.290397
[INFO] Epoch: 43 , batch: 229 , training loss: 4.147738
[INFO] Epoch: 43 , batch: 230 , training loss: 4.020044
[INFO] Epoch: 43 , batch: 231 , training loss: 3.887596
[INFO] Epoch: 43 , batch: 232 , training loss: 4.015750
[INFO] Epoch: 43 , batch: 233 , training loss: 4.019364
[INFO] Epoch: 43 , batch: 234 , training loss: 3.737378
[INFO] Epoch: 43 , batch: 235 , training loss: 3.817176
[INFO] Epoch: 43 , batch: 236 , training loss: 3.932894
[INFO] Epoch: 43 , batch: 237 , training loss: 4.143796
[INFO] Epoch: 43 , batch: 238 , training loss: 3.922266
[INFO] Epoch: 43 , batch: 239 , training loss: 3.960273
[INFO] Epoch: 43 , batch: 240 , training loss: 4.037121
[INFO] Epoch: 43 , batch: 241 , training loss: 3.843131
[INFO] Epoch: 43 , batch: 242 , training loss: 3.864491
[INFO] Epoch: 43 , batch: 243 , training loss: 4.157916
[INFO] Epoch: 43 , batch: 244 , training loss: 4.094619
[INFO] Epoch: 43 , batch: 245 , training loss: 4.029409
[INFO] Epoch: 43 , batch: 246 , training loss: 3.760923
[INFO] Epoch: 43 , batch: 247 , training loss: 3.932187
[INFO] Epoch: 43 , batch: 248 , training loss: 3.986631
[INFO] Epoch: 43 , batch: 249 , training loss: 4.028158
[INFO] Epoch: 43 , batch: 250 , training loss: 3.786111
[INFO] Epoch: 43 , batch: 251 , training loss: 4.216229
[INFO] Epoch: 43 , batch: 252 , training loss: 3.923207
[INFO] Epoch: 43 , batch: 253 , training loss: 3.854261
[INFO] Epoch: 43 , batch: 254 , training loss: 4.114910
[INFO] Epoch: 43 , batch: 255 , training loss: 4.095845
[INFO] Epoch: 43 , batch: 256 , training loss: 4.110388
[INFO] Epoch: 43 , batch: 257 , training loss: 4.215235
[INFO] Epoch: 43 , batch: 258 , training loss: 4.286630
[INFO] Epoch: 43 , batch: 259 , training loss: 4.315336
[INFO] Epoch: 43 , batch: 260 , training loss: 4.064847
[INFO] Epoch: 43 , batch: 261 , training loss: 4.184953
[INFO] Epoch: 43 , batch: 262 , training loss: 4.370343
[INFO] Epoch: 43 , batch: 263 , training loss: 4.567496
[INFO] Epoch: 43 , batch: 264 , training loss: 3.885904
[INFO] Epoch: 43 , batch: 265 , training loss: 4.020071
[INFO] Epoch: 43 , batch: 266 , training loss: 4.443688
[INFO] Epoch: 43 , batch: 267 , training loss: 4.207933
[INFO] Epoch: 43 , batch: 268 , training loss: 4.120079
[INFO] Epoch: 43 , batch: 269 , training loss: 4.119401
[INFO] Epoch: 43 , batch: 270 , training loss: 4.129083
[INFO] Epoch: 43 , batch: 271 , training loss: 4.147736
[INFO] Epoch: 43 , batch: 272 , training loss: 4.127220
[INFO] Epoch: 43 , batch: 273 , training loss: 4.164393
[INFO] Epoch: 43 , batch: 274 , training loss: 4.229219
[INFO] Epoch: 43 , batch: 275 , training loss: 4.097227
[INFO] Epoch: 43 , batch: 276 , training loss: 4.135680
[INFO] Epoch: 43 , batch: 277 , training loss: 4.304482
[INFO] Epoch: 43 , batch: 278 , training loss: 3.991677
[INFO] Epoch: 43 , batch: 279 , training loss: 3.994812
[INFO] Epoch: 43 , batch: 280 , training loss: 3.950725
[INFO] Epoch: 43 , batch: 281 , training loss: 4.095996
[INFO] Epoch: 43 , batch: 282 , training loss: 4.005177
[INFO] Epoch: 43 , batch: 283 , training loss: 4.034502
[INFO] Epoch: 43 , batch: 284 , training loss: 4.052617
[INFO] Epoch: 43 , batch: 285 , training loss: 3.979446
[INFO] Epoch: 43 , batch: 286 , training loss: 3.968712
[INFO] Epoch: 43 , batch: 287 , training loss: 3.925327
[INFO] Epoch: 43 , batch: 288 , training loss: 3.921551
[INFO] Epoch: 43 , batch: 289 , training loss: 3.976577
[INFO] Epoch: 43 , batch: 290 , training loss: 3.775956
[INFO] Epoch: 43 , batch: 291 , training loss: 3.734062
[INFO] Epoch: 43 , batch: 292 , training loss: 3.889143
[INFO] Epoch: 43 , batch: 293 , training loss: 3.785432
[INFO] Epoch: 43 , batch: 294 , training loss: 4.446484
[INFO] Epoch: 43 , batch: 295 , training loss: 4.211797
[INFO] Epoch: 43 , batch: 296 , training loss: 4.147738
[INFO] Epoch: 43 , batch: 297 , training loss: 4.093435
[INFO] Epoch: 43 , batch: 298 , training loss: 3.933850
[INFO] Epoch: 43 , batch: 299 , training loss: 3.968710
[INFO] Epoch: 43 , batch: 300 , training loss: 3.939661
[INFO] Epoch: 43 , batch: 301 , training loss: 3.882829
[INFO] Epoch: 43 , batch: 302 , training loss: 4.059624
[INFO] Epoch: 43 , batch: 303 , training loss: 4.061184
[INFO] Epoch: 43 , batch: 304 , training loss: 4.212842
[INFO] Epoch: 43 , batch: 305 , training loss: 4.024331
[INFO] Epoch: 43 , batch: 306 , training loss: 4.143795
[INFO] Epoch: 43 , batch: 307 , training loss: 4.129807
[INFO] Epoch: 43 , batch: 308 , training loss: 3.999719
[INFO] Epoch: 43 , batch: 309 , training loss: 3.990915
[INFO] Epoch: 43 , batch: 310 , training loss: 3.905057
[INFO] Epoch: 43 , batch: 311 , training loss: 3.892556
[INFO] Epoch: 43 , batch: 312 , training loss: 3.821565
[INFO] Epoch: 43 , batch: 313 , training loss: 3.929382
[INFO] Epoch: 43 , batch: 314 , training loss: 3.984880
[INFO] Epoch: 43 , batch: 315 , training loss: 4.016164
[INFO] Epoch: 43 , batch: 316 , training loss: 4.276110
[INFO] Epoch: 43 , batch: 317 , training loss: 4.677835
[INFO] Epoch: 43 , batch: 318 , training loss: 4.840305
[INFO] Epoch: 43 , batch: 319 , training loss: 4.470097
[INFO] Epoch: 43 , batch: 320 , training loss: 3.993875
[INFO] Epoch: 43 , batch: 321 , training loss: 3.830639
[INFO] Epoch: 43 , batch: 322 , training loss: 3.935741
[INFO] Epoch: 43 , batch: 323 , training loss: 3.973217
[INFO] Epoch: 43 , batch: 324 , training loss: 3.920864
[INFO] Epoch: 43 , batch: 325 , training loss: 4.086533
[INFO] Epoch: 43 , batch: 326 , training loss: 4.158163
[INFO] Epoch: 43 , batch: 327 , training loss: 4.042298
[INFO] Epoch: 43 , batch: 328 , training loss: 4.053304
[INFO] Epoch: 43 , batch: 329 , training loss: 3.953875
[INFO] Epoch: 43 , batch: 330 , training loss: 3.955980
[INFO] Epoch: 43 , batch: 331 , training loss: 4.126408
[INFO] Epoch: 43 , batch: 332 , training loss: 3.935453
[INFO] Epoch: 43 , batch: 333 , training loss: 3.946327
[INFO] Epoch: 43 , batch: 334 , training loss: 3.934954
[INFO] Epoch: 43 , batch: 335 , training loss: 4.086857
[INFO] Epoch: 43 , batch: 336 , training loss: 4.106092
[INFO] Epoch: 43 , batch: 337 , training loss: 4.140200
[INFO] Epoch: 43 , batch: 338 , training loss: 4.356516
[INFO] Epoch: 43 , batch: 339 , training loss: 4.180127
[INFO] Epoch: 43 , batch: 340 , training loss: 4.320508
[INFO] Epoch: 43 , batch: 341 , training loss: 4.087686
[INFO] Epoch: 43 , batch: 342 , training loss: 3.869262
[INFO] Epoch: 43 , batch: 343 , training loss: 3.943254
[INFO] Epoch: 43 , batch: 344 , training loss: 3.825910
[INFO] Epoch: 43 , batch: 345 , training loss: 3.942443
[INFO] Epoch: 43 , batch: 346 , training loss: 3.981033
[INFO] Epoch: 43 , batch: 347 , training loss: 3.901594
[INFO] Epoch: 43 , batch: 348 , training loss: 3.978798
[INFO] Epoch: 43 , batch: 349 , training loss: 4.155964
[INFO] Epoch: 43 , batch: 350 , training loss: 3.959887
[INFO] Epoch: 43 , batch: 351 , training loss: 4.025831
[INFO] Epoch: 43 , batch: 352 , training loss: 4.024052
[INFO] Epoch: 43 , batch: 353 , training loss: 4.004053
[INFO] Epoch: 43 , batch: 354 , training loss: 4.116625
[INFO] Epoch: 43 , batch: 355 , training loss: 4.147896
[INFO] Epoch: 43 , batch: 356 , training loss: 3.973477
[INFO] Epoch: 43 , batch: 357 , training loss: 4.060362
[INFO] Epoch: 43 , batch: 358 , training loss: 3.971869
[INFO] Epoch: 43 , batch: 359 , training loss: 3.959497
[INFO] Epoch: 43 , batch: 360 , training loss: 4.077328
[INFO] Epoch: 43 , batch: 361 , training loss: 4.035820
[INFO] Epoch: 43 , batch: 362 , training loss: 4.132094
[INFO] Epoch: 43 , batch: 363 , training loss: 3.998950
[INFO] Epoch: 43 , batch: 364 , training loss: 4.048536
[INFO] Epoch: 43 , batch: 365 , training loss: 4.004023
[INFO] Epoch: 43 , batch: 366 , training loss: 4.104304
[INFO] Epoch: 43 , batch: 367 , training loss: 4.179387
[INFO] Epoch: 43 , batch: 368 , training loss: 4.573131
[INFO] Epoch: 43 , batch: 369 , training loss: 4.232598
[INFO] Epoch: 43 , batch: 370 , training loss: 4.002993
[INFO] Epoch: 43 , batch: 371 , training loss: 4.382527
[INFO] Epoch: 43 , batch: 372 , training loss: 4.635466
[INFO] Epoch: 43 , batch: 373 , training loss: 4.712472
[INFO] Epoch: 43 , batch: 374 , training loss: 4.824069
[INFO] Epoch: 43 , batch: 375 , training loss: 4.822029
[INFO] Epoch: 43 , batch: 376 , training loss: 4.716099
[INFO] Epoch: 43 , batch: 377 , training loss: 4.503953
[INFO] Epoch: 43 , batch: 378 , training loss: 4.585065
[INFO] Epoch: 43 , batch: 379 , training loss: 4.552447
[INFO] Epoch: 43 , batch: 380 , training loss: 4.695962
[INFO] Epoch: 43 , batch: 381 , training loss: 4.413766
[INFO] Epoch: 43 , batch: 382 , training loss: 4.649718
[INFO] Epoch: 43 , batch: 383 , training loss: 4.654315
[INFO] Epoch: 43 , batch: 384 , training loss: 4.687539
[INFO] Epoch: 43 , batch: 385 , training loss: 4.374259
[INFO] Epoch: 43 , batch: 386 , training loss: 4.648578
[INFO] Epoch: 43 , batch: 387 , training loss: 4.619980
[INFO] Epoch: 43 , batch: 388 , training loss: 4.404029
[INFO] Epoch: 43 , batch: 389 , training loss: 4.253036
[INFO] Epoch: 43 , batch: 390 , training loss: 4.239941
[INFO] Epoch: 43 , batch: 391 , training loss: 4.305199
[INFO] Epoch: 43 , batch: 392 , training loss: 4.635029
[INFO] Epoch: 43 , batch: 393 , training loss: 4.525811
[INFO] Epoch: 43 , batch: 394 , training loss: 4.590244
[INFO] Epoch: 43 , batch: 395 , training loss: 4.440078
[INFO] Epoch: 43 , batch: 396 , training loss: 4.234504
[INFO] Epoch: 43 , batch: 397 , training loss: 4.396092
[INFO] Epoch: 43 , batch: 398 , training loss: 4.258793
[INFO] Epoch: 43 , batch: 399 , training loss: 4.316108
[INFO] Epoch: 43 , batch: 400 , training loss: 4.281138
[INFO] Epoch: 43 , batch: 401 , training loss: 4.687375
[INFO] Epoch: 43 , batch: 402 , training loss: 4.405077
[INFO] Epoch: 43 , batch: 403 , training loss: 4.228207
[INFO] Epoch: 43 , batch: 404 , training loss: 4.423854
[INFO] Epoch: 43 , batch: 405 , training loss: 4.472191
[INFO] Epoch: 43 , batch: 406 , training loss: 4.379371
[INFO] Epoch: 43 , batch: 407 , training loss: 4.428666
[INFO] Epoch: 43 , batch: 408 , training loss: 4.389000
[INFO] Epoch: 43 , batch: 409 , training loss: 4.380491
[INFO] Epoch: 43 , batch: 410 , training loss: 4.408490
[INFO] Epoch: 43 , batch: 411 , training loss: 4.616700
[INFO] Epoch: 43 , batch: 412 , training loss: 4.446496
[INFO] Epoch: 43 , batch: 413 , training loss: 4.342660
[INFO] Epoch: 43 , batch: 414 , training loss: 4.379841
[INFO] Epoch: 43 , batch: 415 , training loss: 4.416640
[INFO] Epoch: 43 , batch: 416 , training loss: 4.476134
[INFO] Epoch: 43 , batch: 417 , training loss: 4.398647
[INFO] Epoch: 43 , batch: 418 , training loss: 4.413717
[INFO] Epoch: 43 , batch: 419 , training loss: 4.383905
[INFO] Epoch: 43 , batch: 420 , training loss: 4.349965
[INFO] Epoch: 43 , batch: 421 , training loss: 4.360806
[INFO] Epoch: 43 , batch: 422 , training loss: 4.207956
[INFO] Epoch: 43 , batch: 423 , training loss: 4.406334
[INFO] Epoch: 43 , batch: 424 , training loss: 4.582532
[INFO] Epoch: 43 , batch: 425 , training loss: 4.454302
[INFO] Epoch: 43 , batch: 426 , training loss: 4.192129
[INFO] Epoch: 43 , batch: 427 , training loss: 4.428751
[INFO] Epoch: 43 , batch: 428 , training loss: 4.305746
[INFO] Epoch: 43 , batch: 429 , training loss: 4.161895
[INFO] Epoch: 43 , batch: 430 , training loss: 4.433827
[INFO] Epoch: 43 , batch: 431 , training loss: 4.031810
[INFO] Epoch: 43 , batch: 432 , training loss: 4.088405
[INFO] Epoch: 43 , batch: 433 , training loss: 4.117114
[INFO] Epoch: 43 , batch: 434 , training loss: 4.003873
[INFO] Epoch: 43 , batch: 435 , training loss: 4.375483
[INFO] Epoch: 43 , batch: 436 , training loss: 4.449734
[INFO] Epoch: 43 , batch: 437 , training loss: 4.187740
[INFO] Epoch: 43 , batch: 438 , training loss: 4.059403
[INFO] Epoch: 43 , batch: 439 , training loss: 4.270800
[INFO] Epoch: 43 , batch: 440 , training loss: 4.390446
[INFO] Epoch: 43 , batch: 441 , training loss: 4.475501
[INFO] Epoch: 43 , batch: 442 , training loss: 4.250742
[INFO] Epoch: 43 , batch: 443 , training loss: 4.461193
[INFO] Epoch: 43 , batch: 444 , training loss: 4.075037
[INFO] Epoch: 43 , batch: 445 , training loss: 3.967747
[INFO] Epoch: 43 , batch: 446 , training loss: 3.934407
[INFO] Epoch: 43 , batch: 447 , training loss: 4.110341
[INFO] Epoch: 43 , batch: 448 , training loss: 4.199973
[INFO] Epoch: 43 , batch: 449 , training loss: 4.589881
[INFO] Epoch: 43 , batch: 450 , training loss: 4.649072
[INFO] Epoch: 43 , batch: 451 , training loss: 4.575852
[INFO] Epoch: 43 , batch: 452 , training loss: 4.367913
[INFO] Epoch: 43 , batch: 453 , training loss: 4.150609
[INFO] Epoch: 43 , batch: 454 , training loss: 4.291614
[INFO] Epoch: 43 , batch: 455 , training loss: 4.340579
[INFO] Epoch: 43 , batch: 456 , training loss: 4.333304
[INFO] Epoch: 43 , batch: 457 , training loss: 4.409609
[INFO] Epoch: 43 , batch: 458 , training loss: 4.166756
[INFO] Epoch: 43 , batch: 459 , training loss: 4.146108
[INFO] Epoch: 43 , batch: 460 , training loss: 4.228296
[INFO] Epoch: 43 , batch: 461 , training loss: 4.207702
[INFO] Epoch: 43 , batch: 462 , training loss: 4.285968
[INFO] Epoch: 43 , batch: 463 , training loss: 4.198993
[INFO] Epoch: 43 , batch: 464 , training loss: 4.378063
[INFO] Epoch: 43 , batch: 465 , training loss: 4.288562
[INFO] Epoch: 43 , batch: 466 , training loss: 4.392193
[INFO] Epoch: 43 , batch: 467 , training loss: 4.375207
[INFO] Epoch: 43 , batch: 468 , training loss: 4.332267
[INFO] Epoch: 43 , batch: 469 , training loss: 4.361499
[INFO] Epoch: 43 , batch: 470 , training loss: 4.174594
[INFO] Epoch: 43 , batch: 471 , training loss: 4.271502
[INFO] Epoch: 43 , batch: 472 , training loss: 4.305324
[INFO] Epoch: 43 , batch: 473 , training loss: 4.251593
[INFO] Epoch: 43 , batch: 474 , training loss: 4.042732
[INFO] Epoch: 43 , batch: 475 , training loss: 3.933335
[INFO] Epoch: 43 , batch: 476 , training loss: 4.280130
[INFO] Epoch: 43 , batch: 477 , training loss: 4.429045
[INFO] Epoch: 43 , batch: 478 , training loss: 4.445203
[INFO] Epoch: 43 , batch: 479 , training loss: 4.421882
[INFO] Epoch: 43 , batch: 480 , training loss: 4.528563
[INFO] Epoch: 43 , batch: 481 , training loss: 4.403126
[INFO] Epoch: 43 , batch: 482 , training loss: 4.502156
[INFO] Epoch: 43 , batch: 483 , training loss: 4.386209
[INFO] Epoch: 43 , batch: 484 , training loss: 4.186528
[INFO] Epoch: 43 , batch: 485 , training loss: 4.263665
[INFO] Epoch: 43 , batch: 486 , training loss: 4.141988
[INFO] Epoch: 43 , batch: 487 , training loss: 4.162196
[INFO] Epoch: 43 , batch: 488 , training loss: 4.349328
[INFO] Epoch: 43 , batch: 489 , training loss: 4.209135
[INFO] Epoch: 43 , batch: 490 , training loss: 4.249752
[INFO] Epoch: 43 , batch: 491 , training loss: 4.204540
[INFO] Epoch: 43 , batch: 492 , training loss: 4.169385
[INFO] Epoch: 43 , batch: 493 , training loss: 4.367644
[INFO] Epoch: 43 , batch: 494 , training loss: 4.267805
[INFO] Epoch: 43 , batch: 495 , training loss: 4.389595
[INFO] Epoch: 43 , batch: 496 , training loss: 4.295576
[INFO] Epoch: 43 , batch: 497 , training loss: 4.332295
[INFO] Epoch: 43 , batch: 498 , training loss: 4.309510
[INFO] Epoch: 43 , batch: 499 , training loss: 4.374133
[INFO] Epoch: 43 , batch: 500 , training loss: 4.518979
[INFO] Epoch: 43 , batch: 501 , training loss: 4.794809
[INFO] Epoch: 43 , batch: 502 , training loss: 4.849431
[INFO] Epoch: 43 , batch: 503 , training loss: 4.559294
[INFO] Epoch: 43 , batch: 504 , training loss: 4.665074
[INFO] Epoch: 43 , batch: 505 , training loss: 4.651251
[INFO] Epoch: 43 , batch: 506 , training loss: 4.606517
[INFO] Epoch: 43 , batch: 507 , training loss: 4.684528
[INFO] Epoch: 43 , batch: 508 , training loss: 4.630561
[INFO] Epoch: 43 , batch: 509 , training loss: 4.430433
[INFO] Epoch: 43 , batch: 510 , training loss: 4.492063
[INFO] Epoch: 43 , batch: 511 , training loss: 4.388738
[INFO] Epoch: 43 , batch: 512 , training loss: 4.515114
[INFO] Epoch: 43 , batch: 513 , training loss: 4.746768
[INFO] Epoch: 43 , batch: 514 , training loss: 4.406423
[INFO] Epoch: 43 , batch: 515 , training loss: 4.632094
[INFO] Epoch: 43 , batch: 516 , training loss: 4.442910
[INFO] Epoch: 43 , batch: 517 , training loss: 4.413363
[INFO] Epoch: 43 , batch: 518 , training loss: 4.374931
[INFO] Epoch: 43 , batch: 519 , training loss: 4.224144
[INFO] Epoch: 43 , batch: 520 , training loss: 4.455028
[INFO] Epoch: 43 , batch: 521 , training loss: 4.444693
[INFO] Epoch: 43 , batch: 522 , training loss: 4.489122
[INFO] Epoch: 43 , batch: 523 , training loss: 4.389113
[INFO] Epoch: 43 , batch: 524 , training loss: 4.700709
[INFO] Epoch: 43 , batch: 525 , training loss: 4.640794
[INFO] Epoch: 43 , batch: 526 , training loss: 4.401726
[INFO] Epoch: 43 , batch: 527 , training loss: 4.435347
[INFO] Epoch: 43 , batch: 528 , training loss: 4.443547
[INFO] Epoch: 43 , batch: 529 , training loss: 4.430503
[INFO] Epoch: 43 , batch: 530 , training loss: 4.271967
[INFO] Epoch: 43 , batch: 531 , training loss: 4.402992
[INFO] Epoch: 43 , batch: 532 , training loss: 4.319597
[INFO] Epoch: 43 , batch: 533 , training loss: 4.473128
[INFO] Epoch: 43 , batch: 534 , training loss: 4.430906
[INFO] Epoch: 43 , batch: 535 , training loss: 4.450669
[INFO] Epoch: 43 , batch: 536 , training loss: 4.277228
[INFO] Epoch: 43 , batch: 537 , training loss: 4.290238
[INFO] Epoch: 43 , batch: 538 , training loss: 4.354552
[INFO] Epoch: 43 , batch: 539 , training loss: 4.466333
[INFO] Epoch: 43 , batch: 540 , training loss: 4.949715
[INFO] Epoch: 43 , batch: 541 , training loss: 4.811635
[INFO] Epoch: 43 , batch: 542 , training loss: 4.736845
[INFO] Epoch: 44 , batch: 0 , training loss: 3.321510
[INFO] Epoch: 44 , batch: 1 , training loss: 3.405345
[INFO] Epoch: 44 , batch: 2 , training loss: 3.604642
[INFO] Epoch: 44 , batch: 3 , training loss: 3.389067
[INFO] Epoch: 44 , batch: 4 , training loss: 3.786196
[INFO] Epoch: 44 , batch: 5 , training loss: 3.434325
[INFO] Epoch: 44 , batch: 6 , training loss: 3.789652
[INFO] Epoch: 44 , batch: 7 , training loss: 3.751472
[INFO] Epoch: 44 , batch: 8 , training loss: 3.428397
[INFO] Epoch: 44 , batch: 9 , training loss: 3.719481
[INFO] Epoch: 44 , batch: 10 , training loss: 3.695927
[INFO] Epoch: 44 , batch: 11 , training loss: 3.658506
[INFO] Epoch: 44 , batch: 12 , training loss: 3.526085
[INFO] Epoch: 44 , batch: 13 , training loss: 3.609696
[INFO] Epoch: 44 , batch: 14 , training loss: 3.466582
[INFO] Epoch: 44 , batch: 15 , training loss: 3.656935
[INFO] Epoch: 44 , batch: 16 , training loss: 3.510708
[INFO] Epoch: 44 , batch: 17 , training loss: 3.665592
[INFO] Epoch: 44 , batch: 18 , training loss: 3.619059
[INFO] Epoch: 44 , batch: 19 , training loss: 3.380002
[INFO] Epoch: 44 , batch: 20 , training loss: 3.345695
[INFO] Epoch: 44 , batch: 21 , training loss: 3.476246
[INFO] Epoch: 44 , batch: 22 , training loss: 3.344099
[INFO] Epoch: 44 , batch: 23 , training loss: 3.578310
[INFO] Epoch: 44 , batch: 24 , training loss: 3.383084
[INFO] Epoch: 44 , batch: 25 , training loss: 3.553273
[INFO] Epoch: 44 , batch: 26 , training loss: 3.446784
[INFO] Epoch: 44 , batch: 27 , training loss: 3.404756
[INFO] Epoch: 44 , batch: 28 , training loss: 3.540862
[INFO] Epoch: 44 , batch: 29 , training loss: 3.390321
[INFO] Epoch: 44 , batch: 30 , training loss: 3.443228
[INFO] Epoch: 44 , batch: 31 , training loss: 3.479928
[INFO] Epoch: 44 , batch: 32 , training loss: 3.492573
[INFO] Epoch: 44 , batch: 33 , training loss: 3.516598
[INFO] Epoch: 44 , batch: 34 , training loss: 3.459066
[INFO] Epoch: 44 , batch: 35 , training loss: 3.497188
[INFO] Epoch: 44 , batch: 36 , training loss: 3.507213
[INFO] Epoch: 44 , batch: 37 , training loss: 3.450253
[INFO] Epoch: 44 , batch: 38 , training loss: 3.423059
[INFO] Epoch: 44 , batch: 39 , training loss: 3.322191
[INFO] Epoch: 44 , batch: 40 , training loss: 3.547480
[INFO] Epoch: 44 , batch: 41 , training loss: 3.443846
[INFO] Epoch: 44 , batch: 42 , training loss: 3.877973
[INFO] Epoch: 44 , batch: 43 , training loss: 3.606235
[INFO] Epoch: 44 , batch: 44 , training loss: 3.952770
[INFO] Epoch: 44 , batch: 45 , training loss: 3.864022
[INFO] Epoch: 44 , batch: 46 , training loss: 3.753801
[INFO] Epoch: 44 , batch: 47 , training loss: 3.560588
[INFO] Epoch: 44 , batch: 48 , training loss: 3.567955
[INFO] Epoch: 44 , batch: 49 , training loss: 3.716910
[INFO] Epoch: 44 , batch: 50 , training loss: 3.506215
[INFO] Epoch: 44 , batch: 51 , training loss: 3.745774
[INFO] Epoch: 44 , batch: 52 , training loss: 3.629240
[INFO] Epoch: 44 , batch: 53 , training loss: 3.704731
[INFO] Epoch: 44 , batch: 54 , training loss: 3.743601
[INFO] Epoch: 44 , batch: 55 , training loss: 3.807651
[INFO] Epoch: 44 , batch: 56 , training loss: 3.717025
[INFO] Epoch: 44 , batch: 57 , training loss: 3.568031
[INFO] Epoch: 44 , batch: 58 , training loss: 3.622168
[INFO] Epoch: 44 , batch: 59 , training loss: 3.720516
[INFO] Epoch: 44 , batch: 60 , training loss: 3.675708
[INFO] Epoch: 44 , batch: 61 , training loss: 3.754439
[INFO] Epoch: 44 , batch: 62 , training loss: 3.622528
[INFO] Epoch: 44 , batch: 63 , training loss: 3.865986
[INFO] Epoch: 44 , batch: 64 , training loss: 4.019740
[INFO] Epoch: 44 , batch: 65 , training loss: 3.700646
[INFO] Epoch: 44 , batch: 66 , training loss: 3.559154
[INFO] Epoch: 44 , batch: 67 , training loss: 3.644797
[INFO] Epoch: 44 , batch: 68 , training loss: 3.790727
[INFO] Epoch: 44 , batch: 69 , training loss: 3.665092
[INFO] Epoch: 44 , batch: 70 , training loss: 3.868669
[INFO] Epoch: 44 , batch: 71 , training loss: 3.787225
[INFO] Epoch: 44 , batch: 72 , training loss: 3.841884
[INFO] Epoch: 44 , batch: 73 , training loss: 3.770526
[INFO] Epoch: 44 , batch: 74 , training loss: 3.892885
[INFO] Epoch: 44 , batch: 75 , training loss: 3.784489
[INFO] Epoch: 44 , batch: 76 , training loss: 3.822520
[INFO] Epoch: 44 , batch: 77 , training loss: 3.814746
[INFO] Epoch: 44 , batch: 78 , training loss: 3.901130
[INFO] Epoch: 44 , batch: 79 , training loss: 3.743650
[INFO] Epoch: 44 , batch: 80 , training loss: 3.920726
[INFO] Epoch: 44 , batch: 81 , training loss: 3.884971
[INFO] Epoch: 44 , batch: 82 , training loss: 3.803632
[INFO] Epoch: 44 , batch: 83 , training loss: 3.966244
[INFO] Epoch: 44 , batch: 84 , training loss: 3.853218
[INFO] Epoch: 44 , batch: 85 , training loss: 3.987288
[INFO] Epoch: 44 , batch: 86 , training loss: 3.910235
[INFO] Epoch: 44 , batch: 87 , training loss: 3.897941
[INFO] Epoch: 44 , batch: 88 , training loss: 4.004419
[INFO] Epoch: 44 , batch: 89 , training loss: 3.822853
[INFO] Epoch: 44 , batch: 90 , training loss: 3.880018
[INFO] Epoch: 44 , batch: 91 , training loss: 3.843088
[INFO] Epoch: 44 , batch: 92 , training loss: 3.822498
[INFO] Epoch: 44 , batch: 93 , training loss: 3.916129
[INFO] Epoch: 44 , batch: 94 , training loss: 4.045418
[INFO] Epoch: 44 , batch: 95 , training loss: 3.823423
[INFO] Epoch: 44 , batch: 96 , training loss: 3.860729
[INFO] Epoch: 44 , batch: 97 , training loss: 3.747071
[INFO] Epoch: 44 , batch: 98 , training loss: 3.726046
[INFO] Epoch: 44 , batch: 99 , training loss: 3.815823
[INFO] Epoch: 44 , batch: 100 , training loss: 3.774011
[INFO] Epoch: 44 , batch: 101 , training loss: 3.781525
[INFO] Epoch: 44 , batch: 102 , training loss: 3.897299
[INFO] Epoch: 44 , batch: 103 , training loss: 3.719408
[INFO] Epoch: 44 , batch: 104 , training loss: 3.679663
[INFO] Epoch: 44 , batch: 105 , training loss: 3.929837
[INFO] Epoch: 44 , batch: 106 , training loss: 3.906332
[INFO] Epoch: 44 , batch: 107 , training loss: 3.752109
[INFO] Epoch: 44 , batch: 108 , training loss: 3.690979
[INFO] Epoch: 44 , batch: 109 , training loss: 3.636431
[INFO] Epoch: 44 , batch: 110 , training loss: 3.816699
[INFO] Epoch: 44 , batch: 111 , training loss: 3.902914
[INFO] Epoch: 44 , batch: 112 , training loss: 3.818134
[INFO] Epoch: 44 , batch: 113 , training loss: 3.820197
[INFO] Epoch: 44 , batch: 114 , training loss: 3.788096
[INFO] Epoch: 44 , batch: 115 , training loss: 3.784751
[INFO] Epoch: 44 , batch: 116 , training loss: 3.723632
[INFO] Epoch: 44 , batch: 117 , training loss: 3.955164
[INFO] Epoch: 44 , batch: 118 , training loss: 3.860227
[INFO] Epoch: 44 , batch: 119 , training loss: 4.028748
[INFO] Epoch: 44 , batch: 120 , training loss: 4.047388
[INFO] Epoch: 44 , batch: 121 , training loss: 3.869981
[INFO] Epoch: 44 , batch: 122 , training loss: 3.806463
[INFO] Epoch: 44 , batch: 123 , training loss: 3.826942
[INFO] Epoch: 44 , batch: 124 , training loss: 3.931127
[INFO] Epoch: 44 , batch: 125 , training loss: 3.749522
[INFO] Epoch: 44 , batch: 126 , training loss: 3.729523
[INFO] Epoch: 44 , batch: 127 , training loss: 3.731723
[INFO] Epoch: 44 , batch: 128 , training loss: 3.865825
[INFO] Epoch: 44 , batch: 129 , training loss: 3.817788
[INFO] Epoch: 44 , batch: 130 , training loss: 3.817381
[INFO] Epoch: 44 , batch: 131 , training loss: 3.847368
[INFO] Epoch: 44 , batch: 132 , training loss: 3.797540
[INFO] Epoch: 44 , batch: 133 , training loss: 3.802857
[INFO] Epoch: 44 , batch: 134 , training loss: 3.627871
[INFO] Epoch: 44 , batch: 135 , training loss: 3.643840
[INFO] Epoch: 44 , batch: 136 , training loss: 3.942172
[INFO] Epoch: 44 , batch: 137 , training loss: 3.862189
[INFO] Epoch: 44 , batch: 138 , training loss: 3.894911
[INFO] Epoch: 44 , batch: 139 , training loss: 4.451947
[INFO] Epoch: 44 , batch: 140 , training loss: 4.203556
[INFO] Epoch: 44 , batch: 141 , training loss: 3.989206
[INFO] Epoch: 44 , batch: 142 , training loss: 3.791386
[INFO] Epoch: 44 , batch: 143 , training loss: 3.911643
[INFO] Epoch: 44 , batch: 144 , training loss: 3.703515
[INFO] Epoch: 44 , batch: 145 , training loss: 3.759183
[INFO] Epoch: 44 , batch: 146 , training loss: 3.937159
[INFO] Epoch: 44 , batch: 147 , training loss: 3.649651
[INFO] Epoch: 44 , batch: 148 , training loss: 3.666589
[INFO] Epoch: 44 , batch: 149 , training loss: 3.742491
[INFO] Epoch: 44 , batch: 150 , training loss: 3.913296
[INFO] Epoch: 44 , batch: 151 , training loss: 3.838212
[INFO] Epoch: 44 , batch: 152 , training loss: 3.889617
[INFO] Epoch: 44 , batch: 153 , training loss: 3.852727
[INFO] Epoch: 44 , batch: 154 , training loss: 3.956026
[INFO] Epoch: 44 , batch: 155 , training loss: 4.167853
[INFO] Epoch: 44 , batch: 156 , training loss: 3.872464
[INFO] Epoch: 44 , batch: 157 , training loss: 3.897642
[INFO] Epoch: 44 , batch: 158 , training loss: 3.988475
[INFO] Epoch: 44 , batch: 159 , training loss: 3.853761
[INFO] Epoch: 44 , batch: 160 , training loss: 4.160777
[INFO] Epoch: 44 , batch: 161 , training loss: 4.261951
[INFO] Epoch: 44 , batch: 162 , training loss: 4.227832
[INFO] Epoch: 44 , batch: 163 , training loss: 4.382878
[INFO] Epoch: 44 , batch: 164 , training loss: 4.391314
[INFO] Epoch: 44 , batch: 165 , training loss: 4.288981
[INFO] Epoch: 44 , batch: 166 , training loss: 4.105875
[INFO] Epoch: 44 , batch: 167 , training loss: 4.075655
[INFO] Epoch: 44 , batch: 168 , training loss: 3.803724
[INFO] Epoch: 44 , batch: 169 , training loss: 3.755253
[INFO] Epoch: 44 , batch: 170 , training loss: 4.017414
[INFO] Epoch: 44 , batch: 171 , training loss: 3.419979
[INFO] Epoch: 44 , batch: 172 , training loss: 3.637031
[INFO] Epoch: 44 , batch: 173 , training loss: 4.040533
[INFO] Epoch: 44 , batch: 174 , training loss: 4.440588
[INFO] Epoch: 44 , batch: 175 , training loss: 4.781627
[INFO] Epoch: 44 , batch: 176 , training loss: 4.380445
[INFO] Epoch: 44 , batch: 177 , training loss: 4.054163
[INFO] Epoch: 44 , batch: 178 , training loss: 4.049769
[INFO] Epoch: 44 , batch: 179 , training loss: 4.100592
[INFO] Epoch: 44 , batch: 180 , training loss: 4.069828
[INFO] Epoch: 44 , batch: 181 , training loss: 4.343895
[INFO] Epoch: 44 , batch: 182 , training loss: 4.300188
[INFO] Epoch: 44 , batch: 183 , training loss: 4.266575
[INFO] Epoch: 44 , batch: 184 , training loss: 4.159414
[INFO] Epoch: 44 , batch: 185 , training loss: 4.106348
[INFO] Epoch: 44 , batch: 186 , training loss: 4.289331
[INFO] Epoch: 44 , batch: 187 , training loss: 4.383570
[INFO] Epoch: 44 , batch: 188 , training loss: 4.372891
[INFO] Epoch: 44 , batch: 189 , training loss: 4.254972
[INFO] Epoch: 44 , batch: 190 , training loss: 4.298725
[INFO] Epoch: 44 , batch: 191 , training loss: 4.401454
[INFO] Epoch: 44 , batch: 192 , training loss: 4.224771
[INFO] Epoch: 44 , batch: 193 , training loss: 4.335500
[INFO] Epoch: 44 , batch: 194 , training loss: 4.257471
[INFO] Epoch: 44 , batch: 195 , training loss: 4.202239
[INFO] Epoch: 44 , batch: 196 , training loss: 4.061265
[INFO] Epoch: 44 , batch: 197 , training loss: 4.151469
[INFO] Epoch: 44 , batch: 198 , training loss: 4.058588
[INFO] Epoch: 44 , batch: 199 , training loss: 4.217089
[INFO] Epoch: 44 , batch: 200 , training loss: 4.103365
[INFO] Epoch: 44 , batch: 201 , training loss: 4.040604
[INFO] Epoch: 44 , batch: 202 , training loss: 4.027735
[INFO] Epoch: 44 , batch: 203 , training loss: 4.138488
[INFO] Epoch: 44 , batch: 204 , training loss: 4.279973
[INFO] Epoch: 44 , batch: 205 , training loss: 3.837703
[INFO] Epoch: 44 , batch: 206 , training loss: 3.775523
[INFO] Epoch: 44 , batch: 207 , training loss: 3.793146
[INFO] Epoch: 44 , batch: 208 , training loss: 4.096542
[INFO] Epoch: 44 , batch: 209 , training loss: 4.041922
[INFO] Epoch: 44 , batch: 210 , training loss: 4.079995
[INFO] Epoch: 44 , batch: 211 , training loss: 4.060112
[INFO] Epoch: 44 , batch: 212 , training loss: 4.168892
[INFO] Epoch: 44 , batch: 213 , training loss: 4.105540
[INFO] Epoch: 44 , batch: 214 , training loss: 4.184662
[INFO] Epoch: 44 , batch: 215 , training loss: 4.428766
[INFO] Epoch: 44 , batch: 216 , training loss: 4.099764
[INFO] Epoch: 44 , batch: 217 , training loss: 4.072227
[INFO] Epoch: 44 , batch: 218 , training loss: 4.057037
[INFO] Epoch: 44 , batch: 219 , training loss: 4.145019
[INFO] Epoch: 44 , batch: 220 , training loss: 3.948126
[INFO] Epoch: 44 , batch: 221 , training loss: 3.974954
[INFO] Epoch: 44 , batch: 222 , training loss: 4.130494
[INFO] Epoch: 44 , batch: 223 , training loss: 4.218355
[INFO] Epoch: 44 , batch: 224 , training loss: 4.265276
[INFO] Epoch: 44 , batch: 225 , training loss: 4.173934
[INFO] Epoch: 44 , batch: 226 , training loss: 4.265791
[INFO] Epoch: 44 , batch: 227 , training loss: 4.256404
[INFO] Epoch: 44 , batch: 228 , training loss: 4.284882
[INFO] Epoch: 44 , batch: 229 , training loss: 4.152564
[INFO] Epoch: 44 , batch: 230 , training loss: 4.006665
[INFO] Epoch: 44 , batch: 231 , training loss: 3.879964
[INFO] Epoch: 44 , batch: 232 , training loss: 4.007666
[INFO] Epoch: 44 , batch: 233 , training loss: 4.033814
[INFO] Epoch: 44 , batch: 234 , training loss: 3.729216
[INFO] Epoch: 44 , batch: 235 , training loss: 3.818351
[INFO] Epoch: 44 , batch: 236 , training loss: 3.918756
[INFO] Epoch: 44 , batch: 237 , training loss: 4.131431
[INFO] Epoch: 44 , batch: 238 , training loss: 3.939799
[INFO] Epoch: 44 , batch: 239 , training loss: 3.964171
[INFO] Epoch: 44 , batch: 240 , training loss: 4.027134
[INFO] Epoch: 44 , batch: 241 , training loss: 3.852350
[INFO] Epoch: 44 , batch: 242 , training loss: 3.849267
[INFO] Epoch: 44 , batch: 243 , training loss: 4.144245
[INFO] Epoch: 44 , batch: 244 , training loss: 4.081776
[INFO] Epoch: 44 , batch: 245 , training loss: 4.034995
[INFO] Epoch: 44 , batch: 246 , training loss: 3.768557
[INFO] Epoch: 44 , batch: 247 , training loss: 3.927148
[INFO] Epoch: 44 , batch: 248 , training loss: 3.984231
[INFO] Epoch: 44 , batch: 249 , training loss: 4.032990
[INFO] Epoch: 44 , batch: 250 , training loss: 3.783523
[INFO] Epoch: 44 , batch: 251 , training loss: 4.197492
[INFO] Epoch: 44 , batch: 252 , training loss: 3.924414
[INFO] Epoch: 44 , batch: 253 , training loss: 3.852246
[INFO] Epoch: 44 , batch: 254 , training loss: 4.109098
[INFO] Epoch: 44 , batch: 255 , training loss: 4.082399
[INFO] Epoch: 44 , batch: 256 , training loss: 4.117915
[INFO] Epoch: 44 , batch: 257 , training loss: 4.235303
[INFO] Epoch: 44 , batch: 258 , training loss: 4.277091
[INFO] Epoch: 44 , batch: 259 , training loss: 4.328631
[INFO] Epoch: 44 , batch: 260 , training loss: 4.069936
[INFO] Epoch: 44 , batch: 261 , training loss: 4.199326
[INFO] Epoch: 44 , batch: 262 , training loss: 4.378881
[INFO] Epoch: 44 , batch: 263 , training loss: 4.555883
[INFO] Epoch: 44 , batch: 264 , training loss: 3.891462
[INFO] Epoch: 44 , batch: 265 , training loss: 4.022492
[INFO] Epoch: 44 , batch: 266 , training loss: 4.417908
[INFO] Epoch: 44 , batch: 267 , training loss: 4.214103
[INFO] Epoch: 44 , batch: 268 , training loss: 4.109709
[INFO] Epoch: 44 , batch: 269 , training loss: 4.107417
[INFO] Epoch: 44 , batch: 270 , training loss: 4.119932
[INFO] Epoch: 44 , batch: 271 , training loss: 4.144472
[INFO] Epoch: 44 , batch: 272 , training loss: 4.121541
[INFO] Epoch: 44 , batch: 273 , training loss: 4.166546
[INFO] Epoch: 44 , batch: 274 , training loss: 4.218470
[INFO] Epoch: 44 , batch: 275 , training loss: 4.085416
[INFO] Epoch: 44 , batch: 276 , training loss: 4.135189
[INFO] Epoch: 44 , batch: 277 , training loss: 4.290339
[INFO] Epoch: 44 , batch: 278 , training loss: 4.005429
[INFO] Epoch: 44 , batch: 279 , training loss: 3.987250
[INFO] Epoch: 44 , batch: 280 , training loss: 3.943170
[INFO] Epoch: 44 , batch: 281 , training loss: 4.097156
[INFO] Epoch: 44 , batch: 282 , training loss: 4.006117
[INFO] Epoch: 44 , batch: 283 , training loss: 4.027556
[INFO] Epoch: 44 , batch: 284 , training loss: 4.028540
[INFO] Epoch: 44 , batch: 285 , training loss: 3.968897
[INFO] Epoch: 44 , batch: 286 , training loss: 3.956981
[INFO] Epoch: 44 , batch: 287 , training loss: 3.923737
[INFO] Epoch: 44 , batch: 288 , training loss: 3.942296
[INFO] Epoch: 44 , batch: 289 , training loss: 3.964159
[INFO] Epoch: 44 , batch: 290 , training loss: 3.786025
[INFO] Epoch: 44 , batch: 291 , training loss: 3.735695
[INFO] Epoch: 44 , batch: 292 , training loss: 3.868768
[INFO] Epoch: 44 , batch: 293 , training loss: 3.780288
[INFO] Epoch: 44 , batch: 294 , training loss: 4.444619
[INFO] Epoch: 44 , batch: 295 , training loss: 4.215748
[INFO] Epoch: 44 , batch: 296 , training loss: 4.142441
[INFO] Epoch: 44 , batch: 297 , training loss: 4.108777
[INFO] Epoch: 44 , batch: 298 , training loss: 3.931665
[INFO] Epoch: 44 , batch: 299 , training loss: 3.964997
[INFO] Epoch: 44 , batch: 300 , training loss: 3.922675
[INFO] Epoch: 44 , batch: 301 , training loss: 3.877581
[INFO] Epoch: 44 , batch: 302 , training loss: 4.035559
[INFO] Epoch: 44 , batch: 303 , training loss: 4.056512
[INFO] Epoch: 44 , batch: 304 , training loss: 4.199561
[INFO] Epoch: 44 , batch: 305 , training loss: 4.009996
[INFO] Epoch: 44 , batch: 306 , training loss: 4.154756
[INFO] Epoch: 44 , batch: 307 , training loss: 4.113560
[INFO] Epoch: 44 , batch: 308 , training loss: 3.996606
[INFO] Epoch: 44 , batch: 309 , training loss: 3.974550
[INFO] Epoch: 44 , batch: 310 , training loss: 3.901633
[INFO] Epoch: 44 , batch: 311 , training loss: 3.898131
[INFO] Epoch: 44 , batch: 312 , training loss: 3.821366
[INFO] Epoch: 44 , batch: 313 , training loss: 3.923400
[INFO] Epoch: 44 , batch: 314 , training loss: 3.970502
[INFO] Epoch: 44 , batch: 315 , training loss: 4.017831
[INFO] Epoch: 44 , batch: 316 , training loss: 4.258051
[INFO] Epoch: 44 , batch: 317 , training loss: 4.678604
[INFO] Epoch: 44 , batch: 318 , training loss: 4.822761
[INFO] Epoch: 44 , batch: 319 , training loss: 4.457963
[INFO] Epoch: 44 , batch: 320 , training loss: 3.988290
[INFO] Epoch: 44 , batch: 321 , training loss: 3.832783
[INFO] Epoch: 44 , batch: 322 , training loss: 3.932581
[INFO] Epoch: 44 , batch: 323 , training loss: 3.975803
[INFO] Epoch: 44 , batch: 324 , training loss: 3.916115
[INFO] Epoch: 44 , batch: 325 , training loss: 4.075655
[INFO] Epoch: 44 , batch: 326 , training loss: 4.140265
[INFO] Epoch: 44 , batch: 327 , training loss: 4.025779
[INFO] Epoch: 44 , batch: 328 , training loss: 4.046681
[INFO] Epoch: 44 , batch: 329 , training loss: 3.952658
[INFO] Epoch: 44 , batch: 330 , training loss: 3.965493
[INFO] Epoch: 44 , batch: 331 , training loss: 4.112049
[INFO] Epoch: 44 , batch: 332 , training loss: 3.923427
[INFO] Epoch: 44 , batch: 333 , training loss: 3.948673
[INFO] Epoch: 44 , batch: 334 , training loss: 3.935817
[INFO] Epoch: 44 , batch: 335 , training loss: 4.070567
[INFO] Epoch: 44 , batch: 336 , training loss: 4.097818
[INFO] Epoch: 44 , batch: 337 , training loss: 4.103069
[INFO] Epoch: 44 , batch: 338 , training loss: 4.324382
[INFO] Epoch: 44 , batch: 339 , training loss: 4.176963
[INFO] Epoch: 44 , batch: 340 , training loss: 4.312105
[INFO] Epoch: 44 , batch: 341 , training loss: 4.074307
[INFO] Epoch: 44 , batch: 342 , training loss: 3.853293
[INFO] Epoch: 44 , batch: 343 , training loss: 3.937233
[INFO] Epoch: 44 , batch: 344 , training loss: 3.828476
[INFO] Epoch: 44 , batch: 345 , training loss: 3.930776
[INFO] Epoch: 44 , batch: 346 , training loss: 3.973626
[INFO] Epoch: 44 , batch: 347 , training loss: 3.885246
[INFO] Epoch: 44 , batch: 348 , training loss: 3.979769
[INFO] Epoch: 44 , batch: 349 , training loss: 4.146235
[INFO] Epoch: 44 , batch: 350 , training loss: 3.950389
[INFO] Epoch: 44 , batch: 351 , training loss: 4.019961
[INFO] Epoch: 44 , batch: 352 , training loss: 4.039375
[INFO] Epoch: 44 , batch: 353 , training loss: 3.996995
[INFO] Epoch: 44 , batch: 354 , training loss: 4.107142
[INFO] Epoch: 44 , batch: 355 , training loss: 4.152104
[INFO] Epoch: 44 , batch: 356 , training loss: 3.979841
[INFO] Epoch: 44 , batch: 357 , training loss: 4.060419
[INFO] Epoch: 44 , batch: 358 , training loss: 3.951529
[INFO] Epoch: 44 , batch: 359 , training loss: 3.951348
[INFO] Epoch: 44 , batch: 360 , training loss: 4.056350
[INFO] Epoch: 44 , batch: 361 , training loss: 4.045510
[INFO] Epoch: 44 , batch: 362 , training loss: 4.125911
[INFO] Epoch: 44 , batch: 363 , training loss: 4.004781
[INFO] Epoch: 44 , batch: 364 , training loss: 4.043045
[INFO] Epoch: 44 , batch: 365 , training loss: 4.004427
[INFO] Epoch: 44 , batch: 366 , training loss: 4.111852
[INFO] Epoch: 44 , batch: 367 , training loss: 4.166514
[INFO] Epoch: 44 , batch: 368 , training loss: 4.563064
[INFO] Epoch: 44 , batch: 369 , training loss: 4.224545
[INFO] Epoch: 44 , batch: 370 , training loss: 3.985770
[INFO] Epoch: 44 , batch: 371 , training loss: 4.357944
[INFO] Epoch: 44 , batch: 372 , training loss: 4.624997
[INFO] Epoch: 44 , batch: 373 , training loss: 4.741011
[INFO] Epoch: 44 , batch: 374 , training loss: 4.818751
[INFO] Epoch: 44 , batch: 375 , training loss: 4.817949
[INFO] Epoch: 44 , batch: 376 , training loss: 4.708879
[INFO] Epoch: 44 , batch: 377 , training loss: 4.469821
[INFO] Epoch: 44 , batch: 378 , training loss: 4.568204
[INFO] Epoch: 44 , batch: 379 , training loss: 4.539808
[INFO] Epoch: 44 , batch: 380 , training loss: 4.676727
[INFO] Epoch: 44 , batch: 381 , training loss: 4.413831
[INFO] Epoch: 44 , batch: 382 , training loss: 4.631085
[INFO] Epoch: 44 , batch: 383 , training loss: 4.646847
[INFO] Epoch: 44 , batch: 384 , training loss: 4.671804
[INFO] Epoch: 44 , batch: 385 , training loss: 4.375233
[INFO] Epoch: 44 , batch: 386 , training loss: 4.657970
[INFO] Epoch: 44 , batch: 387 , training loss: 4.597984
[INFO] Epoch: 44 , batch: 388 , training loss: 4.393941
[INFO] Epoch: 44 , batch: 389 , training loss: 4.244607
[INFO] Epoch: 44 , batch: 390 , training loss: 4.244883
[INFO] Epoch: 44 , batch: 391 , training loss: 4.290950
[INFO] Epoch: 44 , batch: 392 , training loss: 4.637558
[INFO] Epoch: 44 , batch: 393 , training loss: 4.525358
[INFO] Epoch: 44 , batch: 394 , training loss: 4.580357
[INFO] Epoch: 44 , batch: 395 , training loss: 4.439023
[INFO] Epoch: 44 , batch: 396 , training loss: 4.223346
[INFO] Epoch: 44 , batch: 397 , training loss: 4.376342
[INFO] Epoch: 44 , batch: 398 , training loss: 4.238986
[INFO] Epoch: 44 , batch: 399 , training loss: 4.307654
[INFO] Epoch: 44 , batch: 400 , training loss: 4.281869
[INFO] Epoch: 44 , batch: 401 , training loss: 4.672557
[INFO] Epoch: 44 , batch: 402 , training loss: 4.413864
[INFO] Epoch: 44 , batch: 403 , training loss: 4.221784
[INFO] Epoch: 44 , batch: 404 , training loss: 4.413037
[INFO] Epoch: 44 , batch: 405 , training loss: 4.473160
[INFO] Epoch: 44 , batch: 406 , training loss: 4.366204
[INFO] Epoch: 44 , batch: 407 , training loss: 4.442843
[INFO] Epoch: 44 , batch: 408 , training loss: 4.401977
[INFO] Epoch: 44 , batch: 409 , training loss: 4.374013
[INFO] Epoch: 44 , batch: 410 , training loss: 4.418279
[INFO] Epoch: 44 , batch: 411 , training loss: 4.606755
[INFO] Epoch: 44 , batch: 412 , training loss: 4.446861
[INFO] Epoch: 44 , batch: 413 , training loss: 4.345290
[INFO] Epoch: 44 , batch: 414 , training loss: 4.371295
[INFO] Epoch: 44 , batch: 415 , training loss: 4.404885
[INFO] Epoch: 44 , batch: 416 , training loss: 4.477915
[INFO] Epoch: 44 , batch: 417 , training loss: 4.384338
[INFO] Epoch: 44 , batch: 418 , training loss: 4.401037
[INFO] Epoch: 44 , batch: 419 , training loss: 4.368709
[INFO] Epoch: 44 , batch: 420 , training loss: 4.358539
[INFO] Epoch: 44 , batch: 421 , training loss: 4.349117
[INFO] Epoch: 44 , batch: 422 , training loss: 4.201256
[INFO] Epoch: 44 , batch: 423 , training loss: 4.414236
[INFO] Epoch: 44 , batch: 424 , training loss: 4.582405
[INFO] Epoch: 44 , batch: 425 , training loss: 4.452039
[INFO] Epoch: 44 , batch: 426 , training loss: 4.191834
[INFO] Epoch: 44 , batch: 427 , training loss: 4.422943
[INFO] Epoch: 44 , batch: 428 , training loss: 4.294160
[INFO] Epoch: 44 , batch: 429 , training loss: 4.163671
[INFO] Epoch: 44 , batch: 430 , training loss: 4.429698
[INFO] Epoch: 44 , batch: 431 , training loss: 4.029258
[INFO] Epoch: 44 , batch: 432 , training loss: 4.088285
[INFO] Epoch: 44 , batch: 433 , training loss: 4.131668
[INFO] Epoch: 44 , batch: 434 , training loss: 4.005630
[INFO] Epoch: 44 , batch: 435 , training loss: 4.380885
[INFO] Epoch: 44 , batch: 436 , training loss: 4.430178
[INFO] Epoch: 44 , batch: 437 , training loss: 4.188370
[INFO] Epoch: 44 , batch: 438 , training loss: 4.048081
[INFO] Epoch: 44 , batch: 439 , training loss: 4.269249
[INFO] Epoch: 44 , batch: 440 , training loss: 4.380003
[INFO] Epoch: 44 , batch: 441 , training loss: 4.484864
[INFO] Epoch: 44 , batch: 442 , training loss: 4.241333
[INFO] Epoch: 44 , batch: 443 , training loss: 4.434074
[INFO] Epoch: 44 , batch: 444 , training loss: 4.065852
[INFO] Epoch: 44 , batch: 445 , training loss: 3.966658
[INFO] Epoch: 44 , batch: 446 , training loss: 3.931760
[INFO] Epoch: 44 , batch: 447 , training loss: 4.115214
[INFO] Epoch: 44 , batch: 448 , training loss: 4.192548
[INFO] Epoch: 44 , batch: 449 , training loss: 4.604731
[INFO] Epoch: 44 , batch: 450 , training loss: 4.638553
[INFO] Epoch: 44 , batch: 451 , training loss: 4.559022
[INFO] Epoch: 44 , batch: 452 , training loss: 4.380704
[INFO] Epoch: 44 , batch: 453 , training loss: 4.147760
[INFO] Epoch: 44 , batch: 454 , training loss: 4.285940
[INFO] Epoch: 44 , batch: 455 , training loss: 4.336547
[INFO] Epoch: 44 , batch: 456 , training loss: 4.333029
[INFO] Epoch: 44 , batch: 457 , training loss: 4.419550
[INFO] Epoch: 44 , batch: 458 , training loss: 4.155133
[INFO] Epoch: 44 , batch: 459 , training loss: 4.134679
[INFO] Epoch: 44 , batch: 460 , training loss: 4.235263
[INFO] Epoch: 44 , batch: 461 , training loss: 4.207785
[INFO] Epoch: 44 , batch: 462 , training loss: 4.282655
[INFO] Epoch: 44 , batch: 463 , training loss: 4.200905
[INFO] Epoch: 44 , batch: 464 , training loss: 4.374609
[INFO] Epoch: 44 , batch: 465 , training loss: 4.287049
[INFO] Epoch: 44 , batch: 466 , training loss: 4.380175
[INFO] Epoch: 44 , batch: 467 , training loss: 4.359536
[INFO] Epoch: 44 , batch: 468 , training loss: 4.331458
[INFO] Epoch: 44 , batch: 469 , training loss: 4.367648
[INFO] Epoch: 44 , batch: 470 , training loss: 4.173131
[INFO] Epoch: 44 , batch: 471 , training loss: 4.254107
[INFO] Epoch: 44 , batch: 472 , training loss: 4.292053
[INFO] Epoch: 44 , batch: 473 , training loss: 4.244811
[INFO] Epoch: 44 , batch: 474 , training loss: 4.055593
[INFO] Epoch: 44 , batch: 475 , training loss: 3.929984
[INFO] Epoch: 44 , batch: 476 , training loss: 4.276649
[INFO] Epoch: 44 , batch: 477 , training loss: 4.439811
[INFO] Epoch: 44 , batch: 478 , training loss: 4.452899
[INFO] Epoch: 44 , batch: 479 , training loss: 4.416777
[INFO] Epoch: 44 , batch: 480 , training loss: 4.516656
[INFO] Epoch: 44 , batch: 481 , training loss: 4.404085
[INFO] Epoch: 44 , batch: 482 , training loss: 4.488460
[INFO] Epoch: 44 , batch: 483 , training loss: 4.373024
[INFO] Epoch: 44 , batch: 484 , training loss: 4.165932
[INFO] Epoch: 44 , batch: 485 , training loss: 4.251665
[INFO] Epoch: 44 , batch: 486 , training loss: 4.141615
[INFO] Epoch: 44 , batch: 487 , training loss: 4.150367
[INFO] Epoch: 44 , batch: 488 , training loss: 4.324579
[INFO] Epoch: 44 , batch: 489 , training loss: 4.224336
[INFO] Epoch: 44 , batch: 490 , training loss: 4.245910
[INFO] Epoch: 44 , batch: 491 , training loss: 4.196263
[INFO] Epoch: 44 , batch: 492 , training loss: 4.166916
[INFO] Epoch: 44 , batch: 493 , training loss: 4.361512
[INFO] Epoch: 44 , batch: 494 , training loss: 4.278916
[INFO] Epoch: 44 , batch: 495 , training loss: 4.393486
[INFO] Epoch: 44 , batch: 496 , training loss: 4.285729
[INFO] Epoch: 44 , batch: 497 , training loss: 4.313145
[INFO] Epoch: 44 , batch: 498 , training loss: 4.316795
[INFO] Epoch: 44 , batch: 499 , training loss: 4.373421
[INFO] Epoch: 44 , batch: 500 , training loss: 4.506337
[INFO] Epoch: 44 , batch: 501 , training loss: 4.797593
[INFO] Epoch: 44 , batch: 502 , training loss: 4.866035
[INFO] Epoch: 44 , batch: 503 , training loss: 4.566552
[INFO] Epoch: 44 , batch: 504 , training loss: 4.688802
[INFO] Epoch: 44 , batch: 505 , training loss: 4.657804
[INFO] Epoch: 44 , batch: 506 , training loss: 4.605619
[INFO] Epoch: 44 , batch: 507 , training loss: 4.689139
[INFO] Epoch: 44 , batch: 508 , training loss: 4.642780
[INFO] Epoch: 44 , batch: 509 , training loss: 4.410179
[INFO] Epoch: 44 , batch: 510 , training loss: 4.501024
[INFO] Epoch: 44 , batch: 511 , training loss: 4.394026
[INFO] Epoch: 44 , batch: 512 , training loss: 4.518819
[INFO] Epoch: 44 , batch: 513 , training loss: 4.743104
[INFO] Epoch: 44 , batch: 514 , training loss: 4.413252
[INFO] Epoch: 44 , batch: 515 , training loss: 4.620796
[INFO] Epoch: 44 , batch: 516 , training loss: 4.447615
[INFO] Epoch: 44 , batch: 517 , training loss: 4.406344
[INFO] Epoch: 44 , batch: 518 , training loss: 4.367057
[INFO] Epoch: 44 , batch: 519 , training loss: 4.218822
[INFO] Epoch: 44 , batch: 520 , training loss: 4.453173
[INFO] Epoch: 44 , batch: 521 , training loss: 4.428713
[INFO] Epoch: 44 , batch: 522 , training loss: 4.489227
[INFO] Epoch: 44 , batch: 523 , training loss: 4.395738
[INFO] Epoch: 44 , batch: 524 , training loss: 4.678755
[INFO] Epoch: 44 , batch: 525 , training loss: 4.644921
[INFO] Epoch: 44 , batch: 526 , training loss: 4.394209
[INFO] Epoch: 44 , batch: 527 , training loss: 4.430937
[INFO] Epoch: 44 , batch: 528 , training loss: 4.430550
[INFO] Epoch: 44 , batch: 529 , training loss: 4.413993
[INFO] Epoch: 44 , batch: 530 , training loss: 4.275322
[INFO] Epoch: 44 , batch: 531 , training loss: 4.395963
[INFO] Epoch: 44 , batch: 532 , training loss: 4.312379
[INFO] Epoch: 44 , batch: 533 , training loss: 4.449090
[INFO] Epoch: 44 , batch: 534 , training loss: 4.439672
[INFO] Epoch: 44 , batch: 535 , training loss: 4.445896
[INFO] Epoch: 44 , batch: 536 , training loss: 4.269386
[INFO] Epoch: 44 , batch: 537 , training loss: 4.291248
[INFO] Epoch: 44 , batch: 538 , training loss: 4.349435
[INFO] Epoch: 44 , batch: 539 , training loss: 4.461242
[INFO] Epoch: 44 , batch: 540 , training loss: 4.945312
[INFO] Epoch: 44 , batch: 541 , training loss: 4.792478
[INFO] Epoch: 44 , batch: 542 , training loss: 4.743427
[INFO] Epoch: 45 , batch: 0 , training loss: 3.270181
[INFO] Epoch: 45 , batch: 1 , training loss: 3.337821
[INFO] Epoch: 45 , batch: 2 , training loss: 3.620821
[INFO] Epoch: 45 , batch: 3 , training loss: 3.383060
[INFO] Epoch: 45 , batch: 4 , training loss: 3.766691
[INFO] Epoch: 45 , batch: 5 , training loss: 3.385643
[INFO] Epoch: 45 , batch: 6 , training loss: 3.795214
[INFO] Epoch: 45 , batch: 7 , training loss: 3.747340
[INFO] Epoch: 45 , batch: 8 , training loss: 3.403479
[INFO] Epoch: 45 , batch: 9 , training loss: 3.690357
[INFO] Epoch: 45 , batch: 10 , training loss: 3.684093
[INFO] Epoch: 45 , batch: 11 , training loss: 3.640910
[INFO] Epoch: 45 , batch: 12 , training loss: 3.529997
[INFO] Epoch: 45 , batch: 13 , training loss: 3.590954
[INFO] Epoch: 45 , batch: 14 , training loss: 3.446597
[INFO] Epoch: 45 , batch: 15 , training loss: 3.655622
[INFO] Epoch: 45 , batch: 16 , training loss: 3.518630
[INFO] Epoch: 45 , batch: 17 , training loss: 3.655855
[INFO] Epoch: 45 , batch: 18 , training loss: 3.621069
[INFO] Epoch: 45 , batch: 19 , training loss: 3.369384
[INFO] Epoch: 45 , batch: 20 , training loss: 3.323941
[INFO] Epoch: 45 , batch: 21 , training loss: 3.503262
[INFO] Epoch: 45 , batch: 22 , training loss: 3.330909
[INFO] Epoch: 45 , batch: 23 , training loss: 3.570808
[INFO] Epoch: 45 , batch: 24 , training loss: 3.368319
[INFO] Epoch: 45 , batch: 25 , training loss: 3.586910
[INFO] Epoch: 45 , batch: 26 , training loss: 3.455828
[INFO] Epoch: 45 , batch: 27 , training loss: 3.390124
[INFO] Epoch: 45 , batch: 28 , training loss: 3.520429
[INFO] Epoch: 45 , batch: 29 , training loss: 3.402755
[INFO] Epoch: 45 , batch: 30 , training loss: 3.451828
[INFO] Epoch: 45 , batch: 31 , training loss: 3.474359
[INFO] Epoch: 45 , batch: 32 , training loss: 3.475824
[INFO] Epoch: 45 , batch: 33 , training loss: 3.504915
[INFO] Epoch: 45 , batch: 34 , training loss: 3.466920
[INFO] Epoch: 45 , batch: 35 , training loss: 3.493388
[INFO] Epoch: 45 , batch: 36 , training loss: 3.509088
[INFO] Epoch: 45 , batch: 37 , training loss: 3.440282
[INFO] Epoch: 45 , batch: 38 , training loss: 3.447605
[INFO] Epoch: 45 , batch: 39 , training loss: 3.352188
[INFO] Epoch: 45 , batch: 40 , training loss: 3.549799
[INFO] Epoch: 45 , batch: 41 , training loss: 3.431941
[INFO] Epoch: 45 , batch: 42 , training loss: 3.872947
[INFO] Epoch: 45 , batch: 43 , training loss: 3.594351
[INFO] Epoch: 45 , batch: 44 , training loss: 3.958770
[INFO] Epoch: 45 , batch: 45 , training loss: 3.864535
[INFO] Epoch: 45 , batch: 46 , training loss: 3.828712
[INFO] Epoch: 45 , batch: 47 , training loss: 3.530697
[INFO] Epoch: 45 , batch: 48 , training loss: 3.556565
[INFO] Epoch: 45 , batch: 49 , training loss: 3.763521
[INFO] Epoch: 45 , batch: 50 , training loss: 3.558932
[INFO] Epoch: 45 , batch: 51 , training loss: 3.751629
[INFO] Epoch: 45 , batch: 52 , training loss: 3.641304
[INFO] Epoch: 45 , batch: 53 , training loss: 3.717864
[INFO] Epoch: 45 , batch: 54 , training loss: 3.766436
[INFO] Epoch: 45 , batch: 55 , training loss: 3.812016
[INFO] Epoch: 45 , batch: 56 , training loss: 3.713965
[INFO] Epoch: 45 , batch: 57 , training loss: 3.576966
[INFO] Epoch: 45 , batch: 58 , training loss: 3.641129
[INFO] Epoch: 45 , batch: 59 , training loss: 3.721832
[INFO] Epoch: 45 , batch: 60 , training loss: 3.698775
[INFO] Epoch: 45 , batch: 61 , training loss: 3.750943
[INFO] Epoch: 45 , batch: 62 , training loss: 3.627470
[INFO] Epoch: 45 , batch: 63 , training loss: 3.860013
[INFO] Epoch: 45 , batch: 64 , training loss: 4.008812
[INFO] Epoch: 45 , batch: 65 , training loss: 3.738059
[INFO] Epoch: 45 , batch: 66 , training loss: 3.586397
[INFO] Epoch: 45 , batch: 67 , training loss: 3.640310
[INFO] Epoch: 45 , batch: 68 , training loss: 3.787134
[INFO] Epoch: 45 , batch: 69 , training loss: 3.684124
[INFO] Epoch: 45 , batch: 70 , training loss: 3.890388
[INFO] Epoch: 45 , batch: 71 , training loss: 3.787383
[INFO] Epoch: 45 , batch: 72 , training loss: 3.864120
[INFO] Epoch: 45 , batch: 73 , training loss: 3.782659
[INFO] Epoch: 45 , batch: 74 , training loss: 3.902779
[INFO] Epoch: 45 , batch: 75 , training loss: 3.779054
[INFO] Epoch: 45 , batch: 76 , training loss: 3.815345
[INFO] Epoch: 45 , batch: 77 , training loss: 3.791333
[INFO] Epoch: 45 , batch: 78 , training loss: 3.915302
[INFO] Epoch: 45 , batch: 79 , training loss: 3.762076
[INFO] Epoch: 45 , batch: 80 , training loss: 3.912438
[INFO] Epoch: 45 , batch: 81 , training loss: 3.862329
[INFO] Epoch: 45 , batch: 82 , training loss: 3.830324
[INFO] Epoch: 45 , batch: 83 , training loss: 3.990486
[INFO] Epoch: 45 , batch: 84 , training loss: 3.886928
[INFO] Epoch: 45 , batch: 85 , training loss: 3.982841
[INFO] Epoch: 45 , batch: 86 , training loss: 3.931573
[INFO] Epoch: 45 , batch: 87 , training loss: 3.889886
[INFO] Epoch: 45 , batch: 88 , training loss: 3.967939
[INFO] Epoch: 45 , batch: 89 , training loss: 3.805689
[INFO] Epoch: 45 , batch: 90 , training loss: 3.902879
[INFO] Epoch: 45 , batch: 91 , training loss: 3.850054
[INFO] Epoch: 45 , batch: 92 , training loss: 3.825852
[INFO] Epoch: 45 , batch: 93 , training loss: 3.921759
[INFO] Epoch: 45 , batch: 94 , training loss: 4.075938
[INFO] Epoch: 45 , batch: 95 , training loss: 3.825914
[INFO] Epoch: 45 , batch: 96 , training loss: 3.857167
[INFO] Epoch: 45 , batch: 97 , training loss: 3.755046
[INFO] Epoch: 45 , batch: 98 , training loss: 3.727840
[INFO] Epoch: 45 , batch: 99 , training loss: 3.825126
[INFO] Epoch: 45 , batch: 100 , training loss: 3.771629
[INFO] Epoch: 45 , batch: 101 , training loss: 3.810466
[INFO] Epoch: 45 , batch: 102 , training loss: 3.900472
[INFO] Epoch: 45 , batch: 103 , training loss: 3.730196
[INFO] Epoch: 45 , batch: 104 , training loss: 3.690114
[INFO] Epoch: 45 , batch: 105 , training loss: 3.930780
[INFO] Epoch: 45 , batch: 106 , training loss: 3.915345
[INFO] Epoch: 45 , batch: 107 , training loss: 3.762903
[INFO] Epoch: 45 , batch: 108 , training loss: 3.694552
[INFO] Epoch: 45 , batch: 109 , training loss: 3.652708
[INFO] Epoch: 45 , batch: 110 , training loss: 3.827567
[INFO] Epoch: 45 , batch: 111 , training loss: 3.887750
[INFO] Epoch: 45 , batch: 112 , training loss: 3.822375
[INFO] Epoch: 45 , batch: 113 , training loss: 3.850614
[INFO] Epoch: 45 , batch: 114 , training loss: 3.803805
[INFO] Epoch: 45 , batch: 115 , training loss: 3.823878
[INFO] Epoch: 45 , batch: 116 , training loss: 3.737944
[INFO] Epoch: 45 , batch: 117 , training loss: 3.982825
[INFO] Epoch: 45 , batch: 118 , training loss: 3.892874
[INFO] Epoch: 45 , batch: 119 , training loss: 4.049498
[INFO] Epoch: 45 , batch: 120 , training loss: 4.077442
[INFO] Epoch: 45 , batch: 121 , training loss: 3.876831
[INFO] Epoch: 45 , batch: 122 , training loss: 3.803171
[INFO] Epoch: 45 , batch: 123 , training loss: 3.803041
[INFO] Epoch: 45 , batch: 124 , training loss: 3.930487
[INFO] Epoch: 45 , batch: 125 , training loss: 3.756180
[INFO] Epoch: 45 , batch: 126 , training loss: 3.741461
[INFO] Epoch: 45 , batch: 127 , training loss: 3.728075
[INFO] Epoch: 45 , batch: 128 , training loss: 3.871744
[INFO] Epoch: 45 , batch: 129 , training loss: 3.824010
[INFO] Epoch: 45 , batch: 130 , training loss: 3.841513
[INFO] Epoch: 45 , batch: 131 , training loss: 3.848654
[INFO] Epoch: 45 , batch: 132 , training loss: 3.813196
[INFO] Epoch: 45 , batch: 133 , training loss: 3.818196
[INFO] Epoch: 45 , batch: 134 , training loss: 3.636936
[INFO] Epoch: 45 , batch: 135 , training loss: 3.633694
[INFO] Epoch: 45 , batch: 136 , training loss: 3.952637
[INFO] Epoch: 45 , batch: 137 , training loss: 3.865380
[INFO] Epoch: 45 , batch: 138 , training loss: 3.902576
[INFO] Epoch: 45 , batch: 139 , training loss: 4.493607
[INFO] Epoch: 45 , batch: 140 , training loss: 4.243950
[INFO] Epoch: 45 , batch: 141 , training loss: 3.997770
[INFO] Epoch: 45 , batch: 142 , training loss: 3.780722
[INFO] Epoch: 45 , batch: 143 , training loss: 3.896137
[INFO] Epoch: 45 , batch: 144 , training loss: 3.712291
[INFO] Epoch: 45 , batch: 145 , training loss: 3.800731
[INFO] Epoch: 45 , batch: 146 , training loss: 3.967489
[INFO] Epoch: 45 , batch: 147 , training loss: 3.665871
[INFO] Epoch: 45 , batch: 148 , training loss: 3.675810
[INFO] Epoch: 45 , batch: 149 , training loss: 3.763770
[INFO] Epoch: 45 , batch: 150 , training loss: 3.937031
[INFO] Epoch: 45 , batch: 151 , training loss: 3.828468
[INFO] Epoch: 45 , batch: 152 , training loss: 3.896783
[INFO] Epoch: 45 , batch: 153 , training loss: 3.851470
[INFO] Epoch: 45 , batch: 154 , training loss: 3.951142
[INFO] Epoch: 45 , batch: 155 , training loss: 4.210443
[INFO] Epoch: 45 , batch: 156 , training loss: 3.884912
[INFO] Epoch: 45 , batch: 157 , training loss: 3.890819
[INFO] Epoch: 45 , batch: 158 , training loss: 3.996407
[INFO] Epoch: 45 , batch: 159 , training loss: 3.848593
[INFO] Epoch: 45 , batch: 160 , training loss: 4.116694
[INFO] Epoch: 45 , batch: 161 , training loss: 4.219151
[INFO] Epoch: 45 , batch: 162 , training loss: 4.213420
[INFO] Epoch: 45 , batch: 163 , training loss: 4.384505
[INFO] Epoch: 45 , batch: 164 , training loss: 4.385838
[INFO] Epoch: 45 , batch: 165 , training loss: 4.267649
[INFO] Epoch: 45 , batch: 166 , training loss: 4.118870
[INFO] Epoch: 45 , batch: 167 , training loss: 4.075025
[INFO] Epoch: 45 , batch: 168 , training loss: 3.782586
[INFO] Epoch: 45 , batch: 169 , training loss: 3.759257
[INFO] Epoch: 45 , batch: 170 , training loss: 3.989480
[INFO] Epoch: 45 , batch: 171 , training loss: 3.396119
[INFO] Epoch: 45 , batch: 172 , training loss: 3.592921
[INFO] Epoch: 45 , batch: 173 , training loss: 4.014393
[INFO] Epoch: 45 , batch: 174 , training loss: 4.464550
[INFO] Epoch: 45 , batch: 175 , training loss: 4.756531
[INFO] Epoch: 45 , batch: 176 , training loss: 4.370979
[INFO] Epoch: 45 , batch: 177 , training loss: 4.061218
[INFO] Epoch: 45 , batch: 178 , training loss: 4.010561
[INFO] Epoch: 45 , batch: 179 , training loss: 4.093836
[INFO] Epoch: 45 , batch: 180 , training loss: 4.055974
[INFO] Epoch: 45 , batch: 181 , training loss: 4.318561
[INFO] Epoch: 45 , batch: 182 , training loss: 4.268106
[INFO] Epoch: 45 , batch: 183 , training loss: 4.274724
[INFO] Epoch: 45 , batch: 184 , training loss: 4.142283
[INFO] Epoch: 45 , batch: 185 , training loss: 4.089375
[INFO] Epoch: 45 , batch: 186 , training loss: 4.276044
[INFO] Epoch: 45 , batch: 187 , training loss: 4.396158
[INFO] Epoch: 45 , batch: 188 , training loss: 4.354880
[INFO] Epoch: 45 , batch: 189 , training loss: 4.252125
[INFO] Epoch: 45 , batch: 190 , training loss: 4.290808
[INFO] Epoch: 45 , batch: 191 , training loss: 4.401436
[INFO] Epoch: 45 , batch: 192 , training loss: 4.213602
[INFO] Epoch: 45 , batch: 193 , training loss: 4.316582
[INFO] Epoch: 45 , batch: 194 , training loss: 4.266025
[INFO] Epoch: 45 , batch: 195 , training loss: 4.161780
[INFO] Epoch: 45 , batch: 196 , training loss: 4.046202
[INFO] Epoch: 45 , batch: 197 , training loss: 4.151313
[INFO] Epoch: 45 , batch: 198 , training loss: 4.065209
[INFO] Epoch: 45 , batch: 199 , training loss: 4.216660
[INFO] Epoch: 45 , batch: 200 , training loss: 4.113607
[INFO] Epoch: 45 , batch: 201 , training loss: 4.029480
[INFO] Epoch: 45 , batch: 202 , training loss: 4.019177
[INFO] Epoch: 45 , batch: 203 , training loss: 4.118640
[INFO] Epoch: 45 , batch: 204 , training loss: 4.277073
[INFO] Epoch: 45 , batch: 205 , training loss: 3.826105
[INFO] Epoch: 45 , batch: 206 , training loss: 3.764458
[INFO] Epoch: 45 , batch: 207 , training loss: 3.772729
[INFO] Epoch: 45 , batch: 208 , training loss: 4.101061
[INFO] Epoch: 45 , batch: 209 , training loss: 4.010492
[INFO] Epoch: 45 , batch: 210 , training loss: 4.068258
[INFO] Epoch: 45 , batch: 211 , training loss: 4.049653
[INFO] Epoch: 45 , batch: 212 , training loss: 4.161836
[INFO] Epoch: 45 , batch: 213 , training loss: 4.109136
[INFO] Epoch: 45 , batch: 214 , training loss: 4.180297
[INFO] Epoch: 45 , batch: 215 , training loss: 4.416461
[INFO] Epoch: 45 , batch: 216 , training loss: 4.098177
[INFO] Epoch: 45 , batch: 217 , training loss: 4.069907
[INFO] Epoch: 45 , batch: 218 , training loss: 4.049541
[INFO] Epoch: 45 , batch: 219 , training loss: 4.147910
[INFO] Epoch: 45 , batch: 220 , training loss: 3.949472
[INFO] Epoch: 45 , batch: 221 , training loss: 3.982339
[INFO] Epoch: 45 , batch: 222 , training loss: 4.123465
[INFO] Epoch: 45 , batch: 223 , training loss: 4.216009
[INFO] Epoch: 45 , batch: 224 , training loss: 4.266142
[INFO] Epoch: 45 , batch: 225 , training loss: 4.170899
[INFO] Epoch: 45 , batch: 226 , training loss: 4.264977
[INFO] Epoch: 45 , batch: 227 , training loss: 4.245865
[INFO] Epoch: 45 , batch: 228 , training loss: 4.268332
[INFO] Epoch: 45 , batch: 229 , training loss: 4.136797
[INFO] Epoch: 45 , batch: 230 , training loss: 4.002156
[INFO] Epoch: 45 , batch: 231 , training loss: 3.866220
[INFO] Epoch: 45 , batch: 232 , training loss: 4.009535
[INFO] Epoch: 45 , batch: 233 , training loss: 4.013325
[INFO] Epoch: 45 , batch: 234 , training loss: 3.744845
[INFO] Epoch: 45 , batch: 235 , training loss: 3.810940
[INFO] Epoch: 45 , batch: 236 , training loss: 3.918290
[INFO] Epoch: 45 , batch: 237 , training loss: 4.147223
[INFO] Epoch: 45 , batch: 238 , training loss: 3.905674
[INFO] Epoch: 45 , batch: 239 , training loss: 3.965611
[INFO] Epoch: 45 , batch: 240 , training loss: 4.035357
[INFO] Epoch: 45 , batch: 241 , training loss: 3.832116
[INFO] Epoch: 45 , batch: 242 , training loss: 3.842156
[INFO] Epoch: 45 , batch: 243 , training loss: 4.138813
[INFO] Epoch: 45 , batch: 244 , training loss: 4.069959
[INFO] Epoch: 45 , batch: 245 , training loss: 4.062160
[INFO] Epoch: 45 , batch: 246 , training loss: 3.764368
[INFO] Epoch: 45 , batch: 247 , training loss: 3.912091
[INFO] Epoch: 45 , batch: 248 , training loss: 3.990024
[INFO] Epoch: 45 , batch: 249 , training loss: 4.028767
[INFO] Epoch: 45 , batch: 250 , training loss: 3.785359
[INFO] Epoch: 45 , batch: 251 , training loss: 4.200927
[INFO] Epoch: 45 , batch: 252 , training loss: 3.905812
[INFO] Epoch: 45 , batch: 253 , training loss: 3.838477
[INFO] Epoch: 45 , batch: 254 , training loss: 4.110446
[INFO] Epoch: 45 , batch: 255 , training loss: 4.093566
[INFO] Epoch: 45 , batch: 256 , training loss: 4.101906
[INFO] Epoch: 45 , batch: 257 , training loss: 4.225067
[INFO] Epoch: 45 , batch: 258 , training loss: 4.268386
[INFO] Epoch: 45 , batch: 259 , training loss: 4.314463
[INFO] Epoch: 45 , batch: 260 , training loss: 4.078053
[INFO] Epoch: 45 , batch: 261 , training loss: 4.187048
[INFO] Epoch: 45 , batch: 262 , training loss: 4.371028
[INFO] Epoch: 45 , batch: 263 , training loss: 4.552076
[INFO] Epoch: 45 , batch: 264 , training loss: 3.876140
[INFO] Epoch: 45 , batch: 265 , training loss: 4.009426
[INFO] Epoch: 45 , batch: 266 , training loss: 4.410822
[INFO] Epoch: 45 , batch: 267 , training loss: 4.195614
[INFO] Epoch: 45 , batch: 268 , training loss: 4.124764
[INFO] Epoch: 45 , batch: 269 , training loss: 4.095753
[INFO] Epoch: 45 , batch: 270 , training loss: 4.122444
[INFO] Epoch: 45 , batch: 271 , training loss: 4.140267
[INFO] Epoch: 45 , batch: 272 , training loss: 4.115315
[INFO] Epoch: 45 , batch: 273 , training loss: 4.182059
[INFO] Epoch: 45 , batch: 274 , training loss: 4.221363
[INFO] Epoch: 45 , batch: 275 , training loss: 4.093461
[INFO] Epoch: 45 , batch: 276 , training loss: 4.134536
[INFO] Epoch: 45 , batch: 277 , training loss: 4.293083
[INFO] Epoch: 45 , batch: 278 , training loss: 3.993564
[INFO] Epoch: 45 , batch: 279 , training loss: 4.003016
[INFO] Epoch: 45 , batch: 280 , training loss: 3.948948
[INFO] Epoch: 45 , batch: 281 , training loss: 4.078982
[INFO] Epoch: 45 , batch: 282 , training loss: 4.025976
[INFO] Epoch: 45 , batch: 283 , training loss: 4.031882
[INFO] Epoch: 45 , batch: 284 , training loss: 4.051490
[INFO] Epoch: 45 , batch: 285 , training loss: 3.968898
[INFO] Epoch: 45 , batch: 286 , training loss: 3.956499
[INFO] Epoch: 45 , batch: 287 , training loss: 3.924115
[INFO] Epoch: 45 , batch: 288 , training loss: 3.935787
[INFO] Epoch: 45 , batch: 289 , training loss: 3.966220
[INFO] Epoch: 45 , batch: 290 , training loss: 3.776846
[INFO] Epoch: 45 , batch: 291 , training loss: 3.733121
[INFO] Epoch: 45 , batch: 292 , training loss: 3.858092
[INFO] Epoch: 45 , batch: 293 , training loss: 3.785126
[INFO] Epoch: 45 , batch: 294 , training loss: 4.441232
[INFO] Epoch: 45 , batch: 295 , training loss: 4.189827
[INFO] Epoch: 45 , batch: 296 , training loss: 4.141809
[INFO] Epoch: 45 , batch: 297 , training loss: 4.109028
[INFO] Epoch: 45 , batch: 298 , training loss: 3.926449
[INFO] Epoch: 45 , batch: 299 , training loss: 3.964982
[INFO] Epoch: 45 , batch: 300 , training loss: 3.918616
[INFO] Epoch: 45 , batch: 301 , training loss: 3.877524
[INFO] Epoch: 45 , batch: 302 , training loss: 4.034161
[INFO] Epoch: 45 , batch: 303 , training loss: 4.059219
[INFO] Epoch: 45 , batch: 304 , training loss: 4.207353
[INFO] Epoch: 45 , batch: 305 , training loss: 4.021232
[INFO] Epoch: 45 , batch: 306 , training loss: 4.158038
[INFO] Epoch: 45 , batch: 307 , training loss: 4.132416
[INFO] Epoch: 45 , batch: 308 , training loss: 3.993082
[INFO] Epoch: 45 , batch: 309 , training loss: 3.968787
[INFO] Epoch: 45 , batch: 310 , training loss: 3.889983
[INFO] Epoch: 45 , batch: 311 , training loss: 3.885582
[INFO] Epoch: 45 , batch: 312 , training loss: 3.813324
[INFO] Epoch: 45 , batch: 313 , training loss: 3.917968
[INFO] Epoch: 45 , batch: 314 , training loss: 3.966761
[INFO] Epoch: 45 , batch: 315 , training loss: 4.022275
[INFO] Epoch: 45 , batch: 316 , training loss: 4.265415
[INFO] Epoch: 45 , batch: 317 , training loss: 4.665812
[INFO] Epoch: 45 , batch: 318 , training loss: 4.830770
[INFO] Epoch: 45 , batch: 319 , training loss: 4.456822
[INFO] Epoch: 45 , batch: 320 , training loss: 3.987962
[INFO] Epoch: 45 , batch: 321 , training loss: 3.833787
[INFO] Epoch: 45 , batch: 322 , training loss: 3.936480
[INFO] Epoch: 45 , batch: 323 , training loss: 3.972125
[INFO] Epoch: 45 , batch: 324 , training loss: 3.909980
[INFO] Epoch: 45 , batch: 325 , training loss: 4.077338
[INFO] Epoch: 45 , batch: 326 , training loss: 4.141875
[INFO] Epoch: 45 , batch: 327 , training loss: 4.026694
[INFO] Epoch: 45 , batch: 328 , training loss: 4.050265
[INFO] Epoch: 45 , batch: 329 , training loss: 3.952067
[INFO] Epoch: 45 , batch: 330 , training loss: 3.963010
[INFO] Epoch: 45 , batch: 331 , training loss: 4.126812
[INFO] Epoch: 45 , batch: 332 , training loss: 3.937257
[INFO] Epoch: 45 , batch: 333 , training loss: 3.928493
[INFO] Epoch: 45 , batch: 334 , training loss: 3.914046
[INFO] Epoch: 45 , batch: 335 , training loss: 4.072359
[INFO] Epoch: 45 , batch: 336 , training loss: 4.083485
[INFO] Epoch: 45 , batch: 337 , training loss: 4.104077
[INFO] Epoch: 45 , batch: 338 , training loss: 4.325696
[INFO] Epoch: 45 , batch: 339 , training loss: 4.168516
[INFO] Epoch: 45 , batch: 340 , training loss: 4.312718
[INFO] Epoch: 45 , batch: 341 , training loss: 4.079251
[INFO] Epoch: 45 , batch: 342 , training loss: 3.839882
[INFO] Epoch: 45 , batch: 343 , training loss: 3.929684
[INFO] Epoch: 45 , batch: 344 , training loss: 3.818142
[INFO] Epoch: 45 , batch: 345 , training loss: 3.945366
[INFO] Epoch: 45 , batch: 346 , training loss: 3.973562
[INFO] Epoch: 45 , batch: 347 , training loss: 3.884612
[INFO] Epoch: 45 , batch: 348 , training loss: 3.976246
[INFO] Epoch: 45 , batch: 349 , training loss: 4.154216
[INFO] Epoch: 45 , batch: 350 , training loss: 3.940308
[INFO] Epoch: 45 , batch: 351 , training loss: 4.011550
[INFO] Epoch: 45 , batch: 352 , training loss: 4.033316
[INFO] Epoch: 45 , batch: 353 , training loss: 3.995625
[INFO] Epoch: 45 , batch: 354 , training loss: 4.114102
[INFO] Epoch: 45 , batch: 355 , training loss: 4.149328
[INFO] Epoch: 45 , batch: 356 , training loss: 3.970920
[INFO] Epoch: 45 , batch: 357 , training loss: 4.060526
[INFO] Epoch: 45 , batch: 358 , training loss: 3.948759
[INFO] Epoch: 45 , batch: 359 , training loss: 3.950265
[INFO] Epoch: 45 , batch: 360 , training loss: 4.063656
[INFO] Epoch: 45 , batch: 361 , training loss: 4.026142
[INFO] Epoch: 45 , batch: 362 , training loss: 4.123636
[INFO] Epoch: 45 , batch: 363 , training loss: 4.011230
[INFO] Epoch: 45 , batch: 364 , training loss: 4.046921
[INFO] Epoch: 45 , batch: 365 , training loss: 3.997573
[INFO] Epoch: 45 , batch: 366 , training loss: 4.094613
[INFO] Epoch: 45 , batch: 367 , training loss: 4.163576
[INFO] Epoch: 45 , batch: 368 , training loss: 4.553105
[INFO] Epoch: 45 , batch: 369 , training loss: 4.210626
[INFO] Epoch: 45 , batch: 370 , training loss: 3.988652
[INFO] Epoch: 45 , batch: 371 , training loss: 4.359233
[INFO] Epoch: 45 , batch: 372 , training loss: 4.629955
[INFO] Epoch: 45 , batch: 373 , training loss: 4.718999
[INFO] Epoch: 45 , batch: 374 , training loss: 4.804399
[INFO] Epoch: 45 , batch: 375 , training loss: 4.787442
[INFO] Epoch: 45 , batch: 376 , training loss: 4.692044
[INFO] Epoch: 45 , batch: 377 , training loss: 4.456049
[INFO] Epoch: 45 , batch: 378 , training loss: 4.570567
[INFO] Epoch: 45 , batch: 379 , training loss: 4.532040
[INFO] Epoch: 45 , batch: 380 , training loss: 4.673242
[INFO] Epoch: 45 , batch: 381 , training loss: 4.407425
[INFO] Epoch: 45 , batch: 382 , training loss: 4.624499
[INFO] Epoch: 45 , batch: 383 , training loss: 4.637757
[INFO] Epoch: 45 , batch: 384 , training loss: 4.675115
[INFO] Epoch: 45 , batch: 385 , training loss: 4.347437
[INFO] Epoch: 45 , batch: 386 , training loss: 4.628817
[INFO] Epoch: 45 , batch: 387 , training loss: 4.580848
[INFO] Epoch: 45 , batch: 388 , training loss: 4.406044
[INFO] Epoch: 45 , batch: 389 , training loss: 4.250180
[INFO] Epoch: 45 , batch: 390 , training loss: 4.233830
[INFO] Epoch: 45 , batch: 391 , training loss: 4.292946
[INFO] Epoch: 45 , batch: 392 , training loss: 4.645363
[INFO] Epoch: 45 , batch: 393 , training loss: 4.523050
[INFO] Epoch: 45 , batch: 394 , training loss: 4.583504
[INFO] Epoch: 45 , batch: 395 , training loss: 4.433936
[INFO] Epoch: 45 , batch: 396 , training loss: 4.239419
[INFO] Epoch: 45 , batch: 397 , training loss: 4.379158
[INFO] Epoch: 45 , batch: 398 , training loss: 4.246225
[INFO] Epoch: 45 , batch: 399 , training loss: 4.304822
[INFO] Epoch: 45 , batch: 400 , training loss: 4.262890
[INFO] Epoch: 45 , batch: 401 , training loss: 4.672880
[INFO] Epoch: 45 , batch: 402 , training loss: 4.405315
[INFO] Epoch: 45 , batch: 403 , training loss: 4.226362
[INFO] Epoch: 45 , batch: 404 , training loss: 4.417081
[INFO] Epoch: 45 , batch: 405 , training loss: 4.471157
[INFO] Epoch: 45 , batch: 406 , training loss: 4.367630
[INFO] Epoch: 45 , batch: 407 , training loss: 4.426929
[INFO] Epoch: 45 , batch: 408 , training loss: 4.402134
[INFO] Epoch: 45 , batch: 409 , training loss: 4.375808
[INFO] Epoch: 45 , batch: 410 , training loss: 4.413566
[INFO] Epoch: 45 , batch: 411 , training loss: 4.601908
[INFO] Epoch: 45 , batch: 412 , training loss: 4.449274
[INFO] Epoch: 45 , batch: 413 , training loss: 4.346049
[INFO] Epoch: 45 , batch: 414 , training loss: 4.375969
[INFO] Epoch: 45 , batch: 415 , training loss: 4.401311
[INFO] Epoch: 45 , batch: 416 , training loss: 4.476546
[INFO] Epoch: 45 , batch: 417 , training loss: 4.381282
[INFO] Epoch: 45 , batch: 418 , training loss: 4.421064
[INFO] Epoch: 45 , batch: 419 , training loss: 4.376182
[INFO] Epoch: 45 , batch: 420 , training loss: 4.353093
[INFO] Epoch: 45 , batch: 421 , training loss: 4.335864
[INFO] Epoch: 45 , batch: 422 , training loss: 4.208513
[INFO] Epoch: 45 , batch: 423 , training loss: 4.396166
[INFO] Epoch: 45 , batch: 424 , training loss: 4.569808
[INFO] Epoch: 45 , batch: 425 , training loss: 4.428383
[INFO] Epoch: 45 , batch: 426 , training loss: 4.181674
[INFO] Epoch: 45 , batch: 427 , training loss: 4.432889
[INFO] Epoch: 45 , batch: 428 , training loss: 4.290501
[INFO] Epoch: 45 , batch: 429 , training loss: 4.160137
[INFO] Epoch: 45 , batch: 430 , training loss: 4.419734
[INFO] Epoch: 45 , batch: 431 , training loss: 4.024100
[INFO] Epoch: 45 , batch: 432 , training loss: 4.084648
[INFO] Epoch: 45 , batch: 433 , training loss: 4.124633
[INFO] Epoch: 45 , batch: 434 , training loss: 3.999552
[INFO] Epoch: 45 , batch: 435 , training loss: 4.374112
[INFO] Epoch: 45 , batch: 436 , training loss: 4.431880
[INFO] Epoch: 45 , batch: 437 , training loss: 4.177019
[INFO] Epoch: 45 , batch: 438 , training loss: 4.047851
[INFO] Epoch: 45 , batch: 439 , training loss: 4.261675
[INFO] Epoch: 45 , batch: 440 , training loss: 4.380081
[INFO] Epoch: 45 , batch: 441 , training loss: 4.492792
[INFO] Epoch: 45 , batch: 442 , training loss: 4.248395
[INFO] Epoch: 45 , batch: 443 , training loss: 4.439597
[INFO] Epoch: 45 , batch: 444 , training loss: 4.069852
[INFO] Epoch: 45 , batch: 445 , training loss: 3.963334
[INFO] Epoch: 45 , batch: 446 , training loss: 3.935894
[INFO] Epoch: 45 , batch: 447 , training loss: 4.118166
[INFO] Epoch: 45 , batch: 448 , training loss: 4.196476
[INFO] Epoch: 45 , batch: 449 , training loss: 4.607255
[INFO] Epoch: 45 , batch: 450 , training loss: 4.636755
[INFO] Epoch: 45 , batch: 451 , training loss: 4.558042
[INFO] Epoch: 45 , batch: 452 , training loss: 4.358972
[INFO] Epoch: 45 , batch: 453 , training loss: 4.143468
[INFO] Epoch: 45 , batch: 454 , training loss: 4.269534
[INFO] Epoch: 45 , batch: 455 , training loss: 4.340452
[INFO] Epoch: 45 , batch: 456 , training loss: 4.324568
[INFO] Epoch: 45 , batch: 457 , training loss: 4.404638
[INFO] Epoch: 45 , batch: 458 , training loss: 4.165313
[INFO] Epoch: 45 , batch: 459 , training loss: 4.131514
[INFO] Epoch: 45 , batch: 460 , training loss: 4.222860
[INFO] Epoch: 45 , batch: 461 , training loss: 4.199843
[INFO] Epoch: 45 , batch: 462 , training loss: 4.277733
[INFO] Epoch: 45 , batch: 463 , training loss: 4.190407
[INFO] Epoch: 45 , batch: 464 , training loss: 4.379977
[INFO] Epoch: 45 , batch: 465 , training loss: 4.297218
[INFO] Epoch: 45 , batch: 466 , training loss: 4.372156
[INFO] Epoch: 45 , batch: 467 , training loss: 4.365060
[INFO] Epoch: 45 , batch: 468 , training loss: 4.331735
[INFO] Epoch: 45 , batch: 469 , training loss: 4.353117
[INFO] Epoch: 45 , batch: 470 , training loss: 4.174035
[INFO] Epoch: 45 , batch: 471 , training loss: 4.257637
[INFO] Epoch: 45 , batch: 472 , training loss: 4.292076
[INFO] Epoch: 45 , batch: 473 , training loss: 4.246408
[INFO] Epoch: 45 , batch: 474 , training loss: 4.048134
[INFO] Epoch: 45 , batch: 475 , training loss: 3.919644
[INFO] Epoch: 45 , batch: 476 , training loss: 4.285718
[INFO] Epoch: 45 , batch: 477 , training loss: 4.441628
[INFO] Epoch: 45 , batch: 478 , training loss: 4.452140
[INFO] Epoch: 45 , batch: 479 , training loss: 4.399275
[INFO] Epoch: 45 , batch: 480 , training loss: 4.508002
[INFO] Epoch: 45 , batch: 481 , training loss: 4.390849
[INFO] Epoch: 45 , batch: 482 , training loss: 4.491653
[INFO] Epoch: 45 , batch: 483 , training loss: 4.367959
[INFO] Epoch: 45 , batch: 484 , training loss: 4.167537
[INFO] Epoch: 45 , batch: 485 , training loss: 4.248146
[INFO] Epoch: 45 , batch: 486 , training loss: 4.141378
[INFO] Epoch: 45 , batch: 487 , training loss: 4.143867
[INFO] Epoch: 45 , batch: 488 , training loss: 4.336015
[INFO] Epoch: 45 , batch: 489 , training loss: 4.205484
[INFO] Epoch: 45 , batch: 490 , training loss: 4.244863
[INFO] Epoch: 45 , batch: 491 , training loss: 4.210344
[INFO] Epoch: 45 , batch: 492 , training loss: 4.168019
[INFO] Epoch: 45 , batch: 493 , training loss: 4.367995
[INFO] Epoch: 45 , batch: 494 , training loss: 4.276027
[INFO] Epoch: 45 , batch: 495 , training loss: 4.394120
[INFO] Epoch: 45 , batch: 496 , training loss: 4.280422
[INFO] Epoch: 45 , batch: 497 , training loss: 4.303919
[INFO] Epoch: 45 , batch: 498 , training loss: 4.315056
[INFO] Epoch: 45 , batch: 499 , training loss: 4.366380
[INFO] Epoch: 45 , batch: 500 , training loss: 4.490978
[INFO] Epoch: 45 , batch: 501 , training loss: 4.794314
[INFO] Epoch: 45 , batch: 502 , training loss: 4.863301
[INFO] Epoch: 45 , batch: 503 , training loss: 4.555645
[INFO] Epoch: 45 , batch: 504 , training loss: 4.686908
[INFO] Epoch: 45 , batch: 505 , training loss: 4.661018
[INFO] Epoch: 45 , batch: 506 , training loss: 4.598463
[INFO] Epoch: 45 , batch: 507 , training loss: 4.686699
[INFO] Epoch: 45 , batch: 508 , training loss: 4.634025
[INFO] Epoch: 45 , batch: 509 , training loss: 4.409041
[INFO] Epoch: 45 , batch: 510 , training loss: 4.497368
[INFO] Epoch: 45 , batch: 511 , training loss: 4.389403
[INFO] Epoch: 45 , batch: 512 , training loss: 4.510305
[INFO] Epoch: 45 , batch: 513 , training loss: 4.755647
[INFO] Epoch: 45 , batch: 514 , training loss: 4.410079
[INFO] Epoch: 45 , batch: 515 , training loss: 4.624163
[INFO] Epoch: 45 , batch: 516 , training loss: 4.447342
[INFO] Epoch: 45 , batch: 517 , training loss: 4.414683
[INFO] Epoch: 45 , batch: 518 , training loss: 4.369847
[INFO] Epoch: 45 , batch: 519 , training loss: 4.220375
[INFO] Epoch: 45 , batch: 520 , training loss: 4.451180
[INFO] Epoch: 45 , batch: 521 , training loss: 4.432689
[INFO] Epoch: 45 , batch: 522 , training loss: 4.484469
[INFO] Epoch: 45 , batch: 523 , training loss: 4.393959
[INFO] Epoch: 45 , batch: 524 , training loss: 4.688044
[INFO] Epoch: 45 , batch: 525 , training loss: 4.625456
[INFO] Epoch: 45 , batch: 526 , training loss: 4.391601
[INFO] Epoch: 45 , batch: 527 , training loss: 4.431952
[INFO] Epoch: 45 , batch: 528 , training loss: 4.430358
[INFO] Epoch: 45 , batch: 529 , training loss: 4.425562
[INFO] Epoch: 45 , batch: 530 , training loss: 4.264151
[INFO] Epoch: 45 , batch: 531 , training loss: 4.400741
[INFO] Epoch: 45 , batch: 532 , training loss: 4.304449
[INFO] Epoch: 45 , batch: 533 , training loss: 4.440084
[INFO] Epoch: 45 , batch: 534 , training loss: 4.421052
[INFO] Epoch: 45 , batch: 535 , training loss: 4.444182
[INFO] Epoch: 45 , batch: 536 , training loss: 4.281584
[INFO] Epoch: 45 , batch: 537 , training loss: 4.287746
[INFO] Epoch: 45 , batch: 538 , training loss: 4.355912
[INFO] Epoch: 45 , batch: 539 , training loss: 4.454421
[INFO] Epoch: 45 , batch: 540 , training loss: 4.936635
[INFO] Epoch: 45 , batch: 541 , training loss: 4.808396
[INFO] Epoch: 45 , batch: 542 , training loss: 4.733255
[INFO] Epoch: 46 , batch: 0 , training loss: 3.244731
[INFO] Epoch: 46 , batch: 1 , training loss: 3.374788
[INFO] Epoch: 46 , batch: 2 , training loss: 3.603836
[INFO] Epoch: 46 , batch: 3 , training loss: 3.379191
[INFO] Epoch: 46 , batch: 4 , training loss: 3.739082
[INFO] Epoch: 46 , batch: 5 , training loss: 3.421486
[INFO] Epoch: 46 , batch: 6 , training loss: 3.798208
[INFO] Epoch: 46 , batch: 7 , training loss: 3.745716
[INFO] Epoch: 46 , batch: 8 , training loss: 3.419909
[INFO] Epoch: 46 , batch: 9 , training loss: 3.689584
[INFO] Epoch: 46 , batch: 10 , training loss: 3.666530
[INFO] Epoch: 46 , batch: 11 , training loss: 3.634165
[INFO] Epoch: 46 , batch: 12 , training loss: 3.538410
[INFO] Epoch: 46 , batch: 13 , training loss: 3.591260
[INFO] Epoch: 46 , batch: 14 , training loss: 3.472553
[INFO] Epoch: 46 , batch: 15 , training loss: 3.661602
[INFO] Epoch: 46 , batch: 16 , training loss: 3.531225
[INFO] Epoch: 46 , batch: 17 , training loss: 3.686812
[INFO] Epoch: 46 , batch: 18 , training loss: 3.611877
[INFO] Epoch: 46 , batch: 19 , training loss: 3.379762
[INFO] Epoch: 46 , batch: 20 , training loss: 3.343157
[INFO] Epoch: 46 , batch: 21 , training loss: 3.486454
[INFO] Epoch: 46 , batch: 22 , training loss: 3.336287
[INFO] Epoch: 46 , batch: 23 , training loss: 3.595046
[INFO] Epoch: 46 , batch: 24 , training loss: 3.380214
[INFO] Epoch: 46 , batch: 25 , training loss: 3.583868
[INFO] Epoch: 46 , batch: 26 , training loss: 3.457768
[INFO] Epoch: 46 , batch: 27 , training loss: 3.388521
[INFO] Epoch: 46 , batch: 28 , training loss: 3.544106
[INFO] Epoch: 46 , batch: 29 , training loss: 3.395815
[INFO] Epoch: 46 , batch: 30 , training loss: 3.473008
[INFO] Epoch: 46 , batch: 31 , training loss: 3.510756
[INFO] Epoch: 46 , batch: 32 , training loss: 3.487562
[INFO] Epoch: 46 , batch: 33 , training loss: 3.477323
[INFO] Epoch: 46 , batch: 34 , training loss: 3.484546
[INFO] Epoch: 46 , batch: 35 , training loss: 3.472391
[INFO] Epoch: 46 , batch: 36 , training loss: 3.513327
[INFO] Epoch: 46 , batch: 37 , training loss: 3.438909
[INFO] Epoch: 46 , batch: 38 , training loss: 3.436125
[INFO] Epoch: 46 , batch: 39 , training loss: 3.296947
[INFO] Epoch: 46 , batch: 40 , training loss: 3.550816
[INFO] Epoch: 46 , batch: 41 , training loss: 3.436277
[INFO] Epoch: 46 , batch: 42 , training loss: 3.878076
[INFO] Epoch: 46 , batch: 43 , training loss: 3.562063
[INFO] Epoch: 46 , batch: 44 , training loss: 3.901915
[INFO] Epoch: 46 , batch: 45 , training loss: 3.813467
[INFO] Epoch: 46 , batch: 46 , training loss: 3.742606
[INFO] Epoch: 46 , batch: 47 , training loss: 3.539985
[INFO] Epoch: 46 , batch: 48 , training loss: 3.539440
[INFO] Epoch: 46 , batch: 49 , training loss: 3.738977
[INFO] Epoch: 46 , batch: 50 , training loss: 3.543204
[INFO] Epoch: 46 , batch: 51 , training loss: 3.756720
[INFO] Epoch: 46 , batch: 52 , training loss: 3.646944
[INFO] Epoch: 46 , batch: 53 , training loss: 3.733492
[INFO] Epoch: 46 , batch: 54 , training loss: 3.744347
[INFO] Epoch: 46 , batch: 55 , training loss: 3.836910
[INFO] Epoch: 46 , batch: 56 , training loss: 3.715227
[INFO] Epoch: 46 , batch: 57 , training loss: 3.597057
[INFO] Epoch: 46 , batch: 58 , training loss: 3.655823
[INFO] Epoch: 46 , batch: 59 , training loss: 3.745483
[INFO] Epoch: 46 , batch: 60 , training loss: 3.688658
[INFO] Epoch: 46 , batch: 61 , training loss: 3.758924
[INFO] Epoch: 46 , batch: 62 , training loss: 3.608148
[INFO] Epoch: 46 , batch: 63 , training loss: 3.883739
[INFO] Epoch: 46 , batch: 64 , training loss: 4.017666
[INFO] Epoch: 46 , batch: 65 , training loss: 3.726546
[INFO] Epoch: 46 , batch: 66 , training loss: 3.566678
[INFO] Epoch: 46 , batch: 67 , training loss: 3.676723
[INFO] Epoch: 46 , batch: 68 , training loss: 3.789160
[INFO] Epoch: 46 , batch: 69 , training loss: 3.688791
[INFO] Epoch: 46 , batch: 70 , training loss: 3.890589
[INFO] Epoch: 46 , batch: 71 , training loss: 3.784642
[INFO] Epoch: 46 , batch: 72 , training loss: 3.841517
[INFO] Epoch: 46 , batch: 73 , training loss: 3.775601
[INFO] Epoch: 46 , batch: 74 , training loss: 3.898045
[INFO] Epoch: 46 , batch: 75 , training loss: 3.790663
[INFO] Epoch: 46 , batch: 76 , training loss: 3.800273
[INFO] Epoch: 46 , batch: 77 , training loss: 3.783151
[INFO] Epoch: 46 , batch: 78 , training loss: 3.915545
[INFO] Epoch: 46 , batch: 79 , training loss: 3.772220
[INFO] Epoch: 46 , batch: 80 , training loss: 3.915957
[INFO] Epoch: 46 , batch: 81 , training loss: 3.870546
[INFO] Epoch: 46 , batch: 82 , training loss: 3.819505
[INFO] Epoch: 46 , batch: 83 , training loss: 3.980284
[INFO] Epoch: 46 , batch: 84 , training loss: 3.884649
[INFO] Epoch: 46 , batch: 85 , training loss: 3.988568
[INFO] Epoch: 46 , batch: 86 , training loss: 3.927996
[INFO] Epoch: 46 , batch: 87 , training loss: 3.876968
[INFO] Epoch: 46 , batch: 88 , training loss: 4.000501
[INFO] Epoch: 46 , batch: 89 , training loss: 3.806573
[INFO] Epoch: 46 , batch: 90 , training loss: 3.906562
[INFO] Epoch: 46 , batch: 91 , training loss: 3.855111
[INFO] Epoch: 46 , batch: 92 , training loss: 3.841175
[INFO] Epoch: 46 , batch: 93 , training loss: 3.946867
[INFO] Epoch: 46 , batch: 94 , training loss: 4.070353
[INFO] Epoch: 46 , batch: 95 , training loss: 3.811913
[INFO] Epoch: 46 , batch: 96 , training loss: 3.866294
[INFO] Epoch: 46 , batch: 97 , training loss: 3.754655
[INFO] Epoch: 46 , batch: 98 , training loss: 3.729460
[INFO] Epoch: 46 , batch: 99 , training loss: 3.828715
[INFO] Epoch: 46 , batch: 100 , training loss: 3.786043
[INFO] Epoch: 46 , batch: 101 , training loss: 3.811000
[INFO] Epoch: 46 , batch: 102 , training loss: 3.920815
[INFO] Epoch: 46 , batch: 103 , training loss: 3.719687
[INFO] Epoch: 46 , batch: 104 , training loss: 3.678556
[INFO] Epoch: 46 , batch: 105 , training loss: 3.942071
[INFO] Epoch: 46 , batch: 106 , training loss: 3.916142
[INFO] Epoch: 46 , batch: 107 , training loss: 3.781694
[INFO] Epoch: 46 , batch: 108 , training loss: 3.705198
[INFO] Epoch: 46 , batch: 109 , training loss: 3.659056
[INFO] Epoch: 46 , batch: 110 , training loss: 3.851545
[INFO] Epoch: 46 , batch: 111 , training loss: 3.910609
[INFO] Epoch: 46 , batch: 112 , training loss: 3.815113
[INFO] Epoch: 46 , batch: 113 , training loss: 3.827831
[INFO] Epoch: 46 , batch: 114 , training loss: 3.785271
[INFO] Epoch: 46 , batch: 115 , training loss: 3.794539
[INFO] Epoch: 46 , batch: 116 , training loss: 3.729526
[INFO] Epoch: 46 , batch: 117 , training loss: 3.982301
[INFO] Epoch: 46 , batch: 118 , training loss: 3.913526
[INFO] Epoch: 46 , batch: 119 , training loss: 4.051211
[INFO] Epoch: 46 , batch: 120 , training loss: 4.048409
[INFO] Epoch: 46 , batch: 121 , training loss: 3.869343
[INFO] Epoch: 46 , batch: 122 , training loss: 3.816851
[INFO] Epoch: 46 , batch: 123 , training loss: 3.814381
[INFO] Epoch: 46 , batch: 124 , training loss: 3.944978
[INFO] Epoch: 46 , batch: 125 , training loss: 3.754527
[INFO] Epoch: 46 , batch: 126 , training loss: 3.731709
[INFO] Epoch: 46 , batch: 127 , training loss: 3.742856
[INFO] Epoch: 46 , batch: 128 , training loss: 3.872633
[INFO] Epoch: 46 , batch: 129 , training loss: 3.843209
[INFO] Epoch: 46 , batch: 130 , training loss: 3.821521
[INFO] Epoch: 46 , batch: 131 , training loss: 3.850339
[INFO] Epoch: 46 , batch: 132 , training loss: 3.841937
[INFO] Epoch: 46 , batch: 133 , training loss: 3.827446
[INFO] Epoch: 46 , batch: 134 , training loss: 3.629807
[INFO] Epoch: 46 , batch: 135 , training loss: 3.643341
[INFO] Epoch: 46 , batch: 136 , training loss: 3.947297
[INFO] Epoch: 46 , batch: 137 , training loss: 3.883616
[INFO] Epoch: 46 , batch: 138 , training loss: 3.914704
[INFO] Epoch: 46 , batch: 139 , training loss: 4.482342
[INFO] Epoch: 46 , batch: 140 , training loss: 4.198865
[INFO] Epoch: 46 , batch: 141 , training loss: 3.997658
[INFO] Epoch: 46 , batch: 142 , training loss: 3.779274
[INFO] Epoch: 46 , batch: 143 , training loss: 3.916439
[INFO] Epoch: 46 , batch: 144 , training loss: 3.726771
[INFO] Epoch: 46 , batch: 145 , training loss: 3.782110
[INFO] Epoch: 46 , batch: 146 , training loss: 3.968439
[INFO] Epoch: 46 , batch: 147 , training loss: 3.680632
[INFO] Epoch: 46 , batch: 148 , training loss: 3.663296
[INFO] Epoch: 46 , batch: 149 , training loss: 3.773275
[INFO] Epoch: 46 , batch: 150 , training loss: 3.956398
[INFO] Epoch: 46 , batch: 151 , training loss: 3.820979
[INFO] Epoch: 46 , batch: 152 , training loss: 3.900689
[INFO] Epoch: 46 , batch: 153 , training loss: 3.863044
[INFO] Epoch: 46 , batch: 154 , training loss: 3.930090
[INFO] Epoch: 46 , batch: 155 , training loss: 4.185260
[INFO] Epoch: 46 , batch: 156 , training loss: 3.897222
[INFO] Epoch: 46 , batch: 157 , training loss: 3.891404
[INFO] Epoch: 46 , batch: 158 , training loss: 3.991971
[INFO] Epoch: 46 , batch: 159 , training loss: 3.890760
[INFO] Epoch: 46 , batch: 160 , training loss: 4.148470
[INFO] Epoch: 46 , batch: 161 , training loss: 4.241526
[INFO] Epoch: 46 , batch: 162 , training loss: 4.216711
[INFO] Epoch: 46 , batch: 163 , training loss: 4.364555
[INFO] Epoch: 46 , batch: 164 , training loss: 4.370790
[INFO] Epoch: 46 , batch: 165 , training loss: 4.278795
[INFO] Epoch: 46 , batch: 166 , training loss: 4.133757
[INFO] Epoch: 46 , batch: 167 , training loss: 4.149374
[INFO] Epoch: 46 , batch: 168 , training loss: 3.828883
[INFO] Epoch: 46 , batch: 169 , training loss: 3.758128
[INFO] Epoch: 46 , batch: 170 , training loss: 4.011430
[INFO] Epoch: 46 , batch: 171 , training loss: 3.415748
[INFO] Epoch: 46 , batch: 172 , training loss: 3.646999
[INFO] Epoch: 46 , batch: 173 , training loss: 4.010634
[INFO] Epoch: 46 , batch: 174 , training loss: 4.446060
[INFO] Epoch: 46 , batch: 175 , training loss: 4.772388
[INFO] Epoch: 46 , batch: 176 , training loss: 4.378281
[INFO] Epoch: 46 , batch: 177 , training loss: 4.068271
[INFO] Epoch: 46 , batch: 178 , training loss: 4.042480
[INFO] Epoch: 46 , batch: 179 , training loss: 4.112685
[INFO] Epoch: 46 , batch: 180 , training loss: 4.061127
[INFO] Epoch: 46 , batch: 181 , training loss: 4.341664
[INFO] Epoch: 46 , batch: 182 , training loss: 4.323962
[INFO] Epoch: 46 , batch: 183 , training loss: 4.268597
[INFO] Epoch: 46 , batch: 184 , training loss: 4.155657
[INFO] Epoch: 46 , batch: 185 , training loss: 4.124718
[INFO] Epoch: 46 , batch: 186 , training loss: 4.254789
[INFO] Epoch: 46 , batch: 187 , training loss: 4.398025
[INFO] Epoch: 46 , batch: 188 , training loss: 4.395235
[INFO] Epoch: 46 , batch: 189 , training loss: 4.281590
[INFO] Epoch: 46 , batch: 190 , training loss: 4.294335
[INFO] Epoch: 46 , batch: 191 , training loss: 4.380566
[INFO] Epoch: 46 , batch: 192 , training loss: 4.236361
[INFO] Epoch: 46 , batch: 193 , training loss: 4.342428
[INFO] Epoch: 46 , batch: 194 , training loss: 4.274377
[INFO] Epoch: 46 , batch: 195 , training loss: 4.188029
[INFO] Epoch: 46 , batch: 196 , training loss: 4.072272
[INFO] Epoch: 46 , batch: 197 , training loss: 4.160177
[INFO] Epoch: 46 , batch: 198 , training loss: 4.070010
[INFO] Epoch: 46 , batch: 199 , training loss: 4.225077
[INFO] Epoch: 46 , batch: 200 , training loss: 4.124766
[INFO] Epoch: 46 , batch: 201 , training loss: 4.020279
[INFO] Epoch: 46 , batch: 202 , training loss: 4.030090
[INFO] Epoch: 46 , batch: 203 , training loss: 4.128688
[INFO] Epoch: 46 , batch: 204 , training loss: 4.299585
[INFO] Epoch: 46 , batch: 205 , training loss: 3.816662
[INFO] Epoch: 46 , batch: 206 , training loss: 3.764263
[INFO] Epoch: 46 , batch: 207 , training loss: 3.780982
[INFO] Epoch: 46 , batch: 208 , training loss: 4.114586
[INFO] Epoch: 46 , batch: 209 , training loss: 4.050378
[INFO] Epoch: 46 , batch: 210 , training loss: 4.084704
[INFO] Epoch: 46 , batch: 211 , training loss: 4.061619
[INFO] Epoch: 46 , batch: 212 , training loss: 4.159415
[INFO] Epoch: 46 , batch: 213 , training loss: 4.120583
[INFO] Epoch: 46 , batch: 214 , training loss: 4.176375
[INFO] Epoch: 46 , batch: 215 , training loss: 4.433495
[INFO] Epoch: 46 , batch: 216 , training loss: 4.097954
[INFO] Epoch: 46 , batch: 217 , training loss: 4.093035
[INFO] Epoch: 46 , batch: 218 , training loss: 4.050909
[INFO] Epoch: 46 , batch: 219 , training loss: 4.135080
[INFO] Epoch: 46 , batch: 220 , training loss: 3.964917
[INFO] Epoch: 46 , batch: 221 , training loss: 3.985258
[INFO] Epoch: 46 , batch: 222 , training loss: 4.133623
[INFO] Epoch: 46 , batch: 223 , training loss: 4.218913
[INFO] Epoch: 46 , batch: 224 , training loss: 4.269545
[INFO] Epoch: 46 , batch: 225 , training loss: 4.169762
[INFO] Epoch: 46 , batch: 226 , training loss: 4.287626
[INFO] Epoch: 46 , batch: 227 , training loss: 4.258523
[INFO] Epoch: 46 , batch: 228 , training loss: 4.258824
[INFO] Epoch: 46 , batch: 229 , training loss: 4.150875
[INFO] Epoch: 46 , batch: 230 , training loss: 4.002291
[INFO] Epoch: 46 , batch: 231 , training loss: 3.881030
[INFO] Epoch: 46 , batch: 232 , training loss: 4.009363
[INFO] Epoch: 46 , batch: 233 , training loss: 4.014744
[INFO] Epoch: 46 , batch: 234 , training loss: 3.739042
[INFO] Epoch: 46 , batch: 235 , training loss: 3.825044
[INFO] Epoch: 46 , batch: 236 , training loss: 3.910485
[INFO] Epoch: 46 , batch: 237 , training loss: 4.149958
[INFO] Epoch: 46 , batch: 238 , training loss: 3.918586
[INFO] Epoch: 46 , batch: 239 , training loss: 3.965063
[INFO] Epoch: 46 , batch: 240 , training loss: 4.038085
[INFO] Epoch: 46 , batch: 241 , training loss: 3.834740
[INFO] Epoch: 46 , batch: 242 , training loss: 3.843397
[INFO] Epoch: 46 , batch: 243 , training loss: 4.157995
[INFO] Epoch: 46 , batch: 244 , training loss: 4.096484
[INFO] Epoch: 46 , batch: 245 , training loss: 4.052853
[INFO] Epoch: 46 , batch: 246 , training loss: 3.785365
[INFO] Epoch: 46 , batch: 247 , training loss: 3.931057
[INFO] Epoch: 46 , batch: 248 , training loss: 3.988003
[INFO] Epoch: 46 , batch: 249 , training loss: 4.039046
[INFO] Epoch: 46 , batch: 250 , training loss: 3.777278
[INFO] Epoch: 46 , batch: 251 , training loss: 4.221538
[INFO] Epoch: 46 , batch: 252 , training loss: 3.916129
[INFO] Epoch: 46 , batch: 253 , training loss: 3.849856
[INFO] Epoch: 46 , batch: 254 , training loss: 4.116959
[INFO] Epoch: 46 , batch: 255 , training loss: 4.084250
[INFO] Epoch: 46 , batch: 256 , training loss: 4.105939
[INFO] Epoch: 46 , batch: 257 , training loss: 4.244235
[INFO] Epoch: 46 , batch: 258 , training loss: 4.289489
[INFO] Epoch: 46 , batch: 259 , training loss: 4.339793
[INFO] Epoch: 46 , batch: 260 , training loss: 4.082460
[INFO] Epoch: 46 , batch: 261 , training loss: 4.193692
[INFO] Epoch: 46 , batch: 262 , training loss: 4.361666
[INFO] Epoch: 46 , batch: 263 , training loss: 4.572739
[INFO] Epoch: 46 , batch: 264 , training loss: 3.901141
[INFO] Epoch: 46 , batch: 265 , training loss: 4.022573
[INFO] Epoch: 46 , batch: 266 , training loss: 4.423025
[INFO] Epoch: 46 , batch: 267 , training loss: 4.204910
[INFO] Epoch: 46 , batch: 268 , training loss: 4.125533
[INFO] Epoch: 46 , batch: 269 , training loss: 4.101678
[INFO] Epoch: 46 , batch: 270 , training loss: 4.124608
[INFO] Epoch: 46 , batch: 271 , training loss: 4.142138
[INFO] Epoch: 46 , batch: 272 , training loss: 4.122341
[INFO] Epoch: 46 , batch: 273 , training loss: 4.166719
[INFO] Epoch: 46 , batch: 274 , training loss: 4.227683
[INFO] Epoch: 46 , batch: 275 , training loss: 4.084329
[INFO] Epoch: 46 , batch: 276 , training loss: 4.138994
[INFO] Epoch: 46 , batch: 277 , training loss: 4.285069
[INFO] Epoch: 46 , batch: 278 , training loss: 3.992230
[INFO] Epoch: 46 , batch: 279 , training loss: 4.001725
[INFO] Epoch: 46 , batch: 280 , training loss: 3.940858
[INFO] Epoch: 46 , batch: 281 , training loss: 4.088270
[INFO] Epoch: 46 , batch: 282 , training loss: 4.004305
[INFO] Epoch: 46 , batch: 283 , training loss: 4.020832
[INFO] Epoch: 46 , batch: 284 , training loss: 4.051955
[INFO] Epoch: 46 , batch: 285 , training loss: 3.974731
[INFO] Epoch: 46 , batch: 286 , training loss: 3.968618
[INFO] Epoch: 46 , batch: 287 , training loss: 3.919850
[INFO] Epoch: 46 , batch: 288 , training loss: 3.941188
[INFO] Epoch: 46 , batch: 289 , training loss: 3.975208
[INFO] Epoch: 46 , batch: 290 , training loss: 3.783712
[INFO] Epoch: 46 , batch: 291 , training loss: 3.740543
[INFO] Epoch: 46 , batch: 292 , training loss: 3.872005
[INFO] Epoch: 46 , batch: 293 , training loss: 3.788236
[INFO] Epoch: 46 , batch: 294 , training loss: 4.437490
[INFO] Epoch: 46 , batch: 295 , training loss: 4.215590
[INFO] Epoch: 46 , batch: 296 , training loss: 4.137120
[INFO] Epoch: 46 , batch: 297 , training loss: 4.101342
[INFO] Epoch: 46 , batch: 298 , training loss: 3.939339
[INFO] Epoch: 46 , batch: 299 , training loss: 3.960739
[INFO] Epoch: 46 , batch: 300 , training loss: 3.921646
[INFO] Epoch: 46 , batch: 301 , training loss: 3.893231
[INFO] Epoch: 46 , batch: 302 , training loss: 4.037646
[INFO] Epoch: 46 , batch: 303 , training loss: 4.063716
[INFO] Epoch: 46 , batch: 304 , training loss: 4.216277
[INFO] Epoch: 46 , batch: 305 , training loss: 4.008551
[INFO] Epoch: 46 , batch: 306 , training loss: 4.143247
[INFO] Epoch: 46 , batch: 307 , training loss: 4.129518
[INFO] Epoch: 46 , batch: 308 , training loss: 4.001423
[INFO] Epoch: 46 , batch: 309 , training loss: 3.974777
[INFO] Epoch: 46 , batch: 310 , training loss: 3.906350
[INFO] Epoch: 46 , batch: 311 , training loss: 3.889227
[INFO] Epoch: 46 , batch: 312 , training loss: 3.834391
[INFO] Epoch: 46 , batch: 313 , training loss: 3.913451
[INFO] Epoch: 46 , batch: 314 , training loss: 3.954878
[INFO] Epoch: 46 , batch: 315 , training loss: 4.027848
[INFO] Epoch: 46 , batch: 316 , training loss: 4.270538
[INFO] Epoch: 46 , batch: 317 , training loss: 4.692824
[INFO] Epoch: 46 , batch: 318 , training loss: 4.830440
[INFO] Epoch: 46 , batch: 319 , training loss: 4.458192
[INFO] Epoch: 46 , batch: 320 , training loss: 3.987647
[INFO] Epoch: 46 , batch: 321 , training loss: 3.837160
[INFO] Epoch: 46 , batch: 322 , training loss: 3.934455
[INFO] Epoch: 46 , batch: 323 , training loss: 3.965552
[INFO] Epoch: 46 , batch: 324 , training loss: 3.927072
[INFO] Epoch: 46 , batch: 325 , training loss: 4.085593
[INFO] Epoch: 46 , batch: 326 , training loss: 4.136390
[INFO] Epoch: 46 , batch: 327 , training loss: 4.034930
[INFO] Epoch: 46 , batch: 328 , training loss: 4.053464
[INFO] Epoch: 46 , batch: 329 , training loss: 3.974038
[INFO] Epoch: 46 , batch: 330 , training loss: 3.957259
[INFO] Epoch: 46 , batch: 331 , training loss: 4.127905
[INFO] Epoch: 46 , batch: 332 , training loss: 3.937662
[INFO] Epoch: 46 , batch: 333 , training loss: 3.938179
[INFO] Epoch: 46 , batch: 334 , training loss: 3.933566
[INFO] Epoch: 46 , batch: 335 , training loss: 4.079227
[INFO] Epoch: 46 , batch: 336 , training loss: 4.097620
[INFO] Epoch: 46 , batch: 337 , training loss: 4.090607
[INFO] Epoch: 46 , batch: 338 , training loss: 4.325722
[INFO] Epoch: 46 , batch: 339 , training loss: 4.179748
[INFO] Epoch: 46 , batch: 340 , training loss: 4.319433
[INFO] Epoch: 46 , batch: 341 , training loss: 4.081871
[INFO] Epoch: 46 , batch: 342 , training loss: 3.854445
[INFO] Epoch: 46 , batch: 343 , training loss: 3.954302
[INFO] Epoch: 46 , batch: 344 , training loss: 3.818332
[INFO] Epoch: 46 , batch: 345 , training loss: 3.941977
[INFO] Epoch: 46 , batch: 346 , training loss: 3.975879
[INFO] Epoch: 46 , batch: 347 , training loss: 3.890499
[INFO] Epoch: 46 , batch: 348 , training loss: 3.976008
[INFO] Epoch: 46 , batch: 349 , training loss: 4.156125
[INFO] Epoch: 46 , batch: 350 , training loss: 3.948799
[INFO] Epoch: 46 , batch: 351 , training loss: 4.013983
[INFO] Epoch: 46 , batch: 352 , training loss: 4.031343
[INFO] Epoch: 46 , batch: 353 , training loss: 3.985017
[INFO] Epoch: 46 , batch: 354 , training loss: 4.113285
[INFO] Epoch: 46 , batch: 355 , training loss: 4.164440
[INFO] Epoch: 46 , batch: 356 , training loss: 3.981905
[INFO] Epoch: 46 , batch: 357 , training loss: 4.057145
[INFO] Epoch: 46 , batch: 358 , training loss: 3.954723
[INFO] Epoch: 46 , batch: 359 , training loss: 3.961860
[INFO] Epoch: 46 , batch: 360 , training loss: 4.062614
[INFO] Epoch: 46 , batch: 361 , training loss: 4.041101
[INFO] Epoch: 46 , batch: 362 , training loss: 4.128039
[INFO] Epoch: 46 , batch: 363 , training loss: 4.002888
[INFO] Epoch: 46 , batch: 364 , training loss: 4.053070
[INFO] Epoch: 46 , batch: 365 , training loss: 3.992527
[INFO] Epoch: 46 , batch: 366 , training loss: 4.107771
[INFO] Epoch: 46 , batch: 367 , training loss: 4.165503
[INFO] Epoch: 46 , batch: 368 , training loss: 4.556700
[INFO] Epoch: 46 , batch: 369 , training loss: 4.217847
[INFO] Epoch: 46 , batch: 370 , training loss: 3.987509
[INFO] Epoch: 46 , batch: 371 , training loss: 4.377990
[INFO] Epoch: 46 , batch: 372 , training loss: 4.668029
[INFO] Epoch: 46 , batch: 373 , training loss: 4.718143
[INFO] Epoch: 46 , batch: 374 , training loss: 4.815786
[INFO] Epoch: 46 , batch: 375 , training loss: 4.823986
[INFO] Epoch: 46 , batch: 376 , training loss: 4.740845
[INFO] Epoch: 46 , batch: 377 , training loss: 4.500437
[INFO] Epoch: 46 , batch: 378 , training loss: 4.582836
[INFO] Epoch: 46 , batch: 379 , training loss: 4.555552
[INFO] Epoch: 46 , batch: 380 , training loss: 4.693065
[INFO] Epoch: 46 , batch: 381 , training loss: 4.402566
[INFO] Epoch: 46 , batch: 382 , training loss: 4.629805
[INFO] Epoch: 46 , batch: 383 , training loss: 4.635950
[INFO] Epoch: 46 , batch: 384 , training loss: 4.682328
[INFO] Epoch: 46 , batch: 385 , training loss: 4.361483
[INFO] Epoch: 46 , batch: 386 , training loss: 4.635636
[INFO] Epoch: 46 , batch: 387 , training loss: 4.594910
[INFO] Epoch: 46 , batch: 388 , training loss: 4.401266
[INFO] Epoch: 46 , batch: 389 , training loss: 4.250266
[INFO] Epoch: 46 , batch: 390 , training loss: 4.225261
[INFO] Epoch: 46 , batch: 391 , training loss: 4.285586
[INFO] Epoch: 46 , batch: 392 , training loss: 4.639289
[INFO] Epoch: 46 , batch: 393 , training loss: 4.528884
[INFO] Epoch: 46 , batch: 394 , training loss: 4.582633
[INFO] Epoch: 46 , batch: 395 , training loss: 4.450025
[INFO] Epoch: 46 , batch: 396 , training loss: 4.237250
[INFO] Epoch: 46 , batch: 397 , training loss: 4.377129
[INFO] Epoch: 46 , batch: 398 , training loss: 4.251161
[INFO] Epoch: 46 , batch: 399 , training loss: 4.305278
[INFO] Epoch: 46 , batch: 400 , training loss: 4.278522
[INFO] Epoch: 46 , batch: 401 , training loss: 4.677073
[INFO] Epoch: 46 , batch: 402 , training loss: 4.417658
[INFO] Epoch: 46 , batch: 403 , training loss: 4.228673
[INFO] Epoch: 46 , batch: 404 , training loss: 4.433629
[INFO] Epoch: 46 , batch: 405 , training loss: 4.479333
[INFO] Epoch: 46 , batch: 406 , training loss: 4.384794
[INFO] Epoch: 46 , batch: 407 , training loss: 4.433159
[INFO] Epoch: 46 , batch: 408 , training loss: 4.401667
[INFO] Epoch: 46 , batch: 409 , training loss: 4.381900
[INFO] Epoch: 46 , batch: 410 , training loss: 4.413357
[INFO] Epoch: 46 , batch: 411 , training loss: 4.612905
[INFO] Epoch: 46 , batch: 412 , training loss: 4.446198
[INFO] Epoch: 46 , batch: 413 , training loss: 4.346397
[INFO] Epoch: 46 , batch: 414 , training loss: 4.372856
[INFO] Epoch: 46 , batch: 415 , training loss: 4.402824
[INFO] Epoch: 46 , batch: 416 , training loss: 4.472671
[INFO] Epoch: 46 , batch: 417 , training loss: 4.400708
[INFO] Epoch: 46 , batch: 418 , training loss: 4.405571
[INFO] Epoch: 46 , batch: 419 , training loss: 4.368711
[INFO] Epoch: 46 , batch: 420 , training loss: 4.357401
[INFO] Epoch: 46 , batch: 421 , training loss: 4.354263
[INFO] Epoch: 46 , batch: 422 , training loss: 4.207344
[INFO] Epoch: 46 , batch: 423 , training loss: 4.418715
[INFO] Epoch: 46 , batch: 424 , training loss: 4.583728
[INFO] Epoch: 46 , batch: 425 , training loss: 4.447616
[INFO] Epoch: 46 , batch: 426 , training loss: 4.194445
[INFO] Epoch: 46 , batch: 427 , training loss: 4.433529
[INFO] Epoch: 46 , batch: 428 , training loss: 4.305929
[INFO] Epoch: 46 , batch: 429 , training loss: 4.167870
[INFO] Epoch: 46 , batch: 430 , training loss: 4.421774
[INFO] Epoch: 46 , batch: 431 , training loss: 4.036312
[INFO] Epoch: 46 , batch: 432 , training loss: 4.091431
[INFO] Epoch: 46 , batch: 433 , training loss: 4.123541
[INFO] Epoch: 46 , batch: 434 , training loss: 4.000108
[INFO] Epoch: 46 , batch: 435 , training loss: 4.368584
[INFO] Epoch: 46 , batch: 436 , training loss: 4.432846
[INFO] Epoch: 46 , batch: 437 , training loss: 4.184260
[INFO] Epoch: 46 , batch: 438 , training loss: 4.044536
[INFO] Epoch: 46 , batch: 439 , training loss: 4.266266
[INFO] Epoch: 46 , batch: 440 , training loss: 4.387138
[INFO] Epoch: 46 , batch: 441 , training loss: 4.479463
[INFO] Epoch: 46 , batch: 442 , training loss: 4.259872
[INFO] Epoch: 46 , batch: 443 , training loss: 4.440457
[INFO] Epoch: 46 , batch: 444 , training loss: 4.075695
[INFO] Epoch: 46 , batch: 445 , training loss: 3.975838
[INFO] Epoch: 46 , batch: 446 , training loss: 3.938663
[INFO] Epoch: 46 , batch: 447 , training loss: 4.103514
[INFO] Epoch: 46 , batch: 448 , training loss: 4.204788
[INFO] Epoch: 46 , batch: 449 , training loss: 4.605430
[INFO] Epoch: 46 , batch: 450 , training loss: 4.654488
[INFO] Epoch: 46 , batch: 451 , training loss: 4.568262
[INFO] Epoch: 46 , batch: 452 , training loss: 4.372215
[INFO] Epoch: 46 , batch: 453 , training loss: 4.156564
[INFO] Epoch: 46 , batch: 454 , training loss: 4.282600
[INFO] Epoch: 46 , batch: 455 , training loss: 4.341427
[INFO] Epoch: 46 , batch: 456 , training loss: 4.331337
[INFO] Epoch: 46 , batch: 457 , training loss: 4.426403
[INFO] Epoch: 46 , batch: 458 , training loss: 4.166109
[INFO] Epoch: 46 , batch: 459 , training loss: 4.136689
[INFO] Epoch: 46 , batch: 460 , training loss: 4.225486
[INFO] Epoch: 46 , batch: 461 , training loss: 4.204914
[INFO] Epoch: 46 , batch: 462 , training loss: 4.280587
[INFO] Epoch: 46 , batch: 463 , training loss: 4.196094
[INFO] Epoch: 46 , batch: 464 , training loss: 4.394270
[INFO] Epoch: 46 , batch: 465 , training loss: 4.293375
[INFO] Epoch: 46 , batch: 466 , training loss: 4.389127
[INFO] Epoch: 46 , batch: 467 , training loss: 4.362461
[INFO] Epoch: 46 , batch: 468 , training loss: 4.341294
[INFO] Epoch: 46 , batch: 469 , training loss: 4.363167
[INFO] Epoch: 46 , batch: 470 , training loss: 4.182866
[INFO] Epoch: 46 , batch: 471 , training loss: 4.254836
[INFO] Epoch: 46 , batch: 472 , training loss: 4.307017
[INFO] Epoch: 46 , batch: 473 , training loss: 4.254079
[INFO] Epoch: 46 , batch: 474 , training loss: 4.047804
[INFO] Epoch: 46 , batch: 475 , training loss: 3.931726
[INFO] Epoch: 46 , batch: 476 , training loss: 4.278293
[INFO] Epoch: 46 , batch: 477 , training loss: 4.438106
[INFO] Epoch: 46 , batch: 478 , training loss: 4.454184
[INFO] Epoch: 46 , batch: 479 , training loss: 4.410215
[INFO] Epoch: 46 , batch: 480 , training loss: 4.503082
[INFO] Epoch: 46 , batch: 481 , training loss: 4.395315
[INFO] Epoch: 46 , batch: 482 , training loss: 4.485542
[INFO] Epoch: 46 , batch: 483 , training loss: 4.375388
[INFO] Epoch: 46 , batch: 484 , training loss: 4.167576
[INFO] Epoch: 46 , batch: 485 , training loss: 4.248646
[INFO] Epoch: 46 , batch: 486 , training loss: 4.139558
[INFO] Epoch: 46 , batch: 487 , training loss: 4.158916
[INFO] Epoch: 46 , batch: 488 , training loss: 4.345605
[INFO] Epoch: 46 , batch: 489 , training loss: 4.215306
[INFO] Epoch: 46 , batch: 490 , training loss: 4.242052
[INFO] Epoch: 46 , batch: 491 , training loss: 4.212265
[INFO] Epoch: 46 , batch: 492 , training loss: 4.165107
[INFO] Epoch: 46 , batch: 493 , training loss: 4.353239
[INFO] Epoch: 46 , batch: 494 , training loss: 4.274563
[INFO] Epoch: 46 , batch: 495 , training loss: 4.393120
[INFO] Epoch: 46 , batch: 496 , training loss: 4.278968
[INFO] Epoch: 46 , batch: 497 , training loss: 4.311175
[INFO] Epoch: 46 , batch: 498 , training loss: 4.314713
[INFO] Epoch: 46 , batch: 499 , training loss: 4.376285
[INFO] Epoch: 46 , batch: 500 , training loss: 4.498088
[INFO] Epoch: 46 , batch: 501 , training loss: 4.802413
[INFO] Epoch: 46 , batch: 502 , training loss: 4.863801
[INFO] Epoch: 46 , batch: 503 , training loss: 4.553799
[INFO] Epoch: 46 , batch: 504 , training loss: 4.678337
[INFO] Epoch: 46 , batch: 505 , training loss: 4.662011
[INFO] Epoch: 46 , batch: 506 , training loss: 4.606400
[INFO] Epoch: 46 , batch: 507 , training loss: 4.683220
[INFO] Epoch: 46 , batch: 508 , training loss: 4.624800
[INFO] Epoch: 46 , batch: 509 , training loss: 4.400764
[INFO] Epoch: 46 , batch: 510 , training loss: 4.495937
[INFO] Epoch: 46 , batch: 511 , training loss: 4.388017
[INFO] Epoch: 46 , batch: 512 , training loss: 4.510329
[INFO] Epoch: 46 , batch: 513 , training loss: 4.756658
[INFO] Epoch: 46 , batch: 514 , training loss: 4.417762
[INFO] Epoch: 46 , batch: 515 , training loss: 4.631129
[INFO] Epoch: 46 , batch: 516 , training loss: 4.457767
[INFO] Epoch: 46 , batch: 517 , training loss: 4.413078
[INFO] Epoch: 46 , batch: 518 , training loss: 4.375779
[INFO] Epoch: 46 , batch: 519 , training loss: 4.216489
[INFO] Epoch: 46 , batch: 520 , training loss: 4.440677
[INFO] Epoch: 46 , batch: 521 , training loss: 4.430763
[INFO] Epoch: 46 , batch: 522 , training loss: 4.484961
[INFO] Epoch: 46 , batch: 523 , training loss: 4.405975
[INFO] Epoch: 46 , batch: 524 , training loss: 4.690128
[INFO] Epoch: 46 , batch: 525 , training loss: 4.642419
[INFO] Epoch: 46 , batch: 526 , training loss: 4.390640
[INFO] Epoch: 46 , batch: 527 , training loss: 4.425609
[INFO] Epoch: 46 , batch: 528 , training loss: 4.441763
[INFO] Epoch: 46 , batch: 529 , training loss: 4.448197
[INFO] Epoch: 46 , batch: 530 , training loss: 4.271360
[INFO] Epoch: 46 , batch: 531 , training loss: 4.406083
[INFO] Epoch: 46 , batch: 532 , training loss: 4.319937
[INFO] Epoch: 46 , batch: 533 , training loss: 4.460475
[INFO] Epoch: 46 , batch: 534 , training loss: 4.426985
[INFO] Epoch: 46 , batch: 535 , training loss: 4.443933
[INFO] Epoch: 46 , batch: 536 , training loss: 4.273192
[INFO] Epoch: 46 , batch: 537 , training loss: 4.287849
[INFO] Epoch: 46 , batch: 538 , training loss: 4.352169
[INFO] Epoch: 46 , batch: 539 , training loss: 4.460040
[INFO] Epoch: 46 , batch: 540 , training loss: 4.950959
[INFO] Epoch: 46 , batch: 541 , training loss: 4.789589
[INFO] Epoch: 46 , batch: 542 , training loss: 4.721628
[INFO] Epoch: 47 , batch: 0 , training loss: 3.325682
[INFO] Epoch: 47 , batch: 1 , training loss: 3.377360
[INFO] Epoch: 47 , batch: 2 , training loss: 3.626402
[INFO] Epoch: 47 , batch: 3 , training loss: 3.398620
[INFO] Epoch: 47 , batch: 4 , training loss: 3.750147
[INFO] Epoch: 47 , batch: 5 , training loss: 3.440687
[INFO] Epoch: 47 , batch: 6 , training loss: 3.805297
[INFO] Epoch: 47 , batch: 7 , training loss: 3.760097
[INFO] Epoch: 47 , batch: 8 , training loss: 3.440390
[INFO] Epoch: 47 , batch: 9 , training loss: 3.685165
[INFO] Epoch: 47 , batch: 10 , training loss: 3.692697
[INFO] Epoch: 47 , batch: 11 , training loss: 3.648977
[INFO] Epoch: 47 , batch: 12 , training loss: 3.560661
[INFO] Epoch: 47 , batch: 13 , training loss: 3.598370
[INFO] Epoch: 47 , batch: 14 , training loss: 3.473105
[INFO] Epoch: 47 , batch: 15 , training loss: 3.659283
[INFO] Epoch: 47 , batch: 16 , training loss: 3.523202
[INFO] Epoch: 47 , batch: 17 , training loss: 3.675582
[INFO] Epoch: 47 , batch: 18 , training loss: 3.628479
[INFO] Epoch: 47 , batch: 19 , training loss: 3.396626
[INFO] Epoch: 47 , batch: 20 , training loss: 3.357502
[INFO] Epoch: 47 , batch: 21 , training loss: 3.515846
[INFO] Epoch: 47 , batch: 22 , training loss: 3.335320
[INFO] Epoch: 47 , batch: 23 , training loss: 3.597346
[INFO] Epoch: 47 , batch: 24 , training loss: 3.354864
[INFO] Epoch: 47 , batch: 25 , training loss: 3.607127
[INFO] Epoch: 47 , batch: 26 , training loss: 3.469983
[INFO] Epoch: 47 , batch: 27 , training loss: 3.411183
[INFO] Epoch: 47 , batch: 28 , training loss: 3.572410
[INFO] Epoch: 47 , batch: 29 , training loss: 3.420233
[INFO] Epoch: 47 , batch: 30 , training loss: 3.472568
[INFO] Epoch: 47 , batch: 31 , training loss: 3.511320
[INFO] Epoch: 47 , batch: 32 , training loss: 3.498689
[INFO] Epoch: 47 , batch: 33 , training loss: 3.524080
[INFO] Epoch: 47 , batch: 34 , training loss: 3.480211
[INFO] Epoch: 47 , batch: 35 , training loss: 3.488525
[INFO] Epoch: 47 , batch: 36 , training loss: 3.543221
[INFO] Epoch: 47 , batch: 37 , training loss: 3.442297
[INFO] Epoch: 47 , batch: 38 , training loss: 3.436571
[INFO] Epoch: 47 , batch: 39 , training loss: 3.318870
[INFO] Epoch: 47 , batch: 40 , training loss: 3.572515
[INFO] Epoch: 47 , batch: 41 , training loss: 3.471246
[INFO] Epoch: 47 , batch: 42 , training loss: 3.882471
[INFO] Epoch: 47 , batch: 43 , training loss: 3.613055
[INFO] Epoch: 47 , batch: 44 , training loss: 3.970879
[INFO] Epoch: 47 , batch: 45 , training loss: 3.863695
[INFO] Epoch: 47 , batch: 46 , training loss: 3.783802
[INFO] Epoch: 47 , batch: 47 , training loss: 3.560830
[INFO] Epoch: 47 , batch: 48 , training loss: 3.529411
[INFO] Epoch: 47 , batch: 49 , training loss: 3.726716
[INFO] Epoch: 47 , batch: 50 , training loss: 3.533007
[INFO] Epoch: 47 , batch: 51 , training loss: 3.763899
[INFO] Epoch: 47 , batch: 52 , training loss: 3.619946
[INFO] Epoch: 47 , batch: 53 , training loss: 3.728308
[INFO] Epoch: 47 , batch: 54 , training loss: 3.757254
[INFO] Epoch: 47 , batch: 55 , training loss: 3.833370
[INFO] Epoch: 47 , batch: 56 , training loss: 3.709549
[INFO] Epoch: 47 , batch: 57 , training loss: 3.586518
[INFO] Epoch: 47 , batch: 58 , training loss: 3.640767
[INFO] Epoch: 47 , batch: 59 , training loss: 3.749659
[INFO] Epoch: 47 , batch: 60 , training loss: 3.679725
[INFO] Epoch: 47 , batch: 61 , training loss: 3.750656
[INFO] Epoch: 47 , batch: 62 , training loss: 3.616914
[INFO] Epoch: 47 , batch: 63 , training loss: 3.840380
[INFO] Epoch: 47 , batch: 64 , training loss: 4.011645
[INFO] Epoch: 47 , batch: 65 , training loss: 3.729272
[INFO] Epoch: 47 , batch: 66 , training loss: 3.576467
[INFO] Epoch: 47 , batch: 67 , training loss: 3.659075
[INFO] Epoch: 47 , batch: 68 , training loss: 3.790822
[INFO] Epoch: 47 , batch: 69 , training loss: 3.668464
[INFO] Epoch: 47 , batch: 70 , training loss: 3.875487
[INFO] Epoch: 47 , batch: 71 , training loss: 3.802396
[INFO] Epoch: 47 , batch: 72 , training loss: 3.838256
[INFO] Epoch: 47 , batch: 73 , training loss: 3.795137
[INFO] Epoch: 47 , batch: 74 , training loss: 3.913066
[INFO] Epoch: 47 , batch: 75 , training loss: 3.794385
[INFO] Epoch: 47 , batch: 76 , training loss: 3.819328
[INFO] Epoch: 47 , batch: 77 , training loss: 3.820288
[INFO] Epoch: 47 , batch: 78 , training loss: 3.927958
[INFO] Epoch: 47 , batch: 79 , training loss: 3.790882
[INFO] Epoch: 47 , batch: 80 , training loss: 3.963580
[INFO] Epoch: 47 , batch: 81 , training loss: 3.902471
[INFO] Epoch: 47 , batch: 82 , training loss: 3.841878
[INFO] Epoch: 47 , batch: 83 , training loss: 4.009246
[INFO] Epoch: 47 , batch: 84 , training loss: 3.898902
[INFO] Epoch: 47 , batch: 85 , training loss: 3.996892
[INFO] Epoch: 47 , batch: 86 , training loss: 3.943286
[INFO] Epoch: 47 , batch: 87 , training loss: 3.944238
[INFO] Epoch: 47 , batch: 88 , training loss: 4.007340
[INFO] Epoch: 47 , batch: 89 , training loss: 3.848974
[INFO] Epoch: 47 , batch: 90 , training loss: 3.912593
[INFO] Epoch: 47 , batch: 91 , training loss: 3.882833
[INFO] Epoch: 47 , batch: 92 , training loss: 3.857088
[INFO] Epoch: 47 , batch: 93 , training loss: 3.958246
[INFO] Epoch: 47 , batch: 94 , training loss: 4.097379
[INFO] Epoch: 47 , batch: 95 , training loss: 3.841810
[INFO] Epoch: 47 , batch: 96 , training loss: 3.875711
[INFO] Epoch: 47 , batch: 97 , training loss: 3.788800
[INFO] Epoch: 47 , batch: 98 , training loss: 3.766373
[INFO] Epoch: 47 , batch: 99 , training loss: 3.843412
[INFO] Epoch: 47 , batch: 100 , training loss: 3.796553
[INFO] Epoch: 47 , batch: 101 , training loss: 3.809132
[INFO] Epoch: 47 , batch: 102 , training loss: 3.923601
[INFO] Epoch: 47 , batch: 103 , training loss: 3.739633
[INFO] Epoch: 47 , batch: 104 , training loss: 3.699235
[INFO] Epoch: 47 , batch: 105 , training loss: 3.976699
[INFO] Epoch: 47 , batch: 106 , training loss: 3.908957
[INFO] Epoch: 47 , batch: 107 , training loss: 3.810353
[INFO] Epoch: 47 , batch: 108 , training loss: 3.719982
[INFO] Epoch: 47 , batch: 109 , training loss: 3.637499
[INFO] Epoch: 47 , batch: 110 , training loss: 3.905531
[INFO] Epoch: 47 , batch: 111 , training loss: 3.906769
[INFO] Epoch: 47 , batch: 112 , training loss: 3.810545
[INFO] Epoch: 47 , batch: 113 , training loss: 3.856155
[INFO] Epoch: 47 , batch: 114 , training loss: 3.816440
[INFO] Epoch: 47 , batch: 115 , training loss: 3.821395
[INFO] Epoch: 47 , batch: 116 , training loss: 3.743039
[INFO] Epoch: 47 , batch: 117 , training loss: 3.984752
[INFO] Epoch: 47 , batch: 118 , training loss: 3.944033
[INFO] Epoch: 47 , batch: 119 , training loss: 4.034103
[INFO] Epoch: 47 , batch: 120 , training loss: 4.090309
[INFO] Epoch: 47 , batch: 121 , training loss: 3.906912
[INFO] Epoch: 47 , batch: 122 , training loss: 3.808626
[INFO] Epoch: 47 , batch: 123 , training loss: 3.825486
[INFO] Epoch: 47 , batch: 124 , training loss: 3.949683
[INFO] Epoch: 47 , batch: 125 , training loss: 3.766295
[INFO] Epoch: 47 , batch: 126 , training loss: 3.783590
[INFO] Epoch: 47 , batch: 127 , training loss: 3.772681
[INFO] Epoch: 47 , batch: 128 , training loss: 3.895144
[INFO] Epoch: 47 , batch: 129 , training loss: 3.880510
[INFO] Epoch: 47 , batch: 130 , training loss: 3.834880
[INFO] Epoch: 47 , batch: 131 , training loss: 3.885404
[INFO] Epoch: 47 , batch: 132 , training loss: 3.844082
[INFO] Epoch: 47 , batch: 133 , training loss: 3.834159
[INFO] Epoch: 47 , batch: 134 , training loss: 3.622180
[INFO] Epoch: 47 , batch: 135 , training loss: 3.656245
[INFO] Epoch: 47 , batch: 136 , training loss: 3.946020
[INFO] Epoch: 47 , batch: 137 , training loss: 3.893423
[INFO] Epoch: 47 , batch: 138 , training loss: 3.917614
[INFO] Epoch: 47 , batch: 139 , training loss: 4.530530
[INFO] Epoch: 47 , batch: 140 , training loss: 4.238813
[INFO] Epoch: 47 , batch: 141 , training loss: 4.016422
[INFO] Epoch: 47 , batch: 142 , training loss: 3.783692
[INFO] Epoch: 47 , batch: 143 , training loss: 3.925901
[INFO] Epoch: 47 , batch: 144 , training loss: 3.748473
[INFO] Epoch: 47 , batch: 145 , training loss: 3.778695
[INFO] Epoch: 47 , batch: 146 , training loss: 3.989976
[INFO] Epoch: 47 , batch: 147 , training loss: 3.683789
[INFO] Epoch: 47 , batch: 148 , training loss: 3.678691
[INFO] Epoch: 47 , batch: 149 , training loss: 3.771108
[INFO] Epoch: 47 , batch: 150 , training loss: 3.959703
[INFO] Epoch: 47 , batch: 151 , training loss: 3.823866
[INFO] Epoch: 47 , batch: 152 , training loss: 3.899855
[INFO] Epoch: 47 , batch: 153 , training loss: 3.876511
[INFO] Epoch: 47 , batch: 154 , training loss: 3.981170
[INFO] Epoch: 47 , batch: 155 , training loss: 4.204645
[INFO] Epoch: 47 , batch: 156 , training loss: 3.909680
[INFO] Epoch: 47 , batch: 157 , training loss: 3.921351
[INFO] Epoch: 47 , batch: 158 , training loss: 4.026790
[INFO] Epoch: 47 , batch: 159 , training loss: 3.834117
[INFO] Epoch: 47 , batch: 160 , training loss: 4.151615
[INFO] Epoch: 47 , batch: 161 , training loss: 4.237339
[INFO] Epoch: 47 , batch: 162 , training loss: 4.208903
[INFO] Epoch: 47 , batch: 163 , training loss: 4.382855
[INFO] Epoch: 47 , batch: 164 , training loss: 4.358553
[INFO] Epoch: 47 , batch: 165 , training loss: 4.281317
[INFO] Epoch: 47 , batch: 166 , training loss: 4.130148
[INFO] Epoch: 47 , batch: 167 , training loss: 4.075045
[INFO] Epoch: 47 , batch: 168 , training loss: 3.828707
[INFO] Epoch: 47 , batch: 169 , training loss: 3.810081
[INFO] Epoch: 47 , batch: 170 , training loss: 4.046091
[INFO] Epoch: 47 , batch: 171 , training loss: 3.445659
[INFO] Epoch: 47 , batch: 172 , training loss: 3.658658
[INFO] Epoch: 47 , batch: 173 , training loss: 4.032917
[INFO] Epoch: 47 , batch: 174 , training loss: 4.450446
[INFO] Epoch: 47 , batch: 175 , training loss: 4.761605
[INFO] Epoch: 47 , batch: 176 , training loss: 4.424308
[INFO] Epoch: 47 , batch: 177 , training loss: 4.084110
[INFO] Epoch: 47 , batch: 178 , training loss: 4.071158
[INFO] Epoch: 47 , batch: 179 , training loss: 4.124967
[INFO] Epoch: 47 , batch: 180 , training loss: 4.075937
[INFO] Epoch: 47 , batch: 181 , training loss: 4.370112
[INFO] Epoch: 47 , batch: 182 , training loss: 4.345927
[INFO] Epoch: 47 , batch: 183 , training loss: 4.264723
[INFO] Epoch: 47 , batch: 184 , training loss: 4.187085
[INFO] Epoch: 47 , batch: 185 , training loss: 4.106641
[INFO] Epoch: 47 , batch: 186 , training loss: 4.290155
[INFO] Epoch: 47 , batch: 187 , training loss: 4.393402
[INFO] Epoch: 47 , batch: 188 , training loss: 4.396278
[INFO] Epoch: 47 , batch: 189 , training loss: 4.284881
[INFO] Epoch: 47 , batch: 190 , training loss: 4.291090
[INFO] Epoch: 47 , batch: 191 , training loss: 4.390050
[INFO] Epoch: 47 , batch: 192 , training loss: 4.246789
[INFO] Epoch: 47 , batch: 193 , training loss: 4.338314
[INFO] Epoch: 47 , batch: 194 , training loss: 4.277913
[INFO] Epoch: 47 , batch: 195 , training loss: 4.196753
[INFO] Epoch: 47 , batch: 196 , training loss: 4.059971
[INFO] Epoch: 47 , batch: 197 , training loss: 4.168502
[INFO] Epoch: 47 , batch: 198 , training loss: 4.069589
[INFO] Epoch: 47 , batch: 199 , training loss: 4.224778
[INFO] Epoch: 47 , batch: 200 , training loss: 4.127272
[INFO] Epoch: 47 , batch: 201 , training loss: 4.041784
[INFO] Epoch: 47 , batch: 202 , training loss: 4.013695
[INFO] Epoch: 47 , batch: 203 , training loss: 4.134388
[INFO] Epoch: 47 , batch: 204 , training loss: 4.288608
[INFO] Epoch: 47 , batch: 205 , training loss: 3.845763
[INFO] Epoch: 47 , batch: 206 , training loss: 3.777108
[INFO] Epoch: 47 , batch: 207 , training loss: 3.787071
[INFO] Epoch: 47 , batch: 208 , training loss: 4.116620
[INFO] Epoch: 47 , batch: 209 , training loss: 4.039459
[INFO] Epoch: 47 , batch: 210 , training loss: 4.082547
[INFO] Epoch: 47 , batch: 211 , training loss: 4.070010
[INFO] Epoch: 47 , batch: 212 , training loss: 4.182381
[INFO] Epoch: 47 , batch: 213 , training loss: 4.106341
[INFO] Epoch: 47 , batch: 214 , training loss: 4.212686
[INFO] Epoch: 47 , batch: 215 , training loss: 4.435838
[INFO] Epoch: 47 , batch: 216 , training loss: 4.115943
[INFO] Epoch: 47 , batch: 217 , training loss: 4.093335
[INFO] Epoch: 47 , batch: 218 , training loss: 4.076557
[INFO] Epoch: 47 , batch: 219 , training loss: 4.160390
[INFO] Epoch: 47 , batch: 220 , training loss: 3.983154
[INFO] Epoch: 47 , batch: 221 , training loss: 4.001389
[INFO] Epoch: 47 , batch: 222 , training loss: 4.145197
[INFO] Epoch: 47 , batch: 223 , training loss: 4.247270
[INFO] Epoch: 47 , batch: 224 , training loss: 4.299939
[INFO] Epoch: 47 , batch: 225 , training loss: 4.182285
[INFO] Epoch: 47 , batch: 226 , training loss: 4.276389
[INFO] Epoch: 47 , batch: 227 , training loss: 4.259753
[INFO] Epoch: 47 , batch: 228 , training loss: 4.275685
[INFO] Epoch: 47 , batch: 229 , training loss: 4.150418
[INFO] Epoch: 47 , batch: 230 , training loss: 4.029542
[INFO] Epoch: 47 , batch: 231 , training loss: 3.887887
[INFO] Epoch: 47 , batch: 232 , training loss: 4.018042
[INFO] Epoch: 47 , batch: 233 , training loss: 4.040895
[INFO] Epoch: 47 , batch: 234 , training loss: 3.738567
[INFO] Epoch: 47 , batch: 235 , training loss: 3.837628
[INFO] Epoch: 47 , batch: 236 , training loss: 3.923142
[INFO] Epoch: 47 , batch: 237 , training loss: 4.161582
[INFO] Epoch: 47 , batch: 238 , training loss: 3.947201
[INFO] Epoch: 47 , batch: 239 , training loss: 3.972487
[INFO] Epoch: 47 , batch: 240 , training loss: 4.056853
[INFO] Epoch: 47 , batch: 241 , training loss: 3.850224
[INFO] Epoch: 47 , batch: 242 , training loss: 3.870262
[INFO] Epoch: 47 , batch: 243 , training loss: 4.159762
[INFO] Epoch: 47 , batch: 244 , training loss: 4.097783
[INFO] Epoch: 47 , batch: 245 , training loss: 4.069247
[INFO] Epoch: 47 , batch: 246 , training loss: 3.777892
[INFO] Epoch: 47 , batch: 247 , training loss: 3.925162
[INFO] Epoch: 47 , batch: 248 , training loss: 4.006549
[INFO] Epoch: 47 , batch: 249 , training loss: 4.051596
[INFO] Epoch: 47 , batch: 250 , training loss: 3.801827
[INFO] Epoch: 47 , batch: 251 , training loss: 4.230361
[INFO] Epoch: 47 , batch: 252 , training loss: 3.942233
[INFO] Epoch: 47 , batch: 253 , training loss: 3.848211
[INFO] Epoch: 47 , batch: 254 , training loss: 4.137281
[INFO] Epoch: 47 , batch: 255 , training loss: 4.104190
[INFO] Epoch: 47 , batch: 256 , training loss: 4.104555
[INFO] Epoch: 47 , batch: 257 , training loss: 4.255076
[INFO] Epoch: 47 , batch: 258 , training loss: 4.287446
[INFO] Epoch: 47 , batch: 259 , training loss: 4.336013
[INFO] Epoch: 47 , batch: 260 , training loss: 4.075684
[INFO] Epoch: 47 , batch: 261 , training loss: 4.204656
[INFO] Epoch: 47 , batch: 262 , training loss: 4.370813
[INFO] Epoch: 47 , batch: 263 , training loss: 4.592816
[INFO] Epoch: 47 , batch: 264 , training loss: 3.907370
[INFO] Epoch: 47 , batch: 265 , training loss: 4.037976
[INFO] Epoch: 47 , batch: 266 , training loss: 4.452041
[INFO] Epoch: 47 , batch: 267 , training loss: 4.212906
[INFO] Epoch: 47 , batch: 268 , training loss: 4.127430
[INFO] Epoch: 47 , batch: 269 , training loss: 4.106452
[INFO] Epoch: 47 , batch: 270 , training loss: 4.123304
[INFO] Epoch: 47 , batch: 271 , training loss: 4.140079
[INFO] Epoch: 47 , batch: 272 , training loss: 4.121415
[INFO] Epoch: 47 , batch: 273 , training loss: 4.183289
[INFO] Epoch: 47 , batch: 274 , training loss: 4.233285
[INFO] Epoch: 47 , batch: 275 , training loss: 4.102437
[INFO] Epoch: 47 , batch: 276 , training loss: 4.125638
[INFO] Epoch: 47 , batch: 277 , training loss: 4.284340
[INFO] Epoch: 47 , batch: 278 , training loss: 3.996706
[INFO] Epoch: 47 , batch: 279 , training loss: 4.004347
[INFO] Epoch: 47 , batch: 280 , training loss: 3.966146
[INFO] Epoch: 47 , batch: 281 , training loss: 4.104691
[INFO] Epoch: 47 , batch: 282 , training loss: 4.005455
[INFO] Epoch: 47 , batch: 283 , training loss: 4.029261
[INFO] Epoch: 47 , batch: 284 , training loss: 4.063791
[INFO] Epoch: 47 , batch: 285 , training loss: 4.002394
[INFO] Epoch: 47 , batch: 286 , training loss: 3.973011
[INFO] Epoch: 47 , batch: 287 , training loss: 3.935503
[INFO] Epoch: 47 , batch: 288 , training loss: 3.932856
[INFO] Epoch: 47 , batch: 289 , training loss: 3.989985
[INFO] Epoch: 47 , batch: 290 , training loss: 3.792078
[INFO] Epoch: 47 , batch: 291 , training loss: 3.753029
[INFO] Epoch: 47 , batch: 292 , training loss: 3.864403
[INFO] Epoch: 47 , batch: 293 , training loss: 3.790452
[INFO] Epoch: 47 , batch: 294 , training loss: 4.463630
[INFO] Epoch: 47 , batch: 295 , training loss: 4.212342
[INFO] Epoch: 47 , batch: 296 , training loss: 4.153496
[INFO] Epoch: 47 , batch: 297 , training loss: 4.116325
[INFO] Epoch: 47 , batch: 298 , training loss: 3.934271
[INFO] Epoch: 47 , batch: 299 , training loss: 3.966261
[INFO] Epoch: 47 , batch: 300 , training loss: 3.919311
[INFO] Epoch: 47 , batch: 301 , training loss: 3.886935
[INFO] Epoch: 47 , batch: 302 , training loss: 4.062538
[INFO] Epoch: 47 , batch: 303 , training loss: 4.076179
[INFO] Epoch: 47 , batch: 304 , training loss: 4.215061
[INFO] Epoch: 47 , batch: 305 , training loss: 4.024705
[INFO] Epoch: 47 , batch: 306 , training loss: 4.140720
[INFO] Epoch: 47 , batch: 307 , training loss: 4.134994
[INFO] Epoch: 47 , batch: 308 , training loss: 4.001879
[INFO] Epoch: 47 , batch: 309 , training loss: 3.969373
[INFO] Epoch: 47 , batch: 310 , training loss: 3.909883
[INFO] Epoch: 47 , batch: 311 , training loss: 3.897167
[INFO] Epoch: 47 , batch: 312 , training loss: 3.827118
[INFO] Epoch: 47 , batch: 313 , training loss: 3.927854
[INFO] Epoch: 47 , batch: 314 , training loss: 3.977174
[INFO] Epoch: 47 , batch: 315 , training loss: 4.023245
[INFO] Epoch: 47 , batch: 316 , training loss: 4.266566
[INFO] Epoch: 47 , batch: 317 , training loss: 4.689226
[INFO] Epoch: 47 , batch: 318 , training loss: 4.858676
[INFO] Epoch: 47 , batch: 319 , training loss: 4.464723
[INFO] Epoch: 47 , batch: 320 , training loss: 3.978035
[INFO] Epoch: 47 , batch: 321 , training loss: 3.851510
[INFO] Epoch: 47 , batch: 322 , training loss: 3.937893
[INFO] Epoch: 47 , batch: 323 , training loss: 3.978054
[INFO] Epoch: 47 , batch: 324 , training loss: 3.924886
[INFO] Epoch: 47 , batch: 325 , training loss: 4.095344
[INFO] Epoch: 47 , batch: 326 , training loss: 4.156460
[INFO] Epoch: 47 , batch: 327 , training loss: 4.045092
[INFO] Epoch: 47 , batch: 328 , training loss: 4.042462
[INFO] Epoch: 47 , batch: 329 , training loss: 3.974125
[INFO] Epoch: 47 , batch: 330 , training loss: 3.976638
[INFO] Epoch: 47 , batch: 331 , training loss: 4.125838
[INFO] Epoch: 47 , batch: 332 , training loss: 3.934883
[INFO] Epoch: 47 , batch: 333 , training loss: 3.946205
[INFO] Epoch: 47 , batch: 334 , training loss: 3.928401
[INFO] Epoch: 47 , batch: 335 , training loss: 4.076264
[INFO] Epoch: 47 , batch: 336 , training loss: 4.076235
[INFO] Epoch: 47 , batch: 337 , training loss: 4.105639
[INFO] Epoch: 47 , batch: 338 , training loss: 4.333454
[INFO] Epoch: 47 , batch: 339 , training loss: 4.181338
[INFO] Epoch: 47 , batch: 340 , training loss: 4.326226
[INFO] Epoch: 47 , batch: 341 , training loss: 4.111966
[INFO] Epoch: 47 , batch: 342 , training loss: 3.861668
[INFO] Epoch: 47 , batch: 343 , training loss: 3.938119
[INFO] Epoch: 47 , batch: 344 , training loss: 3.819200
[INFO] Epoch: 47 , batch: 345 , training loss: 3.931489
[INFO] Epoch: 47 , batch: 346 , training loss: 3.991561
[INFO] Epoch: 47 , batch: 347 , training loss: 3.890895
[INFO] Epoch: 47 , batch: 348 , training loss: 3.976265
[INFO] Epoch: 47 , batch: 349 , training loss: 4.154293
[INFO] Epoch: 47 , batch: 350 , training loss: 3.952854
[INFO] Epoch: 47 , batch: 351 , training loss: 4.018769
[INFO] Epoch: 47 , batch: 352 , training loss: 4.032301
[INFO] Epoch: 47 , batch: 353 , training loss: 4.000772
[INFO] Epoch: 47 , batch: 354 , training loss: 4.102752
[INFO] Epoch: 47 , batch: 355 , training loss: 4.168123
[INFO] Epoch: 47 , batch: 356 , training loss: 3.984648
[INFO] Epoch: 47 , batch: 357 , training loss: 4.072961
[INFO] Epoch: 47 , batch: 358 , training loss: 3.956160
[INFO] Epoch: 47 , batch: 359 , training loss: 3.978314
[INFO] Epoch: 47 , batch: 360 , training loss: 4.066194
[INFO] Epoch: 47 , batch: 361 , training loss: 4.044359
[INFO] Epoch: 47 , batch: 362 , training loss: 4.121936
[INFO] Epoch: 47 , batch: 363 , training loss: 3.996009
[INFO] Epoch: 47 , batch: 364 , training loss: 4.049170
[INFO] Epoch: 47 , batch: 365 , training loss: 3.988966
[INFO] Epoch: 47 , batch: 366 , training loss: 4.120596
[INFO] Epoch: 47 , batch: 367 , training loss: 4.170958
[INFO] Epoch: 47 , batch: 368 , training loss: 4.578126
[INFO] Epoch: 47 , batch: 369 , training loss: 4.240744
[INFO] Epoch: 47 , batch: 370 , training loss: 3.998079
[INFO] Epoch: 47 , batch: 371 , training loss: 4.370479
[INFO] Epoch: 47 , batch: 372 , training loss: 4.627925
[INFO] Epoch: 47 , batch: 373 , training loss: 4.722987
[INFO] Epoch: 47 , batch: 374 , training loss: 4.809755
[INFO] Epoch: 47 , batch: 375 , training loss: 4.801691
[INFO] Epoch: 47 , batch: 376 , training loss: 4.721097
[INFO] Epoch: 47 , batch: 377 , training loss: 4.492268
[INFO] Epoch: 47 , batch: 378 , training loss: 4.591169
[INFO] Epoch: 47 , batch: 379 , training loss: 4.562954
[INFO] Epoch: 47 , batch: 380 , training loss: 4.689275
[INFO] Epoch: 47 , batch: 381 , training loss: 4.453712
[INFO] Epoch: 47 , batch: 382 , training loss: 4.648687
[INFO] Epoch: 47 , batch: 383 , training loss: 4.640388
[INFO] Epoch: 47 , batch: 384 , training loss: 4.655869
[INFO] Epoch: 47 , batch: 385 , training loss: 4.350383
[INFO] Epoch: 47 , batch: 386 , training loss: 4.626009
[INFO] Epoch: 47 , batch: 387 , training loss: 4.573265
[INFO] Epoch: 47 , batch: 388 , training loss: 4.402656
[INFO] Epoch: 47 , batch: 389 , training loss: 4.249488
[INFO] Epoch: 47 , batch: 390 , training loss: 4.237864
[INFO] Epoch: 47 , batch: 391 , training loss: 4.284799
[INFO] Epoch: 47 , batch: 392 , training loss: 4.641072
[INFO] Epoch: 47 , batch: 393 , training loss: 4.526593
[INFO] Epoch: 47 , batch: 394 , training loss: 4.582252
[INFO] Epoch: 47 , batch: 395 , training loss: 4.444028
[INFO] Epoch: 47 , batch: 396 , training loss: 4.228629
[INFO] Epoch: 47 , batch: 397 , training loss: 4.379983
[INFO] Epoch: 47 , batch: 398 , training loss: 4.251779
[INFO] Epoch: 47 , batch: 399 , training loss: 4.314954
[INFO] Epoch: 47 , batch: 400 , training loss: 4.277150
[INFO] Epoch: 47 , batch: 401 , training loss: 4.685174
[INFO] Epoch: 47 , batch: 402 , training loss: 4.410927
[INFO] Epoch: 47 , batch: 403 , training loss: 4.236019
[INFO] Epoch: 47 , batch: 404 , training loss: 4.420732
[INFO] Epoch: 47 , batch: 405 , training loss: 4.468757
[INFO] Epoch: 47 , batch: 406 , training loss: 4.391881
[INFO] Epoch: 47 , batch: 407 , training loss: 4.428927
[INFO] Epoch: 47 , batch: 408 , training loss: 4.396172
[INFO] Epoch: 47 , batch: 409 , training loss: 4.382849
[INFO] Epoch: 47 , batch: 410 , training loss: 4.437293
[INFO] Epoch: 47 , batch: 411 , training loss: 4.611833
[INFO] Epoch: 47 , batch: 412 , training loss: 4.460371
[INFO] Epoch: 47 , batch: 413 , training loss: 4.348326
[INFO] Epoch: 47 , batch: 414 , training loss: 4.394562
[INFO] Epoch: 47 , batch: 415 , training loss: 4.413280
[INFO] Epoch: 47 , batch: 416 , training loss: 4.491418
[INFO] Epoch: 47 , batch: 417 , training loss: 4.411074
[INFO] Epoch: 47 , batch: 418 , training loss: 4.434912
[INFO] Epoch: 47 , batch: 419 , training loss: 4.401515
[INFO] Epoch: 47 , batch: 420 , training loss: 4.359758
[INFO] Epoch: 47 , batch: 421 , training loss: 4.343788
[INFO] Epoch: 47 , batch: 422 , training loss: 4.227074
[INFO] Epoch: 47 , batch: 423 , training loss: 4.423783
[INFO] Epoch: 47 , batch: 424 , training loss: 4.586221
[INFO] Epoch: 47 , batch: 425 , training loss: 4.462209
[INFO] Epoch: 47 , batch: 426 , training loss: 4.209691
[INFO] Epoch: 47 , batch: 427 , training loss: 4.435939
[INFO] Epoch: 47 , batch: 428 , training loss: 4.306240
[INFO] Epoch: 47 , batch: 429 , training loss: 4.168489
[INFO] Epoch: 47 , batch: 430 , training loss: 4.427895
[INFO] Epoch: 47 , batch: 431 , training loss: 4.044300
[INFO] Epoch: 47 , batch: 432 , training loss: 4.098350
[INFO] Epoch: 47 , batch: 433 , training loss: 4.124177
[INFO] Epoch: 47 , batch: 434 , training loss: 4.000747
[INFO] Epoch: 47 , batch: 435 , training loss: 4.382201
[INFO] Epoch: 47 , batch: 436 , training loss: 4.452548
[INFO] Epoch: 47 , batch: 437 , training loss: 4.199231
[INFO] Epoch: 47 , batch: 438 , training loss: 4.047897
[INFO] Epoch: 47 , batch: 439 , training loss: 4.273536
[INFO] Epoch: 47 , batch: 440 , training loss: 4.386962
[INFO] Epoch: 47 , batch: 441 , training loss: 4.490052
[INFO] Epoch: 47 , batch: 442 , training loss: 4.266937
[INFO] Epoch: 47 , batch: 443 , training loss: 4.457502
[INFO] Epoch: 47 , batch: 444 , training loss: 4.094433
[INFO] Epoch: 47 , batch: 445 , training loss: 3.981308
[INFO] Epoch: 47 , batch: 446 , training loss: 3.936225
[INFO] Epoch: 47 , batch: 447 , training loss: 4.112046
[INFO] Epoch: 47 , batch: 448 , training loss: 4.205090
[INFO] Epoch: 47 , batch: 449 , training loss: 4.605446
[INFO] Epoch: 47 , batch: 450 , training loss: 4.651880
[INFO] Epoch: 47 , batch: 451 , training loss: 4.570247
[INFO] Epoch: 47 , batch: 452 , training loss: 4.373216
[INFO] Epoch: 47 , batch: 453 , training loss: 4.151907
[INFO] Epoch: 47 , batch: 454 , training loss: 4.291026
[INFO] Epoch: 47 , batch: 455 , training loss: 4.343991
[INFO] Epoch: 47 , batch: 456 , training loss: 4.335358
[INFO] Epoch: 47 , batch: 457 , training loss: 4.422406
[INFO] Epoch: 47 , batch: 458 , training loss: 4.163173
[INFO] Epoch: 47 , batch: 459 , training loss: 4.152874
[INFO] Epoch: 47 , batch: 460 , training loss: 4.229568
[INFO] Epoch: 47 , batch: 461 , training loss: 4.204637
[INFO] Epoch: 47 , batch: 462 , training loss: 4.283410
[INFO] Epoch: 47 , batch: 463 , training loss: 4.198182
[INFO] Epoch: 47 , batch: 464 , training loss: 4.388752
[INFO] Epoch: 47 , batch: 465 , training loss: 4.299719
[INFO] Epoch: 47 , batch: 466 , training loss: 4.401016
[INFO] Epoch: 47 , batch: 467 , training loss: 4.370787
[INFO] Epoch: 47 , batch: 468 , training loss: 4.340070
[INFO] Epoch: 47 , batch: 469 , training loss: 4.354941
[INFO] Epoch: 47 , batch: 470 , training loss: 4.170551
[INFO] Epoch: 47 , batch: 471 , training loss: 4.263004
[INFO] Epoch: 47 , batch: 472 , training loss: 4.311583
[INFO] Epoch: 47 , batch: 473 , training loss: 4.251691
[INFO] Epoch: 47 , batch: 474 , training loss: 4.055224
[INFO] Epoch: 47 , batch: 475 , training loss: 3.917021
[INFO] Epoch: 47 , batch: 476 , training loss: 4.281459
[INFO] Epoch: 47 , batch: 477 , training loss: 4.447540
[INFO] Epoch: 47 , batch: 478 , training loss: 4.451897
[INFO] Epoch: 47 , batch: 479 , training loss: 4.416811
[INFO] Epoch: 47 , batch: 480 , training loss: 4.519013
[INFO] Epoch: 47 , batch: 481 , training loss: 4.401756
[INFO] Epoch: 47 , batch: 482 , training loss: 4.502737
[INFO] Epoch: 47 , batch: 483 , training loss: 4.388299
[INFO] Epoch: 47 , batch: 484 , training loss: 4.171795
[INFO] Epoch: 47 , batch: 485 , training loss: 4.253409
[INFO] Epoch: 47 , batch: 486 , training loss: 4.145215
[INFO] Epoch: 47 , batch: 487 , training loss: 4.163203
[INFO] Epoch: 47 , batch: 488 , training loss: 4.348184
[INFO] Epoch: 47 , batch: 489 , training loss: 4.217751
[INFO] Epoch: 47 , batch: 490 , training loss: 4.258460
[INFO] Epoch: 47 , batch: 491 , training loss: 4.195981
[INFO] Epoch: 47 , batch: 492 , training loss: 4.180563
[INFO] Epoch: 47 , batch: 493 , training loss: 4.361016
[INFO] Epoch: 47 , batch: 494 , training loss: 4.286984
[INFO] Epoch: 47 , batch: 495 , training loss: 4.397916
[INFO] Epoch: 47 , batch: 496 , training loss: 4.282229
[INFO] Epoch: 47 , batch: 497 , training loss: 4.325134
[INFO] Epoch: 47 , batch: 498 , training loss: 4.328898
[INFO] Epoch: 47 , batch: 499 , training loss: 4.373163
[INFO] Epoch: 47 , batch: 500 , training loss: 4.512939
[INFO] Epoch: 47 , batch: 501 , training loss: 4.807535
[INFO] Epoch: 47 , batch: 502 , training loss: 4.856331
[INFO] Epoch: 47 , batch: 503 , training loss: 4.556910
[INFO] Epoch: 47 , batch: 504 , training loss: 4.687337
[INFO] Epoch: 47 , batch: 505 , training loss: 4.655104
[INFO] Epoch: 47 , batch: 506 , training loss: 4.615280
[INFO] Epoch: 47 , batch: 507 , training loss: 4.685986
[INFO] Epoch: 47 , batch: 508 , training loss: 4.643590
[INFO] Epoch: 47 , batch: 509 , training loss: 4.417000
[INFO] Epoch: 47 , batch: 510 , training loss: 4.495766
[INFO] Epoch: 47 , batch: 511 , training loss: 4.386399
[INFO] Epoch: 47 , batch: 512 , training loss: 4.526200
[INFO] Epoch: 47 , batch: 513 , training loss: 4.747529
[INFO] Epoch: 47 , batch: 514 , training loss: 4.416870
[INFO] Epoch: 47 , batch: 515 , training loss: 4.627190
[INFO] Epoch: 47 , batch: 516 , training loss: 4.463676
[INFO] Epoch: 47 , batch: 517 , training loss: 4.407397
[INFO] Epoch: 47 , batch: 518 , training loss: 4.374056
[INFO] Epoch: 47 , batch: 519 , training loss: 4.221445
[INFO] Epoch: 47 , batch: 520 , training loss: 4.445540
[INFO] Epoch: 47 , batch: 521 , training loss: 4.454829
[INFO] Epoch: 47 , batch: 522 , training loss: 4.497265
[INFO] Epoch: 47 , batch: 523 , training loss: 4.389363
[INFO] Epoch: 47 , batch: 524 , training loss: 4.698461
[INFO] Epoch: 47 , batch: 525 , training loss: 4.643201
[INFO] Epoch: 47 , batch: 526 , training loss: 4.393863
[INFO] Epoch: 47 , batch: 527 , training loss: 4.443244
[INFO] Epoch: 47 , batch: 528 , training loss: 4.449920
[INFO] Epoch: 47 , batch: 529 , training loss: 4.428147
[INFO] Epoch: 47 , batch: 530 , training loss: 4.270491
[INFO] Epoch: 47 , batch: 531 , training loss: 4.400606
[INFO] Epoch: 47 , batch: 532 , training loss: 4.310558
[INFO] Epoch: 47 , batch: 533 , training loss: 4.457800
[INFO] Epoch: 47 , batch: 534 , training loss: 4.432125
[INFO] Epoch: 47 , batch: 535 , training loss: 4.444193
[INFO] Epoch: 47 , batch: 536 , training loss: 4.276551
[INFO] Epoch: 47 , batch: 537 , training loss: 4.295777
[INFO] Epoch: 47 , batch: 538 , training loss: 4.368488
[INFO] Epoch: 47 , batch: 539 , training loss: 4.464523
[INFO] Epoch: 47 , batch: 540 , training loss: 4.943141
[INFO] Epoch: 47 , batch: 541 , training loss: 4.798779
[INFO] Epoch: 47 , batch: 542 , training loss: 4.732165
[INFO] Epoch: 48 , batch: 0 , training loss: 3.282464
[INFO] Epoch: 48 , batch: 1 , training loss: 3.369780
[INFO] Epoch: 48 , batch: 2 , training loss: 3.637758
[INFO] Epoch: 48 , batch: 3 , training loss: 3.449248
[INFO] Epoch: 48 , batch: 4 , training loss: 3.789134
[INFO] Epoch: 48 , batch: 5 , training loss: 3.428417
[INFO] Epoch: 48 , batch: 6 , training loss: 3.823726
[INFO] Epoch: 48 , batch: 7 , training loss: 3.770914
[INFO] Epoch: 48 , batch: 8 , training loss: 3.477368
[INFO] Epoch: 48 , batch: 9 , training loss: 3.692379
[INFO] Epoch: 48 , batch: 10 , training loss: 3.706930
[INFO] Epoch: 48 , batch: 11 , training loss: 3.643313
[INFO] Epoch: 48 , batch: 12 , training loss: 3.584017
[INFO] Epoch: 48 , batch: 13 , training loss: 3.597588
[INFO] Epoch: 48 , batch: 14 , training loss: 3.486384
[INFO] Epoch: 48 , batch: 15 , training loss: 3.664193
[INFO] Epoch: 48 , batch: 16 , training loss: 3.515246
[INFO] Epoch: 48 , batch: 17 , training loss: 3.671614
[INFO] Epoch: 48 , batch: 18 , training loss: 3.643543
[INFO] Epoch: 48 , batch: 19 , training loss: 3.411439
[INFO] Epoch: 48 , batch: 20 , training loss: 3.365978
[INFO] Epoch: 48 , batch: 21 , training loss: 3.525148
[INFO] Epoch: 48 , batch: 22 , training loss: 3.373081
[INFO] Epoch: 48 , batch: 23 , training loss: 3.591099
[INFO] Epoch: 48 , batch: 24 , training loss: 3.391410
[INFO] Epoch: 48 , batch: 25 , training loss: 3.598278
[INFO] Epoch: 48 , batch: 26 , training loss: 3.445939
[INFO] Epoch: 48 , batch: 27 , training loss: 3.425930
[INFO] Epoch: 48 , batch: 28 , training loss: 3.613965
[INFO] Epoch: 48 , batch: 29 , training loss: 3.416030
[INFO] Epoch: 48 , batch: 30 , training loss: 3.488117
[INFO] Epoch: 48 , batch: 31 , training loss: 3.534563
[INFO] Epoch: 48 , batch: 32 , training loss: 3.503970
[INFO] Epoch: 48 , batch: 33 , training loss: 3.523467
[INFO] Epoch: 48 , batch: 34 , training loss: 3.502909
[INFO] Epoch: 48 , batch: 35 , training loss: 3.520133
[INFO] Epoch: 48 , batch: 36 , training loss: 3.567399
[INFO] Epoch: 48 , batch: 37 , training loss: 3.453604
[INFO] Epoch: 48 , batch: 38 , training loss: 3.435782
[INFO] Epoch: 48 , batch: 39 , training loss: 3.333947
[INFO] Epoch: 48 , batch: 40 , training loss: 3.560685
[INFO] Epoch: 48 , batch: 41 , training loss: 3.466111
[INFO] Epoch: 48 , batch: 42 , training loss: 3.901608
[INFO] Epoch: 48 , batch: 43 , training loss: 3.614045
[INFO] Epoch: 48 , batch: 44 , training loss: 4.002011
[INFO] Epoch: 48 , batch: 45 , training loss: 3.869369
[INFO] Epoch: 48 , batch: 46 , training loss: 3.820290
[INFO] Epoch: 48 , batch: 47 , training loss: 3.553984
[INFO] Epoch: 48 , batch: 48 , training loss: 3.554256
[INFO] Epoch: 48 , batch: 49 , training loss: 3.725009
[INFO] Epoch: 48 , batch: 50 , training loss: 3.534510
[INFO] Epoch: 48 , batch: 51 , training loss: 3.767250
[INFO] Epoch: 48 , batch: 52 , training loss: 3.614028
[INFO] Epoch: 48 , batch: 53 , training loss: 3.729132
[INFO] Epoch: 48 , batch: 54 , training loss: 3.732252
[INFO] Epoch: 48 , batch: 55 , training loss: 3.809570
[INFO] Epoch: 48 , batch: 56 , training loss: 3.709725
[INFO] Epoch: 48 , batch: 57 , training loss: 3.569867
[INFO] Epoch: 48 , batch: 58 , training loss: 3.654896
[INFO] Epoch: 48 , batch: 59 , training loss: 3.698992
[INFO] Epoch: 48 , batch: 60 , training loss: 3.670784
[INFO] Epoch: 48 , batch: 61 , training loss: 3.756521
[INFO] Epoch: 48 , batch: 62 , training loss: 3.632056
[INFO] Epoch: 48 , batch: 63 , training loss: 3.848090
[INFO] Epoch: 48 , batch: 64 , training loss: 3.998700
[INFO] Epoch: 48 , batch: 65 , training loss: 3.716274
[INFO] Epoch: 48 , batch: 66 , training loss: 3.585945
[INFO] Epoch: 48 , batch: 67 , training loss: 3.682093
[INFO] Epoch: 48 , batch: 68 , training loss: 3.800446
[INFO] Epoch: 48 , batch: 69 , training loss: 3.667670
[INFO] Epoch: 48 , batch: 70 , training loss: 3.888050
[INFO] Epoch: 48 , batch: 71 , training loss: 3.764019
[INFO] Epoch: 48 , batch: 72 , training loss: 3.848417
[INFO] Epoch: 48 , batch: 73 , training loss: 3.778845
[INFO] Epoch: 48 , batch: 74 , training loss: 3.900504
[INFO] Epoch: 48 , batch: 75 , training loss: 3.792976
[INFO] Epoch: 48 , batch: 76 , training loss: 3.810494
[INFO] Epoch: 48 , batch: 77 , training loss: 3.798718
[INFO] Epoch: 48 , batch: 78 , training loss: 3.921926
[INFO] Epoch: 48 , batch: 79 , training loss: 3.737222
[INFO] Epoch: 48 , batch: 80 , training loss: 3.925672
[INFO] Epoch: 48 , batch: 81 , training loss: 3.866005
[INFO] Epoch: 48 , batch: 82 , training loss: 3.820072
[INFO] Epoch: 48 , batch: 83 , training loss: 3.978208
[INFO] Epoch: 48 , batch: 84 , training loss: 3.899945
[INFO] Epoch: 48 , batch: 85 , training loss: 3.986650
[INFO] Epoch: 48 , batch: 86 , training loss: 3.881574
[INFO] Epoch: 48 , batch: 87 , training loss: 3.901161
[INFO] Epoch: 48 , batch: 88 , training loss: 3.985286
[INFO] Epoch: 48 , batch: 89 , training loss: 3.816766
[INFO] Epoch: 48 , batch: 90 , training loss: 3.893802
[INFO] Epoch: 48 , batch: 91 , training loss: 3.829609
[INFO] Epoch: 48 , batch: 92 , training loss: 3.826418
[INFO] Epoch: 48 , batch: 93 , training loss: 3.926526
[INFO] Epoch: 48 , batch: 94 , training loss: 4.071550
[INFO] Epoch: 48 , batch: 95 , training loss: 3.820931
[INFO] Epoch: 48 , batch: 96 , training loss: 3.849928
[INFO] Epoch: 48 , batch: 97 , training loss: 3.756892
[INFO] Epoch: 48 , batch: 98 , training loss: 3.743375
[INFO] Epoch: 48 , batch: 99 , training loss: 3.819275
[INFO] Epoch: 48 , batch: 100 , training loss: 3.787743
[INFO] Epoch: 48 , batch: 101 , training loss: 3.796619
[INFO] Epoch: 48 , batch: 102 , training loss: 3.927695
[INFO] Epoch: 48 , batch: 103 , training loss: 3.696496
[INFO] Epoch: 48 , batch: 104 , training loss: 3.670002
[INFO] Epoch: 48 , batch: 105 , training loss: 3.932943
[INFO] Epoch: 48 , batch: 106 , training loss: 3.906655
[INFO] Epoch: 48 , batch: 107 , training loss: 3.781983
[INFO] Epoch: 48 , batch: 108 , training loss: 3.714298
[INFO] Epoch: 48 , batch: 109 , training loss: 3.654288
[INFO] Epoch: 48 , batch: 110 , training loss: 3.806094
[INFO] Epoch: 48 , batch: 111 , training loss: 3.899652
[INFO] Epoch: 48 , batch: 112 , training loss: 3.811873
[INFO] Epoch: 48 , batch: 113 , training loss: 3.828852
[INFO] Epoch: 48 , batch: 114 , training loss: 3.798906
[INFO] Epoch: 48 , batch: 115 , training loss: 3.823946
[INFO] Epoch: 48 , batch: 116 , training loss: 3.714805
[INFO] Epoch: 48 , batch: 117 , training loss: 3.952662
[INFO] Epoch: 48 , batch: 118 , training loss: 3.908067
[INFO] Epoch: 48 , batch: 119 , training loss: 4.029524
[INFO] Epoch: 48 , batch: 120 , training loss: 4.048807
[INFO] Epoch: 48 , batch: 121 , training loss: 3.876966
[INFO] Epoch: 48 , batch: 122 , training loss: 3.812446
[INFO] Epoch: 48 , batch: 123 , training loss: 3.814745
[INFO] Epoch: 48 , batch: 124 , training loss: 3.917810
[INFO] Epoch: 48 , batch: 125 , training loss: 3.750715
[INFO] Epoch: 48 , batch: 126 , training loss: 3.756641
[INFO] Epoch: 48 , batch: 127 , training loss: 3.745135
[INFO] Epoch: 48 , batch: 128 , training loss: 3.883138
[INFO] Epoch: 48 , batch: 129 , training loss: 3.853120
[INFO] Epoch: 48 , batch: 130 , training loss: 3.837703
[INFO] Epoch: 48 , batch: 131 , training loss: 3.868280
[INFO] Epoch: 48 , batch: 132 , training loss: 3.842207
[INFO] Epoch: 48 , batch: 133 , training loss: 3.815032
[INFO] Epoch: 48 , batch: 134 , training loss: 3.632950
[INFO] Epoch: 48 , batch: 135 , training loss: 3.651829
[INFO] Epoch: 48 , batch: 136 , training loss: 3.954604
[INFO] Epoch: 48 , batch: 137 , training loss: 3.891532
[INFO] Epoch: 48 , batch: 138 , training loss: 3.920254
[INFO] Epoch: 48 , batch: 139 , training loss: 4.461477
[INFO] Epoch: 48 , batch: 140 , training loss: 4.213089
[INFO] Epoch: 48 , batch: 141 , training loss: 4.000090
[INFO] Epoch: 48 , batch: 142 , training loss: 3.774642
[INFO] Epoch: 48 , batch: 143 , training loss: 3.902071
[INFO] Epoch: 48 , batch: 144 , training loss: 3.735868
[INFO] Epoch: 48 , batch: 145 , training loss: 3.770452
[INFO] Epoch: 48 , batch: 146 , training loss: 3.993047
[INFO] Epoch: 48 , batch: 147 , training loss: 3.666179
[INFO] Epoch: 48 , batch: 148 , training loss: 3.659640
[INFO] Epoch: 48 , batch: 149 , training loss: 3.756394
[INFO] Epoch: 48 , batch: 150 , training loss: 3.921902
[INFO] Epoch: 48 , batch: 151 , training loss: 3.837931
[INFO] Epoch: 48 , batch: 152 , training loss: 3.886353
[INFO] Epoch: 48 , batch: 153 , training loss: 3.857693
[INFO] Epoch: 48 , batch: 154 , training loss: 3.940148
[INFO] Epoch: 48 , batch: 155 , training loss: 4.157133
[INFO] Epoch: 48 , batch: 156 , training loss: 3.884501
[INFO] Epoch: 48 , batch: 157 , training loss: 3.855788
[INFO] Epoch: 48 , batch: 158 , training loss: 3.998882
[INFO] Epoch: 48 , batch: 159 , training loss: 3.852475
[INFO] Epoch: 48 , batch: 160 , training loss: 4.130022
[INFO] Epoch: 48 , batch: 161 , training loss: 4.260008
[INFO] Epoch: 48 , batch: 162 , training loss: 4.217626
[INFO] Epoch: 48 , batch: 163 , training loss: 4.355098
[INFO] Epoch: 48 , batch: 164 , training loss: 4.361861
[INFO] Epoch: 48 , batch: 165 , training loss: 4.266997
[INFO] Epoch: 48 , batch: 166 , training loss: 4.091539
[INFO] Epoch: 48 , batch: 167 , training loss: 4.087473
[INFO] Epoch: 48 , batch: 168 , training loss: 3.776490
[INFO] Epoch: 48 , batch: 169 , training loss: 3.783803
[INFO] Epoch: 48 , batch: 170 , training loss: 3.977263
[INFO] Epoch: 48 , batch: 171 , training loss: 3.404216
[INFO] Epoch: 48 , batch: 172 , training loss: 3.639472
[INFO] Epoch: 48 , batch: 173 , training loss: 3.984934
[INFO] Epoch: 48 , batch: 174 , training loss: 4.433107
[INFO] Epoch: 48 , batch: 175 , training loss: 4.769729
[INFO] Epoch: 48 , batch: 176 , training loss: 4.384389
[INFO] Epoch: 48 , batch: 177 , training loss: 4.061167
[INFO] Epoch: 48 , batch: 178 , training loss: 4.038908
[INFO] Epoch: 48 , batch: 179 , training loss: 4.109724
[INFO] Epoch: 48 , batch: 180 , training loss: 4.058743
[INFO] Epoch: 48 , batch: 181 , training loss: 4.356751
[INFO] Epoch: 48 , batch: 182 , training loss: 4.318813
[INFO] Epoch: 48 , batch: 183 , training loss: 4.239246
[INFO] Epoch: 48 , batch: 184 , training loss: 4.159975
[INFO] Epoch: 48 , batch: 185 , training loss: 4.089259
[INFO] Epoch: 48 , batch: 186 , training loss: 4.275527
[INFO] Epoch: 48 , batch: 187 , training loss: 4.390001
[INFO] Epoch: 48 , batch: 188 , training loss: 4.375656
[INFO] Epoch: 48 , batch: 189 , training loss: 4.281066
[INFO] Epoch: 48 , batch: 190 , training loss: 4.298237
[INFO] Epoch: 48 , batch: 191 , training loss: 4.388795
[INFO] Epoch: 48 , batch: 192 , training loss: 4.217526
[INFO] Epoch: 48 , batch: 193 , training loss: 4.327833
[INFO] Epoch: 48 , batch: 194 , training loss: 4.249433
[INFO] Epoch: 48 , batch: 195 , training loss: 4.191745
[INFO] Epoch: 48 , batch: 196 , training loss: 4.053525
[INFO] Epoch: 48 , batch: 197 , training loss: 4.125060
[INFO] Epoch: 48 , batch: 198 , training loss: 4.038065
[INFO] Epoch: 48 , batch: 199 , training loss: 4.195450
[INFO] Epoch: 48 , batch: 200 , training loss: 4.125240
[INFO] Epoch: 48 , batch: 201 , training loss: 4.015825
[INFO] Epoch: 48 , batch: 202 , training loss: 4.003103
[INFO] Epoch: 48 , batch: 203 , training loss: 4.126684
[INFO] Epoch: 48 , batch: 204 , training loss: 4.262906
[INFO] Epoch: 48 , batch: 205 , training loss: 3.832673
[INFO] Epoch: 48 , batch: 206 , training loss: 3.755676
[INFO] Epoch: 48 , batch: 207 , training loss: 3.770459
[INFO] Epoch: 48 , batch: 208 , training loss: 4.084677
[INFO] Epoch: 48 , batch: 209 , training loss: 4.013851
[INFO] Epoch: 48 , batch: 210 , training loss: 4.072298
[INFO] Epoch: 48 , batch: 211 , training loss: 4.072680
[INFO] Epoch: 48 , batch: 212 , training loss: 4.158541
[INFO] Epoch: 48 , batch: 213 , training loss: 4.086067
[INFO] Epoch: 48 , batch: 214 , training loss: 4.195681
[INFO] Epoch: 48 , batch: 215 , training loss: 4.414805
[INFO] Epoch: 48 , batch: 216 , training loss: 4.090452
[INFO] Epoch: 48 , batch: 217 , training loss: 4.075570
[INFO] Epoch: 48 , batch: 218 , training loss: 4.037620
[INFO] Epoch: 48 , batch: 219 , training loss: 4.146005
[INFO] Epoch: 48 , batch: 220 , training loss: 3.974296
[INFO] Epoch: 48 , batch: 221 , training loss: 3.995726
[INFO] Epoch: 48 , batch: 222 , training loss: 4.112239
[INFO] Epoch: 48 , batch: 223 , training loss: 4.224826
[INFO] Epoch: 48 , batch: 224 , training loss: 4.292393
[INFO] Epoch: 48 , batch: 225 , training loss: 4.167627
[INFO] Epoch: 48 , batch: 226 , training loss: 4.293939
[INFO] Epoch: 48 , batch: 227 , training loss: 4.254807
[INFO] Epoch: 48 , batch: 228 , training loss: 4.262946
[INFO] Epoch: 48 , batch: 229 , training loss: 4.135887
[INFO] Epoch: 48 , batch: 230 , training loss: 4.004196
[INFO] Epoch: 48 , batch: 231 , training loss: 3.873821
[INFO] Epoch: 48 , batch: 232 , training loss: 4.001203
[INFO] Epoch: 48 , batch: 233 , training loss: 4.029720
[INFO] Epoch: 48 , batch: 234 , training loss: 3.738016
[INFO] Epoch: 48 , batch: 235 , training loss: 3.827870
[INFO] Epoch: 48 , batch: 236 , training loss: 3.913980
[INFO] Epoch: 48 , batch: 237 , training loss: 4.138719
[INFO] Epoch: 48 , batch: 238 , training loss: 3.910967
[INFO] Epoch: 48 , batch: 239 , training loss: 3.963531
[INFO] Epoch: 48 , batch: 240 , training loss: 4.032186
[INFO] Epoch: 48 , batch: 241 , training loss: 3.840702
[INFO] Epoch: 48 , batch: 242 , training loss: 3.860053
[INFO] Epoch: 48 , batch: 243 , training loss: 4.151547
[INFO] Epoch: 48 , batch: 244 , training loss: 4.086286
[INFO] Epoch: 48 , batch: 245 , training loss: 4.049306
[INFO] Epoch: 48 , batch: 246 , training loss: 3.791611
[INFO] Epoch: 48 , batch: 247 , training loss: 3.927356
[INFO] Epoch: 48 , batch: 248 , training loss: 3.996948
[INFO] Epoch: 48 , batch: 249 , training loss: 4.042093
[INFO] Epoch: 48 , batch: 250 , training loss: 3.787447
[INFO] Epoch: 48 , batch: 251 , training loss: 4.215497
[INFO] Epoch: 48 , batch: 252 , training loss: 3.912091
[INFO] Epoch: 48 , batch: 253 , training loss: 3.853737
[INFO] Epoch: 48 , batch: 254 , training loss: 4.138446
[INFO] Epoch: 48 , batch: 255 , training loss: 4.099295
[INFO] Epoch: 48 , batch: 256 , training loss: 4.107473
[INFO] Epoch: 48 , batch: 257 , training loss: 4.252497
[INFO] Epoch: 48 , batch: 258 , training loss: 4.280808
[INFO] Epoch: 48 , batch: 259 , training loss: 4.337264
[INFO] Epoch: 48 , batch: 260 , training loss: 4.065830
[INFO] Epoch: 48 , batch: 261 , training loss: 4.200198
[INFO] Epoch: 48 , batch: 262 , training loss: 4.362494
[INFO] Epoch: 48 , batch: 263 , training loss: 4.561412
[INFO] Epoch: 48 , batch: 264 , training loss: 3.903672
[INFO] Epoch: 48 , batch: 265 , training loss: 4.028776
[INFO] Epoch: 48 , batch: 266 , training loss: 4.409921
[INFO] Epoch: 48 , batch: 267 , training loss: 4.181520
[INFO] Epoch: 48 , batch: 268 , training loss: 4.110294
[INFO] Epoch: 48 , batch: 269 , training loss: 4.105368
[INFO] Epoch: 48 , batch: 270 , training loss: 4.127188
[INFO] Epoch: 48 , batch: 271 , training loss: 4.145874
[INFO] Epoch: 48 , batch: 272 , training loss: 4.125495
[INFO] Epoch: 48 , batch: 273 , training loss: 4.169915
[INFO] Epoch: 48 , batch: 274 , training loss: 4.225440
[INFO] Epoch: 48 , batch: 275 , training loss: 4.083566
[INFO] Epoch: 48 , batch: 276 , training loss: 4.141035
[INFO] Epoch: 48 , batch: 277 , training loss: 4.292592
[INFO] Epoch: 48 , batch: 278 , training loss: 3.995051
[INFO] Epoch: 48 , batch: 279 , training loss: 4.006726
[INFO] Epoch: 48 , batch: 280 , training loss: 3.955998
[INFO] Epoch: 48 , batch: 281 , training loss: 4.100105
[INFO] Epoch: 48 , batch: 282 , training loss: 4.015514
[INFO] Epoch: 48 , batch: 283 , training loss: 4.039680
[INFO] Epoch: 48 , batch: 284 , training loss: 4.049046
[INFO] Epoch: 48 , batch: 285 , training loss: 3.993767
[INFO] Epoch: 48 , batch: 286 , training loss: 3.966031
[INFO] Epoch: 48 , batch: 287 , training loss: 3.940272
[INFO] Epoch: 48 , batch: 288 , training loss: 3.933423
[INFO] Epoch: 48 , batch: 289 , training loss: 3.975208
[INFO] Epoch: 48 , batch: 290 , training loss: 3.787843
[INFO] Epoch: 48 , batch: 291 , training loss: 3.751338
[INFO] Epoch: 48 , batch: 292 , training loss: 3.878584
[INFO] Epoch: 48 , batch: 293 , training loss: 3.778764
[INFO] Epoch: 48 , batch: 294 , training loss: 4.464038
[INFO] Epoch: 48 , batch: 295 , training loss: 4.208557
[INFO] Epoch: 48 , batch: 296 , training loss: 4.142060
[INFO] Epoch: 48 , batch: 297 , training loss: 4.115047
[INFO] Epoch: 48 , batch: 298 , training loss: 3.930916
[INFO] Epoch: 48 , batch: 299 , training loss: 3.968102
[INFO] Epoch: 48 , batch: 300 , training loss: 3.927071
[INFO] Epoch: 48 , batch: 301 , training loss: 3.894238
[INFO] Epoch: 48 , batch: 302 , training loss: 4.049173
[INFO] Epoch: 48 , batch: 303 , training loss: 4.061839
[INFO] Epoch: 48 , batch: 304 , training loss: 4.221242
[INFO] Epoch: 48 , batch: 305 , training loss: 4.009704
[INFO] Epoch: 48 , batch: 306 , training loss: 4.135819
[INFO] Epoch: 48 , batch: 307 , training loss: 4.125449
[INFO] Epoch: 48 , batch: 308 , training loss: 3.981135
[INFO] Epoch: 48 , batch: 309 , training loss: 3.973835
[INFO] Epoch: 48 , batch: 310 , training loss: 3.916011
[INFO] Epoch: 48 , batch: 311 , training loss: 3.885067
[INFO] Epoch: 48 , batch: 312 , training loss: 3.810018
[INFO] Epoch: 48 , batch: 313 , training loss: 3.920162
[INFO] Epoch: 48 , batch: 314 , training loss: 3.973703
[INFO] Epoch: 48 , batch: 315 , training loss: 4.030598
[INFO] Epoch: 48 , batch: 316 , training loss: 4.263246
[INFO] Epoch: 48 , batch: 317 , training loss: 4.675720
[INFO] Epoch: 48 , batch: 318 , training loss: 4.830348
[INFO] Epoch: 48 , batch: 319 , training loss: 4.466511
[INFO] Epoch: 48 , batch: 320 , training loss: 3.991950
[INFO] Epoch: 48 , batch: 321 , training loss: 3.837580
[INFO] Epoch: 48 , batch: 322 , training loss: 3.945117
[INFO] Epoch: 48 , batch: 323 , training loss: 3.975705
[INFO] Epoch: 48 , batch: 324 , training loss: 3.927164
[INFO] Epoch: 48 , batch: 325 , training loss: 4.088829
[INFO] Epoch: 48 , batch: 326 , training loss: 4.150509
[INFO] Epoch: 48 , batch: 327 , training loss: 4.047655
[INFO] Epoch: 48 , batch: 328 , training loss: 4.042604
[INFO] Epoch: 48 , batch: 329 , training loss: 3.967055
[INFO] Epoch: 48 , batch: 330 , training loss: 3.965825
[INFO] Epoch: 48 , batch: 331 , training loss: 4.117340
[INFO] Epoch: 48 , batch: 332 , training loss: 3.935371
[INFO] Epoch: 48 , batch: 333 , training loss: 3.948933
[INFO] Epoch: 48 , batch: 334 , training loss: 3.923011
[INFO] Epoch: 48 , batch: 335 , training loss: 4.076754
[INFO] Epoch: 48 , batch: 336 , training loss: 4.078586
[INFO] Epoch: 48 , batch: 337 , training loss: 4.113696
[INFO] Epoch: 48 , batch: 338 , training loss: 4.343163
[INFO] Epoch: 48 , batch: 339 , training loss: 4.175522
[INFO] Epoch: 48 , batch: 340 , training loss: 4.342363
[INFO] Epoch: 48 , batch: 341 , training loss: 4.098858
[INFO] Epoch: 48 , batch: 342 , training loss: 3.850602
[INFO] Epoch: 48 , batch: 343 , training loss: 3.944686
[INFO] Epoch: 48 , batch: 344 , training loss: 3.813395
[INFO] Epoch: 48 , batch: 345 , training loss: 3.946772
[INFO] Epoch: 48 , batch: 346 , training loss: 3.983176
[INFO] Epoch: 48 , batch: 347 , training loss: 3.895048
[INFO] Epoch: 48 , batch: 348 , training loss: 3.971355
[INFO] Epoch: 48 , batch: 349 , training loss: 4.160340
[INFO] Epoch: 48 , batch: 350 , training loss: 3.952394
[INFO] Epoch: 48 , batch: 351 , training loss: 4.014496
[INFO] Epoch: 48 , batch: 352 , training loss: 4.039317
[INFO] Epoch: 48 , batch: 353 , training loss: 4.003712
[INFO] Epoch: 48 , batch: 354 , training loss: 4.109035
[INFO] Epoch: 48 , batch: 355 , training loss: 4.152993
[INFO] Epoch: 48 , batch: 356 , training loss: 3.985086
[INFO] Epoch: 48 , batch: 357 , training loss: 4.062056
[INFO] Epoch: 48 , batch: 358 , training loss: 3.939602
[INFO] Epoch: 48 , batch: 359 , training loss: 3.971252
[INFO] Epoch: 48 , batch: 360 , training loss: 4.065604
[INFO] Epoch: 48 , batch: 361 , training loss: 4.050238
[INFO] Epoch: 48 , batch: 362 , training loss: 4.132615
[INFO] Epoch: 48 , batch: 363 , training loss: 4.021503
[INFO] Epoch: 48 , batch: 364 , training loss: 4.044821
[INFO] Epoch: 48 , batch: 365 , training loss: 3.996095
[INFO] Epoch: 48 , batch: 366 , training loss: 4.108696
[INFO] Epoch: 48 , batch: 367 , training loss: 4.172144
[INFO] Epoch: 48 , batch: 368 , training loss: 4.565531
[INFO] Epoch: 48 , batch: 369 , training loss: 4.233235
[INFO] Epoch: 48 , batch: 370 , training loss: 3.998465
[INFO] Epoch: 48 , batch: 371 , training loss: 4.368704
[INFO] Epoch: 48 , batch: 372 , training loss: 4.631892
[INFO] Epoch: 48 , batch: 373 , training loss: 4.714969
[INFO] Epoch: 48 , batch: 374 , training loss: 4.806200
[INFO] Epoch: 48 , batch: 375 , training loss: 4.790780
[INFO] Epoch: 48 , batch: 376 , training loss: 4.716455
[INFO] Epoch: 48 , batch: 377 , training loss: 4.472907
[INFO] Epoch: 48 , batch: 378 , training loss: 4.568716
[INFO] Epoch: 48 , batch: 379 , training loss: 4.546021
[INFO] Epoch: 48 , batch: 380 , training loss: 4.688324
[INFO] Epoch: 48 , batch: 381 , training loss: 4.436555
[INFO] Epoch: 48 , batch: 382 , training loss: 4.630069
[INFO] Epoch: 48 , batch: 383 , training loss: 4.619213
[INFO] Epoch: 48 , batch: 384 , training loss: 4.661416
[INFO] Epoch: 48 , batch: 385 , training loss: 4.360238
[INFO] Epoch: 48 , batch: 386 , training loss: 4.631482
[INFO] Epoch: 48 , batch: 387 , training loss: 4.588032
[INFO] Epoch: 48 , batch: 388 , training loss: 4.399323
[INFO] Epoch: 48 , batch: 389 , training loss: 4.232249
[INFO] Epoch: 48 , batch: 390 , training loss: 4.227299
[INFO] Epoch: 48 , batch: 391 , training loss: 4.279040
[INFO] Epoch: 48 , batch: 392 , training loss: 4.638178
[INFO] Epoch: 48 , batch: 393 , training loss: 4.520618
[INFO] Epoch: 48 , batch: 394 , training loss: 4.583664
[INFO] Epoch: 48 , batch: 395 , training loss: 4.421011
[INFO] Epoch: 48 , batch: 396 , training loss: 4.222090
[INFO] Epoch: 48 , batch: 397 , training loss: 4.392958
[INFO] Epoch: 48 , batch: 398 , training loss: 4.243227
[INFO] Epoch: 48 , batch: 399 , training loss: 4.304065
[INFO] Epoch: 48 , batch: 400 , training loss: 4.272596
[INFO] Epoch: 48 , batch: 401 , training loss: 4.661250
[INFO] Epoch: 48 , batch: 402 , training loss: 4.429813
[INFO] Epoch: 48 , batch: 403 , training loss: 4.222709
[INFO] Epoch: 48 , batch: 404 , training loss: 4.414436
[INFO] Epoch: 48 , batch: 405 , training loss: 4.481175
[INFO] Epoch: 48 , batch: 406 , training loss: 4.367568
[INFO] Epoch: 48 , batch: 407 , training loss: 4.416124
[INFO] Epoch: 48 , batch: 408 , training loss: 4.377369
[INFO] Epoch: 48 , batch: 409 , training loss: 4.373658
[INFO] Epoch: 48 , batch: 410 , training loss: 4.429732
[INFO] Epoch: 48 , batch: 411 , training loss: 4.616411
[INFO] Epoch: 48 , batch: 412 , training loss: 4.445550
[INFO] Epoch: 48 , batch: 413 , training loss: 4.339890
[INFO] Epoch: 48 , batch: 414 , training loss: 4.375727
[INFO] Epoch: 48 , batch: 415 , training loss: 4.401607
[INFO] Epoch: 48 , batch: 416 , training loss: 4.469017
[INFO] Epoch: 48 , batch: 417 , training loss: 4.399935
[INFO] Epoch: 48 , batch: 418 , training loss: 4.422113
[INFO] Epoch: 48 , batch: 419 , training loss: 4.398981
[INFO] Epoch: 48 , batch: 420 , training loss: 4.361621
[INFO] Epoch: 48 , batch: 421 , training loss: 4.334883
[INFO] Epoch: 48 , batch: 422 , training loss: 4.213707
[INFO] Epoch: 48 , batch: 423 , training loss: 4.413567
[INFO] Epoch: 48 , batch: 424 , training loss: 4.591610
[INFO] Epoch: 48 , batch: 425 , training loss: 4.443575
[INFO] Epoch: 48 , batch: 426 , training loss: 4.191059
[INFO] Epoch: 48 , batch: 427 , training loss: 4.419386
[INFO] Epoch: 48 , batch: 428 , training loss: 4.307528
[INFO] Epoch: 48 , batch: 429 , training loss: 4.156401
[INFO] Epoch: 48 , batch: 430 , training loss: 4.431590
[INFO] Epoch: 48 , batch: 431 , training loss: 4.037195
[INFO] Epoch: 48 , batch: 432 , training loss: 4.105397
[INFO] Epoch: 48 , batch: 433 , training loss: 4.117118
[INFO] Epoch: 48 , batch: 434 , training loss: 4.002800
[INFO] Epoch: 48 , batch: 435 , training loss: 4.374413
[INFO] Epoch: 48 , batch: 436 , training loss: 4.430150
[INFO] Epoch: 48 , batch: 437 , training loss: 4.188577
[INFO] Epoch: 48 , batch: 438 , training loss: 4.055036
[INFO] Epoch: 48 , batch: 439 , training loss: 4.266167
[INFO] Epoch: 48 , batch: 440 , training loss: 4.385139
[INFO] Epoch: 48 , batch: 441 , training loss: 4.481932
[INFO] Epoch: 48 , batch: 442 , training loss: 4.263033
[INFO] Epoch: 48 , batch: 443 , training loss: 4.445551
[INFO] Epoch: 48 , batch: 444 , training loss: 4.084768
[INFO] Epoch: 48 , batch: 445 , training loss: 3.974952
[INFO] Epoch: 48 , batch: 446 , training loss: 3.936728
[INFO] Epoch: 48 , batch: 447 , training loss: 4.109700
[INFO] Epoch: 48 , batch: 448 , training loss: 4.209718
[INFO] Epoch: 48 , batch: 449 , training loss: 4.597589
[INFO] Epoch: 48 , batch: 450 , training loss: 4.659400
[INFO] Epoch: 48 , batch: 451 , training loss: 4.561069
[INFO] Epoch: 48 , batch: 452 , training loss: 4.373251
[INFO] Epoch: 48 , batch: 453 , training loss: 4.151649
[INFO] Epoch: 48 , batch: 454 , training loss: 4.278885
[INFO] Epoch: 48 , batch: 455 , training loss: 4.333214
[INFO] Epoch: 48 , batch: 456 , training loss: 4.329243
[INFO] Epoch: 48 , batch: 457 , training loss: 4.418952
[INFO] Epoch: 48 , batch: 458 , training loss: 4.157856
[INFO] Epoch: 48 , batch: 459 , training loss: 4.143929
[INFO] Epoch: 48 , batch: 460 , training loss: 4.222575
[INFO] Epoch: 48 , batch: 461 , training loss: 4.208509
[INFO] Epoch: 48 , batch: 462 , training loss: 4.274986
[INFO] Epoch: 48 , batch: 463 , training loss: 4.203015
[INFO] Epoch: 48 , batch: 464 , training loss: 4.378554
[INFO] Epoch: 48 , batch: 465 , training loss: 4.294374
[INFO] Epoch: 48 , batch: 466 , training loss: 4.381765
[INFO] Epoch: 48 , batch: 467 , training loss: 4.361122
[INFO] Epoch: 48 , batch: 468 , training loss: 4.335446
[INFO] Epoch: 48 , batch: 469 , training loss: 4.346794
[INFO] Epoch: 48 , batch: 470 , training loss: 4.185175
[INFO] Epoch: 48 , batch: 471 , training loss: 4.254251
[INFO] Epoch: 48 , batch: 472 , training loss: 4.298744
[INFO] Epoch: 48 , batch: 473 , training loss: 4.248047
[INFO] Epoch: 48 , batch: 474 , training loss: 4.045155
[INFO] Epoch: 48 , batch: 475 , training loss: 3.914988
[INFO] Epoch: 48 , batch: 476 , training loss: 4.283342
[INFO] Epoch: 48 , batch: 477 , training loss: 4.433402
[INFO] Epoch: 48 , batch: 478 , training loss: 4.442483
[INFO] Epoch: 48 , batch: 479 , training loss: 4.411546
[INFO] Epoch: 48 , batch: 480 , training loss: 4.510406
[INFO] Epoch: 48 , batch: 481 , training loss: 4.389874
[INFO] Epoch: 48 , batch: 482 , training loss: 4.500334
[INFO] Epoch: 48 , batch: 483 , training loss: 4.376233
[INFO] Epoch: 48 , batch: 484 , training loss: 4.151593
[INFO] Epoch: 48 , batch: 485 , training loss: 4.248932
[INFO] Epoch: 48 , batch: 486 , training loss: 4.137778
[INFO] Epoch: 48 , batch: 487 , training loss: 4.152061
[INFO] Epoch: 48 , batch: 488 , training loss: 4.345503
[INFO] Epoch: 48 , batch: 489 , training loss: 4.207438
[INFO] Epoch: 48 , batch: 490 , training loss: 4.252713
[INFO] Epoch: 48 , batch: 491 , training loss: 4.191173
[INFO] Epoch: 48 , batch: 492 , training loss: 4.162517
[INFO] Epoch: 48 , batch: 493 , training loss: 4.349282
[INFO] Epoch: 48 , batch: 494 , training loss: 4.281756
[INFO] Epoch: 48 , batch: 495 , training loss: 4.394577
[INFO] Epoch: 48 , batch: 496 , training loss: 4.279613
[INFO] Epoch: 48 , batch: 497 , training loss: 4.330062
[INFO] Epoch: 48 , batch: 498 , training loss: 4.321528
[INFO] Epoch: 48 , batch: 499 , training loss: 4.355402
[INFO] Epoch: 48 , batch: 500 , training loss: 4.495113
[INFO] Epoch: 48 , batch: 501 , training loss: 4.794157
[INFO] Epoch: 48 , batch: 502 , training loss: 4.844207
[INFO] Epoch: 48 , batch: 503 , training loss: 4.539333
[INFO] Epoch: 48 , batch: 504 , training loss: 4.673664
[INFO] Epoch: 48 , batch: 505 , training loss: 4.661592
[INFO] Epoch: 48 , batch: 506 , training loss: 4.610648
[INFO] Epoch: 48 , batch: 507 , training loss: 4.671085
[INFO] Epoch: 48 , batch: 508 , training loss: 4.629741
[INFO] Epoch: 48 , batch: 509 , training loss: 4.410609
[INFO] Epoch: 48 , batch: 510 , training loss: 4.497046
[INFO] Epoch: 48 , batch: 511 , training loss: 4.395759
[INFO] Epoch: 48 , batch: 512 , training loss: 4.522690
[INFO] Epoch: 48 , batch: 513 , training loss: 4.747292
[INFO] Epoch: 48 , batch: 514 , training loss: 4.425848
[INFO] Epoch: 48 , batch: 515 , training loss: 4.612731
[INFO] Epoch: 48 , batch: 516 , training loss: 4.450655
[INFO] Epoch: 48 , batch: 517 , training loss: 4.397111
[INFO] Epoch: 48 , batch: 518 , training loss: 4.362382
[INFO] Epoch: 48 , batch: 519 , training loss: 4.210429
[INFO] Epoch: 48 , batch: 520 , training loss: 4.442889
[INFO] Epoch: 48 , batch: 521 , training loss: 4.440803
[INFO] Epoch: 48 , batch: 522 , training loss: 4.486930
[INFO] Epoch: 48 , batch: 523 , training loss: 4.390298
[INFO] Epoch: 48 , batch: 524 , training loss: 4.690979
[INFO] Epoch: 48 , batch: 525 , training loss: 4.638172
[INFO] Epoch: 48 , batch: 526 , training loss: 4.396355
[INFO] Epoch: 48 , batch: 527 , training loss: 4.443372
[INFO] Epoch: 48 , batch: 528 , training loss: 4.436650
[INFO] Epoch: 48 , batch: 529 , training loss: 4.422163
[INFO] Epoch: 48 , batch: 530 , training loss: 4.280972
[INFO] Epoch: 48 , batch: 531 , training loss: 4.401512
[INFO] Epoch: 48 , batch: 532 , training loss: 4.315539
[INFO] Epoch: 48 , batch: 533 , training loss: 4.453723
[INFO] Epoch: 48 , batch: 534 , training loss: 4.429717
[INFO] Epoch: 48 , batch: 535 , training loss: 4.459410
[INFO] Epoch: 48 , batch: 536 , training loss: 4.269676
[INFO] Epoch: 48 , batch: 537 , training loss: 4.293459
[INFO] Epoch: 48 , batch: 538 , training loss: 4.357764
[INFO] Epoch: 48 , batch: 539 , training loss: 4.461226
[INFO] Epoch: 48 , batch: 540 , training loss: 4.923374
[INFO] Epoch: 48 , batch: 541 , training loss: 4.764687
[INFO] Epoch: 48 , batch: 542 , training loss: 4.720044
[INFO] Epoch: 49 , batch: 0 , training loss: 3.291658
[INFO] Epoch: 49 , batch: 1 , training loss: 3.392916
[INFO] Epoch: 49 , batch: 2 , training loss: 3.596055
[INFO] Epoch: 49 , batch: 3 , training loss: 3.427426
[INFO] Epoch: 49 , batch: 4 , training loss: 3.799155
[INFO] Epoch: 49 , batch: 5 , training loss: 3.419979
[INFO] Epoch: 49 , batch: 6 , training loss: 3.794989
[INFO] Epoch: 49 , batch: 7 , training loss: 3.775437
[INFO] Epoch: 49 , batch: 8 , training loss: 3.452204
[INFO] Epoch: 49 , batch: 9 , training loss: 3.704987
[INFO] Epoch: 49 , batch: 10 , training loss: 3.694644
[INFO] Epoch: 49 , batch: 11 , training loss: 3.659172
[INFO] Epoch: 49 , batch: 12 , training loss: 3.556316
[INFO] Epoch: 49 , batch: 13 , training loss: 3.601917
[INFO] Epoch: 49 , batch: 14 , training loss: 3.471555
[INFO] Epoch: 49 , batch: 15 , training loss: 3.679106
[INFO] Epoch: 49 , batch: 16 , training loss: 3.503237
[INFO] Epoch: 49 , batch: 17 , training loss: 3.685536
[INFO] Epoch: 49 , batch: 18 , training loss: 3.611992
[INFO] Epoch: 49 , batch: 19 , training loss: 3.406583
[INFO] Epoch: 49 , batch: 20 , training loss: 3.386210
[INFO] Epoch: 49 , batch: 21 , training loss: 3.503321
[INFO] Epoch: 49 , batch: 22 , training loss: 3.337164
[INFO] Epoch: 49 , batch: 23 , training loss: 3.609178
[INFO] Epoch: 49 , batch: 24 , training loss: 3.392564
[INFO] Epoch: 49 , batch: 25 , training loss: 3.596931
[INFO] Epoch: 49 , batch: 26 , training loss: 3.466339
[INFO] Epoch: 49 , batch: 27 , training loss: 3.419344
[INFO] Epoch: 49 , batch: 28 , training loss: 3.560092
[INFO] Epoch: 49 , batch: 29 , training loss: 3.400439
[INFO] Epoch: 49 , batch: 30 , training loss: 3.488568
[INFO] Epoch: 49 , batch: 31 , training loss: 3.502261
[INFO] Epoch: 49 , batch: 32 , training loss: 3.525542
[INFO] Epoch: 49 , batch: 33 , training loss: 3.508679
[INFO] Epoch: 49 , batch: 34 , training loss: 3.489274
[INFO] Epoch: 49 , batch: 35 , training loss: 3.496814
[INFO] Epoch: 49 , batch: 36 , training loss: 3.536653
[INFO] Epoch: 49 , batch: 37 , training loss: 3.429790
[INFO] Epoch: 49 , batch: 38 , training loss: 3.436890
[INFO] Epoch: 49 , batch: 39 , training loss: 3.333180
[INFO] Epoch: 49 , batch: 40 , training loss: 3.581058
[INFO] Epoch: 49 , batch: 41 , training loss: 3.416701
[INFO] Epoch: 49 , batch: 42 , training loss: 3.882423
[INFO] Epoch: 49 , batch: 43 , training loss: 3.580631
[INFO] Epoch: 49 , batch: 44 , training loss: 3.984569
[INFO] Epoch: 49 , batch: 45 , training loss: 3.827112
[INFO] Epoch: 49 , batch: 46 , training loss: 3.839877
[INFO] Epoch: 49 , batch: 47 , training loss: 3.555606
[INFO] Epoch: 49 , batch: 48 , training loss: 3.549457
[INFO] Epoch: 49 , batch: 49 , training loss: 3.725571
[INFO] Epoch: 49 , batch: 50 , training loss: 3.551467
[INFO] Epoch: 49 , batch: 51 , training loss: 3.785113
[INFO] Epoch: 49 , batch: 52 , training loss: 3.642357
[INFO] Epoch: 49 , batch: 53 , training loss: 3.720163
[INFO] Epoch: 49 , batch: 54 , training loss: 3.741237
[INFO] Epoch: 49 , batch: 55 , training loss: 3.809106
[INFO] Epoch: 49 , batch: 56 , training loss: 3.699923
[INFO] Epoch: 49 , batch: 57 , training loss: 3.589850
[INFO] Epoch: 49 , batch: 58 , training loss: 3.637752
[INFO] Epoch: 49 , batch: 59 , training loss: 3.726894
[INFO] Epoch: 49 , batch: 60 , training loss: 3.654250
[INFO] Epoch: 49 , batch: 61 , training loss: 3.735020
[INFO] Epoch: 49 , batch: 62 , training loss: 3.610759
[INFO] Epoch: 49 , batch: 63 , training loss: 3.830032
[INFO] Epoch: 49 , batch: 64 , training loss: 4.012793
[INFO] Epoch: 49 , batch: 65 , training loss: 3.726714
[INFO] Epoch: 49 , batch: 66 , training loss: 3.568892
[INFO] Epoch: 49 , batch: 67 , training loss: 3.649858
[INFO] Epoch: 49 , batch: 68 , training loss: 3.757067
[INFO] Epoch: 49 , batch: 69 , training loss: 3.669270
[INFO] Epoch: 49 , batch: 70 , training loss: 3.872766
[INFO] Epoch: 49 , batch: 71 , training loss: 3.783694
[INFO] Epoch: 49 , batch: 72 , training loss: 3.860250
[INFO] Epoch: 49 , batch: 73 , training loss: 3.788247
[INFO] Epoch: 49 , batch: 74 , training loss: 3.898401
[INFO] Epoch: 49 , batch: 75 , training loss: 3.778184
[INFO] Epoch: 49 , batch: 76 , training loss: 3.813338
[INFO] Epoch: 49 , batch: 77 , training loss: 3.776959
[INFO] Epoch: 49 , batch: 78 , training loss: 3.928193
[INFO] Epoch: 49 , batch: 79 , training loss: 3.746200
[INFO] Epoch: 49 , batch: 80 , training loss: 3.937141
[INFO] Epoch: 49 , batch: 81 , training loss: 3.840348
[INFO] Epoch: 49 , batch: 82 , training loss: 3.806637
[INFO] Epoch: 49 , batch: 83 , training loss: 3.965493
[INFO] Epoch: 49 , batch: 84 , training loss: 3.886673
[INFO] Epoch: 49 , batch: 85 , training loss: 3.968959
[INFO] Epoch: 49 , batch: 86 , training loss: 3.892870
[INFO] Epoch: 49 , batch: 87 , training loss: 3.886146
[INFO] Epoch: 49 , batch: 88 , training loss: 3.958054
[INFO] Epoch: 49 , batch: 89 , training loss: 3.798327
[INFO] Epoch: 49 , batch: 90 , training loss: 3.887022
[INFO] Epoch: 49 , batch: 91 , training loss: 3.839861
[INFO] Epoch: 49 , batch: 92 , training loss: 3.839977
[INFO] Epoch: 49 , batch: 93 , training loss: 3.922010
[INFO] Epoch: 49 , batch: 94 , training loss: 4.072619
[INFO] Epoch: 49 , batch: 95 , training loss: 3.838368
[INFO] Epoch: 49 , batch: 96 , training loss: 3.857471
[INFO] Epoch: 49 , batch: 97 , training loss: 3.754111
[INFO] Epoch: 49 , batch: 98 , training loss: 3.715300
[INFO] Epoch: 49 , batch: 99 , training loss: 3.801336
[INFO] Epoch: 49 , batch: 100 , training loss: 3.774140
[INFO] Epoch: 49 , batch: 101 , training loss: 3.785602
[INFO] Epoch: 49 , batch: 102 , training loss: 3.916098
[INFO] Epoch: 49 , batch: 103 , training loss: 3.692687
[INFO] Epoch: 49 , batch: 104 , training loss: 3.661873
[INFO] Epoch: 49 , batch: 105 , training loss: 3.937864
[INFO] Epoch: 49 , batch: 106 , training loss: 3.906382
[INFO] Epoch: 49 , batch: 107 , training loss: 3.776697
[INFO] Epoch: 49 , batch: 108 , training loss: 3.678281
[INFO] Epoch: 49 , batch: 109 , training loss: 3.640061
[INFO] Epoch: 49 , batch: 110 , training loss: 3.799940
[INFO] Epoch: 49 , batch: 111 , training loss: 3.900900
[INFO] Epoch: 49 , batch: 112 , training loss: 3.791559
[INFO] Epoch: 49 , batch: 113 , training loss: 3.832058
[INFO] Epoch: 49 , batch: 114 , training loss: 3.776523
[INFO] Epoch: 49 , batch: 115 , training loss: 3.792058
[INFO] Epoch: 49 , batch: 116 , training loss: 3.717271
[INFO] Epoch: 49 , batch: 117 , training loss: 3.969392
[INFO] Epoch: 49 , batch: 118 , training loss: 3.894699
[INFO] Epoch: 49 , batch: 119 , training loss: 4.031790
[INFO] Epoch: 49 , batch: 120 , training loss: 4.049001
[INFO] Epoch: 49 , batch: 121 , training loss: 3.887058
[INFO] Epoch: 49 , batch: 122 , training loss: 3.808381
[INFO] Epoch: 49 , batch: 123 , training loss: 3.791943
[INFO] Epoch: 49 , batch: 124 , training loss: 3.908395
[INFO] Epoch: 49 , batch: 125 , training loss: 3.737160
[INFO] Epoch: 49 , batch: 126 , training loss: 3.741758
[INFO] Epoch: 49 , batch: 127 , training loss: 3.725134
[INFO] Epoch: 49 , batch: 128 , training loss: 3.867062
[INFO] Epoch: 49 , batch: 129 , training loss: 3.816660
[INFO] Epoch: 49 , batch: 130 , training loss: 3.793539
[INFO] Epoch: 49 , batch: 131 , training loss: 3.875402
[INFO] Epoch: 49 , batch: 132 , training loss: 3.823944
[INFO] Epoch: 49 , batch: 133 , training loss: 3.788340
[INFO] Epoch: 49 , batch: 134 , training loss: 3.595236
[INFO] Epoch: 49 , batch: 135 , training loss: 3.651600
[INFO] Epoch: 49 , batch: 136 , training loss: 3.915915
[INFO] Epoch: 49 , batch: 137 , training loss: 3.846578
[INFO] Epoch: 49 , batch: 138 , training loss: 3.895621
[INFO] Epoch: 49 , batch: 139 , training loss: 4.469338
[INFO] Epoch: 49 , batch: 140 , training loss: 4.188094
[INFO] Epoch: 49 , batch: 141 , training loss: 3.975018
[INFO] Epoch: 49 , batch: 142 , training loss: 3.781778
[INFO] Epoch: 49 , batch: 143 , training loss: 3.888176
[INFO] Epoch: 49 , batch: 144 , training loss: 3.712423
[INFO] Epoch: 49 , batch: 145 , training loss: 3.780483
[INFO] Epoch: 49 , batch: 146 , training loss: 3.972156
[INFO] Epoch: 49 , batch: 147 , training loss: 3.693132
[INFO] Epoch: 49 , batch: 148 , training loss: 3.659155
[INFO] Epoch: 49 , batch: 149 , training loss: 3.750276
[INFO] Epoch: 49 , batch: 150 , training loss: 3.898501
[INFO] Epoch: 49 , batch: 151 , training loss: 3.836823
[INFO] Epoch: 49 , batch: 152 , training loss: 3.883884
[INFO] Epoch: 49 , batch: 153 , training loss: 3.859135
[INFO] Epoch: 49 , batch: 154 , training loss: 3.954996
[INFO] Epoch: 49 , batch: 155 , training loss: 4.167684
[INFO] Epoch: 49 , batch: 156 , training loss: 3.875243
[INFO] Epoch: 49 , batch: 157 , training loss: 3.863009
[INFO] Epoch: 49 , batch: 158 , training loss: 4.001738
[INFO] Epoch: 49 , batch: 159 , training loss: 3.840526
[INFO] Epoch: 49 , batch: 160 , training loss: 4.145572
[INFO] Epoch: 49 , batch: 161 , training loss: 4.222800
[INFO] Epoch: 49 , batch: 162 , training loss: 4.214970
[INFO] Epoch: 49 , batch: 163 , training loss: 4.362822
[INFO] Epoch: 49 , batch: 164 , training loss: 4.356199
[INFO] Epoch: 49 , batch: 165 , training loss: 4.300365
[INFO] Epoch: 49 , batch: 166 , training loss: 4.068435
[INFO] Epoch: 49 , batch: 167 , training loss: 4.088294
[INFO] Epoch: 49 , batch: 168 , training loss: 3.778412
[INFO] Epoch: 49 , batch: 169 , training loss: 3.756576
[INFO] Epoch: 49 , batch: 170 , training loss: 3.969751
[INFO] Epoch: 49 , batch: 171 , training loss: 3.397290
[INFO] Epoch: 49 , batch: 172 , training loss: 3.637642
[INFO] Epoch: 49 , batch: 173 , training loss: 4.004292
[INFO] Epoch: 49 , batch: 174 , training loss: 4.404474
[INFO] Epoch: 49 , batch: 175 , training loss: 4.706917
[INFO] Epoch: 49 , batch: 176 , training loss: 4.367198
[INFO] Epoch: 49 , batch: 177 , training loss: 4.047050
[INFO] Epoch: 49 , batch: 178 , training loss: 4.009207
[INFO] Epoch: 49 , batch: 179 , training loss: 4.114844
[INFO] Epoch: 49 , batch: 180 , training loss: 4.049396
[INFO] Epoch: 49 , batch: 181 , training loss: 4.340117
[INFO] Epoch: 49 , batch: 182 , training loss: 4.310237
[INFO] Epoch: 49 , batch: 183 , training loss: 4.229896
[INFO] Epoch: 49 , batch: 184 , training loss: 4.148181
[INFO] Epoch: 49 , batch: 185 , training loss: 4.059945
[INFO] Epoch: 49 , batch: 186 , training loss: 4.249450
[INFO] Epoch: 49 , batch: 187 , training loss: 4.393028
[INFO] Epoch: 49 , batch: 188 , training loss: 4.368777
[INFO] Epoch: 49 , batch: 189 , training loss: 4.248705
[INFO] Epoch: 49 , batch: 190 , training loss: 4.289397
[INFO] Epoch: 49 , batch: 191 , training loss: 4.377907
[INFO] Epoch: 49 , batch: 192 , training loss: 4.212861
[INFO] Epoch: 49 , batch: 193 , training loss: 4.322460
[INFO] Epoch: 49 , batch: 194 , training loss: 4.280100
[INFO] Epoch: 49 , batch: 195 , training loss: 4.160112
[INFO] Epoch: 49 , batch: 196 , training loss: 4.068014
[INFO] Epoch: 49 , batch: 197 , training loss: 4.134672
[INFO] Epoch: 49 , batch: 198 , training loss: 4.070698
[INFO] Epoch: 49 , batch: 199 , training loss: 4.212227
[INFO] Epoch: 49 , batch: 200 , training loss: 4.092069
[INFO] Epoch: 49 , batch: 201 , training loss: 4.010260
[INFO] Epoch: 49 , batch: 202 , training loss: 4.012522
[INFO] Epoch: 49 , batch: 203 , training loss: 4.103765
[INFO] Epoch: 49 , batch: 204 , training loss: 4.261034
[INFO] Epoch: 49 , batch: 205 , training loss: 3.812608
[INFO] Epoch: 49 , batch: 206 , training loss: 3.763259
[INFO] Epoch: 49 , batch: 207 , training loss: 3.764025
[INFO] Epoch: 49 , batch: 208 , training loss: 4.084061
[INFO] Epoch: 49 , batch: 209 , training loss: 4.018475
[INFO] Epoch: 49 , batch: 210 , training loss: 4.070694
[INFO] Epoch: 49 , batch: 211 , training loss: 4.059343
[INFO] Epoch: 49 , batch: 212 , training loss: 4.163619
[INFO] Epoch: 49 , batch: 213 , training loss: 4.107235
[INFO] Epoch: 49 , batch: 214 , training loss: 4.182905
[INFO] Epoch: 49 , batch: 215 , training loss: 4.419470
[INFO] Epoch: 49 , batch: 216 , training loss: 4.085638
[INFO] Epoch: 49 , batch: 217 , training loss: 4.084272
[INFO] Epoch: 49 , batch: 218 , training loss: 4.040248
[INFO] Epoch: 49 , batch: 219 , training loss: 4.138410
[INFO] Epoch: 49 , batch: 220 , training loss: 3.966388
[INFO] Epoch: 49 , batch: 221 , training loss: 3.986394
[INFO] Epoch: 49 , batch: 222 , training loss: 4.134222
[INFO] Epoch: 49 , batch: 223 , training loss: 4.221997
[INFO] Epoch: 49 , batch: 224 , training loss: 4.256750
[INFO] Epoch: 49 , batch: 225 , training loss: 4.166733
[INFO] Epoch: 49 , batch: 226 , training loss: 4.276744
[INFO] Epoch: 49 , batch: 227 , training loss: 4.218698
[INFO] Epoch: 49 , batch: 228 , training loss: 4.268903
[INFO] Epoch: 49 , batch: 229 , training loss: 4.138196
[INFO] Epoch: 49 , batch: 230 , training loss: 4.014714
[INFO] Epoch: 49 , batch: 231 , training loss: 3.880431
[INFO] Epoch: 49 , batch: 232 , training loss: 4.013119
[INFO] Epoch: 49 , batch: 233 , training loss: 4.034713
[INFO] Epoch: 49 , batch: 234 , training loss: 3.739288
[INFO] Epoch: 49 , batch: 235 , training loss: 3.831753
[INFO] Epoch: 49 , batch: 236 , training loss: 3.915373
[INFO] Epoch: 49 , batch: 237 , training loss: 4.145271
[INFO] Epoch: 49 , batch: 238 , training loss: 3.926360
[INFO] Epoch: 49 , batch: 239 , training loss: 3.964818
[INFO] Epoch: 49 , batch: 240 , training loss: 4.024826
[INFO] Epoch: 49 , batch: 241 , training loss: 3.836454
[INFO] Epoch: 49 , batch: 242 , training loss: 3.859616
[INFO] Epoch: 49 , batch: 243 , training loss: 4.144458
[INFO] Epoch: 49 , batch: 244 , training loss: 4.069598
[INFO] Epoch: 49 , batch: 245 , training loss: 4.044060
[INFO] Epoch: 49 , batch: 246 , training loss: 3.773982
[INFO] Epoch: 49 , batch: 247 , training loss: 3.909813
[INFO] Epoch: 49 , batch: 248 , training loss: 3.999133
[INFO] Epoch: 49 , batch: 249 , training loss: 4.018903
[INFO] Epoch: 49 , batch: 250 , training loss: 3.776718
[INFO] Epoch: 49 , batch: 251 , training loss: 4.202403
[INFO] Epoch: 49 , batch: 252 , training loss: 3.905420
[INFO] Epoch: 49 , batch: 253 , training loss: 3.839398
[INFO] Epoch: 49 , batch: 254 , training loss: 4.109908
[INFO] Epoch: 49 , batch: 255 , training loss: 4.078407
[INFO] Epoch: 49 , batch: 256 , training loss: 4.105860
[INFO] Epoch: 49 , batch: 257 , training loss: 4.231574
[INFO] Epoch: 49 , batch: 258 , training loss: 4.270375
[INFO] Epoch: 49 , batch: 259 , training loss: 4.306923
[INFO] Epoch: 49 , batch: 260 , training loss: 4.060869
[INFO] Epoch: 49 , batch: 261 , training loss: 4.179226
[INFO] Epoch: 49 , batch: 262 , training loss: 4.357155
[INFO] Epoch: 49 , batch: 263 , training loss: 4.582071
[INFO] Epoch: 49 , batch: 264 , training loss: 3.891657
[INFO] Epoch: 49 , batch: 265 , training loss: 4.029464
[INFO] Epoch: 49 , batch: 266 , training loss: 4.406048
[INFO] Epoch: 49 , batch: 267 , training loss: 4.184334
[INFO] Epoch: 49 , batch: 268 , training loss: 4.106027
[INFO] Epoch: 49 , batch: 269 , training loss: 4.109994
[INFO] Epoch: 49 , batch: 270 , training loss: 4.140970
[INFO] Epoch: 49 , batch: 271 , training loss: 4.149515
[INFO] Epoch: 49 , batch: 272 , training loss: 4.115712
[INFO] Epoch: 49 , batch: 273 , training loss: 4.163325
[INFO] Epoch: 49 , batch: 274 , training loss: 4.212695
[INFO] Epoch: 49 , batch: 275 , training loss: 4.082514
[INFO] Epoch: 49 , batch: 276 , training loss: 4.147803
[INFO] Epoch: 49 , batch: 277 , training loss: 4.284503
[INFO] Epoch: 49 , batch: 278 , training loss: 3.986006
[INFO] Epoch: 49 , batch: 279 , training loss: 3.997195
[INFO] Epoch: 49 , batch: 280 , training loss: 3.949380
[INFO] Epoch: 49 , batch: 281 , training loss: 4.096619
[INFO] Epoch: 49 , batch: 282 , training loss: 3.986537
[INFO] Epoch: 49 , batch: 283 , training loss: 4.015247
[INFO] Epoch: 49 , batch: 284 , training loss: 4.041509
[INFO] Epoch: 49 , batch: 285 , training loss: 3.979837
[INFO] Epoch: 49 , batch: 286 , training loss: 3.958333
[INFO] Epoch: 49 , batch: 287 , training loss: 3.924484
[INFO] Epoch: 49 , batch: 288 , training loss: 3.910916
[INFO] Epoch: 49 , batch: 289 , training loss: 3.983376
[INFO] Epoch: 49 , batch: 290 , training loss: 3.803524
[INFO] Epoch: 49 , batch: 291 , training loss: 3.749607
[INFO] Epoch: 49 , batch: 292 , training loss: 3.867307
[INFO] Epoch: 49 , batch: 293 , training loss: 3.778494
[INFO] Epoch: 49 , batch: 294 , training loss: 4.460456
[INFO] Epoch: 49 , batch: 295 , training loss: 4.198323
[INFO] Epoch: 49 , batch: 296 , training loss: 4.135013
[INFO] Epoch: 49 , batch: 297 , training loss: 4.107846
[INFO] Epoch: 49 , batch: 298 , training loss: 3.930645
[INFO] Epoch: 49 , batch: 299 , training loss: 3.956768
[INFO] Epoch: 49 , batch: 300 , training loss: 3.922673
[INFO] Epoch: 49 , batch: 301 , training loss: 3.887590
[INFO] Epoch: 49 , batch: 302 , training loss: 4.049819
[INFO] Epoch: 49 , batch: 303 , training loss: 4.052808
[INFO] Epoch: 49 , batch: 304 , training loss: 4.206141
[INFO] Epoch: 49 , batch: 305 , training loss: 4.018624
[INFO] Epoch: 49 , batch: 306 , training loss: 4.117496
[INFO] Epoch: 49 , batch: 307 , training loss: 4.106467
[INFO] Epoch: 49 , batch: 308 , training loss: 3.980069
[INFO] Epoch: 49 , batch: 309 , training loss: 3.980667
[INFO] Epoch: 49 , batch: 310 , training loss: 3.898549
[INFO] Epoch: 49 , batch: 311 , training loss: 3.890056
[INFO] Epoch: 49 , batch: 312 , training loss: 3.811441
[INFO] Epoch: 49 , batch: 313 , training loss: 3.917704
[INFO] Epoch: 49 , batch: 314 , training loss: 3.974860
[INFO] Epoch: 49 , batch: 315 , training loss: 4.016118
[INFO] Epoch: 49 , batch: 316 , training loss: 4.256201
[INFO] Epoch: 49 , batch: 317 , training loss: 4.656246
[INFO] Epoch: 49 , batch: 318 , training loss: 4.803236
[INFO] Epoch: 49 , batch: 319 , training loss: 4.438680
[INFO] Epoch: 49 , batch: 320 , training loss: 3.991455
[INFO] Epoch: 49 , batch: 321 , training loss: 3.836996
[INFO] Epoch: 49 , batch: 322 , training loss: 3.934662
[INFO] Epoch: 49 , batch: 323 , training loss: 3.967164
[INFO] Epoch: 49 , batch: 324 , training loss: 3.918046
[INFO] Epoch: 49 , batch: 325 , training loss: 4.078557
[INFO] Epoch: 49 , batch: 326 , training loss: 4.136393
[INFO] Epoch: 49 , batch: 327 , training loss: 4.028489
[INFO] Epoch: 49 , batch: 328 , training loss: 4.038916
[INFO] Epoch: 49 , batch: 329 , training loss: 3.959276
[INFO] Epoch: 49 , batch: 330 , training loss: 3.947277
[INFO] Epoch: 49 , batch: 331 , training loss: 4.116958
[INFO] Epoch: 49 , batch: 332 , training loss: 3.929917
[INFO] Epoch: 49 , batch: 333 , training loss: 3.946723
[INFO] Epoch: 49 , batch: 334 , training loss: 3.913610
[INFO] Epoch: 49 , batch: 335 , training loss: 4.059391
[INFO] Epoch: 49 , batch: 336 , training loss: 4.075218
[INFO] Epoch: 49 , batch: 337 , training loss: 4.105642
[INFO] Epoch: 49 , batch: 338 , training loss: 4.318898
[INFO] Epoch: 49 , batch: 339 , training loss: 4.170715
[INFO] Epoch: 49 , batch: 340 , training loss: 4.332247
[INFO] Epoch: 49 , batch: 341 , training loss: 4.078386
[INFO] Epoch: 49 , batch: 342 , training loss: 3.846483
[INFO] Epoch: 49 , batch: 343 , training loss: 3.947210
[INFO] Epoch: 49 , batch: 344 , training loss: 3.810065
[INFO] Epoch: 49 , batch: 345 , training loss: 3.931024
[INFO] Epoch: 49 , batch: 346 , training loss: 3.970437
[INFO] Epoch: 49 , batch: 347 , training loss: 3.897100
[INFO] Epoch: 49 , batch: 348 , training loss: 3.964448
[INFO] Epoch: 49 , batch: 349 , training loss: 4.153309
[INFO] Epoch: 49 , batch: 350 , training loss: 3.948957
[INFO] Epoch: 49 , batch: 351 , training loss: 4.018194
[INFO] Epoch: 49 , batch: 352 , training loss: 4.012941
[INFO] Epoch: 49 , batch: 353 , training loss: 3.996242
[INFO] Epoch: 49 , batch: 354 , training loss: 4.111942
[INFO] Epoch: 49 , batch: 355 , training loss: 4.139473
[INFO] Epoch: 49 , batch: 356 , training loss: 3.982041
[INFO] Epoch: 49 , batch: 357 , training loss: 4.043246
[INFO] Epoch: 49 , batch: 358 , training loss: 3.925443
[INFO] Epoch: 49 , batch: 359 , training loss: 3.961723
[INFO] Epoch: 49 , batch: 360 , training loss: 4.058718
[INFO] Epoch: 49 , batch: 361 , training loss: 4.048934
[INFO] Epoch: 49 , batch: 362 , training loss: 4.115652
[INFO] Epoch: 49 , batch: 363 , training loss: 4.005077
[INFO] Epoch: 49 , batch: 364 , training loss: 4.043128
[INFO] Epoch: 49 , batch: 365 , training loss: 3.993262
[INFO] Epoch: 49 , batch: 366 , training loss: 4.086174
[INFO] Epoch: 49 , batch: 367 , training loss: 4.161129
[INFO] Epoch: 49 , batch: 368 , training loss: 4.576239
[INFO] Epoch: 49 , batch: 369 , training loss: 4.225167
[INFO] Epoch: 49 , batch: 370 , training loss: 3.987736
[INFO] Epoch: 49 , batch: 371 , training loss: 4.357274
[INFO] Epoch: 49 , batch: 372 , training loss: 4.645579
[INFO] Epoch: 49 , batch: 373 , training loss: 4.695149
[INFO] Epoch: 49 , batch: 374 , training loss: 4.787014
[INFO] Epoch: 49 , batch: 375 , training loss: 4.761641
[INFO] Epoch: 49 , batch: 376 , training loss: 4.715759
[INFO] Epoch: 49 , batch: 377 , training loss: 4.490146
[INFO] Epoch: 49 , batch: 378 , training loss: 4.569928
[INFO] Epoch: 49 , batch: 379 , training loss: 4.553503
[INFO] Epoch: 49 , batch: 380 , training loss: 4.696142
[INFO] Epoch: 49 , batch: 381 , training loss: 4.426385
[INFO] Epoch: 49 , batch: 382 , training loss: 4.629865
[INFO] Epoch: 49 , batch: 383 , training loss: 4.625847
[INFO] Epoch: 49 , batch: 384 , training loss: 4.670682
[INFO] Epoch: 49 , batch: 385 , training loss: 4.357063
[INFO] Epoch: 49 , batch: 386 , training loss: 4.625223
[INFO] Epoch: 49 , batch: 387 , training loss: 4.579444
[INFO] Epoch: 49 , batch: 388 , training loss: 4.370308
[INFO] Epoch: 49 , batch: 389 , training loss: 4.228347
[INFO] Epoch: 49 , batch: 390 , training loss: 4.216104
[INFO] Epoch: 49 , batch: 391 , training loss: 4.276210
[INFO] Epoch: 49 , batch: 392 , training loss: 4.640370
[INFO] Epoch: 49 , batch: 393 , training loss: 4.513258
[INFO] Epoch: 49 , batch: 394 , training loss: 4.574504
[INFO] Epoch: 49 , batch: 395 , training loss: 4.425921
[INFO] Epoch: 49 , batch: 396 , training loss: 4.209935
[INFO] Epoch: 49 , batch: 397 , training loss: 4.391777
[INFO] Epoch: 49 , batch: 398 , training loss: 4.242819
[INFO] Epoch: 49 , batch: 399 , training loss: 4.315111
[INFO] Epoch: 49 , batch: 400 , training loss: 4.272924
[INFO] Epoch: 49 , batch: 401 , training loss: 4.669815
[INFO] Epoch: 49 , batch: 402 , training loss: 4.428843
[INFO] Epoch: 49 , batch: 403 , training loss: 4.207749
[INFO] Epoch: 49 , batch: 404 , training loss: 4.410120
[INFO] Epoch: 49 , batch: 405 , training loss: 4.473472
[INFO] Epoch: 49 , batch: 406 , training loss: 4.377725
[INFO] Epoch: 49 , batch: 407 , training loss: 4.413816
[INFO] Epoch: 49 , batch: 408 , training loss: 4.385133
[INFO] Epoch: 49 , batch: 409 , training loss: 4.373546
[INFO] Epoch: 49 , batch: 410 , training loss: 4.420966
[INFO] Epoch: 49 , batch: 411 , training loss: 4.606343
[INFO] Epoch: 49 , batch: 412 , training loss: 4.436734
[INFO] Epoch: 49 , batch: 413 , training loss: 4.335497
[INFO] Epoch: 49 , batch: 414 , training loss: 4.382905
[INFO] Epoch: 49 , batch: 415 , training loss: 4.406107
[INFO] Epoch: 49 , batch: 416 , training loss: 4.476256
[INFO] Epoch: 49 , batch: 417 , training loss: 4.407177
[INFO] Epoch: 49 , batch: 418 , training loss: 4.420371
[INFO] Epoch: 49 , batch: 419 , training loss: 4.379319
[INFO] Epoch: 49 , batch: 420 , training loss: 4.363166
[INFO] Epoch: 49 , batch: 421 , training loss: 4.335284
[INFO] Epoch: 49 , batch: 422 , training loss: 4.213758
[INFO] Epoch: 49 , batch: 423 , training loss: 4.414194
[INFO] Epoch: 49 , batch: 424 , training loss: 4.573461
[INFO] Epoch: 49 , batch: 425 , training loss: 4.453792
[INFO] Epoch: 49 , batch: 426 , training loss: 4.212569
[INFO] Epoch: 49 , batch: 427 , training loss: 4.414865
[INFO] Epoch: 49 , batch: 428 , training loss: 4.299397
[INFO] Epoch: 49 , batch: 429 , training loss: 4.152922
[INFO] Epoch: 49 , batch: 430 , training loss: 4.419537
[INFO] Epoch: 49 , batch: 431 , training loss: 4.038356
[INFO] Epoch: 49 , batch: 432 , training loss: 4.085851
[INFO] Epoch: 49 , batch: 433 , training loss: 4.122500
[INFO] Epoch: 49 , batch: 434 , training loss: 4.008621
[INFO] Epoch: 49 , batch: 435 , training loss: 4.376225
[INFO] Epoch: 49 , batch: 436 , training loss: 4.438397
[INFO] Epoch: 49 , batch: 437 , training loss: 4.194459
[INFO] Epoch: 49 , batch: 438 , training loss: 4.053964
[INFO] Epoch: 49 , batch: 439 , training loss: 4.269859
[INFO] Epoch: 49 , batch: 440 , training loss: 4.386397
[INFO] Epoch: 49 , batch: 441 , training loss: 4.479106
[INFO] Epoch: 49 , batch: 442 , training loss: 4.268084
[INFO] Epoch: 49 , batch: 443 , training loss: 4.440509
[INFO] Epoch: 49 , batch: 444 , training loss: 4.083565
[INFO] Epoch: 49 , batch: 445 , training loss: 3.967232
[INFO] Epoch: 49 , batch: 446 , training loss: 3.932932
[INFO] Epoch: 49 , batch: 447 , training loss: 4.104033
[INFO] Epoch: 49 , batch: 448 , training loss: 4.196742
[INFO] Epoch: 49 , batch: 449 , training loss: 4.606913
[INFO] Epoch: 49 , batch: 450 , training loss: 4.636095
[INFO] Epoch: 49 , batch: 451 , training loss: 4.566190
[INFO] Epoch: 49 , batch: 452 , training loss: 4.356626
[INFO] Epoch: 49 , batch: 453 , training loss: 4.142756
[INFO] Epoch: 49 , batch: 454 , training loss: 4.287402
[INFO] Epoch: 49 , batch: 455 , training loss: 4.339027
[INFO] Epoch: 49 , batch: 456 , training loss: 4.325523
[INFO] Epoch: 49 , batch: 457 , training loss: 4.410792
[INFO] Epoch: 49 , batch: 458 , training loss: 4.157069
[INFO] Epoch: 49 , batch: 459 , training loss: 4.142758
[INFO] Epoch: 49 , batch: 460 , training loss: 4.219587
[INFO] Epoch: 49 , batch: 461 , training loss: 4.206377
[INFO] Epoch: 49 , batch: 462 , training loss: 4.268147
[INFO] Epoch: 49 , batch: 463 , training loss: 4.197773
[INFO] Epoch: 49 , batch: 464 , training loss: 4.380530
[INFO] Epoch: 49 , batch: 465 , training loss: 4.297995
[INFO] Epoch: 49 , batch: 466 , training loss: 4.380860
[INFO] Epoch: 49 , batch: 467 , training loss: 4.362587
[INFO] Epoch: 49 , batch: 468 , training loss: 4.332526
[INFO] Epoch: 49 , batch: 469 , training loss: 4.359115
[INFO] Epoch: 49 , batch: 470 , training loss: 4.184470
[INFO] Epoch: 49 , batch: 471 , training loss: 4.254682
[INFO] Epoch: 49 , batch: 472 , training loss: 4.313347
[INFO] Epoch: 49 , batch: 473 , training loss: 4.262241
[INFO] Epoch: 49 , batch: 474 , training loss: 4.050443
[INFO] Epoch: 49 , batch: 475 , training loss: 3.920033
[INFO] Epoch: 49 , batch: 476 , training loss: 4.278792
[INFO] Epoch: 49 , batch: 477 , training loss: 4.434828
[INFO] Epoch: 49 , batch: 478 , training loss: 4.451461
[INFO] Epoch: 49 , batch: 479 , training loss: 4.418129
[INFO] Epoch: 49 , batch: 480 , training loss: 4.509526
[INFO] Epoch: 49 , batch: 481 , training loss: 4.401130
[INFO] Epoch: 49 , batch: 482 , training loss: 4.508679
[INFO] Epoch: 49 , batch: 483 , training loss: 4.370609
[INFO] Epoch: 49 , batch: 484 , training loss: 4.171695
[INFO] Epoch: 49 , batch: 485 , training loss: 4.256441
[INFO] Epoch: 49 , batch: 486 , training loss: 4.137239
[INFO] Epoch: 49 , batch: 487 , training loss: 4.154099
[INFO] Epoch: 49 , batch: 488 , training loss: 4.346564
[INFO] Epoch: 49 , batch: 489 , training loss: 4.215617
[INFO] Epoch: 49 , batch: 490 , training loss: 4.262841
[INFO] Epoch: 49 , batch: 491 , training loss: 4.192580
[INFO] Epoch: 49 , batch: 492 , training loss: 4.173194
[INFO] Epoch: 49 , batch: 493 , training loss: 4.354707
[INFO] Epoch: 49 , batch: 494 , training loss: 4.277704
[INFO] Epoch: 49 , batch: 495 , training loss: 4.388171
[INFO] Epoch: 49 , batch: 496 , training loss: 4.276709
[INFO] Epoch: 49 , batch: 497 , training loss: 4.336061
[INFO] Epoch: 49 , batch: 498 , training loss: 4.323153
[INFO] Epoch: 49 , batch: 499 , training loss: 4.381343
[INFO] Epoch: 49 , batch: 500 , training loss: 4.491913
[INFO] Epoch: 49 , batch: 501 , training loss: 4.794355
[INFO] Epoch: 49 , batch: 502 , training loss: 4.853269
[INFO] Epoch: 49 , batch: 503 , training loss: 4.539166
[INFO] Epoch: 49 , batch: 504 , training loss: 4.676986
[INFO] Epoch: 49 , batch: 505 , training loss: 4.658935
[INFO] Epoch: 49 , batch: 506 , training loss: 4.612858
[INFO] Epoch: 49 , batch: 507 , training loss: 4.678041
[INFO] Epoch: 49 , batch: 508 , training loss: 4.643266
[INFO] Epoch: 49 , batch: 509 , training loss: 4.419530
[INFO] Epoch: 49 , batch: 510 , training loss: 4.494203
[INFO] Epoch: 49 , batch: 511 , training loss: 4.387268
[INFO] Epoch: 49 , batch: 512 , training loss: 4.522142
[INFO] Epoch: 49 , batch: 513 , training loss: 4.739994
[INFO] Epoch: 49 , batch: 514 , training loss: 4.397275
[INFO] Epoch: 49 , batch: 515 , training loss: 4.628195
[INFO] Epoch: 49 , batch: 516 , training loss: 4.452151
[INFO] Epoch: 49 , batch: 517 , training loss: 4.402657
[INFO] Epoch: 49 , batch: 518 , training loss: 4.373101
[INFO] Epoch: 49 , batch: 519 , training loss: 4.218624
[INFO] Epoch: 49 , batch: 520 , training loss: 4.443928
[INFO] Epoch: 49 , batch: 521 , training loss: 4.445415
[INFO] Epoch: 49 , batch: 522 , training loss: 4.480602
[INFO] Epoch: 49 , batch: 523 , training loss: 4.390825
[INFO] Epoch: 49 , batch: 524 , training loss: 4.682777
[INFO] Epoch: 49 , batch: 525 , training loss: 4.635814
[INFO] Epoch: 49 , batch: 526 , training loss: 4.399419
[INFO] Epoch: 49 , batch: 527 , training loss: 4.433540
[INFO] Epoch: 49 , batch: 528 , training loss: 4.431565
[INFO] Epoch: 49 , batch: 529 , training loss: 4.423779
[INFO] Epoch: 49 , batch: 530 , training loss: 4.264864
[INFO] Epoch: 49 , batch: 531 , training loss: 4.398128
[INFO] Epoch: 49 , batch: 532 , training loss: 4.320416
[INFO] Epoch: 49 , batch: 533 , training loss: 4.453209
[INFO] Epoch: 49 , batch: 534 , training loss: 4.420083
[INFO] Epoch: 49 , batch: 535 , training loss: 4.455315
[INFO] Epoch: 49 , batch: 536 , training loss: 4.280499
[INFO] Epoch: 49 , batch: 537 , training loss: 4.289161
[INFO] Epoch: 49 , batch: 538 , training loss: 4.353201
[INFO] Epoch: 49 , batch: 539 , training loss: 4.463713
[INFO] Epoch: 49 , batch: 540 , training loss: 4.923390
[INFO] Epoch: 49 , batch: 541 , training loss: 4.782371
[INFO] Epoch: 49 , batch: 542 , training loss: 4.720603
[INFO] write tang poem...
###########  ##########






#########################
###########  ##########





#########################
###########  ##########





#########################
###########  ##########


#########################
###########  ##########






#########################
###########  ##########






#########################
###########  ##########






#########################

Process finished with exit code 0
    

